[ Info: Welcome to Julia
[ Info: Importing Hypergraph
[ Info: Aggregation, the aggregation function, is (Orchid.AggregateMean, Orchid.AggregateMax)
[ Info: Edges is [[299, 93, 122], [299, 93, 122], [299, 93, 122], [607, 93], [607, 93], [495, 241, 1417, 93], [495, 241, 1417, 93], [495, 241, 1417, 93], [495, 241, 1417, 93], [93, 241, 190, 712, 711], [93, 241, 190, 712, 711], [93, 241, 190, 712, 711], [93, 241, 190, 712, 711], [93, 241, 190, 712, 711], [622, 93], [622, 93], [317, 93, 658], [317, 93, 658], [317, 93, 658], [768, 294, 1184, 93], [768, 294, 1184, 93], [768, 294, 1184, 93], [768, 294, 1184, 93], [72, 93, 1065, 1152, 941], [72, 93, 1065, 1152, 941], [72, 93, 1065, 1152, 941], [72, 93, 1065, 1152, 941], [72, 93, 1065, 1152, 941], [995, 93, 29], [995, 93, 29], [995, 93, 29], [632, 93, 55, 759, 758], [632, 93, 55, 759, 758], [632, 93, 55, 759, 758], [632, 93, 55, 759, 758], [632, 93, 55, 759, 758], [648, 93, 103], [648, 93, 103], [648, 93, 103], [1328, 93], [1328, 93], [947, 948, 93, 561, 453], [947, 948, 93, 561, 453], [947, 948, 93, 561, 453], [947, 948, 93, 561, 453], [947, 948, 93, 561, 453], [1287, 93, 283, 284], [1287, 93, 283, 284], [1287, 93, 283, 284], [1287, 93, 283, 284], [1343, 83, 93, 1151], [1343, 83, 93, 1151], [1343, 83, 93, 1151], [1343, 83, 93, 1151], [331, 1111, 93, 244], [331, 1111, 93, 244], [331, 1111, 93, 244], [331, 1111, 93, 244], [632, 93], [632, 93], [926, 453, 620, 93], [926, 453, 620, 93], [926, 453, 620, 93], [926, 453, 620, 93], [93, 830], [93, 830], [774, 93, 1175, 412], [774, 93, 1175, 412], [774, 93, 1175, 412], [774, 93, 1175, 412], [926, 93, 1051], [926, 93, 1051], [926, 93, 1051], [424, 93, 1091, 423], [424, 93, 1091, 423], [424, 93, 1091, 423], [424, 93, 1091, 423], [93, 437], [93, 437], [648, 93, 838, 480], [648, 93, 838, 480], [648, 93, 838, 480], [648, 93, 838, 480], [93, 522], [93, 522], [632, 93], [632, 93], [93, 531, 509], [93, 531, 509], [93, 531, 509], [676, 93, 938, 873, 453], [676, 93, 938, 873, 453], [676, 93, 938, 873, 453], [676, 93, 938, 873, 453], [676, 93, 938, 873, 453], [926, 684, 93, 964], [926, 684, 93, 964], [926, 684, 93, 964], [926, 684, 93, 964], [1220, 83, 93], [1220, 83, 93], [1220, 83, 93], [83, 93], [83, 93], [782, 93, 759, 524], [782, 93, 759, 524], [782, 93, 759, 524], [782, 93, 759, 524], [622, 1055, 93, 975], [622, 1055, 93, 975], [622, 1055, 93, 975], [622, 1055, 93, 975], [93, 1194], [93, 1194], [1228, 93], [1228, 93], [298, 93], [298, 93], [622, 93], [622, 93], [622, 712, 93], [622, 712, 93], [622, 712, 93], [561, 453, 632, 93], [561, 453, 632, 93], [561, 453, 632, 93], [561, 453, 632, 93], [622, 93], [622, 93], [561, 453, 632, 93], [561, 453, 632, 93], [561, 453, 632, 93], [561, 453, 632, 93], [662, 663, 93, 190, 1173], [662, 663, 93, 190, 1173], [662, 663, 93, 190, 1173], [662, 663, 93, 190, 1173], [662, 663, 93, 190, 1173], [926, 93, 1179, 549], [926, 93, 1179, 549], [926, 93, 1179, 549], [926, 93, 1179, 549], [632, 93, 523, 413, 453], [632, 93, 523, 413, 453], [632, 93, 523, 413, 453], [632, 93, 523, 413, 453], [632, 93, 523, 413, 453], [93, 190], [93, 190], [561, 453, 632, 93], [561, 453, 632, 93], [561, 453, 632, 93], [561, 453, 632, 93], [97, 93], [97, 93], [316, 294, 93, 505], [316, 294, 93, 505], [316, 294, 93, 505], [316, 294, 93, 505], [361, 93, 1174, 480], [361, 93, 1174, 480], [361, 93, 1174, 480], [361, 93, 1174, 480], [93, 164], [93, 164], [93, 531, 509], [93, 531, 509], [93, 531, 509], [561, 93], [561, 93], [93, 830], [93, 830], [93, 795, 319, 470, 471], [93, 795, 319, 470, 471], [93, 795, 319, 470, 471], [93, 795, 319, 470, 471], [93, 795, 319, 470, 471], [93, 609, 727], [93, 609, 727], [93, 609, 727], [93, 1184, 870, 122, 1278], [93, 1184, 870, 122, 1278], [93, 1184, 870, 122, 1278], [93, 1184, 870, 122, 1278], [93, 1184, 870, 122, 1278], [299, 93, 122], [299, 93, 122], [299, 93, 122], [839, 162, 93, 331], [839, 162, 93, 331], [839, 162, 93, 331], [839, 162, 93, 331], [93, 19, 20, 133, 122], [93, 19, 20, 133, 122], [93, 19, 20, 133, 122], [93, 19, 20, 133, 122], [93, 19, 20, 133, 122], [1057, 284, 93, 870], [1057, 284, 93, 870], [1057, 284, 93, 870], [1057, 284, 93, 870], [93, 103], [93, 103], [93, 583, 1094], [93, 583, 1094], [93, 583, 1094], [72, 93, 23, 82, 104], [72, 93, 23, 82, 104], [72, 93, 23, 82, 104], [72, 93, 23, 82, 104], [72, 93, 23, 82, 104], [93, 509, 133], [93, 509, 133], [93, 509, 133], [971, 93], [971, 93], [93, 73, 81], [93, 73, 81], [93, 73, 81], [666, 93, 362], [666, 93, 362], [666, 93, 362], [606, 93, 453], [606, 93, 453], [606, 93, 453], [211, 93, 727, 244], [211, 93, 727, 244], [211, 93, 727, 244], [211, 93, 727, 244], [72, 93], [72, 93], [294, 93], [294, 93], [453, 93, 103, 104], [453, 93, 103, 104], [453, 93, 103, 104], [453, 93, 103, 104], [93, 377], [93, 377], [93, 509], [93, 509], [1106, 93, 228], [1106, 93, 228], [1106, 93, 228], [93, 901], [93, 901], [93, 1091], [93, 1091], [424, 1411, 93, 1091], [424, 1411, 93, 1091], [424, 1411, 93, 1091], [424, 1411, 93, 1091], [93, 1091], [93, 1091], [453, 93], [453, 93], [624, 93], [624, 93], [1173, 93, 190], [1173, 93, 190], [1173, 93, 190], [561, 601, 93, 413], [561, 601, 93, 413], [561, 601, 93, 413], [561, 601, 93, 413], [947, 93, 8], [947, 93, 8], [947, 93, 8], [294, 103, 93], [294, 103, 93], [294, 103, 93], [93, 361, 1157, 480], [93, 361, 1157, 480], [93, 361, 1157, 480], [93, 361, 1157, 480], [83, 93, 727], [83, 93, 727], [83, 93, 727], [93, 480], [93, 480], [622, 561, 93], [622, 561, 93], [622, 561, 93], [93, 105], [93, 105], [93, 244, 392, 1056], [93, 244, 392, 1056], [93, 244, 392, 1056], [93, 244, 392, 1056], [1065, 93], [1065, 93], [72, 93, 105], [72, 93, 105], [72, 93, 105], [83, 93], [83, 93], [93, 1067], [93, 1067], [453, 93, 920], [453, 93, 920], [453, 93, 920], [926, 1261, 93, 684], [926, 1261, 93, 684], [926, 1261, 93, 684], [926, 1261, 93, 684], [93, 530, 531, 505, 509], [93, 530, 531, 505, 509], [93, 530, 531, 505, 509], [93, 530, 531, 505, 509], [93, 530, 531, 505, 509], [632, 93], [632, 93], [93, 1106, 480, 122, 103], [93, 1106, 480, 122, 103], [93, 1106, 480, 122, 103], [93, 1106, 480, 122, 103], [93, 1106, 480, 122, 103], [93, 589], [93, 589], [453, 93, 238], [453, 93, 238], [453, 93, 238], [632, 93], [632, 93], [93, 103], [93, 103], [947, 93, 116], [947, 93, 116], [947, 93, 116], [798, 93], [798, 93], [622, 93], [622, 93], [93, 851, 635], [93, 851, 635], [93, 851, 635], [648, 646, 93, 1158], [648, 646, 93, 1158], [648, 646, 93, 1158], [648, 646, 93, 1158], [648, 93, 213], [648, 93, 213], [648, 93, 213], [632, 93, 586, 601, 712], [632, 93, 586, 601, 712], [632, 93, 586, 601, 712], [632, 93, 586, 601, 712], [632, 93, 586, 601, 712], [93, 1184, 1036, 998], [93, 1184, 1036, 998], [93, 1184, 1036, 998], [93, 1184, 1036, 998], [1149, 93, 838, 1319], [1149, 93, 838, 1319], [1149, 93, 838, 1319], [1149, 93, 838, 1319], [93, 104, 105], [93, 104, 105], [93, 104, 105], [712, 93, 190, 1036], [712, 93, 190, 1036], [712, 93, 190, 1036], [712, 93, 190, 1036], [93, 122], [93, 122], [646, 93], [646, 93], [523, 93, 213, 1214], [523, 93, 213, 1214], [523, 93, 213, 1214], [523, 93, 213, 1214], [1006, 93], [1006, 93], [453, 93], [453, 93], [83, 93], [83, 93], [83, 93], [83, 93], [83, 93], [83, 93], [1201, 83, 93, 1204], [1201, 83, 93, 1204], [1201, 83, 93, 1204], [1201, 83, 93, 1204], [632, 93], [632, 93], [1122, 663, 93], [1122, 663, 93], [1122, 663, 93], [1173, 93], [1173, 93], [607, 93, 1248], [607, 93, 1248], [607, 93, 1248], [1122, 83, 93, 663], [1122, 83, 93, 663], [1122, 83, 93, 663], [1122, 83, 93, 663], [632, 93, 213], [632, 93, 213], [632, 93, 213], [684, 93, 1210, 1158], [684, 93, 1210, 1158], [684, 93, 1210, 1158], [684, 93, 1210, 1158], [93, 533], [93, 533], [93, 531, 127, 622, 830], [93, 531, 127, 622, 830], [93, 531, 127, 622, 830], [93, 531, 127, 622, 830], [93, 531, 127, 622, 830], [1025, 93, 583, 1207], [1025, 93, 583, 1207], [1025, 93, 583, 1207], [1025, 93, 583, 1207], [622, 93], [622, 93], [93, 967], [93, 967], [947, 972, 974, 93], [947, 972, 974, 93], [947, 972, 974, 93], [947, 972, 974, 93], [93, 589, 920], [93, 589, 920], [93, 589, 920], [1411, 93, 423, 424, 1091], [1411, 93, 423, 424, 1091], [1411, 93, 423, 424, 1091], [1411, 93, 423, 424, 1091], [1411, 93, 423, 424, 1091], [93, 144], [93, 144], [93, 241], [93, 241], [328, 93, 622, 453, 799], [328, 93, 622, 453, 799], [328, 93, 622, 453, 799], [328, 93, 622, 453, 799], [328, 93, 622, 453, 799], [528, 93, 545], [528, 93, 545], [528, 93, 545], [93, 103, 408], [93, 103, 408], [93, 103, 408], [453, 93, 727], [453, 93, 727], [453, 93, 727], [97, 425, 1379, 1385, 911], [97, 425, 1379, 1385, 911], [97, 425, 1379, 1385, 911], [97, 425, 1379, 1385, 911], [97, 425, 1379, 1385, 911], [1011, 1012, 306, 47, 316], [1011, 1012, 306, 47, 316], [1011, 1012, 306, 47, 316], [1011, 1012, 306, 47, 316], [1011, 1012, 306, 47, 316], [306, 257], [306, 257], [306, 1347, 1416], [306, 1347, 1416], [306, 1347, 1416], [306, 371, 317, 555], [306, 371, 317, 555], [306, 371, 317, 555], [306, 371, 317, 555], [306, 227, 316, 267], [306, 227, 316, 267], [306, 227, 316, 267], [306, 227, 316, 267], [306, 47, 316, 227], [306, 47, 316, 227], [306, 47, 316, 227], [306, 47, 316, 227], [306, 757, 316, 47], [306, 757, 316, 47], [306, 757, 316, 47], [306, 757, 316, 47], [306, 307], [306, 307], [306, 119, 316], [306, 119, 316], [306, 119, 316], [306, 278, 25, 165], [306, 278, 25, 165], [306, 278, 25, 165], [306, 278, 25, 165], [306, 224], [306, 224], [306, 1324], [306, 1324], [306, 927, 264, 317], [306, 927, 264, 317], [306, 927, 264, 317], [306, 927, 264, 317], [92, 164, 306, 316, 317], [92, 164, 306, 316, 317], [92, 164, 306, 316, 317], [92, 164, 306, 316, 317], [92, 164, 306, 316, 317], [306, 491, 316, 164], [306, 491, 316, 164], [306, 491, 316, 164], [306, 491, 316, 164], [306, 757, 1251, 96], [306, 757, 1251, 96], [306, 757, 1251, 96], [306, 757, 1251, 96], [306, 263, 316, 164], [306, 263, 316, 164], [306, 263, 316, 164], [306, 263, 316, 164], [306, 33, 116, 317, 229], [306, 33, 116, 317, 229], [306, 33, 116, 317, 229], [306, 33, 116, 317, 229], [306, 33, 116, 317, 229], [306, 420, 316], [306, 420, 316], [306, 420, 316], [863, 164, 306, 437, 1425], [863, 164, 306, 437, 1425], [863, 164, 306, 437, 1425], [863, 164, 306, 437, 1425], [863, 164, 306, 437, 1425], [306, 316], [306, 316], [306, 792, 1327, 273], [306, 792, 1327, 273], [306, 792, 1327, 273], [306, 792, 1327, 273], [306, 792, 316, 533], [306, 792, 316, 533], [306, 792, 316, 533], [306, 792, 316, 533], [306, 313, 316], [306, 313, 316], [306, 313, 316], [420, 165, 306, 792, 35], [420, 165, 306, 792, 35], [420, 165, 306, 792, 35], [420, 165, 306, 792, 35], [420, 165, 306, 792, 35], [306, 313, 792, 316, 317], [306, 313, 792, 316, 317], [306, 313, 792, 316, 317], [306, 313, 792, 316, 317], [306, 313, 792, 316, 317], [306, 307, 792, 311], [306, 307, 792, 311], [306, 307, 792, 311], [306, 307, 792, 311], [306, 76, 685, 316, 271], [306, 76, 685, 316, 271], [306, 76, 685, 316, 271], [306, 76, 685, 316, 271], [306, 76, 685, 316, 271], [306, 904, 1410, 316], [306, 904, 1410, 316], [306, 904, 1410, 316], [306, 904, 1410, 316], [306, 273, 316], [306, 273, 316], [306, 273, 316], [164, 306, 263, 316, 317], [164, 306, 263, 316, 317], [164, 306, 263, 316, 317], [164, 306, 263, 316, 317], [164, 306, 263, 316, 317], [306, 792], [306, 792], [306, 164, 317], [306, 164, 317], [306, 164, 317], [420, 306, 479, 99, 316], [420, 306, 479, 99, 316], [420, 306, 479, 99, 316], [420, 306, 479, 99, 316], [420, 306, 479, 99, 316], [306, 227], [306, 227], [343, 2, 306, 953, 316], [343, 2, 306, 953, 316], [343, 2, 306, 953, 316], [343, 2, 306, 953, 316], [343, 2, 306, 953, 316], [306, 313, 491], [306, 313, 491], [306, 313, 491], [533, 306, 307, 316, 30], [533, 306, 307, 316, 30], [533, 306, 307, 316, 30], [533, 306, 307, 316, 30], [533, 306, 307, 316, 30], [273, 164, 306, 294, 318], [273, 164, 306, 294, 318], [273, 164, 306, 294, 318], [273, 164, 306, 294, 318], [273, 164, 306, 294, 318], [1285, 165, 1323, 792, 1068], [1285, 165, 1323, 792, 1068], [1285, 165, 1323, 792, 1068], [1285, 165, 1323, 792, 1068], [1285, 165, 1323, 792, 1068], [792, 1283, 317, 833], [792, 1283, 317, 833], [792, 1283, 317, 833], [792, 1283, 317, 833], [904, 504, 294, 792], [904, 504, 294, 792], [904, 504, 294, 792], [904, 504, 294, 792], [92, 491, 792], [92, 491, 792], [92, 491, 792], [757, 792, 316, 610], [757, 792, 316, 610], [757, 792, 316, 610], [757, 792, 316, 610], [343, 164, 792, 316, 317], [343, 164, 792, 316, 317], [343, 164, 792, 316, 317], [343, 164, 792, 316, 317], [343, 164, 792, 316, 317], [278, 792, 280], [278, 792, 280], [278, 792, 280], [792, 310, 921], [792, 310, 921], [792, 310, 921], [263, 792, 316, 164], [263, 792, 316, 164], [263, 792, 316, 164], [263, 792, 316, 164], [927, 792], [927, 792], [307, 316, 349, 267], [307, 316, 349, 267], [307, 316, 349, 267], [307, 316, 349, 267], [37, 17, 311, 316, 317], [37, 17, 311, 316, 317], [37, 17, 311, 316, 317], [37, 17, 311, 316, 317], [37, 17, 311, 316, 317], [1049, 310, 311, 1327, 317], [1049, 310, 311, 1327, 317], [1049, 310, 311, 1327, 317], [1049, 310, 311, 1327, 317], [1049, 310, 311, 1327, 317], [390, 1011, 852], [390, 1011, 852], [390, 1011, 852], [390, 164, 1401], [390, 164, 1401], [390, 164, 1401], [51, 362, 356], [51, 362, 356], [51, 362, 356], [1198, 51, 1320], [1198, 51, 1320], [1198, 51, 1320], [1071, 1072], [1071, 1072], [976, 1071], [976, 1071], [1071, 1072], [1071, 1072], [976, 1072, 1230], [976, 1072, 1230], [976, 1072, 1230], [294, 316, 271], [294, 316, 271], [294, 316, 271], [294, 921], [294, 921], [520, 1134, 40, 294, 316], [520, 1134, 40, 294, 316], [520, 1134, 40, 294, 316], [520, 1134, 40, 294, 316], [520, 1134, 40, 294, 316], [496, 294, 316], [496, 294, 316], [496, 294, 316], [757, 294, 316, 835, 325], [757, 294, 316, 835, 325], [757, 294, 316, 835, 325], [757, 294, 316, 835, 325], [757, 294, 316, 835, 325], [294, 316, 426], [294, 316, 426], [294, 316, 426], [294, 332], [294, 332], [1391, 294], [1391, 294], [294, 1066], [294, 1066], [347, 406, 294, 921, 1248], [347, 406, 294, 921, 1248], [347, 406, 294, 921, 1248], [347, 406, 294, 921, 1248], [347, 406, 294, 921, 1248], [1265, 294, 316], [1265, 294, 316], [1265, 294, 316], [42, 294, 116, 316, 628], [42, 294, 116, 316, 628], [42, 294, 116, 316, 628], [42, 294, 116, 316, 628], [42, 294, 116, 316, 628], [233, 294, 1243], [233, 294, 1243], [233, 294, 1243], [294, 316, 12, 1248, 231], [294, 316, 12, 1248, 231], [294, 316, 12, 1248, 231], [294, 316, 12, 1248, 231], [294, 316, 12, 1248, 231], [294, 316, 164], [294, 316, 164], [294, 316, 164], [263, 294, 164], [263, 294, 164], [263, 294, 164], [294, 316, 317], [294, 316, 317], [294, 316, 317], [767, 1281, 294, 1047], [767, 1281, 294, 1047], [767, 1281, 294, 1047], [767, 1281, 294, 1047], [342, 68, 74], [342, 68, 74], [342, 68, 74], [342, 343, 87, 1126], [342, 343, 87, 1126], [342, 343, 87, 1126], [342, 343, 87, 1126], [342, 42, 426, 53], [342, 42, 426, 53], [342, 42, 426, 53], [342, 42, 426, 53], [342, 42, 44, 593, 384], [342, 42, 44, 593, 384], [342, 42, 44, 593, 384], [342, 42, 44, 593, 384], [342, 42, 44, 593, 384], [342, 343, 182], [342, 343, 182], [342, 343, 182], [342, 343], [342, 343], [342, 1271, 1050, 409, 593], [342, 1271, 1050, 409, 593], [342, 1271, 1050, 409, 593], [342, 1271, 1050, 409, 593], [342, 1271, 1050, 409, 593], [914, 343, 856, 857], [914, 343, 856, 857], [914, 343, 856, 857], [914, 343, 856, 857], [343, 1209], [343, 1209], [343, 42, 866, 241, 593], [343, 42, 866, 241, 593], [343, 42, 866, 241, 593], [343, 42, 866, 241, 593], [343, 42, 866, 241, 593], [343, 1050], [343, 1050], [343, 1103], [343, 1103], [343, 150], [343, 150], [343, 593], [343, 593], [343, 1332, 202], [343, 1332, 202], [343, 1332, 202], [1302, 343], [1302, 343], [343, 425, 1383, 225, 363], [343, 425, 1383, 225, 363], [343, 425, 1383, 225, 363], [343, 425, 1383, 225, 363], [343, 425, 1383, 225, 363], [1271, 343], [1271, 343], [425, 343, 426], [425, 343, 426], [425, 343, 426], [343, 87], [343, 87], [343, 899, 1295], [343, 899, 1295], [343, 899, 1295], [42, 343], [42, 343], [90, 343, 87], [90, 343, 87], [90, 343, 87], [343, 1422, 1332], [343, 1422, 1332], [343, 1422, 1332], [809, 1271, 343, 1035], [809, 1271, 343, 1035], [809, 1271, 343, 1035], [809, 1271, 343, 1035], [880, 343, 940], [880, 343, 940], [880, 343, 940], [914, 1420, 343], [914, 1420, 343], [914, 1420, 343], [400, 343, 426], [400, 343, 426], [400, 343, 426], [343, 949], [343, 949], [874, 343], [874, 343], [914, 425, 343, 393], [914, 425, 343, 393], [914, 425, 343, 393], [914, 425, 343, 393], [343, 426], [343, 426], [811, 343, 540, 241], [811, 343, 540, 241], [811, 343, 540, 241], [811, 343, 540, 241], [312, 343], [312, 343], [343, 945], [343, 945], [343, 1332], [343, 1332], [343, 713, 578, 717], [343, 713, 578, 717], [343, 713, 578, 717], [343, 713, 578, 717], [384, 343, 1310, 593], [384, 343, 1310, 593], [384, 343, 1310, 593], [384, 343, 1310, 593], [425, 1348, 343], [425, 1348, 343], [425, 1348, 343], [623, 629, 839, 275], [623, 629, 839, 275], [623, 629, 839, 275], [623, 629, 839, 275], [816, 817, 839], [816, 817, 839], [816, 817, 839], [842, 839], [842, 839], [839, 162, 275, 332], [839, 162, 275, 332], [839, 162, 275, 332], [839, 162, 275, 332], [328, 408, 839], [328, 408, 839], [328, 408, 839], [842, 344, 839, 840], [842, 344, 839, 840], [842, 344, 839, 840], [842, 344, 839, 840], [839, 159, 14, 275], [839, 159, 14, 275], [839, 159, 14, 275], [839, 159, 14, 275], [842, 840], [842, 840], [842, 840], [842, 840], [162, 344, 332, 408, 1352], [162, 344, 332, 408, 1352], [162, 344, 332, 408, 1352], [162, 344, 332, 408, 1352], [162, 344, 332, 408, 1352], [737, 1433, 344, 613], [737, 1433, 344, 613], [737, 1433, 344, 613], [737, 1433, 344, 613], [221, 1237, 1231], [221, 1237, 1231], [221, 1237, 1231], [1231, 522, 1239, 164], [1231, 522, 1239, 164], [1231, 522, 1239, 164], [1231, 522, 1239, 164], [1231, 1098], [1231, 1098], [222, 1231], [222, 1231], [406, 221, 1237], [406, 221, 1237], [406, 221, 1237], [1156, 122, 1237, 643], [1156, 122, 1237, 643], [1156, 122, 1237, 643], [1156, 122, 1237, 643], [1403, 1341, 419], [1403, 1341, 419], [1403, 1341, 419], [1059, 1341], [1059, 1341], [1347, 1004, 1045], [1347, 1004, 1045], [1347, 1004, 1045], [853, 1347, 1429], [853, 1347, 1429], [853, 1347, 1429], [1347, 1003, 1004], [1347, 1003, 1004], [1347, 1003, 1004], [853, 435, 60, 387], [853, 435, 60, 387], [853, 435, 60, 387], [853, 435, 60, 387], [853, 447, 552, 553], [853, 447, 552, 553], [853, 447, 552, 553], [853, 447, 552, 553], [1409, 931, 853, 1295, 873], [1409, 931, 853, 1295, 873], [1409, 931, 853, 1295, 873], [1409, 931, 853, 1295, 873], [1409, 931, 853, 1295, 873], [387, 304, 319, 853, 13], [387, 304, 319, 853, 13], [387, 304, 319, 853, 13], [387, 304, 319, 853, 13], [387, 304, 319, 853, 13], [1105, 124, 362], [1105, 124, 362], [1105, 124, 362], [1164, 1030, 552, 553, 362], [1164, 1030, 552, 553, 362], [1164, 1030, 552, 553, 362], [1164, 1030, 552, 553, 362], [1164, 1030, 552, 553, 362], [899, 362, 8], [899, 362, 8], [899, 362, 8], [983, 1295, 544, 563, 362], [983, 1295, 544, 563, 362], [983, 1295, 544, 563, 362], [983, 1295, 544, 563, 362], [983, 1295, 544, 563, 362], [360, 362], [360, 362], [1178, 362], [1178, 362], [898, 362, 921], [898, 362, 921], [898, 362, 921], [362, 270], [362, 270], [387, 362], [387, 362], [234, 777, 692, 191, 362], [234, 777, 692, 191, 362], [234, 777, 692, 191, 362], [234, 777, 692, 191, 362], [234, 777, 692, 191, 362], [482, 362, 481], [482, 362, 481], [482, 362, 481], [362, 1027, 57], [362, 1027, 57], [362, 1027, 57], [191, 1092, 362], [191, 1092, 362], [191, 1092, 362], [362, 80, 248, 211, 270], [362, 80, 248, 211, 270], [362, 80, 248, 211, 270], [362, 80, 248, 211, 270], [362, 80, 248, 211, 270], [123, 669, 398, 248, 362], [123, 669, 398, 248, 362], [123, 669, 398, 248, 362], [123, 669, 398, 248, 362], [123, 669, 398, 248, 362], [1419, 362, 392], [1419, 362, 392], [1419, 362, 392], [860, 362], [860, 362], [360, 362], [360, 362], [426, 657, 362, 303], [426, 657, 362, 303], [426, 657, 362, 303], [426, 657, 362, 303], [191, 362], [191, 362], [362, 360, 244], [362, 360, 244], [362, 360, 244], [177, 721, 729, 211, 362], [177, 721, 729, 211, 362], [177, 721, 729, 211, 362], [177, 721, 729, 211, 362], [177, 721, 729, 211, 362], [685, 191, 595, 596, 362], [685, 191, 595, 596, 362], [685, 191, 595, 596, 362], [685, 191, 595, 596, 362], [685, 191, 595, 596, 362], [191, 362, 556], [191, 362, 556], [191, 362, 556], [360, 362], [360, 362], [1296, 362, 556], [1296, 362, 556], [1296, 362, 556], [362, 843, 317, 271], [362, 843, 317, 271], [362, 843, 317, 271], [362, 843, 317, 271], [426, 164], [426, 164], [425, 426, 579, 114], [425, 426, 579, 114], [425, 426, 579, 114], [425, 426, 579, 114], [426, 1332, 202], [426, 1332, 202], [426, 1332, 202], [352, 495, 426, 322], [352, 495, 426, 322], [352, 495, 426, 322], [352, 495, 426, 322], [426, 8], [426, 8], [425, 426], [425, 426], [426, 377, 1063], [426, 377, 1063], [426, 377, 1063], [426, 374], [426, 374], [164, 1346, 426, 316, 318], [164, 1346, 426, 316, 318], [164, 1346, 426, 316, 318], [164, 1346, 426, 316, 318], [164, 1346, 426, 316, 318], [426, 1384], [426, 1384], [1389, 426], [1389, 426], [1254, 426, 377], [1254, 426, 377], [1254, 426, 377], [1089, 426], [1089, 426], [426, 241, 164], [426, 241, 164], [426, 241, 164], [164, 426, 334, 317], [164, 426, 334, 317], [164, 426, 334, 317], [164, 426, 334, 317], [426, 241], [426, 241], [426, 409], [426, 409], [426, 241], [426, 241], [326, 198, 202, 426, 416], [326, 198, 202, 426, 416], [326, 198, 202, 426, 416], [326, 198, 202, 426, 416], [326, 198, 202, 426, 416], [426, 269], [426, 269], [352, 353, 426, 493], [352, 353, 426, 493], [352, 353, 426, 493], [352, 353, 426, 493], [426, 241], [426, 241], [906, 1414, 426, 1064], [906, 1414, 426, 1064], [906, 1414, 426, 1064], [906, 1414, 426, 1064], [426, 1020], [426, 1020], [426, 241], [426, 241], [440, 426], [440, 426], [41, 425, 112, 426], [41, 425, 112, 426], [41, 425, 112, 426], [41, 425, 112, 426], [352, 495, 426], [352, 495, 426], [352, 495, 426], [164, 426, 334, 317], [164, 426, 334, 317], [164, 426, 334, 317], [164, 426, 334, 317], [425, 426, 393], [425, 426, 393], [425, 426, 393], [41, 426, 113, 241], [41, 426, 113, 241], [41, 426, 113, 241], [41, 426, 113, 241], [112, 426, 900, 1288], [112, 426, 900, 1288], [112, 426, 900, 1288], [112, 426, 900, 1288], [425, 440, 426], [425, 440, 426], [425, 440, 426], [426, 1384], [426, 1384], [121, 426], [121, 426], [425, 426, 241], [425, 426, 241], [425, 426, 241], [443, 657, 426], [443, 657, 426], [443, 657, 426], [164, 426, 402, 317], [164, 426, 402, 317], [164, 426, 402, 317], [164, 426, 402, 317], [426, 113], [426, 113], [42, 426, 384], [42, 426, 384], [42, 426, 384], [425, 65, 426, 70], [425, 65, 426, 70], [425, 65, 426, 70], [425, 65, 426, 70], [425, 9, 426, 65], [425, 9, 426, 65], [425, 9, 426, 65], [425, 9, 426, 65], [426, 164, 35], [426, 164, 35], [426, 164, 35], [425, 426, 241], [425, 426, 241], [425, 426, 241], [426, 241], [426, 241], [425, 426, 241], [425, 426, 241], [425, 426, 241], [426, 241], [426, 241], [425, 426], [425, 426], [426, 1169], [426, 1169], [1339, 37], [1339, 37], [394, 37, 475], [394, 37, 475], [394, 37, 475], [1343, 37, 370, 1273, 625], [1343, 37, 370, 1273, 625], [1343, 37, 370, 1273, 625], [1343, 37, 370, 1273, 625], [1343, 37, 370, 1273, 625], [37, 1273, 1108, 400, 270], [37, 1273, 1108, 400, 270], [37, 1273, 1108, 400, 270], [37, 1273, 1108, 400, 270], [37, 1273, 1108, 400, 270], [37, 1369, 174, 1382], [37, 1369, 174, 1382], [37, 1369, 174, 1382], [37, 1369, 174, 1382], [37, 499, 633, 542], [37, 499, 633, 542], [37, 499, 633, 542], [37, 499, 633, 542], [191, 1370, 37], [191, 1370, 37], [191, 1370, 37], [37, 1014, 43, 1008, 1009], [37, 1014, 43, 1008, 1009], [37, 1014, 43, 1008, 1009], [37, 1014, 43, 1008, 1009], [37, 1014, 43, 1008, 1009], [37, 300, 302, 256], [37, 300, 302, 256], [37, 300, 302, 256], [37, 300, 302, 256], [37, 1376, 244, 392, 891], [37, 1376, 244, 392, 891], [37, 1376, 244, 392, 891], [37, 1376, 244, 392, 891], [37, 1376, 244, 392, 891], [756, 37, 43, 1273, 1093], [756, 37, 43, 1273, 1093], [756, 37, 43, 1273, 1093], [756, 37, 43, 1273, 1093], [756, 37, 43, 1273, 1093], [161, 37], [161, 37], [622, 712], [622, 712], [622, 632], [622, 632], [622, 562], [622, 562], [622, 624, 626], [622, 624, 626], [622, 624, 626], [622, 473, 663], [622, 473, 663], [622, 473, 663], [192, 86, 254, 287], [192, 86, 254, 287], [192, 86, 254, 287], [192, 86, 254, 287], [192, 1293], [192, 1293], [192, 42, 1293], [192, 42, 1293], [192, 42, 1293], [1095, 192, 1244], [1095, 192, 1244], [1095, 192, 1244], [683, 42, 657, 1295], [683, 42, 657, 1295], [683, 42, 657, 1295], [683, 42, 657, 1295], [124, 741, 870, 659, 838], [124, 741, 870, 659, 838], [124, 741, 870, 659, 838], [124, 741, 870, 659, 838], [124, 741, 870, 659, 838], [1251, 239, 1323, 449, 34], [1251, 239, 1323, 449, 34], [1251, 239, 1323, 449, 34], [1251, 239, 1323, 449, 34], [1251, 239, 1323, 449, 34], [816, 329, 618], [816, 329, 618], [816, 329, 618], [815, 95], [815, 95], [816, 849, 848], [816, 849, 848], [816, 849, 848], [816, 330, 618, 661], [816, 330, 618, 661], [816, 330, 618, 661], [816, 330, 618, 661], [367, 330], [367, 330], [367, 330], [367, 330], [367, 330, 84, 332], [367, 330, 84, 332], [367, 330, 84, 332], [367, 330, 84, 332], [617, 331, 465, 660, 661], [617, 331, 465, 660, 661], [617, 331, 465, 660, 661], [617, 331, 465, 660, 661], [617, 331, 465, 660, 661], [176, 124, 331], [176, 124, 331], [176, 124, 331], [817, 1267, 332], [817, 1267, 332], [817, 1267, 332], [1307, 817], [1307, 817], [726, 817, 706], [726, 817, 706], [726, 817, 706], [726, 817, 706], [726, 817, 706], [726, 817, 706], [1307, 817], [1307, 817], [817, 615, 617], [817, 615, 617], [817, 615, 617], [1266, 1267, 332], [1266, 1267, 332], [1266, 1267, 332], [162, 158, 275, 332], [162, 158, 275, 332], [162, 158, 275, 332], [162, 158, 275, 332], [36, 275, 332], [36, 275, 332], [36, 275, 332], [613, 84, 332], [613, 84, 332], [613, 84, 332], [826, 1335], [826, 1335], [826, 191], [826, 191], [826, 42, 882], [826, 42, 882], [826, 42, 882], [39, 826, 796, 672, 194], [39, 826, 796, 672, 194], [39, 826, 796, 672, 194], [39, 826, 796, 672, 194], [39, 826, 796, 672, 194], [826, 580], [826, 580], [826, 730, 732, 690], [826, 730, 732, 690], [826, 730, 732, 690], [826, 730, 732, 690], [826, 191], [826, 191], [1162, 1252, 1335, 564, 1264], [1162, 1252, 1335, 564, 1264], [1162, 1252, 1335, 564, 1264], [1162, 1252, 1335, 564, 1264], [1162, 1252, 1335, 564, 1264], [1335, 244, 245], [1335, 244, 245], [1335, 244, 245], [1335, 244, 869, 1034, 360], [1335, 244, 869, 1034, 360], [1335, 244, 869, 1034, 360], [1335, 244, 869, 1034, 360], [1335, 244, 869, 1034, 360], [90, 533, 1335, 899, 8], [90, 533, 1335, 899, 8], [90, 533, 1335, 899, 8], [90, 533, 1335, 899, 8], [90, 533, 1335, 899, 8], [820, 487, 370, 137, 1357], [820, 487, 370, 137, 1357], [820, 487, 370, 137, 1357], [820, 487, 370, 137, 1357], [820, 487, 370, 137, 1357], [278, 137], [278, 137], [820, 137, 487], [820, 137, 487], [820, 137, 487], [328, 1181, 408, 137, 230], [328, 1181, 408, 137, 230], [328, 1181, 408, 137, 230], [328, 1181, 408, 137, 230], [328, 1181, 408, 137, 230], [328, 137, 6, 305], [328, 137, 6, 305], [328, 137, 6, 305], [328, 137, 6, 305], [487, 137], [487, 137], [718, 721, 865, 777, 191], [718, 721, 865, 777, 191], [718, 721, 865, 777, 191], [718, 721, 865, 777, 191], [718, 721, 865, 777, 191], [124, 1124, 777, 191, 696], [124, 1124, 777, 191, 696], [124, 1124, 777, 191, 696], [124, 1124, 777, 191, 696], [124, 1124, 777, 191, 696], [191, 1117, 1116], [191, 1117, 1116], [191, 1117, 1116], [191, 333, 240, 211], [191, 333, 240, 211], [191, 333, 240, 211], [191, 333, 240, 211], [191, 1307, 1308], [191, 1307, 1308], [191, 1307, 1308], [191, 388, 1137], [191, 388, 1137], [191, 388, 1137], [191, 777, 1415], [191, 777, 1415], [191, 777, 1415], [191, 828], [191, 828], [388, 784, 785, 191, 230], [388, 784, 785, 191, 230], [388, 784, 785, 191, 230], [388, 784, 785, 191, 230], [388, 784, 785, 191, 230], [191, 388, 230], [191, 388, 230], [191, 388, 230], [191, 1246], [191, 1246], [191, 645, 271], [191, 645, 271], [191, 645, 271], [333, 240, 668, 191, 211], [333, 240, 668, 191, 211], [333, 240, 668, 191, 211], [333, 240, 668, 191, 211], [333, 240, 668, 191, 211], [333, 1225, 191, 211, 696], [333, 1225, 191, 211, 696], [333, 1225, 191, 211, 696], [333, 1225, 191, 211, 696], [333, 1225, 191, 211, 696], [191, 333, 211, 696], [191, 333, 211, 696], [191, 333, 211, 696], [191, 333, 211, 696], [718, 1123, 373, 692, 191], [718, 1123, 373, 692, 191], [718, 1123, 373, 692, 191], [718, 1123, 373, 692, 191], [718, 1123, 373, 692, 191], [191, 173, 195, 584], [191, 173, 195, 584], [191, 173, 195, 584], [191, 173, 195, 584], [191, 1181], [191, 1181], [319, 13, 779], [319, 13, 779], [319, 13, 779], [319, 248, 773, 1295], [319, 248, 773, 1295], [319, 248, 773, 1295], [319, 248, 773, 1295], [392, 773, 319, 470, 175], [392, 773, 319, 470, 175], [392, 773, 319, 470, 175], [392, 773, 319, 470, 175], [392, 773, 319, 470, 175], [319, 807], [319, 807], [319, 705, 255, 566], [319, 705, 255, 566], [319, 705, 255, 566], [319, 705, 255, 566], [319, 237, 39, 439], [319, 237, 39, 439], [319, 237, 39, 439], [319, 237, 39, 439], [319, 807], [319, 807], [1250, 39, 726, 319, 1315], [1250, 39, 726, 319, 1315], [1250, 39, 726, 319, 1315], [1250, 39, 726, 319, 1315], [1250, 39, 726, 319, 1315], [319, 807, 743], [319, 807, 743], [319, 807, 743], [258, 319, 1127, 255], [258, 319, 1127, 255], [258, 319, 1127, 255], [258, 319, 1127, 255], [319, 255], [319, 255], [319, 248, 481], [319, 248, 481], [319, 248, 481], [718, 319, 234, 255], [718, 319, 234, 255], [718, 319, 234, 255], [718, 319, 234, 255], [319, 649], [319, 649], [255, 705, 555, 671, 319], [255, 705, 555, 671, 319], [255, 705, 555, 671, 319], [255, 705, 555, 671, 319], [255, 705, 555, 671, 319], [649, 807, 670, 39], [649, 807, 670, 39], [649, 807, 670, 39], [649, 807, 670, 39], [807, 745, 354, 356, 714], [807, 745, 354, 356, 714], [807, 745, 354, 356, 714], [807, 745, 354, 356, 714], [807, 745, 354, 356, 714], [807, 124, 39], [807, 124, 39], [807, 124, 39], [882, 42, 807, 886], [882, 42, 807, 886], [882, 42, 807, 886], [882, 42, 807, 886], [39, 550, 807, 796, 580], [39, 550, 807, 796, 580], [39, 550, 807, 796, 580], [39, 550, 807, 796, 580], [39, 550, 807, 796, 580], [807, 899], [807, 899], [705, 580, 807, 1395], [705, 580, 807, 1395], [705, 580, 807, 1395], [705, 580, 807, 1395], [807, 690, 1162, 671], [807, 690, 1162, 671], [807, 690, 1162, 671], [807, 690, 1162, 671], [791, 470, 807, 773], [791, 470, 807, 773], [791, 470, 807, 773], [791, 470, 807, 773], [326, 98, 320], [326, 98, 320], [326, 98, 320], [326, 320, 1355, 1042], [326, 320, 1355, 1042], [326, 320, 1355, 1042], [326, 320, 1355, 1042], [326, 320, 829], [326, 320, 829], [326, 320, 829], [326, 320, 831], [326, 320, 831], [326, 320, 831], [326, 320, 829], [326, 320, 829], [326, 320, 829], [320, 831, 675], [320, 831, 675], [320, 831, 675], [829, 320], [829, 320], [326, 320, 829], [326, 320, 829], [326, 320, 829], [829, 320, 32, 478], [829, 320, 32, 478], [829, 320, 32, 478], [829, 320, 32, 478], [326, 320], [326, 320], [326, 474, 977, 588, 320], [326, 474, 977, 588, 320], [326, 474, 977, 588, 320], [326, 474, 977, 588, 320], [326, 474, 977, 588, 320], [829, 320, 32], [829, 320, 32], [829, 320, 32], [326, 320, 829, 478], [326, 320, 829, 478], [326, 320, 829, 478], [326, 320, 829, 478], [326, 478, 829, 320, 32], [326, 478, 829, 320, 32], [326, 478, 829, 320, 32], [326, 478, 829, 320, 32], [326, 478, 829, 320, 32], [326, 320, 829, 831], [326, 320, 829, 831], [326, 320, 829, 831], [326, 320, 829, 831], [326, 320, 478], [326, 320, 478], [326, 320, 478], [326, 268, 389], [326, 268, 389], [326, 268, 389], [326, 461, 1332], [326, 461, 1332], [326, 461, 1332], [326, 215], [326, 215], [326, 829, 831, 478], [326, 829, 831, 478], [326, 829, 831, 478], [326, 829, 831, 478], [326, 829], [326, 829], [326, 906], [326, 906], [326, 829, 32], [326, 829, 32], [326, 829, 32], [326, 588, 989], [326, 588, 989], [326, 588, 989], [326, 588, 829, 984, 989], [326, 588, 829, 984, 989], [326, 588, 829, 984, 989], [326, 588, 829, 984, 989], [326, 588, 829, 984, 989], [326, 590, 829, 198], [326, 590, 829, 198], [326, 590, 829, 198], [326, 590, 829, 198], [326, 389], [326, 389], [326, 1089, 425, 9, 1037], [326, 1089, 425, 9, 1037], [326, 1089, 425, 9, 1037], [326, 1089, 425, 9, 1037], [326, 1089, 425, 9, 1037], [326, 425, 854], [326, 425, 854], [326, 425, 854], [1331, 1332, 1085], [1331, 1332, 1085], [1331, 1332, 1085], [1331, 202], [1331, 202], [1331, 1332], [1331, 1332], [1331, 1332, 1333, 202], [1331, 1332, 1333, 202], [1331, 1332, 1333, 202], [1331, 1332, 1333, 202], [1332, 1333, 1294, 202], [1332, 1333, 1294, 202], [1332, 1333, 1294, 202], [1332, 1333, 1294, 202], [1332, 1422, 202], [1332, 1422, 202], [1332, 1422, 202], [1332, 202], [1332, 202], [906, 1332], [906, 1332], [1332, 1333, 202], [1332, 1333, 202], [1332, 1333, 202], [1242, 1332, 1333, 202], [1242, 1332, 1333, 202], [1242, 1332, 1333, 202], [1242, 1332, 1333, 202], [1332, 546, 411], [1332, 546, 411], [1332, 546, 411], [1332, 1333, 202, 411, 546], [1332, 1333, 202, 411, 546], [1332, 1333, 202, 411, 546], [1332, 1333, 202, 411, 546], [1332, 1333, 202, 411, 546], [140, 1364, 383, 1217], [140, 1364, 383, 1217], [140, 1364, 383, 1217], [140, 1364, 383, 1217], [1364, 383, 1136, 42], [1364, 383, 1136, 42], [1364, 383, 1136, 42], [1364, 383, 1136, 42], [1408, 934, 928, 614], [1408, 934, 928, 614], [1408, 934, 928, 614], [1408, 934, 928, 614], [614, 889], [614, 889], [338, 653, 889, 614], [338, 653, 889, 614], [338, 653, 889, 614], [338, 653, 889, 614], [615, 300, 256], [615, 300, 256], [615, 300, 256], [742, 615], [742, 615], [873, 742, 615, 702], [873, 742, 615, 702], [873, 742, 615, 702], [873, 742, 615, 702], [742, 615], [742, 615], [466, 660, 617, 751], [466, 660, 617, 751], [466, 660, 617, 751], [466, 660, 617, 751], [806, 617], [806, 617], [1266, 367, 618, 1267], [1266, 367, 618, 1267], [1266, 367, 618, 1267], [1266, 367, 618, 1267], [517, 618], [517, 618], [633, 465, 466, 660, 661], [633, 465, 466, 660, 661], [633, 465, 466, 660, 661], [633, 465, 466, 660, 661], [633, 465, 466, 660, 661], [848, 849, 660], [848, 849, 660], [848, 849, 660], [1408, 661], [1408, 661], [300, 661], [300, 661], [468, 469, 661], [468, 469, 661], [468, 469, 661], [889, 661], [889, 661], [91, 194], [91, 194], [672, 673, 194, 48], [672, 673, 194, 48], [672, 673, 194, 48], [672, 673, 194, 48], [287, 194], [287, 194], [672, 673, 194], [672, 673, 194], [672, 673, 194], [672, 673], [672, 673], [672, 179], [672, 179], [672, 1360], [672, 1360], [672, 87], [672, 87], [672, 42, 882], [672, 42, 882], [672, 42, 882], [672, 833], [672, 833], [672, 441, 368], [672, 441, 368], [672, 441, 368], [672, 673, 641], [672, 673, 641], [672, 673, 641], [672, 441, 1360], [672, 441, 1360], [672, 441, 1360], [672, 673, 48], [672, 673, 48], [672, 673, 48], [672, 673, 48], [672, 673, 48], [672, 673, 48], [91, 739, 441, 672, 673], [91, 739, 441, 672, 673], [91, 739, 441, 672, 673], [91, 739, 441, 672, 673], [91, 739, 441, 672, 673], [673, 48, 441, 833], [673, 48, 441, 833], [673, 48, 441, 833], [673, 48, 441, 833], [721, 244, 725, 726, 730], [721, 244, 725, 726, 730], [721, 244, 725, 726, 730], [721, 244, 725, 726, 730], [721, 244, 725, 726, 730], [721, 777], [721, 777], [1361, 1318, 124, 721, 79], [1361, 1318, 124, 721, 79], [1361, 1318, 124, 721, 79], [1361, 1318, 124, 721, 79], [1361, 1318, 124, 721, 79], [240, 243, 214, 175], [240, 243, 214, 175], [240, 243, 214, 175], [240, 243, 214, 175], [139, 240, 1382], [139, 240, 1382], [139, 240, 1382], [240, 875, 689, 210], [240, 875, 689, 210], [240, 875, 689, 210], [240, 875, 689, 210], [240, 875, 689, 210], [240, 875, 689, 210], [240, 875, 689, 210], [240, 875, 689, 210], [240, 668], [240, 668], [240, 1296], [240, 1296], [557, 860, 240], [557, 860, 240], [557, 860, 240], [240, 668], [240, 668], [642, 240, 1124, 270], [642, 240, 1124, 270], [642, 240, 1124, 270], [642, 240, 1124, 270], [333, 240, 211], [333, 240, 211], [333, 240, 211], [240, 899], [240, 899], [240, 1296, 1040], [240, 1296, 1040], [240, 1296, 1040], [725, 1369], [725, 1369], [1148, 637, 725, 1093, 891], [1148, 637, 725, 1093, 891], [1148, 637, 725, 1093, 891], [1148, 637, 725, 1093, 891], [1148, 637, 725, 1093, 891], [725, 479, 604], [725, 479, 604], [725, 479, 604], [725, 141], [725, 141], [725, 446, 983, 359, 486], [725, 446, 983, 359, 486], [725, 446, 983, 359, 486], [725, 446, 983, 359, 486], [725, 446, 983, 359, 486], [725, 563], [725, 563], [725, 779, 604], [725, 779, 604], [725, 779, 604], [725, 878, 983, 486], [725, 878, 983, 486], [725, 878, 983, 486], [725, 878, 983, 486], [725, 861], [725, 861], [725, 141, 732, 861], [725, 141, 732, 861], [725, 141, 732, 861], [725, 141, 732, 861], [725, 580, 244], [725, 580, 244], [725, 580, 244], [725, 634, 983, 766], [725, 634, 983, 766], [725, 634, 983, 766], [725, 634, 983, 766], [725, 634, 983, 486], [725, 634, 983, 486], [725, 634, 983, 486], [725, 634, 983, 486], [725, 174, 1369], [725, 174, 1369], [725, 174, 1369], [1250, 42, 580, 728], [1250, 42, 580, 728], [1250, 42, 580, 728], [1250, 42, 580, 728], [729, 1250, 728], [729, 1250, 728], [729, 1250, 728], [1083, 1250, 869, 1034, 732], [1083, 1250, 869, 1034, 732], [1083, 1250, 869, 1034, 732], [1083, 1250, 869, 1034, 732], [1083, 1250, 869, 1034, 732], [726, 1250], [726, 1250], [1250, 732, 1253], [1250, 732, 1253], [1250, 732, 1253], [982, 241, 269], [982, 241, 269], [982, 241, 269], [1159, 241], [1159, 241], [241, 1289], [241, 1289], [1288, 241, 113], [1288, 241, 113], [1288, 241, 113], [241, 988], [241, 988], [1416, 241], [1416, 241], [241, 1020], [241, 1020], [1129, 241], [1129, 241], [241, 1365], [241, 1365], [241, 796], [241, 796], [241, 796], [241, 796], [52, 241, 120, 1196], [52, 241, 120, 1196], [52, 241, 120, 1196], [52, 241, 120, 1196], [241, 1024], [241, 1024], [700, 241, 80, 123], [700, 241, 80, 123], [700, 241, 80, 123], [700, 241, 80, 123], [700, 241, 80, 123], [700, 241, 80, 123], [700, 241, 80, 123], [700, 241, 80, 123], [700, 241, 80, 123], [700, 241, 80, 123], [700, 241, 80, 123], [700, 241, 80, 123], [241, 569], [241, 569], [1099, 241], [1099, 241], [327, 241, 441, 368], [327, 241, 441, 368], [327, 241, 441, 368], [327, 241, 441, 368], [241, 80], [241, 80], [700, 241, 244, 125], [700, 241, 244, 125], [700, 241, 244, 125], [700, 241, 244, 125], [700, 241, 80], [700, 241, 80], [700, 241, 80], [479, 241, 317, 1401, 269], [479, 241, 317, 1401, 269], [479, 241, 317, 1401, 269], [479, 241, 317, 1401, 269], [479, 241, 317, 1401, 269], [677, 241, 8, 678], [677, 241, 8, 678], [677, 241, 8, 678], [677, 241, 8, 678], [1236, 241, 125], [1236, 241, 125], [1236, 241, 125], [241, 316, 1407], [241, 316, 1407], [241, 316, 1407], [327, 58, 44, 241], [327, 58, 44, 241], [327, 58, 44, 241], [327, 58, 44, 241], [136, 241, 363], [136, 241, 363], [136, 241, 363], [241, 59, 794, 1037, 136], [241, 59, 794, 1037, 136], [241, 59, 794, 1037, 136], [241, 59, 794, 1037, 136], [241, 59, 794, 1037, 136], [241, 1340], [241, 1340], [241, 384, 1096, 393], [241, 384, 1096, 393], [241, 384, 1096, 393], [241, 384, 1096, 393], [534, 241], [534, 241], [425, 241], [425, 241], [241, 87, 573], [241, 87, 573], [241, 87, 573], [241, 1075], [241, 1075], [877, 241], [877, 241], [376, 241, 867, 190], [376, 241, 867, 190], [376, 241, 867, 190], [376, 241, 867, 190], [241, 87], [241, 87], [700, 125, 241, 80, 248], [700, 125, 241, 80, 248], [700, 125, 241, 80, 248], [700, 125, 241, 80, 248], [700, 125, 241, 80, 248], [76, 28, 244, 990], [76, 28, 244, 990], [76, 28, 244, 990], [76, 28, 244, 990], [602, 126, 244], [602, 126, 244], [602, 126, 244], [359, 244, 1252], [359, 244, 1252], [359, 244, 1252], [854, 87, 244], [854, 87, 244], [854, 87, 244], [1012, 244, 392, 357, 360], [1012, 244, 392, 357, 360], [1012, 244, 392, 357, 360], [1012, 244, 392, 357, 360], [1012, 244, 392, 357, 360], [244, 1390], [244, 1390], [244, 724], [244, 724], [244, 1252], [244, 1252], [244, 392], [244, 392], [291, 244], [291, 244], [360, 392, 244, 1252], [360, 392, 244, 1252], [360, 392, 244, 1252], [360, 392, 244, 1252], [360, 244], [360, 244], [117, 730, 886, 244], [117, 730, 886, 244], [117, 730, 886, 244], [117, 730, 886, 244], [42, 244], [42, 244], [52, 860, 360, 244], [52, 860, 360, 244], [52, 860, 360, 244], [52, 860, 360, 244], [359, 360, 244, 174], [359, 360, 244, 174], [359, 360, 244, 174], [359, 360, 244, 174], [360, 131, 244], [360, 131, 244], [360, 131, 244], [531, 244, 510, 1170, 360], [531, 244, 510, 1170, 360], [531, 244, 510, 1170, 360], [531, 244, 510, 1170, 360], [531, 244, 510, 1170, 360], [1252, 244, 117, 354, 728], [1252, 244, 117, 354, 728], [1252, 244, 117, 354, 728], [1252, 244, 117, 354, 728], [1252, 244, 117, 354, 728], [124, 244, 392], [124, 244, 392], [124, 244, 392], [124, 244, 270], [124, 244, 270], [124, 244, 270], [244, 693, 694], [244, 693, 694], [244, 693, 694], [1252, 270], [1252, 270], [603, 1093, 1252], [603, 1093, 1252], [603, 1093, 1252], [730, 728], [730, 728], [482, 728, 580, 665], [482, 728, 580, 665], [482, 728, 580, 665], [482, 728, 580, 665], [605, 728], [605, 728], [482, 728, 773, 881], [482, 728, 773, 881], [482, 728, 773, 881], [482, 728, 773, 881], [100, 728], [100, 728], [533, 336, 482, 481, 728], [533, 336, 482, 481, 728], [533, 336, 482, 481, 728], [533, 336, 482, 481, 728], [533, 336, 482, 481, 728], [730, 1397, 271], [730, 1397, 271], [730, 1397, 271], [1322, 732, 340], [1322, 732, 340], [1322, 732, 340], [732, 916], [732, 916], [248, 1177], [248, 1177], [370, 248, 1108, 270], [370, 248, 1108, 270], [370, 248, 1108, 270], [370, 248, 1108, 270], [757, 248, 164, 318], [757, 248, 164, 318], [757, 248, 164, 318], [757, 248, 164, 318], [713, 248], [713, 248], [248, 147, 345, 1097], [248, 147, 345, 1097], [248, 147, 345, 1097], [248, 147, 345, 1097], [211, 248, 650], [211, 248, 650], [211, 248, 650], [76, 1427, 600], [76, 1427, 600], [76, 1427, 600], [543, 1427], [543, 1427], [76, 1427, 89], [76, 1427, 89], [76, 1427, 89], [927, 421, 40, 852], [927, 421, 40, 852], [927, 421, 40, 852], [927, 421, 40, 852], [927, 1410, 421, 40, 932], [927, 1410, 421, 40, 932], [927, 1410, 421, 40, 932], [927, 1410, 421, 40, 932], [927, 1410, 421, 40, 932], [1046, 1132, 1189, 40], [1046, 1132, 1189, 40], [1046, 1132, 1189, 40], [1046, 1132, 1189, 40], [42, 44, 40], [42, 44, 40], [42, 44, 40], [310, 598, 40], [310, 598, 40], [310, 598, 40], [42, 563, 1012], [42, 563, 1012], [42, 563, 1012], [42, 87, 864], [42, 87, 864], [42, 87, 864], [42, 679], [42, 679], [42, 384], [42, 384], [1269, 42, 172, 750, 1196], [1269, 42, 172, 750, 1196], [1269, 42, 172, 750, 1196], [1269, 42, 172, 750, 1196], [1269, 42, 172, 750, 1196], [218, 42], [218, 42], [42, 1293], [42, 1293], [42, 1222, 707, 1219], [42, 1222, 707, 1219], [42, 1222, 707, 1219], [42, 1222, 707, 1219], [42, 1271], [42, 1271], [42, 75], [42, 75], [164, 42, 394, 313, 33], [164, 42, 394, 313, 33], [164, 42, 394, 313, 33], [164, 42, 394, 313, 33], [164, 42, 394, 313, 33], [42, 44, 1293, 350], [42, 44, 1293, 350], [42, 44, 1293, 350], [42, 44, 1293, 350], [704, 42, 703], [704, 42, 703], [704, 42, 703], [42, 1189, 44, 18], [42, 1189, 44, 18], [42, 1189, 44, 18], [42, 1189, 44, 18], [42, 384, 1293], [42, 384, 1293], [42, 384, 1293], [42, 350], [42, 350], [42, 321], [42, 321], [42, 383, 813, 26], [42, 383, 813, 26], [42, 383, 813, 26], [42, 383, 813, 26], [44, 755], [44, 755], [704, 764, 44], [704, 764, 44], [704, 764, 44], [136, 44], [136, 44], [239, 44, 310, 1316], [239, 44, 310, 1316], [239, 44, 310, 1316], [239, 44, 310, 1316], [44, 87, 380, 438], [44, 87, 380, 438], [44, 87, 380, 438], [44, 87, 380, 438], [1290, 44, 438, 289], [1290, 44, 438, 289], [1290, 44, 438, 289], [1290, 44, 438, 289], [1081, 1043, 44], [1081, 1043, 44], [1081, 1043, 44], [44, 101, 380, 438], [44, 101, 380, 438], [44, 101, 380, 438], [44, 101, 380, 438], [316, 164, 460], [316, 164, 460], [316, 164, 460], [71, 164, 263, 316, 267], [71, 164, 263, 316, 267], [71, 164, 263, 316, 267], [71, 164, 263, 316, 267], [71, 164, 263, 316, 267], [1287, 164], [1287, 164], [400, 164], [400, 164], [914, 164], [914, 164], [1321, 164], [1321, 164], [164, 425, 316, 317], [164, 425, 316, 317], [164, 425, 316, 317], [164, 425, 316, 317], [1000, 420, 316, 164], [1000, 420, 316, 164], [1000, 420, 316, 164], [1000, 420, 316, 164], [316, 164], [316, 164], [364, 1199, 336, 747, 324], [364, 1199, 336, 747, 324], [364, 1199, 336, 747, 324], [364, 1199, 336, 747, 324], [364, 1199, 336, 747, 324], [1124, 685], [1124, 685], [685, 1349, 1206, 1137, 1224], [685, 1349, 1206, 1137, 1224], [685, 1349, 1206, 1137, 1224], [685, 1349, 1206, 1137, 1224], [685, 1349, 1206, 1137, 1224], [596, 685], [596, 685], [705, 685, 642], [705, 685, 642], [705, 685, 642], [292, 596, 685], [292, 596, 685], [292, 596, 685], [271, 544, 843, 685], [271, 544, 843, 685], [271, 544, 843, 685], [271, 544, 843, 685], [1336, 692, 255], [1336, 692, 255], [1336, 692, 255], [566, 255, 555, 692], [566, 255, 555, 692], [566, 255, 555, 692], [566, 255, 555, 692], [1318, 336], [1318, 336], [364, 304, 336], [364, 304, 336], [364, 304, 336], [280, 716, 691, 336], [280, 716, 691, 336], [280, 716, 691, 336], [280, 716, 691, 336], [616, 304, 336], [616, 304, 336], [616, 304, 336], [364, 387, 336], [364, 387, 336], [364, 387, 336], [255, 909, 336, 859], [255, 909, 336, 859], [255, 909, 336, 859], [255, 909, 336, 859], [1030, 552, 1079, 336], [1030, 552, 1079, 336], [1030, 552, 1079, 336], [1030, 552, 1079, 336], [336, 481, 533], [336, 481, 533], [336, 481, 533], [364, 336], [364, 336], [304, 60, 336], [304, 60, 336], [304, 60, 336], [567, 1259, 454, 336], [567, 1259, 454, 336], [567, 1259, 454, 336], [567, 1259, 454, 336], [43, 1008], [43, 1008], [1014, 43, 1009, 1229], [1014, 43, 1009, 1229], [1014, 43, 1009, 1229], [1014, 43, 1009, 1229], [1014, 43, 1229, 1114, 1008], [1014, 43, 1229, 1114, 1008], [1014, 43, 1229, 1114, 1008], [1014, 43, 1229, 1114, 1008], [1014, 43, 1229, 1114, 1008], [1326, 1008], [1326, 1008], [1135, 1008, 1009], [1135, 1008, 1009], [1135, 1008, 1009], [48, 5], [48, 5], [54, 48], [54, 48], [771, 776, 48, 671, 87], [771, 776, 48, 671, 87], [771, 776, 48, 671, 87], [771, 776, 48, 671, 87], [771, 776, 48, 671, 87], [574, 575, 569], [574, 575, 569], [574, 575, 569], [809, 811, 569], [809, 811, 569], [809, 811, 569], [575, 569, 572, 115], [575, 569, 572, 115], [575, 569, 572, 115], [575, 569, 572, 115], [569, 572], [569, 572], [461, 709, 202], [461, 709, 202], [461, 709, 202], [416, 202], [416, 202], [461, 709, 1311, 202], [461, 709, 1311, 202], [461, 709, 1311, 202], [461, 709, 1311, 202], [704, 268, 389, 559], [704, 268, 389, 559], [704, 268, 389, 559], [704, 268, 389, 559], [268, 389, 559], [268, 389, 559], [268, 389, 559], [338, 889], [338, 889], [1430, 1105], [1430, 1105], [1105, 668, 824], [1105, 668, 824], [1105, 668, 824], [1105, 668, 450], [1105, 668, 450], [1105, 668, 450], [258, 60, 655], [258, 60, 655], [258, 60, 655], [258, 255], [258, 255], [258, 747, 387], [258, 747, 387], [258, 747, 387], [258, 1164], [258, 1164], [1268, 750, 940, 384, 1196], [1268, 750, 940, 384, 1196], [1268, 750, 940, 384, 1196], [1268, 750, 940, 384, 1196], [1268, 750, 940, 384, 1196], [1268, 750, 940, 384, 1196], [1268, 750, 940, 384, 1196], [1268, 750, 940, 384, 1196], [1268, 750, 940, 384, 1196], [1268, 750, 940, 384, 1196], [384, 750], [384, 750], [263, 361, 316], [263, 361, 316], [263, 361, 316], [361, 1287, 263, 316], [361, 1287, 263, 316], [361, 1287, 263, 316], [361, 1287, 263, 316], [361, 1287, 263, 316], [361, 1287, 263, 316], [361, 1287, 263, 316], [361, 1287, 263, 316], [263, 49], [263, 49], [757, 316, 47], [757, 316, 47], [757, 316, 47], [757, 316, 21], [757, 316, 21], [757, 316, 21], [757, 838, 536, 318], [757, 838, 536, 318], [757, 838, 536, 318], [757, 838, 536, 318], [844, 267], [844, 267], [267, 38, 921], [267, 38, 921], [267, 38, 921], [47, 316, 267], [47, 316, 267], [47, 316, 267], [2, 273, 184, 315, 269], [2, 273, 184, 315, 269], [2, 273, 184, 315, 269], [2, 273, 184, 315, 269], [2, 273, 184, 315, 269], [899, 269, 1017, 1351], [899, 269, 1017, 1351], [899, 269, 1017, 1351], [899, 269, 1017, 1351], [1089, 111, 269, 196], [1089, 111, 269, 196], [1089, 111, 269, 196], [1089, 111, 269, 196], [269, 316, 317], [269, 316, 317], [269, 316, 317], [491, 273], [491, 273], [375, 273, 555], [375, 273, 555], [375, 273, 555], [1381, 849, 313, 316, 525], [1381, 849, 313, 316, 525], [1381, 849, 313, 316, 525], [1381, 849, 313, 316, 525], [1381, 849, 313, 316, 525], [313, 992, 1080], [313, 992, 1080], [313, 992, 1080], [313, 230, 208], [313, 230, 208], [313, 230, 208], [1402, 415, 1371, 313], [1402, 415, 1371, 313], [1402, 415, 1371, 313], [1402, 415, 1371, 313], [1122, 509, 313, 133, 607], [1122, 509, 313, 133, 607], [1122, 509, 313, 133, 607], [1122, 509, 313, 133, 607], [1122, 509, 313, 133, 607], [652, 656, 420, 313], [652, 656, 420, 313], [652, 656, 420, 313], [652, 656, 420, 313], [75, 165, 316, 317, 1245], [75, 165, 316, 317, 1245], [75, 165, 316, 317, 1245], [75, 165, 316, 317, 1245], [75, 165, 316, 317, 1245], [420, 458, 316], [420, 458, 316], [420, 458, 316], [316, 49], [316, 49], [496, 316, 165], [496, 316, 165], [496, 316, 165], [420, 845, 169, 316, 946], [420, 845, 169, 316, 946], [420, 845, 169, 316, 946], [420, 845, 169, 316, 946], [420, 845, 169, 316, 946], [496, 316, 165], [496, 316, 165], [496, 316, 165], [375, 898, 316, 921], [375, 898, 316, 921], [375, 898, 316, 921], [375, 898, 316, 921], [316, 318], [316, 318], [844, 316], [844, 316], [1346, 316], [1346, 316], [904, 420, 316], [904, 420, 316], [904, 420, 316], [1000, 420, 316], [1000, 420, 316], [1000, 420, 316], [316, 35], [316, 35], [316, 278, 1292, 953], [316, 278, 1292, 953], [316, 278, 1292, 953], [316, 278, 1292, 953], [47, 316], [47, 316], [420, 316], [420, 316], [76, 316, 429], [76, 316, 429], [76, 316, 429], [325, 316, 99, 1327], [325, 316, 99, 1327], [325, 316, 99, 1327], [325, 316, 99, 1327], [99, 1324, 316, 317, 325], [99, 1324, 316, 317, 325], [99, 1324, 316, 317, 325], [99, 1324, 316, 317, 325], [99, 1324, 316, 317, 325], [420, 316], [420, 316], [1346, 420, 1393, 316], [1346, 420, 1393, 316], [1346, 420, 1393, 316], [1346, 420, 1393, 316], [316, 940], [316, 940], [827, 868, 317], [827, 868, 317], [827, 868, 317], [597, 1358, 317, 1232], [597, 1358, 317, 1232], [597, 1358, 317, 1232], [597, 1358, 317, 1232], [904, 370, 317], [904, 370, 317], [904, 370, 317], [317, 113, 270], [317, 113, 270], [317, 113, 270], [1234, 1058, 1161, 651, 317], [1234, 1058, 1161, 651, 317], [1234, 1058, 1161, 651, 317], [1234, 1058, 1161, 651, 317], [1234, 1058, 1161, 651, 317], [1084, 1080, 318], [1084, 1080, 318], [1084, 1080, 318], [914, 812], [914, 812], [174, 1369, 57, 1144], [174, 1369, 57, 1144], [174, 1369, 57, 1144], [174, 1369, 57, 1144], [1370, 1113], [1370, 1113], [1370, 1317], [1370, 1317], [1370, 668], [1370, 668], [52, 1298, 392], [52, 1298, 392], [52, 1298, 392], [52, 1298, 392], [52, 1298, 392], [52, 1298, 392], [52, 1298, 392], [52, 1298, 392], [52, 1298, 392], [52, 1298, 392], [52, 1298, 392], [52, 1298, 392], [139, 860, 392, 359, 360], [139, 860, 392, 359, 360], [139, 860, 392, 359, 360], [139, 860, 392, 359, 360], [139, 860, 392, 359, 360], [891, 634, 983, 486], [891, 634, 983, 486], [891, 634, 983, 486], [891, 634, 983, 486], [891, 551, 542], [891, 551, 542], [891, 551, 542], [891, 861], [891, 861], [396, 174], [396, 174], [396, 1067], [396, 1067], [904, 898], [904, 898], [922, 904, 925, 921], [922, 904, 925, 921], [922, 904, 925, 921], [922, 904, 925, 921], [260, 375, 904, 119, 921], [260, 375, 904, 119, 921], [260, 375, 904, 119, 921], [260, 375, 904, 119, 921], [260, 375, 904, 119, 921], [904, 371, 397, 921], [904, 371, 397, 921], [904, 371, 397, 921], [904, 371, 397, 921], [519, 837], [519, 837], [860, 865, 584], [860, 865, 584], [860, 865, 584], [558, 584, 865, 492], [558, 584, 865, 492], [558, 584, 865, 492], [558, 584, 865, 492], [60, 642], [60, 642], [783, 784, 642], [783, 784, 642], [783, 784, 642], [76, 543, 642], [76, 543, 642], [76, 543, 642], [642, 530, 509], [642, 530, 509], [642, 530, 509], [752, 484, 597, 174], [752, 484, 597, 174], [752, 484, 597, 174], [752, 484, 597, 174], [1277, 174], [1277, 174], [585, 654, 627, 630, 631], [585, 654, 627, 630, 631], [585, 654, 627, 630, 631], [585, 654, 627, 630, 631], [585, 654, 627, 630, 631], [654, 657], [654, 657], [495, 657], [495, 657], [495, 657], [495, 657], [495, 657], [495, 657], [1115, 179, 368, 833], [1115, 179, 368, 833], [1115, 179, 368, 833], [1115, 179, 368, 833], [940, 703], [940, 703], [770, 703, 181, 449, 34], [770, 703, 181, 449, 34], [770, 703, 181, 449, 34], [770, 703, 181, 449, 34], [770, 703, 181, 449, 34], [764, 691, 703], [764, 691, 703], [764, 691, 703], [703, 704, 764, 449, 691], [703, 704, 764, 449, 691], [703, 704, 764, 449, 691], [703, 704, 764, 449, 691], [703, 704, 764, 449, 691], [703, 764, 448, 874, 501], [703, 764, 448, 874, 501], [703, 764, 448, 874, 501], [703, 764, 448, 874, 501], [703, 764, 448, 874, 501], [764, 507, 703], [764, 507, 703], [764, 507, 703], [704, 136, 703], [704, 136, 703], [704, 136, 703], [537, 764, 310, 703], [537, 764, 310, 703], [537, 764, 310, 703], [537, 764, 310, 703], [704, 327, 441, 764], [704, 327, 441, 764], [704, 327, 441, 764], [704, 327, 441, 764], [704, 136], [704, 136], [704, 136, 58], [704, 136, 58], [704, 136, 58], [106, 107, 236, 290, 156], [106, 107, 236, 290, 156], [106, 107, 236, 290, 156], [106, 107, 236, 290, 156], [106, 107, 236, 290, 156], [236, 216, 1293, 290], [236, 216, 1293, 290], [236, 216, 1293, 290], [236, 216, 1293, 290], [106, 107, 236, 1306, 1353], [106, 107, 236, 1306, 1353], [106, 107, 236, 1306, 1353], [106, 107, 236, 1306, 1353], [106, 107, 236, 1306, 1353], [106, 236], [106, 236], [1353, 290], [1353, 290], [200, 290, 1187], [200, 290, 1187], [200, 290, 1187], [106, 1035, 290], [106, 1035, 290], [106, 1035, 290], [106, 200, 290, 1187], [106, 200, 290, 1187], [106, 200, 290, 1187], [106, 200, 290, 1187], [1180, 290], [1180, 290], [107, 290], [107, 290], [106, 107, 1353, 290], [106, 107, 1353, 290], [106, 107, 1353, 290], [106, 107, 1353, 290], [107, 1293], [107, 1293], [292, 173, 126, 245], [292, 173, 126, 245], [292, 173, 126, 245], [292, 173, 126, 245], [1296, 1221, 476], [1296, 1221, 476], [1296, 1221, 476], [1296, 777], [1296, 777], [1297, 60, 285], [1297, 60, 285], [1297, 60, 285], [1297, 1274, 285], [1297, 1274, 285], [1297, 1274, 285], [1297, 1206, 1137, 1301], [1297, 1206, 1137, 1301], [1297, 1206, 1137, 1301], [1297, 1206, 1137, 1301], [1297, 285], [1297, 285], [304, 60, 1274, 1297, 454], [304, 60, 1274, 1297, 454], [304, 60, 1274, 1297, 454], [304, 60, 1274, 1297, 454], [304, 60, 1274, 1297, 454], [777, 1372], [777, 1372], [780, 778], [780, 778], [882, 139, 949, 1295], [882, 139, 949, 1295], [882, 139, 949, 1295], [882, 139, 949, 1295], [425, 1371, 1378], [425, 1371, 1378], [425, 1371, 1378], [1371, 1033, 1387], [1371, 1033, 1387], [1371, 1033, 1387], [199, 1120, 1190, 65, 908], [199, 1120, 1190, 65, 908], [199, 1120, 1190, 65, 908], [199, 1120, 1190, 65, 908], [199, 1120, 1190, 65, 908], [67, 65, 1039, 70], [67, 65, 1039, 70], [67, 65, 1039, 70], [67, 65, 1039, 70], [358, 2, 428, 165], [358, 2, 428, 165], [358, 2, 428, 165], [358, 2, 428, 165], [1234, 1058, 1161, 1147], [1234, 1058, 1161, 1147], [1234, 1058, 1161, 1147], [1234, 1058, 1161, 1147], [1322, 491, 1147], [1322, 491, 1147], [1322, 491, 1147], [695, 698, 787], [695, 698, 787], [695, 698, 787], [698, 699, 786], [698, 699, 786], [698, 699, 786], [698, 699, 894, 399, 253], [698, 699, 894, 399, 253], [698, 699, 894, 399, 253], [698, 699, 894, 399, 253], [698, 699, 894, 399, 253], [698, 699, 787, 738], [698, 699, 787, 738], [698, 699, 787, 738], [698, 699, 787, 738], [698, 738], [698, 738], [699, 738], [699, 738], [129, 1405], [129, 1405], [563, 949, 580, 1295], [563, 949, 580, 1295], [563, 949, 580, 1295], [563, 949, 580, 1295], [211, 706, 710], [211, 706, 710], [211, 706, 710], [211, 57, 1144], [211, 57, 1144], [211, 57, 1144], [211, 1060, 160], [211, 1060, 160], [211, 1060, 160], [211, 1298], [211, 1298], [211, 705, 846], [211, 705, 846], [211, 705, 846], [211, 705, 270], [211, 705, 270], [211, 705, 270], [211, 160], [211, 160], [211, 1124], [211, 1124], [211, 705, 1295], [211, 705, 1295], [211, 705, 1295], [211, 563], [211, 563], [211, 160], [211, 160], [211, 270], [211, 270], [211, 57, 1144], [211, 57, 1144], [211, 57, 1144], [461, 416, 935], [461, 416, 935], [461, 416, 935], [24, 935, 416], [24, 935, 416], [24, 935, 416], [1089, 416], [1089, 416], [1403, 50], [1403, 50], [1403, 419], [1403, 419], [109, 419], [109, 419], [109, 419], [109, 419], [226, 470], [226, 470], [470, 471], [470, 471], [76, 987, 28, 990], [76, 987, 28, 990], [76, 987, 28, 990], [76, 987, 28, 990], [878, 987, 340], [878, 987, 340], [878, 987, 340], [76, 1392, 1280], [76, 1392, 1280], [76, 1392, 1280], [76, 1083, 600, 1351], [76, 1083, 600, 1351], [76, 1083, 600, 1351], [76, 1083, 600, 1351], [76, 89], [76, 89], [76, 875, 1351], [76, 875, 1351], [76, 875, 1351], [76, 600], [76, 600], [76, 543], [76, 543], [76, 1396], [76, 1396], [607, 609, 1248], [607, 609, 1248], [607, 609, 1248], [1122, 607, 609, 133], [1122, 607, 609, 133], [1122, 607, 609, 133], [1122, 607, 609, 133], [1122, 607, 609], [1122, 607, 609], [1122, 607, 609], [961, 962, 1305, 12, 133], [961, 962, 1305, 12, 133], [961, 962, 1305, 12, 133], [961, 962, 1305, 12, 133], [961, 962, 1305, 12, 133], [133, 314, 1305, 961], [133, 314, 1305, 961], [133, 314, 1305, 961], [133, 314, 1305, 961], [72, 1309, 133], [72, 1309, 133], [72, 1309, 133], [72, 1309, 133], [72, 1309, 133], [72, 1309, 133], [457, 1122, 130, 298, 855], [457, 1122, 130, 298, 855], [457, 1122, 130, 298, 855], [457, 1122, 130, 298, 855], [457, 1122, 130, 298, 855], [406, 1122, 432], [406, 1122, 432], [406, 1122, 432], [609, 1248], [609, 1248], [1101, 1102], [1101, 1102], [364, 304, 1102, 387], [364, 304, 1102, 387], [364, 304, 1102, 387], [364, 304, 1102, 387], [1426, 1102, 276, 60, 1424], [1426, 1102, 276, 60, 1424], [1426, 1102, 276, 60, 1424], [1426, 1102, 276, 60, 1424], [1426, 1102, 276, 60, 1424], [1118, 1413, 1035], [1118, 1413, 1035], [1118, 1413, 1035], [1118, 1035], [1118, 1035], [973, 1119, 974, 1160, 965], [973, 1119, 974, 1160, 965], [973, 1119, 974, 1160, 965], [973, 1119, 974, 1160, 965], [973, 1119, 974, 1160, 965], [972, 1119, 974, 973, 1163], [972, 1119, 974, 973, 1163], [972, 1119, 974, 973, 1163], [972, 1119, 974, 973, 1163], [972, 1119, 974, 973, 1163], [1233, 462, 230, 516], [1233, 462, 230, 516], [1233, 462, 230, 516], [1233, 462, 230, 516], [387, 843, 793, 716], [387, 843, 793, 716], [387, 843, 793, 716], [387, 843, 793, 716], [360, 1033], [360, 1033], [401, 360], [401, 360], [139, 861, 360], [139, 861, 360], [139, 861, 360], [364, 378, 60], [364, 378, 60], [364, 378, 60], [364, 60], [364, 60], [364, 1406, 387], [364, 1406, 387], [364, 1406, 387], [364, 13, 552, 417], [364, 13, 552, 417], [364, 13, 552, 417], [364, 13, 552, 417], [364, 146, 185, 13, 417], [364, 146, 185, 13, 417], [364, 146, 185, 13, 417], [364, 146, 185, 13, 417], [364, 146, 185, 13, 417], [364, 304, 387], [364, 304, 387], [364, 304, 387], [364, 259, 387], [364, 259, 387], [364, 259, 387], [364, 60], [364, 60], [370, 1282], [370, 1282], [370, 95], [370, 95], [1388, 544, 899, 1386], [1388, 544, 899, 1386], [1388, 544, 899, 1386], [1388, 544, 899, 1386], [756, 544, 804], [756, 544, 804], [756, 544, 804], [544, 899, 1386], [544, 899, 1386], [544, 899, 1386], [141, 308, 7, 1258, 1260], [141, 308, 7, 1258, 1260], [141, 308, 7, 1258, 1260], [141, 308, 7, 1258, 1260], [141, 308, 7, 1258, 1260], [1, 141, 7, 430], [1, 141, 7, 430], [1, 141, 7, 430], [1, 141, 7, 430], [348, 141, 345, 7], [348, 141, 345, 7], [348, 141, 345, 7], [348, 141, 345, 7], [141, 7], [141, 7], [1, 141, 7, 430], [1, 141, 7, 430], [1, 141, 7, 430], [1, 141, 7, 430], [141, 7], [141, 7], [1, 141, 4, 7, 430], [1, 141, 4, 7, 430], [1, 141, 4, 7, 430], [1, 141, 4, 7, 430], [1, 141, 4, 7, 430], [125, 270, 80, 409], [125, 270, 80, 409], [125, 270, 80, 409], [125, 270, 80, 409], [479, 270], [479, 270], [421, 655, 271], [421, 655, 271], [421, 655, 271], [527, 578, 489], [527, 578, 489], [527, 578, 489], [246, 247, 242, 88], [246, 247, 242, 88], [246, 247, 242, 88], [246, 247, 242, 88], [242, 168, 246, 170, 88], [242, 168, 246, 170, 88], [242, 168, 246, 170, 88], [242, 168, 246, 170, 88], [242, 168, 246, 170, 88], [246, 242, 88], [246, 242, 88], [246, 242, 88], [108, 109, 88], [108, 109, 88], [108, 109, 88], [668, 705], [668, 705], [668, 328], [668, 328], [669, 916, 779], [669, 916, 779], [669, 916, 779], [747, 1256], [747, 1256], [747, 553, 735, 552], [747, 553, 735, 552], [747, 553, 735, 552], [747, 553, 735, 552], [735, 171, 800, 599], [735, 171, 800, 599], [735, 171, 800, 599], [735, 171, 800, 599], [1260, 1258, 308, 347], [1260, 1258, 308, 347], [1260, 1258, 308, 347], [1260, 1258, 308, 347], [259, 1259, 800, 748], [259, 1259, 800, 748], [259, 1259, 800, 748], [259, 1259, 800, 748], [1127, 255], [1127, 255], [255, 435, 171, 272], [255, 435, 171, 272], [255, 435, 171, 272], [255, 435, 171, 272], [1404, 255], [1404, 255], [1260, 308, 255, 1312], [1260, 308, 255, 1312], [1260, 308, 255, 1312], [1260, 308, 255, 1312], [616, 255, 859], [616, 255, 859], [616, 255, 859], [255, 60, 744, 451], [255, 60, 744, 451], [255, 60, 744, 451], [255, 60, 744, 451], [566, 255], [566, 255], [327, 255, 310, 1244, 1081], [327, 255, 310, 1244, 1081], [327, 255, 310, 1244, 1081], [327, 255, 310, 1244, 1081], [327, 255, 310, 1244, 1081], [746, 452, 454], [746, 452, 454], [746, 452, 454], [1260, 1095, 387], [1260, 1095, 387], [1260, 1095, 387], [1260, 938, 387], [1260, 938, 387], [1260, 938, 387], [1260, 324], [1260, 324], [1260, 324, 555], [1260, 324, 555], [1260, 324, 555], [259, 417, 11, 185], [259, 417, 11, 185], [259, 417, 11, 185], [259, 417, 11, 185], [387, 747, 553, 552], [387, 747, 553, 552], [387, 747, 553, 552], [387, 747, 553, 552], [747, 553, 552], [747, 553, 552], [747, 553, 552], [553, 552, 748, 909], [553, 552, 748, 909], [553, 552, 748, 909], [553, 552, 748, 909], [308, 346], [308, 346], [383, 18, 1189], [383, 18, 1189], [383, 18, 1189], [383, 18, 1189], [383, 18, 1189], [383, 18, 1189], [873, 877, 880, 387], [873, 877, 880, 387], [873, 877, 880, 387], [873, 877, 880, 387], [873, 702], [873, 702], [873, 226, 702, 225], [873, 226, 702, 225], [873, 226, 702, 225], [873, 226, 702, 225], [387, 705], [387, 705], [387, 1095, 324], [387, 1095, 324], [387, 1095, 324], [387, 553], [387, 553], [387, 552], [387, 552], [1417, 880, 393], [1417, 880, 393], [1417, 880, 393], [530, 863, 531, 532, 509], [530, 863, 531, 532, 509], [530, 863, 531, 532, 509], [530, 863, 531, 532, 509], [530, 863, 531, 532, 509], [509, 680, 532], [509, 680, 532], [509, 680, 532], [863, 530, 531, 509], [863, 530, 531, 509], [863, 530, 531, 509], [863, 530, 531, 509], [530, 531, 532, 509, 541], [530, 531, 532, 509, 541], [530, 531, 532, 509, 541], [530, 531, 532, 509, 541], [530, 531, 532, 509, 541], [680, 625, 437, 509], [680, 625, 437, 509], [680, 625, 437, 509], [680, 625, 437, 509], [509, 505], [509, 505], [361, 863, 509, 1425], [361, 863, 509, 1425], [361, 863, 509, 1425], [361, 863, 509, 1425], [625, 680, 509], [625, 680, 509], [625, 680, 509], [509, 531, 505], [509, 531, 505], [509, 531, 505], [530, 863, 531, 532, 509], [530, 863, 531, 532, 509], [530, 863, 531, 532, 509], [530, 863, 531, 532, 509], [530, 863, 531, 532, 509], [106, 107, 1180], [106, 107, 1180], [106, 107, 1180], [107, 277, 26, 985], [107, 277, 26, 985], [107, 277, 26, 985], [107, 277, 26, 985], [372, 50], [372, 50], [1001, 1002, 38], [1001, 1002, 38], [1001, 1002, 38], [1001, 38], [1001, 38], [38, 493, 1002, 87, 438], [38, 493, 1002, 87, 438], [38, 493, 1002, 87, 438], [38, 493, 1002, 87, 438], [38, 493, 1002, 87, 438], [969, 38, 970], [969, 38, 970], [969, 38, 970], [38, 1062, 538, 539, 64], [38, 1062, 538, 539, 64], [38, 1062, 538, 539, 64], [38, 1062, 538, 539, 64], [38, 1062, 538, 539, 64], [282, 381], [282, 381], [282, 284, 381], [282, 284, 381], [282, 284, 381], [282, 1276], [282, 1276], [1276, 986, 284], [1276, 986, 284], [1276, 986, 284], [1276, 284], [1276, 284], [1057, 1276, 986, 284], [1057, 1276, 986, 284], [1057, 1276, 986, 284], [1057, 1276, 986, 284], [1029, 284], [1029, 284], [148, 21], [148, 21], [901, 453, 413, 561, 798], [901, 453, 413, 561, 798], [901, 453, 413, 561, 798], [901, 453, 413, 561, 798], [901, 453, 413, 561, 798], [561, 663], [561, 663], [561, 413], [561, 413], [570, 565], [570, 565], [27, 22, 570], [27, 22, 570], [27, 22, 570], [819, 821, 822, 818], [819, 821, 822, 818], [819, 821, 822, 818], [819, 821, 822, 818], [819, 821, 832, 818], [819, 821, 832, 818], [819, 821, 832, 818], [819, 821, 832, 818], [821, 715, 818], [821, 715, 818], [821, 715, 818], [819, 821, 715, 818], [819, 821, 715, 818], [819, 821, 715, 818], [819, 821, 715, 818], [819, 821, 818], [819, 821, 818], [819, 821, 818], [819, 821, 818], [819, 821, 818], [819, 821, 818], [819, 821, 832], [819, 821, 832], [819, 821, 832], [1382, 895, 1387, 902], [1382, 895, 1387, 902], [1382, 895, 1387, 902], [1382, 895, 1387, 902], [860, 895, 605, 173, 647], [860, 895, 605, 173, 647], [860, 895, 605, 173, 647], [860, 895, 605, 173, 647], [860, 895, 605, 173, 647], [139, 895, 542], [139, 895, 542], [139, 895, 542], [139, 895, 499], [139, 895, 499], [139, 895, 499], [860, 895, 902, 173, 647], [860, 895, 902, 173, 647], [860, 895, 902, 173, 647], [860, 895, 902, 173, 647], [860, 895, 902, 173, 647], [762, 772, 341], [762, 772, 341], [762, 772, 341], [1095, 60], [1095, 60], [907, 60], [907, 60], [13, 60], [13, 60], [61, 60], [61, 60], [312, 60], [312, 60], [300, 497], [300, 497], [300, 497, 789], [300, 497, 789], [300, 497, 789], [441, 368, 1360], [441, 368, 1360], [441, 368, 1360], [1360, 441, 722, 833], [1360, 441, 722, 833], [1360, 441, 722, 833], [1360, 441, 722, 833], [235, 722, 723, 833, 1360], [235, 722, 723, 833, 1360], [235, 722, 723, 833, 1360], [235, 722, 723, 833, 1360], [235, 722, 723, 833, 1360], [1360, 235, 722, 833], [1360, 235, 722, 833], [1360, 235, 722, 833], [1360, 235, 722, 833], [441, 142, 833], [441, 142, 833], [441, 142, 833], [1121, 441, 1153], [1121, 441, 1153], [1121, 441, 1153], [415, 702, 196], [415, 702, 196], [415, 702, 196], [1235, 228], [1235, 228], [226, 252, 223], [226, 252, 223], [226, 252, 223], [228, 225], [228, 225], [1018, 225], [1018, 225], [1356, 1026, 225], [1356, 1026, 225], [1356, 1026, 225], [1160, 261, 940, 226, 228], [1160, 261, 940, 226, 228], [1160, 261, 940, 226, 228], [1160, 261, 940, 226, 228], [1160, 261, 940, 226, 228], [110, 31, 228, 115], [110, 31, 228, 115], [110, 31, 228, 115], [110, 31, 228, 115], [797, 228, 572], [797, 228, 572], [797, 228, 572], [110, 31, 228, 115], [110, 31, 228, 115], [110, 31, 228, 115], [110, 31, 228, 115], [506, 22], [506, 22], [506, 22, 1150, 936, 502], [506, 22, 1150, 936, 502], [506, 22, 1150, 936, 502], [506, 22, 1150, 936, 502], [506, 22, 1150, 936, 502], [22, 69], [22, 69], [1150, 936, 939, 22], [1150, 936, 939, 22], [1150, 936, 939, 22], [1150, 936, 939, 22], [22, 1074, 939], [22, 1074, 939], [22, 1074, 939], [506, 22, 78, 936, 939], [506, 22, 78, 936, 939], [506, 22, 78, 936, 939], [506, 22, 78, 936, 939], [506, 22, 78, 936, 939], [506, 22, 753], [506, 22, 753], [506, 22, 753], [22, 434], [22, 434], [506, 22, 78, 1150, 936], [506, 22, 78, 1150, 936], [506, 22, 78, 1150, 936], [506, 22, 78, 1150, 936], [506, 22, 78, 1150, 936], [27, 1255], [27, 1255], [27, 1255], [27, 1255], [278, 1151], [278, 1151], [278, 280], [278, 280], [278, 280, 418], [278, 280, 418], [278, 280, 418], [1292, 280, 449, 691], [1292, 280, 449, 691], [1292, 280, 449, 691], [1292, 280, 449, 691], [1165, 1166], [1165, 1166], [1166, 399], [1166, 399], [234, 373], [234, 373], [327, 1314], [327, 1314], [598, 1314, 592], [598, 1314, 592], [598, 1314, 592], [591, 1314, 598], [591, 1314, 598], [591, 1314, 598], [357, 327, 251, 1315], [357, 327, 251, 1315], [357, 327, 251, 1315], [357, 327, 251, 1315], [310, 705, 309, 34], [310, 705, 309, 34], [310, 705, 309, 34], [310, 705, 309, 34], [327, 310], [327, 310], [404, 365, 310], [404, 365, 310], [404, 365, 310], [310, 239], [310, 239], [310, 327, 444, 181], [310, 327, 444, 181], [310, 327, 444, 181], [310, 327, 444, 181], [310, 846], [310, 846], [482, 365], [482, 365], [365, 366, 864], [365, 366, 864], [365, 366, 864], [375, 868], [375, 868], [827, 868], [827, 868], [1391, 1047, 1205, 1216, 697], [1391, 1047, 1205, 1216, 697], [1391, 1047, 1205, 1216, 697], [1391, 1047, 1205, 1216, 697], [1391, 1047, 1205, 1216, 697], [1041, 1047], [1041, 1047], [676, 1047], [676, 1047], [640, 94, 1336, 655], [640, 94, 1336, 655], [640, 94, 1336, 655], [640, 94, 1336, 655], [927, 421, 655, 852], [927, 421, 655, 852], [927, 421, 655, 852], [927, 421, 655, 852], [929, 422, 933, 425, 393], [929, 422, 933, 425, 393], [929, 422, 933, 425, 393], [929, 422, 933, 425, 393], [929, 422, 933, 425, 393], [425, 422], [425, 422], [425, 72, 299], [425, 72, 299], [425, 72, 299], [425, 393], [425, 393], [425, 393], [425, 393], [425, 1048, 140, 872], [425, 1048, 140, 872], [425, 1048, 140, 872], [425, 1048, 140, 872], [425, 1021, 1022], [425, 1021, 1022], [425, 1021, 1022], [425, 1368], [425, 1368], [312, 87, 120, 438], [312, 87, 120, 438], [312, 87, 120, 438], [312, 87, 120, 438], [736, 613, 1433], [736, 613, 1433], [736, 613, 1433], [736, 301], [736, 301], [736, 737, 742, 740], [736, 737, 742, 740], [736, 737, 742, 740], [736, 737, 742, 740], [737, 576, 577], [737, 576, 577], [737, 576, 577], [737, 1433, 301, 613], [737, 1433, 301, 613], [737, 1433, 301, 613], [737, 1433, 301, 613], [740, 355], [740, 355], [756, 455, 897, 742, 297], [756, 455, 897, 742, 297], [756, 455, 897, 742, 297], [756, 455, 897, 742, 297], [756, 455, 897, 742, 297], [742, 623, 577], [742, 623, 577], [742, 623, 577], [613, 742, 623], [613, 742, 623], [613, 742, 623], [333, 742], [333, 742], [918, 742, 915], [918, 742, 915], [918, 742, 915], [742, 915], [742, 915], [47, 849], [47, 849], [875, 57, 1421, 1077], [875, 57, 1421, 1077], [875, 57, 1421, 1077], [875, 57, 1421, 1077], [1033, 57], [1033, 57], [57, 1013, 899, 1421, 875], [57, 1013, 899, 1421, 875], [57, 1013, 899, 1421, 875], [57, 1013, 899, 1421, 875], [57, 1013, 899, 1421, 875], [552, 171, 1406], [552, 171, 1406], [552, 171, 1406], [1005, 552, 553], [1005, 552, 553], [1005, 552, 553], [1221, 556], [1221, 556], [274, 1273, 997, 1033, 1359], [274, 1273, 997, 1033, 1359], [274, 1273, 997, 1033, 1359], [274, 1273, 997, 1033, 1359], [274, 1273, 997, 1033, 1359], [1362, 274, 997], [1362, 274, 997], [1362, 274, 997], [409, 1033, 274, 1273], [409, 1033, 274, 1273], [409, 1033, 274, 1273], [409, 1033, 274, 1273], [274, 1273, 409, 1033, 833], [274, 1273, 409, 1033, 833], [274, 1273, 409, 1033, 833], [274, 1273, 409, 1033, 833], [274, 1273, 409, 1033, 833], [1188, 1184], [1188, 1184], [1188, 549, 1184], [1188, 549, 1184], [1188, 549, 1184], [1188, 1184], [1188, 1184], [72, 1184], [72, 1184], [1184, 1158, 485], [1184, 1158, 485], [1184, 1158, 485], [1188, 1184], [1188, 1184], [159, 275, 14], [159, 275, 14], [159, 275, 14], [275, 14], [275, 14], [275, 14], [275, 14], [1388, 899, 1386], [1388, 899, 1386], [1388, 899, 1386], [875, 1061, 1421, 899], [875, 1061, 1421, 899], [875, 1061, 1421, 899], [875, 1061, 1421, 899], [899, 875, 1061, 1013], [899, 875, 1061, 1013], [899, 875, 1061, 1013], [899, 875, 1061, 1013], [899, 983, 486], [899, 983, 486], [899, 983, 486], [1421, 899, 563], [1421, 899, 563], [1421, 899, 563], [899, 1295], [899, 1295], [121, 436, 1020], [121, 436, 1020], [121, 436, 1020], [1110, 1107, 1100, 1108], [1110, 1107, 1100, 1108], [1110, 1107, 1100, 1108], [1110, 1107, 1100, 1108], [484, 597, 1232], [484, 597, 1232], [484, 597, 1232], [1082, 281, 262, 189, 414], [1082, 281, 262, 189, 414], [1082, 281, 262, 189, 414], [1082, 281, 262, 189, 414], [1082, 281, 262, 189, 414], [1133, 265], [1133, 265], [281, 186, 265, 189, 323], [281, 186, 265, 189, 323], [281, 186, 265, 189, 323], [281, 186, 265, 189, 323], [281, 186, 265, 189, 323], [1133, 1070, 149, 265, 189], [1133, 1070, 149, 265, 189], [1133, 1070, 149, 265, 189], [1133, 1070, 149, 265, 189], [1133, 1070, 149, 265, 189], [189, 186, 265], [189, 186, 265], [189, 186, 265], [265, 189], [265, 189], [312, 788, 87], [312, 788, 87], [312, 788, 87], [312, 1238], [312, 1238], [388, 113], [388, 113], [1099, 113], [1099, 113], [113, 1181], [113, 1181], [1246, 705], [1246, 705], [1246, 705], [1246, 705], [705, 379, 1295], [705, 379, 1295], [705, 379, 1295], [578, 1434], [578, 1434], [182, 1434, 813], [182, 1434, 813], [182, 1434, 813], [976, 1230], [976, 1230], [733, 87], [733, 87], [87, 7], [87, 7], [1375, 56, 87], [1375, 56, 87], [1375, 56, 87], [788, 87], [788, 87], [1298, 87], [1298, 87], [87, 593], [87, 593], [788, 421, 87, 132], [788, 421, 87, 132], [788, 421, 87, 132], [788, 421, 87, 132], [1104, 87], [1104, 87], [52, 87], [52, 87], [1345, 87], [1345, 87], [90, 1010, 46], [90, 1010, 46], [90, 1010, 46], [90, 46, 120], [90, 46, 120], [90, 46, 120], [90, 1350, 68, 1086], [90, 1350, 68, 1086], [90, 1350, 68, 1086], [90, 1350, 68, 1086], [582, 701, 341], [582, 701, 341], [582, 701, 341], [701, 295], [701, 295], [593, 522, 1158], [593, 522, 1158], [593, 522, 1158], [605, 563], [605, 563], [605, 1344], [605, 1344], [605, 182], [605, 182], [610, 1144], [610, 1144], [1079, 1164], [1079, 1164], [1079, 1164], [1079, 1164], [567, 454, 599], [567, 454, 599], [567, 454, 599], [352, 377], [352, 377], [354, 356], [354, 356], [860, 354], [860, 354], [1351, 89], [1351, 89], [927, 386], [927, 386], [927, 421], [927, 421], [927, 421, 458, 852], [927, 421, 458, 852], [927, 421, 458, 852], [927, 421, 458, 852], [927, 421], [927, 421], [1410, 1298, 1300], [1410, 1298, 1300], [1410, 1298, 1300], [361, 437, 863, 1425], [361, 437, 863, 1425], [361, 437, 863, 1425], [361, 437, 863, 1425], [361, 403], [361, 403], [231, 1419, 924, 646], [231, 1419, 924, 646], [231, 1419, 924, 646], [231, 1419, 924, 646], [1430, 1058, 347], [1430, 1058, 347], [1430, 1058, 347], [382, 874, 1058], [382, 874, 1058], [382, 874, 1058], [1234, 1058, 1161, 1186], [1234, 1058, 1161, 1186], [1234, 1058, 1161, 1186], [1234, 1058, 1161, 1186], [1125, 1244], [1125, 1244], [616, 858], [616, 858], [1261, 103], [1261, 103], [327, 102, 178, 183], [327, 102, 178, 183], [327, 102, 178, 183], [327, 102, 178, 183], [196, 81], [196, 81], [196, 197], [196, 197], [196, 1069, 940], [196, 1069, 940], [196, 1069, 940], [874, 794], [874, 794], [1141, 123], [1141, 123], [875, 714, 795, 773], [875, 714, 795, 773], [875, 714, 795, 773], [875, 714, 795, 773], [147, 345, 7], [147, 345, 7], [147, 345, 7], [166, 7], [166, 7], [555, 324, 7], [555, 324, 7], [555, 324, 7], [136, 608, 7], [136, 608, 7], [136, 608, 7], [136, 608, 7], [136, 608, 7], [136, 608, 7], [136, 608, 7], [136, 608, 7], [136, 608, 7], [1330, 1129, 7], [1330, 1129, 7], [1330, 1129, 7], [1083, 1033, 1034, 598], [1083, 1033, 1034, 598], [1083, 1033, 1034, 598], [1083, 1033, 1034, 598], [869, 1034, 598], [869, 1034, 598], [869, 1034, 598], [1334, 1035, 1270], [1334, 1035, 1270], [1334, 1035, 1270], [1035, 1413, 1263], [1035, 1413, 1263], [1035, 1413, 1263], [1413, 1035], [1413, 1035], [1413, 1035], [1413, 1035], [1334, 1035, 1263], [1334, 1035, 1263], [1334, 1035, 1263], [677, 220], [677, 220], [293, 771, 391, 775, 776], [293, 771, 391, 775, 776], [293, 771, 391, 775, 776], [293, 771, 391, 775, 776], [293, 771, 391, 775, 776], [91, 775, 443, 846], [91, 775, 443, 846], [91, 775, 443, 846], [91, 775, 443, 846], [1330, 345, 147, 1019, 1129], [1330, 345, 147, 1019, 1129], [1330, 345, 147, 1019, 1129], [1330, 345, 147, 1019, 1129], [1330, 345, 147, 1019, 1129], [644, 8], [644, 8], [8, 985], [8, 985], [72, 1152, 8], [72, 1152, 8], [72, 1152, 8], [1096, 8], [1096, 8], [578, 717], [578, 717], [578, 944], [578, 944], [991, 1336, 324], [991, 1336, 324], [991, 1336, 324], [1160, 974, 1163], [1160, 974, 1163], [1160, 974, 1163], [1160, 1163], [1160, 1163], [972, 1160, 973, 1163], [972, 1160, 973, 1163], [972, 1160, 973, 1163], [972, 1160, 973, 1163], [1213, 1207, 711], [1213, 1207, 711], [1213, 1207, 711], [462, 720, 619], [462, 720, 619], [462, 720, 619], [462, 885], [462, 885], [462, 885], [462, 885], [462, 250, 720], [462, 250, 720], [462, 250, 720], [462, 720, 619], [462, 720, 619], [462, 720, 619], [462, 463, 464, 749, 734], [462, 463, 464, 749, 734], [462, 463, 464, 749, 734], [462, 463, 464, 749, 734], [462, 463, 464, 749, 734], [462, 749, 734, 464], [462, 749, 734, 464], [462, 749, 734, 464], [462, 749, 734, 464], [594, 462, 720, 619], [594, 462, 720, 619], [594, 462, 720, 619], [594, 462, 720, 619], [462, 619], [462, 619], [462, 463, 720, 619], [462, 463, 720, 619], [462, 463, 720, 619], [462, 463, 720, 619], [462, 463, 464], [462, 463, 464], [462, 463, 464], [543, 1420, 89], [543, 1420, 89], [543, 1420, 89], [72, 768], [72, 768], [72, 299, 1152], [72, 299, 1152], [72, 299, 1152], [1391, 72, 632, 298, 299], [1391, 72, 632, 298, 299], [1391, 72, 632, 298, 299], [1391, 72, 632, 298, 299], [1391, 72, 632, 298, 299], [1236, 1257], [1236, 1257], [896, 301, 385, 577], [896, 301, 385, 577], [896, 301, 385, 577], [896, 301, 385, 577], [34, 183, 180, 181], [34, 183, 180, 181], [34, 183, 180, 181], [34, 183, 180, 181], [1391, 495], [1391, 495], [495, 769, 1279, 1284], [495, 769, 1279, 1284], [495, 769, 1279, 1284], [495, 769, 1279, 1284], [495, 1279, 611], [495, 1279, 611], [495, 1279, 611], [1139, 581, 495], [1139, 581, 495], [1139, 581, 495], [1065, 299], [1065, 299], [608, 794], [608, 794], [136, 3], [136, 3], [136, 58, 59], [136, 58, 59], [136, 58, 59], [136, 930, 872], [136, 930, 872], [136, 930, 872], [1292, 449], [1292, 449], [641, 1251], [641, 1251], [691, 1251], [691, 1251], [691, 1251, 1185], [691, 1251, 1185], [691, 1251, 1185], [1251, 691, 955], [1251, 691, 955], [1251, 691, 955], [768, 658, 231], [768, 658, 231], [768, 658, 231], [406, 104, 1128, 231], [406, 104, 1128, 231], [406, 104, 1128, 231], [406, 104, 1128, 231], [1248, 231], [1248, 231], [646, 213, 231], [646, 213, 231], [646, 213, 231], [993, 996, 29, 1248, 231], [993, 996, 29, 1248, 231], [993, 996, 29, 1248, 231], [993, 996, 29, 1248, 231], [993, 996, 29, 1248, 231], [994, 231, 1248, 993], [994, 231, 1248, 993], [994, 231, 1248, 993], [994, 231, 1248, 993], [1248, 231], [1248, 231], [1248, 231], [1248, 231], [122, 231], [122, 231], [410, 434], [410, 434], [91, 434, 201, 682], [91, 434, 201, 682], [91, 434, 201, 682], [91, 434, 201, 682], [1367, 434, 1423], [1367, 434, 1423], [1367, 434, 1423], [896, 629, 903, 577], [896, 629, 903, 577], [896, 629, 903, 577], [896, 629, 903, 577], [860, 917, 919, 173, 647], [860, 917, 919, 173, 647], [860, 917, 919, 173, 647], [860, 917, 919, 173, 647], [860, 917, 919, 173, 647], [557, 860, 558], [557, 860, 558], [557, 860, 558], [860, 558, 865], [860, 558, 865], [860, 558, 865], [860, 558, 865], [860, 558, 865], [860, 558, 865], [472, 16, 18, 872], [472, 16, 18, 872], [472, 16, 18, 872], [472, 16, 18, 872], [579, 930], [579, 930], [1155, 833], [1155, 833], [722, 833], [722, 833], [142, 1028, 1121, 1153, 833], [142, 1028, 1121, 1153, 833], [142, 1028, 1121, 1153, 833], [142, 1028, 1121, 1153, 833], [142, 1028, 1121, 1153, 833], [1121, 833], [1121, 833], [512, 508, 1028, 833], [512, 508, 1028, 833], [512, 508, 1028, 833], [512, 508, 1028, 833], [512, 1028, 833], [512, 1028, 833], [512, 1028, 833], [723, 1153, 833], [723, 1153, 833], [723, 1153, 833], [142, 723, 219, 833, 134], [142, 723, 219, 833, 134], [142, 723, 219, 833, 134], [142, 723, 219, 833, 134], [142, 723, 219, 833, 134], [1016, 1017], [1016, 1017], [1334, 1197, 1263], [1334, 1197, 1263], [1334, 1197, 1263], [1381, 408], [1381, 408], [849, 1381], [849, 1381], [923, 299, 122], [923, 299, 122], [923, 299, 122], [923, 122], [923, 122], [1099, 1087, 1249], [1099, 1087, 1249], [1099, 1087, 1249], [1142, 214], [1142, 214], [82, 104], [82, 104], [482, 566, 871, 568], [482, 566, 871, 568], [482, 566, 871, 568], [482, 566, 871, 568], [482, 533], [482, 533], [482, 533], [482, 533], [299, 163], [299, 163], [406, 299, 910], [406, 299, 910], [406, 299, 910], [875, 710], [875, 710], [834, 875, 203], [834, 875, 203], [834, 875, 203], [931, 846], [931, 846], [1317, 1198, 498], [1317, 1198, 498], [1317, 1198, 498], [457, 855, 130, 781], [457, 855, 130, 781], [457, 855, 130, 781], [457, 855, 130, 781], [9, 488], [9, 488], [58, 59], [58, 59], [217, 587, 58, 351, 1227], [217, 587, 58, 351, 1227], [217, 587, 58, 351, 1227], [217, 587, 58, 351, 1227], [217, 587, 58, 351, 1227], [521, 681], [521, 681], [1028, 723], [1028, 723], [205, 206, 207], [205, 206, 207], [205, 206, 207], [1131, 206], [1131, 206], [1359, 527, 369], [1359, 527, 369], [1359, 527, 369], [646, 1179, 1377, 667], [646, 1179, 1377, 667], [646, 1179, 1377, 667], [646, 1179, 1377, 667], [646, 549, 1377, 667], [646, 549, 1377, 667], [646, 549, 1377, 667], [646, 549, 1377, 667], [646, 697], [646, 697], [951, 646], [951, 646], [950, 646, 549], [950, 646, 549], [950, 646, 549], [549, 1377, 1319, 667, 646], [549, 1377, 1319, 667, 646], [549, 1377, 1319, 667, 646], [549, 1377, 1319, 667, 646], [549, 1377, 1319, 667, 646], [646, 549, 814], [646, 549, 814], [646, 549, 814], [549, 1179, 1377, 667, 646], [549, 1179, 1377, 667, 646], [549, 1179, 1377, 667, 646], [549, 1179, 1377, 667, 646], [549, 1179, 1377, 667, 646], [1398, 1158], [1398, 1158], [684, 1210, 1158, 1036], [684, 1210, 1158, 1036], [684, 1210, 1158, 1036], [684, 1210, 1158, 1036], [926, 1053, 1158], [926, 1053, 1158], [926, 1053, 1158], [684, 1210, 1158, 1036], [684, 1210, 1158, 1036], [684, 1210, 1158, 1036], [684, 1210, 1158, 1036], [1272, 1210, 1143, 1036, 1158], [1272, 1210, 1143, 1036, 1158], [1272, 1210, 1143, 1036, 1158], [1272, 1210, 1143, 1036, 1158], [1272, 1210, 1143, 1036, 1158], [648, 103, 1158], [648, 103, 1158], [648, 103, 1158], [648, 971, 1143], [648, 971, 1143], [648, 971, 1143], [648, 522, 536, 29, 971], [648, 522, 536, 29, 971], [648, 522, 536, 29, 971], [648, 522, 536, 29, 971], [648, 522, 536, 29, 971], [648, 522, 1036], [648, 522, 1036], [648, 522, 1036], [530, 505], [530, 505], [827, 437], [827, 437], [975, 1248], [975, 1248], [636, 1243], [636, 1243], [1171, 1146, 1168], [1171, 1146, 1168], [1171, 1146, 1168], [1167, 1171, 1146, 1168], [1167, 1171, 1146, 1168], [1167, 1171, 1146, 1168], [1167, 1171, 1146, 1168], [135, 50], [135, 50], [1138, 1241], [1138, 1241], [1138, 1140, 1141], [1138, 1140, 1141], [1138, 1140, 1141], [688, 945], [688, 945], [494, 830], [494, 830], [63, 100, 1208], [63, 100, 1208], [63, 100, 1208], [645, 888], [645, 888], [1299, 621], [1299, 621], [1299, 1239], [1299, 1239], [1299, 765, 1152], [1299, 765, 1152], [1299, 765, 1152], [878, 1246], [878, 1246], [878, 388], [878, 388], [305, 1354], [305, 1354], [879, 186], [879, 186], [186, 723, 535], [186, 723, 535], [186, 723, 535], [414, 1070, 189], [414, 1070, 189], [414, 1070, 189], [1070, 154, 189], [1070, 154, 189], [1070, 154, 189], [414, 154, 189], [414, 154, 189], [414, 154, 189], [1121, 219, 723, 189, 1153], [1121, 219, 723, 189, 1153], [1121, 219, 723, 189, 1153], [1121, 219, 723, 189, 1153], [1121, 219, 723, 189, 1153], [442, 568, 533], [442, 568, 533], [442, 568, 533], [1202, 1203, 1205, 1208, 1195], [1202, 1203, 1205, 1208, 1195], [1202, 1203, 1205, 1208, 1195], [1202, 1203, 1205, 1208, 1195], [1202, 1203, 1205, 1208, 1195], [991, 166, 1338], [991, 166, 1338], [991, 166, 1338], [1007, 1338], [1007, 1338], [1291, 1338], [1291, 1338], [1145, 1338], [1145, 1338], [1291, 1076, 1338], [1291, 1076, 1338], [1291, 1076, 1338], [834, 837], [834, 837], [395, 1394], [395, 1394], [890, 395], [890, 395], [18, 579], [18, 579], [166, 324], [166, 324], [905, 560], [905, 560], [731, 905], [731, 905], [445, 515, 905, 560], [445, 515, 905, 560], [445, 515, 905, 560], [445, 515, 905, 560], [455, 897], [455, 897], [906, 64], [906, 64], [1368, 981], [1368, 981], [810, 556], [810, 556], [723, 1154], [723, 1154], [954, 1112, 723], [954, 1112, 723], [954, 1112, 723], [1121, 723, 219], [1121, 723, 219], [1121, 723, 219], [134, 1303, 723], [134, 1303, 723], [134, 1303, 723], [367, 1358], [367, 1358], [153, 323, 155], [153, 323, 155], [153, 323, 155], [820, 487], [820, 487], [514, 56, 1375], [514, 56, 1375], [514, 56, 1375], [963, 959, 960], [963, 959, 960], [963, 959, 960], [15, 14], [15, 14], [253, 399, 801], [253, 399, 801], [253, 399, 801], [173, 674, 195], [173, 674, 195], [173, 674, 195], [1159, 913], [1159, 913], [691, 1130], [691, 1130], [1031, 351, 691, 1226, 1130], [1031, 351, 691, 1226, 1130], [1031, 351, 691, 1226, 1130], [1031, 351, 691, 1226, 1130], [1031, 351, 691, 1226, 1130], [912, 409], [912, 409], [886, 409], [886, 409], [1087, 409], [1087, 409], [468, 469], [468, 469], [926, 967], [926, 967], [972, 974, 967], [972, 974, 967], [972, 974, 967], [1373, 249], [1373, 249], [850, 846], [850, 846], [391, 846], [391, 846], [453, 620], [453, 620], [453, 790], [453, 790], [453, 638], [453, 638], [453, 103, 104], [453, 103, 104], [453, 103, 104], [453, 632, 589, 920], [453, 632, 589, 920], [453, 632, 589, 920], [453, 632, 589, 920], [947, 453, 803, 802], [947, 453, 803, 802], [947, 453, 803, 802], [947, 453, 803, 802], [453, 1067, 103], [453, 1067, 103], [453, 1067, 103], [483, 806], [483, 806], [1366, 579], [1366, 579], [727, 243], [727, 243], [628, 679], [628, 679], [843, 1304], [843, 1304], [976, 1082], [976, 1082], [277, 138, 985], [277, 138, 985], [277, 138, 985], [277, 128], [277, 128], [277, 138], [277, 138], [277, 167, 128], [277, 167, 128], [277, 167, 128], [522, 1377, 1286, 1380, 1247], [522, 1377, 1286, 1380, 1247], [522, 1377, 1286, 1380, 1247], [522, 1377, 1286, 1380, 1247], [522, 1377, 1286, 1380, 1247], [511, 1026, 1096], [511, 1026, 1096], [511, 1026, 1096], [926, 1247], [926, 1247], [926, 1380, 1377, 893], [926, 1380, 1377, 893], [926, 1380, 1377, 893], [926, 1380, 1377, 893], [1412, 427], [1412, 427], [232, 77, 427, 936, 939], [232, 77, 427, 936, 939], [232, 77, 427, 936, 939], [232, 77, 427, 936, 939], [232, 77, 427, 936, 939], [1064, 1026, 1389, 547], [1064, 1026, 1389, 547], [1064, 1026, 1389, 547], [1064, 1026, 1389, 547], [511, 630, 892], [511, 630, 892], [511, 630, 892], [1031, 178, 180], [1031, 178, 180], [1031, 178, 180], [1109, 549, 667], [1109, 549, 667], [1109, 549, 667], [178, 181], [178, 181], [178, 217, 183], [178, 217, 183], [178, 217, 183], [1226, 181], [1226, 181], [526, 1052, 183, 351], [526, 1052, 183, 351], [526, 1052, 183, 351], [526, 1052, 183, 351], [548, 456], [548, 456], [1249, 1414, 296, 1383, 1313], [1249, 1414, 296, 1383, 1313], [1249, 1414, 296, 1383, 1313], [1249, 1414, 296, 1383, 1313], [1249, 1414, 296, 1383, 1313], [66, 1313], [66, 1313], [143, 393], [143, 393], [979, 980], [979, 980], [1044, 77, 78, 1048, 1054], [1044, 77, 78, 1048, 1054], [1044, 77, 78, 1048, 1054], [1044, 77, 78, 1048, 1054], [1044, 77, 78, 1048, 1054], [77, 78, 1337, 936, 1038], [77, 78, 1337, 936, 1038], [77, 78, 1337, 936, 1038], [77, 78, 1337, 936, 1038], [77, 78, 1337, 936, 1038], [517, 518], [517, 518], [517, 518], [517, 518], [55, 433], [55, 433], [55, 145, 1067], [55, 145, 1067], [55, 145, 1067], [906, 1414], [906, 1414], [710, 911], [710, 911], [91, 560], [91, 560], [1306, 91, 1223], [1306, 91, 1223], [1306, 91, 1223], [947, 802, 213], [947, 802, 213], [947, 802, 213], [952, 621], [952, 621], [1172, 768, 664], [1172, 768, 664], [1172, 768, 664], [1172, 768, 664], [1172, 768, 664], [1172, 768, 664], [212, 431], [212, 431], [944, 384], [944, 384], [1342, 1325], [1342, 1325], [586, 643], [586, 643], [1374, 54], [1374, 54], [54, 1423], [54, 1423], [445, 54, 887, 884], [445, 54, 887, 884], [445, 54, 887, 884], [445, 54, 887, 884], [942, 54, 943], [942, 54, 943], [942, 54, 943], [54, 943], [54, 943], [503, 54, 201], [503, 54, 201], [503, 54, 201], [1193, 1211], [1193, 1211], [1193, 1211], [1193, 1211], [193, 204, 45, 81], [193, 204, 45, 81], [193, 204, 45, 81], [193, 204, 45, 81], [188, 187, 405], [188, 187, 405], [188, 187, 405], [187, 405], [187, 405], [187, 188], [187, 188], [157, 1069], [157, 1069], [1218, 1215], [1218, 1215], [612, 1215], [612, 1215], [991, 166], [991, 166], [31, 459, 966], [31, 459, 966], [31, 459, 966], [31, 321], [31, 321], [639, 295], [639, 295], [152, 20], [152, 20], [641, 708], [641, 708], [288, 351], [288, 351], [1303, 954], [1303, 954], [246, 242], [246, 242], [246, 168], [246, 168], [1129, 147], [1129, 147], [85, 761], [85, 761], [1223, 940], [1223, 940], [829, 198], [829, 198], [529, 679, 467], [529, 679, 467], [529, 679, 467], [679, 635], [679, 635], [679, 467], [679, 467], [1106, 103, 765, 432], [1106, 103, 765, 432], [1106, 103, 765, 432], [1106, 103, 765, 432], [937, 477], [937, 477], [233, 286, 1243], [233, 286, 1243], [233, 286, 1243], [193, 81], [193, 81], [571, 838], [571, 838], [829, 490], [829, 490], [829, 581], [829, 581], [339, 1069], [339, 1069], [1171, 1167], [1171, 1167], [1418, 1167, 1168], [1418, 1167, 1168], [1418, 1167, 1168], [1171, 1168], [1171, 1168], [1171, 1168], [1171, 1168], [841, 1417, 911], [841, 1417, 911], [841, 1417, 911], [424, 423], [424, 423], [794, 825], [794, 825], [624, 876], [624, 876], [624, 862], [624, 862], [151, 847], [151, 847], [1046, 1136], [1046, 1136], [644, 1015], [644, 1015], [554, 760], [554, 760], [118, 1286], [118, 1286], [662, 663, 407], [662, 663, 407], [662, 663, 407], [662, 663, 407, 763], [662, 663, 407, 763], [662, 663, 407, 763], [662, 663, 407, 763], [663, 836], [663, 836], [663, 407, 975], [663, 407, 975], [663, 407, 975], [887, 883], [887, 883], [1109, 667], [1109, 667], [1182, 971, 999, 998], [1182, 971, 999, 998], [1182, 971, 999, 998], [1182, 971, 999, 998], [1182, 971, 999, 998], [1182, 971, 999, 998], [1182, 971, 999, 998], [1182, 971, 999, 998], [1319, 1400], [1319, 1400], [1399, 1319], [1399, 1319], [686, 687], [686, 687], [1275, 1143, 1176], [1275, 1143, 1176], [1275, 1143, 1176], [1272, 1176], [1272, 1176], [1431, 1432], [1431, 1432], [1431, 956], [1431, 956], [1431, 1432], [1431, 1432], [514, 1269], [514, 1269], [514, 1268], [514, 1268], [500, 266, 753], [500, 266, 753], [500, 266, 753], [500, 936, 939], [500, 936, 939], [500, 936, 939], [1090, 279, 266], [1090, 279, 266], [1090, 279, 266], [1150, 1183, 1337, 1240], [1150, 1183, 1337, 1240], [1150, 1183, 1337, 1240], [1150, 1183, 1337, 1240], [1150, 936], [1150, 936], [1150, 936], [1150, 936], [1150, 936, 1337], [1150, 936, 1337], [1150, 936, 1337], [1150, 1183, 1088], [1150, 1183, 1088], [1150, 1183, 1088], [1078, 1150, 1183], [1078, 1150, 1183], [1078, 1150, 1183], [1200, 1212], [1200, 1212], [936, 1262], [936, 1262], [936, 1337], [936, 1337], [936, 939, 1337], [936, 939, 1337], [936, 939, 1337], [936, 1262], [936, 1262], [957, 1191, 1192, 69], [957, 1191, 1192, 69], [957, 1191, 1192, 69], [957, 1191, 1192, 69], [279, 266], [279, 266], [266, 1240], [266, 1240], [1032, 62], [1032, 62], [473, 975], [473, 975], [337, 335], [337, 335], [337, 823], [337, 823], [337, 823], [337, 823], [1075, 1073], [1075, 1073], [209, 1268, 1269, 1196], [209, 1268, 1269, 1196], [209, 1268, 1269, 1196], [209, 1268, 1269, 1196], [754, 1268, 1269], [754, 1268, 1269], [754, 1268, 1269], [1363, 719, 1310], [1363, 719, 1310], [1363, 719, 1310], [968, 978], [968, 978], [513, 847], [513, 847], [321, 1428], [321, 1428], [321, 1329], [321, 1329], [958, 321, 1329], [958, 321, 1329], [958, 321, 1329], [958, 10, 1329], [958, 10, 1329], [958, 10, 1329], [908, 199, 1190, 1120], [908, 199, 1190, 1120], [908, 199, 1190, 1120], [908, 199, 1190, 1120], [199, 1190], [199, 1190], [805, 808], [805, 808], [1021, 1022, 1023], [1021, 1022, 1023], [1021, 1022, 1023], [1021, 1022, 1023], [1021, 1022, 1023], [1021, 1022, 1023]]
[ Info: Preparing Input
[ Info: Preparing Neighborhoods
[ Info: Preparing Cost Matrix
[ Info: Preparing Dispersion
[ Info: Computing Dispersions
[ Info: Computing Directional Curvature
[ Info: Computing Node Curvature Neighborhood
[ Info: Computing Edge Curvature
[ Info: Computing Node Curvature Edges
[ Info: Computing Edge Curvature
[ Info: Computing Node Curvature Edges
[ Info: Converting Curvatures to JSON
[ Info: Writing JSON to /Users/pellegrinraphael/Desktop/Repos_GNN/Hypergraph_Encodings/src/encodings_hnns/results.alpha-0.dispersion-UnweightedClique.orc.json
['/Users/pellegrinraphael/Desktop/Repos_GNN/Hypergraph_Encodings/scripts', '/Users/pellegrinraphael/anaconda3/envs/our_repo_2/lib/python311.zip', '/Users/pellegrinraphael/anaconda3/envs/our_repo_2/lib/python3.11', '/Users/pellegrinraphael/anaconda3/envs/our_repo_2/lib/python3.11/lib-dynload', '/Users/pellegrinraphael/anaconda3/envs/our_repo_2/lib/python3.11/site-packages', '/Users/pellegrinraphael/anaconda3/envs/our_repo_2/lib/python3.11/site-packages/torch_scatter-2.1.2-py3.11-macosx-10.9-x86_64.egg', '/Users/pellegrinraphael/Desktop/Repos_GNN/Hypergraph_Encodings/src']
We are adding the LCP encodings
The split is 1
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
We are adding encodings!
Adding the LCP encodings
Current working directory: /Users/pellegrinraphael/Desktop/Repos_GNN/Hypergraph_Encodings
The node curvatures are 
 [0.3318080008029938, -0.28314283219250763, 0.0, 0.3318080008029938, 0.0, 0.17605302731196085, -0.5663544138272604, -0.8285898864269257, -0.32101504291806904, 0.3332880139350891, 0.23427138725916544, -0.34206488728523254, -0.2512977827679027, -0.03242596983909607, 0.0, 0.1934390366077423, -0.22557741403579712, -0.3852607160806656, -0.0701531171798706, -0.1996682405471802, -0.4429638385772705, -0.41720668971538544, 0.05346184968948364, 0.16665157675743103, -0.13518325487772623, -0.4549834132194519, -0.11114335060119629, -0.06842935085296631, -0.23959129452705383, -0.13059544563293457, -0.3713731567064921, 0.2021106258034706, -0.12393909692764282, -0.2745855748653412, -0.13284831387656076, -0.0069693922996521, -0.7707999481095208, -0.5178757905960083, -0.30026163905858994, -0.5456053018569946, -0.1813509464263916, -0.8351828846438177, -0.09596214029524061, -0.610653630009404, 0.4982505242029826, -0.06946959098180135, -0.23709417879581451, -0.5558789860118519, -0.1355629861354828, -0.2222222089767456, -0.44044029712677, -0.5069930818345811, -0.2371598482131958, -0.474031126499176, -0.139797500201634, -0.03665404518445333, -0.4466385742028554, -0.40491111278533937, -0.08031022548675537, -0.5341167839673849, 0.0, 0.0, 0.1670377552509308, 0.07864162921905518, -0.3062186181545258, 0.0, 0.3659934898217519, -0.07307227849960327, 0.04161563515663147, -0.14852614402770997, -0.05160552263259888, -0.4235929350058238, -0.24017998576164246, -0.09949055314064026, -0.22310954928398133, -0.6213797688484192, 0.10164290070533752, 0.05083443359895186, 0.24952593445777893, -0.14036580589082506, -0.36947975556055707, 0.053461670875549316, -0.3104641040166219, -0.13921888172626495, 0.0, 0.40374039610226947, -0.7403425080435616, 0.2227643302508763, 0.05525535941123962, -0.5503015021483103, -0.7504793660981315, 0.04871211449305216, -0.8115275880825594, 0.20005435744921365, -0.4333330988883972, -0.25346557299296063, -0.05302935838699341, -0.0014384984970092773, 0.02424035221338272, -0.40742258230845135, 0.06367979447046916, 0.017688512802124023, -0.45882079005241394, -0.24313913583755492, -0.11491998036702473, 0.06676648259162903, -0.13610431822863492, 0.13093650341033936, -0.2536557912826538, 0.09683142105738322, -0.14131130774815878, 0.022789764404296874, -0.5809262328677707, -0.13750491539637247, -0.03698893388112386, -0.343495774269104, -0.12990387280782065, 0.0, -0.00362469752629598, -0.4265592694282532, -0.02734529972076416, -0.443778432905674, -0.2905261144042015, -0.635302566346668, -0.22500327229499817, -0.0652506947517395, -0.0584864616394043, 0.24940791726112366, 0.0, 0.32303027510643006, -0.05948859453201294, -0.06897582610448201, -0.39990679919719696, 0.09219752550125122, 0.0, -0.5113338104316166, -0.41082126985896716, 0.18330729007720947, -0.42702837173755354, -0.30989424387613934, -0.4797268211841583, 0.30619072169065475, 0.0, 0.0, 0.1185934841632843, 0.26549002528190613, 0.12233546801975795, 0.0, 0.40976034104824066, 0.0, 0.0, 0.0, 0.24940791726112366, 0.24456298351287842, 0.24940791726112366, 0.30059245228767395, 0.0, 0.09745434919993083, 0.1704911986986796, 0.05074506998062134, 0.0, -0.16309602393044365, 0.0, -0.4859624178636642, -0.2665339708328247, -0.31536804139614105, 0.24940791726112366, 0.5864176601171494, 0.19743824005126953, 0.5864176601171494, -0.1943705976009369, 0.11724039912223816, -0.29809558391571045, -0.5543793099267142, -0.35949105875832693, -0.22292500734329224, -0.0724317878484726, 0.018960544041224887, 0.028659000992774963, 0.07905960083007812, -0.14602666009556164, -0.5307570338249207, -0.11782070398330688, 0.10748210549354553, 0.2529694239298503, -0.1361020462853568, 0.498043417930603, 0.498043417930603, -0.20079894860585532, -0.2812068581581116, -0.6503674096839372, -0.5583853551319667, 0.4982694983482361, -0.40850236266851425, -0.16425955295562744, -0.6531700425677829, 0.0, -0.23624334732691446, 0.5478077232837677, 0.2537178695201874, -0.14780906438827515, -0.23620455339550972, 0.004901707172393799, 0.4982694983482361, 0.41567325592041016, 0.22220206260681152, 0.41567325592041016, -0.3267039656639099, 0.22719204425811768, -0.005339394013086955, -0.6063147536639509, 0.0, -0.29480574528376263, -0.08432792127132416, 0.0, 0.12384535868962605, 0.0952284038066864, 0.0, 0.17973106248038156, 0.0, -0.24222201108932495, 0.0, 0.14901793003082275, 0.0, -0.6335061355070635, -0.486791604757309, -0.009837478399276733, -0.6007501437113836, 0.014011457562446594, -0.5742080899385306, -0.44583506385485333, 0.29735715687274933, -0.023840169111887615, -0.12491045519709587, 0.17900149524211884, 0.20187029242515564, 0.07235573728879292, -0.15095087885856628, -0.16545734235218593, -0.6447596311569214, -0.7823001759392875, 0.576264151930809, -0.15682153403759003, -0.6518246818471838, -0.22991331815719604, 0.576264151930809, 0.36261282364527386, -0.6525093150138855, 0.0, 0.10126233100891113, -0.10715500513712566, 0.15029865503311157, 0.34904959201812746, 0.40374015768369037, -0.6602557034328066, -0.14771052449941635, 0.0, -0.3864736780524254, -0.11418237537145615, 0.16822506487369537, -0.10349889099597931, 0.37277545034885406, -0.15211418000134555, -0.18335982163747153, 0.2237077099936349, -0.19972798824310303, -0.24360862374305725, -0.044251859188079834, -0.5770991479649263, -0.5101068303698585, -0.6698578834533692, -0.0577502449353536, -0.29238178048815044, -0.1681946473462241, -0.1320824490653144, 0.3444816768169403, -0.10281489292780559, -0.4126649119637229, 0.29947778582572937, -0.3925800621509552, 0.20144399574824742, 0.17725559075673422, -0.10983705520629883, -0.2116031289100647, 0.07060291369756062, 0.29163265228271484, -0.009454190731048584, 0.0, 0.10920764009157817, 0.009866779500787909, 0.0, -0.155777907371521, 0.41868655383586884, -0.6610902178855169, -0.16493499279022217, 0.34804874658584595, 0.28311219811439514, -0.3254138496186998, -0.42750636430887073, -0.35793937955583843, 0.1704613651548113, 0.05548505981763204, -0.2658092180887858, -0.28820420801639557, -0.031912773847579956, -0.4972731666131453, -0.12450829893350601, -0.2876063287258148, -0.20515974362691244, -0.6673985307033246, -0.20796658396720885, -0.5852390357426235, -0.6541631863667414, 0.19064350922902426, 0.10748013854026794, -0.5830472550172915, -0.6273557224443981, -0.4622606039047241, -0.6237445563981028, 0.05741983652114868, -0.6462959766387939, -0.11061803499857585, -0.058311392863591514, -0.4332786798477173, 0.0110330730676651, -0.4952297457333269, -0.6374876081943512, -0.5646637380123138, 0.12350213527679443, -0.08418431878089905, -0.6207400560379028, -0.36022550612688065, -0.35499946560178486, -0.23719962437947592, 0.0, -0.6032799625396729, 0.0, 0.44366587201754254, 0.0, -0.3401496708393097, 0.188821941614151, -0.36249319314956663, -0.7546641716250667, -0.2142444968223572, -0.0343370967441135, 0.0, -0.7227886517842611, 0.05688617626825968, -0.045824567476908364, -0.14779305458068848, -0.18096522490183511, -0.14847509066263834, -0.05020642280578613, -0.23874606688817343, 0.0, -0.23049327731132507, -0.35516537938799175, 0.12633457779884338, -0.24659740924835205, -0.4650395427431379, -0.24368525544802347, -0.7485929944298484, -0.42051154375076294, -0.1858440265059471, -0.3952082395553589, 0.1749492585659027, -0.03399773154939924, -0.19135338813066483, 0.124879390001297, -0.5874577522277832, -0.2276324431101481, 0.0, 0.2184873104095459, 0.0, -0.11799307664235432, 0.05931610862414042, -0.30383663177490233, -0.09571760892868042, -0.22672319412231445, -0.033473968505859375, 0.11665073037147522, -0.162509024143219, -0.23261619938744438, -0.4879181757569313, 0.2703170379002889, 0.0, -0.5033677639784636, -0.517382800579071, -0.04428684711456299, -0.3009690046310425, 0.20753913521766662, -0.5158289833502336, -0.3822525372871986, -0.38403374950091046, 0.0, -0.6420171856880188, 0.07153846820195515, 0.017295673489570618, 0.09033238887786865, -0.4667677198137556, 0.0, -0.23719962437947592, 0.0, -0.14185258746147156, 0.498043417930603, -0.5584620970946091, 0.22373012453317642, -0.5200333595275879, -0.6323550144831339, 0.0, 0.33585555851459503, 0.23071696360905966, 0.12575723230838776, 0.3000083863735199, -0.41717822551727296, -0.2289414331316948, 0.15519183022635324, 0.041304558515548706, -6.051858266194662e-05, -0.28496775501652766, -0.2743501446463845, 0.24326926469802856, 0.38348519802093506, 0.38348519802093506, -0.7825308980011358, -0.733733258004916, 0.08183798789978028, 0.12633457779884338, -0.3155273199081421, 0.33167845010757446, 0.0, -0.1774351954460144, 0.0, -0.38064459392002653, -0.1656821370124817, 0.29163265228271484, -0.23545415997505187, -0.28516747734763404, 0.07235593597094218, -0.1272018551826477, -0.4508804753422737, -0.023719966411590576, -0.5708970308303833, -0.03894333044687907, -0.035881996154785156, 0.09151726961135864, -0.043962717056274414, 0.17702019214630127, -0.18597984810670218, 0.032678961753845215, -0.026935229698816936, 0.14901793003082275, -0.39539477331884976, -0.3655806005001068, 0.28311219811439514, 0.0, 0.3226300418376923, -0.199582839012146, 0.24933266639709473, -0.0829399824142456, -0.11314536843981061, -0.11555189390977223, 0.33022325734297436, 0.498143769800663, 0.18292430539925894, 0.24588395158449808, 0.299458384513855, 0.10627049207687378, 0.10627049207687378, -0.33456584215164187, -0.10525675117969513, 0.1934390366077423, 0.012755493323008219, 0.266001358628273, -0.2080230712890625, 0.2025585174560547, 0.0, 0.28307687044143676, -0.48542141914367676, -0.21357896592881945, -0.31424524102892193, -0.3882676179592426, 0.0, 0.12492869049310684, -0.15142005681991577, -0.1394246742129326, 0.2741551995277405, 0.0, 0.08878153562545776, 0.0, -0.3661040597491794, 0.2992557684580485, -0.37426435947418213, 0.0, -0.6310620989118304, -0.16478419303894043, 0.21330666542053223, 0.2911447286605835, -0.2061125695705414, -0.08732078969478607, 0.17702019214630127, 0.17356790602207184, -0.048266977071762085, -0.23222812016805014, -0.35199400782585144, 0.22100079911095755, 0.06967228651046753, 0.19031001130739847, -0.303004198214587, -0.004311919212341309, -0.39866164326667786, 0.19031001130739847, 0.0, -0.23931583762168884, 0.38910019397735596, 0.04391863942146301, -0.3749123215675354, 0.0, 0.0, -0.04560968279838562, 0.0, -0.543829083442688, -0.017334202925364178, 0.1264138917128245, 0.0003889799118041992, 0.14345207810401917, -0.19106310606002808, 0.008455514907836914, 0.299458384513855, -0.10554789751768112, -0.3798352267060961, 0.17562811573346457, -0.6497830525040627, 0.0, -0.09569934010505676, -0.24924603530338832, -0.049327234427134194, 0.4704114496707916, 0.4704114496707916, -0.11373448371887207, 0.19882017374038696, -0.29712546297482084, -0.1183562457561493, -0.454236497481664, 0.008455514907836914, 0.33585555851459503, 0.29190417130788165, 0.0, -0.17360704640547434, 0.06548526883125305, -0.07121869921684265, -0.2090674999513124, -0.17022453993558884, 0.0, -0.6485570924622672, -0.3202576518058777, -0.2998582720756531, 0.08163710832595825, 0.23952303330103555, -0.05062539875507355, -0.02777087688446045, 0.0, -0.46282675633063686, 0.19575555622577667, 0.0, -0.3131062239408493, -0.03985767066478729, -0.06254827976226807, -0.3235770974840437, -0.11114335060119629, 0.0, 0.0412495493888855, -0.28572094440460205, 0.1970265507698059, 0.20879963040351868, 0.05798220634460449, 0.013685656918419732, -0.3882638045719692, -0.39363248149553937, -0.34188398192910585, -0.4358038107554118, 0.2922465205192566, 0.08787795156240463, -0.06972263540540423, 0.6276188045740128, -0.23224633932113647, 0.27877142280340195, 0.22109932558877127, 0.024117544293403625, 0.06130266189575195, 0.09751728177070618, 0.09751728177070618, -0.42440188782555716, 0.2721558411916097, -0.045124053955078125, -0.10401029586791992, -0.04904052118460337, -0.5468093951543173, -0.07638798952102661, -0.05865837633609772, 0.030987878640492756, -0.1125645637512207, -0.2427549958229065, -0.47870393594106037, -0.5706194490194321, -0.15095087885856628, -0.17672482558659144, -0.26900263627370197, -0.11840624610582988, -0.4900628328323364, 0.05355355143547058, 0.0, -0.14534418450461495, 0.1950547198454539, -0.7164711441312518, -0.26186270713806153, -0.2314490344789293, -0.16211926192045212, 0.3374950662255287, -0.24128166834513345, -0.24999845027923584, -0.39332216276842, -0.21003530422846475, -0.3608577728271484, -0.315703136580331, -0.031684041023254395, 0.6276188045740128, -0.35714011192321776, -0.1285000741481781, 0.16723530491193137, 0.6276966333389282, -0.2999306728965358, -0.35352832078933716, 0.09643927812576295, -0.4013142983118693, 0.0, 0.13207556307315826, 0.0, 0.0, 0.20005435744921365, -0.6904406845569611, -0.7808779180049896, -0.12979385256767273, -0.4375, -0.4868197838465373, -0.40158772841095924, 0.11078645501817976, -0.28706830739974976, -0.015265792608261108, -0.23806875944137573, 0.13116717338562012, 0.05011529723803202, 0.443684846162796, 0.151213538646698, -0.5925768481360542, 0.05011529723803202, -0.6031529241138034, -0.3464064747095108, 0.06161421537399292, -0.031519714328977794, -0.5610980902399335, 0.0005173335472742716, -0.2593558109723605, 0.18651646375656128, -0.05597911278406779, -0.40211302042007446, 0.13278528054555258, -0.5204615354537964, -0.256923109292984, 0.06468264261881511, -0.6075973293998025, -0.45933206291759715, -0.18252114206552505, 0.05355355143547058, 0.10120290517807007, -0.44753328561782835, -0.2027231752872467, -0.07213350137074788, -0.40515573024749757, -0.02435639500617981, 0.0, 0.024086912473042805, -0.20497373739878336, -0.023877876145499095, -0.5671660568979051, 0.0, 0.0, 0.0, -0.005356679360071818, -0.32214637597401935, -0.5182617743810017, -0.23504678227684714, 0.026799023151397705, 0.026799023151397705, 0.1867397129535675, -0.03217692034585135, -0.1226508617401123, 0.21261236630380154, 0.25219595219407764, -0.08783196409543355, 0.0017170111338297527, -0.6443843160356794, -0.3828588239848614, -0.5152493623586801, -0.6452306736083258, -0.4891672730445862, 0.24897001187006632, 0.0, 0.2467574973901113, -0.7473955154418945, -0.25292616089185077, -0.2825998332765367, -0.32216978073120117, -0.09760670151029315, 0.3999364376068115, -0.4100840787092845, -0.008847196896870932, -0.11409682929515838, 0.299458384513855, 0.2673120677471161, -0.3643292784690857, 0.1617942214012146, -0.18778175466200886, 0.0, -0.5260023784637451, -0.45165075063705445, -0.6716596228735787, -0.5230431589815352, -0.2579165001710256, -0.38881269097328186, 0.0, -0.5365852202687945, 0.0, 0.498143769800663, -0.08071366945902507, 0.2159282018740972, 0.02050465676519606, 0.3303075035413106, 0.03799642622470856, -0.09717752039432526, 0.06161421537399292, -0.6839147061109543, -0.1835673451423645, -0.026935229698816936, 0.09509754180908203, 0.15029865503311157, -0.1568489909172058, 0.06926732261975606, 0.498143769800663, 0.05345792429787772, 0.259284774462382, 0.17372452219327292, -0.056793034076690674, 0.12497919797897339, 0.0, -0.5254450440406799, -0.430582869797945, 0.018733173608779907, 0.04392768939336141, 0.0, 0.0, 0.37461668252944946, 0.28680254022280377, -0.3111756123029269, -0.1711264133453369, 0.10080091158548991, 0.046744475762049355, -0.40425416082143784, 0.30931461850802106, 0.12200681865215302, -0.03766282967158726, 0.37461668252944946, -0.3821316489151546, 0.23071696360905966, -0.018273366349084035, -0.0379940356527056, -0.2417758978330172, 0.0, -0.5284187495708466, 0.0, 0.3999364376068115, 0.12672832608222961, -0.12444165349006653, -0.18184896310170492, 0.02526184916496277, 0.1339183747768402, 0.26824671030044556, -0.1346272975206375, 0.21330666542053223, 0.0, -0.055360893408457436, -0.4212848749011755, -0.025545756022135418, -0.2591517056737627, -0.2543725201061794, -0.20339307934045792, 0.004239022731781006, 0.1787541389465332, -0.18858355283737183, 0.08684467275937398, 0.18331819772720337, 0.0027977898716926575, 0.07780880729357402, -0.2235509157180786, 0.0, -0.38888269662857056, -0.4653838300704956, 0.0, -0.32008612155914307, 0.0, -0.208715558052063, 0.0, -0.30061017274856566, 0.07228666543960571, 0.0, -0.3248167932033539, -0.7281278636720445, 0.5583335340023041, 0.5583335340023041, 0.2741551995277405, 0.5583335340023041, 0.3999364376068115, 0.0, 0.03267037868499756, 0.0, -0.42309952278931934, -0.24209272861480713, 0.0, -0.0978235403696696, -0.18996937274932862, 0.12670873999595642, 0.3999364376068115, -0.5300075036508066, -0.2541414101918538, -0.031530141830444336, 0.0, -0.16550815105438232, -0.476373369877155, -0.4740782156586647, 0.18879655003547668, -0.07142329216003418, 0.18879655003547668, -0.4902260767088996, -0.1393691897392273, 0.19735947251319885, -0.7222103940116035, 0.0, -0.14284165700276694, -0.4540051519870758, 0.0, -0.07391464710235596, -0.1897346888269697, -0.4746139367421468, -0.8033162653446198, 0.3226300418376923, 0.11031636595726013, 0.11031636595726013, 0.0, -0.08776089549064636, -0.4491182804107666, -0.46858348449071247, 0.0, -0.05910387966367933, -0.4559377133846283, -0.19679520279169083, -0.24970108270645142, 0.05931610862414042, -0.22355914115905762, -0.1381449941545725, -0.4097225844860077, 0.052688995997111, -0.34749849140644073, -0.6754481345415115, -0.46645668148994446, -0.5633725278517779, 0.0, -0.4089869111776352, -0.6563762256077358, 0.0, -0.4625409245491028, -0.14940524101257324, -0.36369539052248, 0.0, 0.16721789042154947, 0.0, -0.4037642223494394, 0.024975597858428955, 0.0, 0.00897209346294403, 0.0, -0.31250086426734924, 0.12590593099594116, 0.11607617139816284, 0.4272882789373398, -0.18441171050071717, 0.21816238164901733, 0.2830190062522888, -0.2281399369239807, -0.6381072687066119, 0.0952707330385844, 0.1787541389465332, 0.013513376315434774, 0.31631263097127277, -0.27072148190604317, 0.24993784725666046, -0.6425970395406088, 0.0, 0.5478077232837677, -0.16452343265215555, -0.2690889835357666, -0.19736585446766444, 0.0, 0.0, -0.4098077416419983, 0.09277766942977905, -0.37068015336990356, 0.25288912653923035, 0.09277766942977905, 0.25288912653923035, 0.024129241704940796, -0.4468251435380233, 0.14323118329048157, -0.22063666582107544, -0.0967111587524414, 0.14323118329048157, -0.3061005584895611, -0.25101478894551593, 0.443684846162796, 0.244147390127182, -0.22169939676920572, -0.28521132469177246, 0.0919789969921112, 0.244147390127182, 0.443684846162796, 0.11992629369099934, 0.09966216179040763, 0.0, -0.4935080905755361, 0.12312642552635887, -0.7953366279602051, 0.07032658904790878, 0.15029865503311157, 0.15029865503311157, -0.29454338550567627, -0.48148149251937866, 0.19735947251319885, -0.4315156719901345, -0.042377129197120667, -0.3311246534188588, 0.07228666543960571, 0.0, 0.0, -0.2669764260450999, 0.11312168836593628, -0.10393261909484863, 0.0, 0.6091012358665466, 0.2657853066921234, 0.498043417930603, 0.498043417930603, 0.3117369502782822, 0.268780916929245, 0.498043417930603, -0.08156770467758179, 0.35014568269252777, 0.24933266639709473, -0.28941091895103455, 0.0, 0.11447721719741821, 0.11447721719741821, -0.22032401959101358, -0.014589857310056686, 0.5068248063325882, 0.026338507731755573, -0.22684190102985927, -0.025022447109222412, 0.2661711275577545, 0.0, 0.0, 0.0, 0.0, -0.2068803310394287, -0.2453548816534189, 0.25721709430217743, -0.3003136157989502, 0.34478890895843506, -0.24721889495849608, 0.0, 0.25721709430217743, -0.06833568215370178, -0.2971707135438919, -0.06781548261642456, 0.14927177429199218, 0.1370516618092855, -0.17782992124557495, 0.1447630077600479, 0.10094359517097473, -0.16465632120768228, 0.3315215806166331, -0.08504803975423177, 0.0807894766330719, -0.11689637899398804, 0.2025585174560547, 0.23806411027908325, 0.06798624992370605, 0.0, 0.0, 0.17119638621807098, 0.22214501102765402, 0.083303302526474, -0.22773807247479758, -0.5585443019866944, 0.13564109802246094, 0.33230897784233093, 0.0, 0.0, -0.3657713830471039, 0.0, 0.4521130099892616, -0.22702082991600037, 0.03152125080426534, 0.03152125080426534, 0.3332880139350891, 0.0, 0.0925053854783376, -0.32485364164624897, -0.24601399898529053, 0.16284172875540598, 0.0, 0.004316846529642741, 0.07131530841191609, 0.0, -0.7440690902563242, -0.21740082403024039, -0.6078188180923462, -0.22254410882790884, -0.3309454172849655, 0.29435569047927856, 0.36605843901634216, -0.21071231365203857, 0.0, 0.14900095264116922, -0.0740567147731781, 0.28553861379623413, 0.2025585174560547, 0.11435830593109131, -0.28144676155514187, -0.41209735189165386, -0.08568617701530457, 0.027790391445159913, -0.18102729320526123, 0.14345207810401917, -0.18751651048660278, 0.2855422794818878, -0.08482170104980469, -0.2769058148066203, -0.12449860572814941, -0.325213623046875, 0.0, 0.05074495077133179, 0.16419732570648193, 0.4704114496707916, -0.10763749480247498, -0.12609388927618662, 0.08114314675331116, 0.0, -0.4230048954486847, 0.09591555595397949, -0.4467695355415344, 0.3216792394717534, 0.29163265228271484, 0.3888435363769531, 0.0, 0.08664298057556152, -0.48571425676345825, 0.24940791726112366, 0.00408172607421875, 0.12316811084747314, 0.05787914991378784, -0.17299982905387878, -0.2636573513348897, -0.025761210918426515, -0.3862168656455146, -0.008949518203735352, 0.0535564124584198, 0.2493173579374949, -0.3162104884783427, 0.12316811084747314, -0.38558601008521187, 0.29947778582572937, 0.38348519802093506, -0.20994704961776733, -0.41036025881767274, -0.1092885434627533, -0.47269709904988605, -0.6270331144332886, -0.03718165556589762, 0.0, -0.41581349074840546, 0.4054151972134908, 0.0, -0.12669479846954346, 0.0, 0.0, -0.4608043630917867, -0.3367948191506522, 0.4054151972134908, -0.3803821934594048, 0.12449094653129578, 0.4066370924313863, -0.31361432870229083, 0.03456065058708191, 0.0, 0.4315389022231102, 0.12664641936620077, 0.033904850482940674, 0.033904850482940674, 0.11665073037147522, 0.5068375915288925, 0.5478077232837677, 0.17593246698379517, -0.49473850641931805, 0.17055462300777435, -0.23124894168641832, 0.0, -0.20314590136210123, -0.11366353432337443, -0.1903743346532186, 0.07562403380870819, 0.14189454913139343, 0.0, 0.17019532124201456, 0.4098820462822914, -0.04560968279838562, 0.14581865072250366, -0.07223497331142426, -0.21208346635103226, 0.2221818765004476, 0.023825109004974365, 0.3333030939102173, -0.1389292081197103, 0.0, -0.003201887011528015, -0.4027896046638489, 0.0, 0.5544954836368561, -0.22726778984069823, 0.13283176720142365, -0.18146049976348877, 0.05755746906453913, -0.4863464832305908, -0.29209502041339874, 0.17593246698379517, 0.0, 0.0, 0.10438030958175659, -0.08894860744476318, -0.32342924177646637, -0.48571425676345825, -0.09355052709579467, 0.17500106990337372, -0.1767068760735648, 0.4673949956893921, -0.07320261994997661, 0.0, -0.332832932472229, 0.4992083013057709, 0.4992083013057709, 0.0, -0.004311114549636841, 0.5547429422537485, 0.18651628494262695, 0.017070241272449493, -0.08894860744476318, 0.23071696360905966, 0.18905862172444662, 0.0, 0.0, 0.010275890429814657, 0.18819000323613486, -0.41233612100283307, 0.3315215806166331, 0.13612942695617675, -0.46654661802145153, -0.10393261909484863, 0.2667504946390788, 0.2537058691183726, -0.1795009970664978, -0.24620428255626134, 0.5478077232837677, 0.6091012358665466, 0.6091012358665466, 0.0, 0.0, 0.5088862627744675, -0.22597009485418146, 0.24997729063034058, 0.06245735287666321, -0.0747414231300354, 0.0, 0.060481240351994835, 0.5088862627744675, 0.5088862627744675, 0.060481240351994835, -0.08498755842447281, 0.14179964860280356, 0.00902022123336792, 0.1254996657371521, 0.0, 0.060271501541137695, 0.0, 0.0, 0.1837543249130249, -0.01428067684173584, 0.0, 0.22904203832149506, 0.22527086734771729, 0.0, 0.24897001187006632, -0.16300243139266968, 0.1758030652999878, 0.24897001187006632, -0.7492134968439738, 0.32823190838098526, 0.02464696764945984, 0.08059403896331788, 0.27877142280340195, 0.0, 0.5090664923191071, 0.29163265228271484, -0.465062005179269, 0.030389495193958282, 0.04391863942146301, 0.17500106990337372, 0.0, -0.39246630668640137, -0.2843390703201294, 0.0, -0.5634069740772247, -0.10599029064178467, 0.0, 0.1843992074330648, -0.10120777785778046, -0.43399540867124287, -0.03049396723508835, -0.613242506980896, 0.19368963837623596, -0.41837191184361777, 0.07401259740193684, -0.41967866053948033, -0.5087457353418524, -0.3796829581260681, -0.05386298894882202, -0.10763749480247498, 0.0, 0.0, 0.0, -0.09295859336853027, -0.05994270245234171, -0.3962956170241038, -0.14623789489269257, 0.0, 0.17633818089962006, 0.1959027498960495, -0.07140117883682251, 0.17966927587985992, 0.0412368655204773, -0.047436825931072235, 0.033981867134571075, 0.07499536871910095, -0.2517666816711426, 0.15322673320770264, -0.557568371295929, 0.07150706052780151, 0.16664820909500122, 0.29283368587493896, 0.0, -0.16481934487819672, 0.25003883987665176, 0.07402104139328003, 0.04677721858024597, 0.0, -0.3388754526774089, 0.3094383478164673, 0.09591555595397949, 0.08776922225952148, -0.38612060035978046, -0.11953650712966919, 0.0, 0.10884669423103333, 0.24940791726112366, -0.30072813232739765, -0.5737176073922051, 0.1843992074330648, -0.5173367365546848, -0.3007586257798331, -0.2832648456096649, -0.5657674272855123, -0.5797790586948395, -0.08302688598632812, 0.15275955200195312, 0.0, 0.08306856950124104, 0.0, 0.31166947782039645, -0.2724677423636119, -0.47191286087036133, -0.0484653115272522, -0.2632216215133667, -0.3532503008842468, 0.2467574973901113, -0.06951038042704265, 0.2162881851196289, -0.17082032561302185, -0.30994503838675364, -0.14049009482065836, -0.199422558148702, -0.13841552734375, -0.2886713875664605, 0.2511899471282959, 0.0, -0.61602982878685, -0.22276021540164948, 0.03700544834136963, 0.0, 0.0, -0.06305450797080994, 0.0, 0.2657853066921234, 0.349506676197052, 0.11806584894657135, -0.23239661114556448, 0.2656817947115217, 0.17228247225284576, -0.4828520928110395, -0.43519445828029085, 0.11446836590766907, -0.19398075342178345, 0.0, 0.0, 0.22216169039408365, 0.0, -0.40802747862679617, 0.0, 0.0, -0.19499916831652322, -0.41989184277398245, -0.2186884880065918, 0.32823190838098526, 0.2493173579374949, -0.4644351005554199, 0.10612168908119202, 0.336412513256073, 0.0, 0.14900095264116922, -0.14890283346176147, 0.2741551995277405, -0.3419096767902374, -0.1893675128618876, 0.042609410626547675, 0.2495216429233551, 0.17141512036323547, 0.29947778582572937, 0.0017166018486022949, 0.0, 0.0, 0.13093650341033936, -0.47560977935791016, -0.3268977105617523, -0.7651658773422241, -0.6068066017968314, 0.0, 0.0, 0.0, -0.03651464978853861, -0.2661200761795044, -0.06459292498501865, -0.24913030862808228, 0.254326656460762, 0.17712054153283438, -0.20983951091766356, -0.3618425652384758, -0.2667372450232506, 0.0, 0.254326656460762, 0.12964505950609842, -0.4314842104911804, 0.12964505950609842, -0.019961722195148468, 0.0, -0.5140252947807312, 0.07402104139328003, -0.05402670303980509, 0.0, -0.19542308648427328, 0.0, -0.274814248085022, 0.0, 0.0, 0.0, -0.31564327081044513, -0.030862172444661457, -6.051858266194662e-05, 0.0, 0.0, -0.07597336173057556, -0.34526288509368896, 0.00897209346294403, -0.04994705319404602, -0.3446611099772983, 0.38348519802093506, 0.0, 0.14402349789937338, -0.1492604443005153, -0.07201981544494629, -0.6162350177764893, -0.5356108461107526, 0.24997729063034058, -0.455852746963501, -0.49882394075393677, 0.025310499327523366, -0.18376674254735312, -0.19677106539408365, 0.34448249638080597, -0.005213012297948201, 0.34448249638080597, 0.18130558729171753, 0.0, -0.1857014298439026, -0.5901415348052979, 0.0, 0.0, 0.20952088832855226, -0.16814001401265463]
The edge curvatures are 
 [-0.727394421895345, -0.727394421895345, -0.727394421895345, -0.7209595441818237, -0.7209595441818237, -1.121214429537455, -1.121214429537455, -1.121214429537455, -1.121214429537455, -0.6587341606616974, -0.6587341606616974, -0.6587341606616974, -0.6587341606616974, -0.6587341606616974, -0.6983116865158081, -0.6983116865158081, -0.8716821670532227, -0.8716821670532227, -0.8716821670532227, -0.8081713318824768, -0.8081713318824768, -0.8081713318824768, -0.8081713318824768, -0.16185975372791295, -0.16185975372791295, -0.16185975372791295, -0.16185975372791295, -0.16185975372791295, -0.4472271998723347, -0.4472271998723347, -0.4472271998723347, -0.28354715108871464, -0.28354715108871464, -0.28354715108871464, -0.28354715108871464, -0.28354715108871464, -0.5851835807164509, -0.5851835807164509, -0.5851835807164509, 0.0, 0.0, -0.25840848088264456, -0.25840848088264456, -0.25840848088264456, -0.25840848088264456, -0.25840848088264456, -0.5062500238418579, -0.5062500238418579, -0.5062500238418579, -0.5062500238418579, -0.5654383699099224, -0.5654383699099224, -0.5654383699099224, -0.5654383699099224, -0.7773370146751404, -0.7773370146751404, -0.7773370146751404, -0.7773370146751404, -0.4571666717529297, -0.4571666717529297, -0.5297709107398987, -0.5297709107398987, -0.5297709107398987, -0.5297709107398987, -0.861522912979126, -0.861522912979126, 0.01423250138759613, 0.01423250138759613, 0.01423250138759613, 0.01423250138759613, -0.4326666593551636, -0.4326666593551636, -0.4326666593551636, 0.020331263542175293, 0.020331263542175293, 0.020331263542175293, 0.020331263542175293, -1.1788816452026367, -1.1788816452026367, -0.6125012636184692, -0.6125012636184692, -0.6125012636184692, -0.6125012636184692, -1.0891773700714111, -1.0891773700714111, -0.4571666717529297, -0.4571666717529297, -0.7695860862731934, -0.7695860862731934, -0.7695860862731934, -0.6308845698833465, -0.6308845698833465, -0.6308845698833465, -0.6308845698833465, -0.6308845698833465, -0.3364862104256947, -0.3364862104256947, -0.3364862104256947, -0.3364862104256947, -0.3399784962336223, -0.3399784962336223, -0.3399784962336223, -0.6939306259155273, -0.6939306259155273, -0.09568411608537031, -0.09568411608537031, -0.09568411608537031, -0.09568411608537031, -0.36156755685806274, -0.36156755685806274, -0.36156755685806274, -0.36156755685806274, 0.0, 0.0, 0.0, 0.0, -0.9809273481369019, -0.9809273481369019, -0.6983116865158081, -0.6983116865158081, -0.540376861890157, -0.540376861890157, -0.540376861890157, -0.31790957848231005, -0.31790957848231005, -0.31790957848231005, -0.31790957848231005, -0.6983116865158081, -0.6983116865158081, -0.31790957848231005, -0.31790957848231005, -0.31790957848231005, -0.31790957848231005, -0.32431810200214395, -0.32431810200214395, -0.32431810200214395, -0.32431810200214395, -0.32431810200214395, -0.5812423030535381, -0.5812423030535381, -0.5812423030535381, -0.5812423030535381, -0.2571895301342011, -0.2571895301342011, -0.2571895301342011, -0.2571895301342011, -0.2571895301342011, -0.8544492721557617, -0.8544492721557617, -0.31790957848231005, -0.31790957848231005, -0.31790957848231005, -0.31790957848231005, -1.3246469497680664, -1.3246469497680664, -0.807444969813029, -0.807444969813029, -0.807444969813029, -0.807444969813029, -0.36275736490885424, -0.36275736490885424, -0.36275736490885424, -0.36275736490885424, -1.190739631652832, -1.190739631652832, -0.7695860862731934, -0.7695860862731934, -0.7695860862731934, -0.5248978137969971, -0.5248978137969971, -0.861522912979126, -0.861522912979126, -0.48604393005371094, -0.48604393005371094, -0.48604393005371094, -0.48604393005371094, -0.48604393005371094, -0.5255361398061116, -0.5255361398061116, -0.5255361398061116, -0.4547164738178253, -0.4547164738178253, -0.4547164738178253, -0.4547164738178253, -0.4547164738178253, -0.727394421895345, -0.727394421895345, -0.727394421895345, -0.8998659948507945, -0.8998659948507945, -0.8998659948507945, -0.8998659948507945, -0.4249953031539917, -0.4249953031539917, -0.4249953031539917, -0.4249953031539917, -0.4249953031539917, -0.7142033974329631, -0.7142033974329631, -0.7142033974329631, -0.7142033974329631, -0.5464562177658081, -0.5464562177658081, -0.3060880700747173, -0.3060880700747173, -0.3060880700747173, -0.18084473609924312, -0.18084473609924312, -0.18084473609924312, -0.18084473609924312, -0.18084473609924312, -0.9162863095601399, -0.9162863095601399, -0.9162863095601399, -1.113257884979248, -1.113257884979248, -0.641254723072052, -0.641254723072052, -0.641254723072052, -0.7140695651372273, -0.7140695651372273, -0.7140695651372273, -0.2800430655479431, -0.2800430655479431, -0.2800430655479431, -1.0259791215260825, -1.0259791215260825, -1.0259791215260825, -1.0259791215260825, -0.5975019931793213, -0.5975019931793213, -1.0394315719604492, -1.0394315719604492, -0.5817679166793823, -0.5817679166793823, -0.5817679166793823, -0.5817679166793823, -1.3247876167297363, -1.3247876167297363, -1.05961012840271, -1.05961012840271, -1.2135533889134726, -1.2135533889134726, -1.2135533889134726, -0.600702166557312, -0.600702166557312, -0.7059766054153442, -0.7059766054153442, 0.020331263542175293, 0.020331263542175293, 0.020331263542175293, 0.020331263542175293, -0.7059766054153442, -0.7059766054153442, -0.5382274389266968, -0.5382274389266968, -1.2993769645690918, -1.2993769645690918, -0.467930297056834, -0.467930297056834, -0.467930297056834, -0.17636123299598694, -0.17636123299598694, -0.17636123299598694, -0.17636123299598694, -0.9793926477432251, -0.9793926477432251, -0.9793926477432251, -0.9012640714645386, -0.9012640714645386, -0.9012640714645386, -0.36275736490885424, -0.36275736490885424, -0.36275736490885424, -0.36275736490885424, -0.5913549264272053, -0.5913549264272053, -0.5913549264272053, -0.6121103763580322, -0.6121103763580322, -0.4889394839604695, -0.4889394839604695, -0.4889394839604695, -0.5471769571304321, -0.5471769571304321, -0.5905506014823914, -0.5905506014823914, -0.5905506014823914, -0.5905506014823914, -0.6742182970046997, -0.6742182970046997, -0.3808641036351521, -0.3808641036351521, -0.3808641036351521, -0.6939306259155273, -0.6939306259155273, -0.9065346717834473, -0.9065346717834473, -0.4002771774927776, -0.4002771774927776, -0.4002771774927776, -0.35747401913007093, -0.35747401913007093, -0.35747401913007093, -0.35747401913007093, -0.3708735585212708, -0.3708735585212708, -0.3708735585212708, -0.3708735585212708, -0.3708735585212708, -0.4571666717529297, -0.4571666717529297, -0.41813841462135315, -0.41813841462135315, -0.41813841462135315, -0.41813841462135315, -0.41813841462135315, -0.509084939956665, -0.509084939956665, -0.2800430655479431, -0.2800430655479431, -0.2800430655479431, -0.4571666717529297, -0.4571666717529297, -0.5464562177658081, -0.5464562177658081, -0.9021085103352864, -0.9021085103352864, -0.9021085103352864, -0.600702166557312, -0.600702166557312, -0.6983116865158081, -0.6983116865158081, -0.3616468906402588, -0.3616468906402588, -0.3616468906402588, -0.7062658667564392, -0.7062658667564392, -0.7062658667564392, -0.7062658667564392, -0.5621614853541057, -0.5621614853541057, -0.5621614853541057, -0.2251505911350251, -0.2251505911350251, -0.2251505911350251, -0.2251505911350251, -0.2251505911350251, -0.6132047375043232, -0.6132047375043232, -0.6132047375043232, -0.6132047375043232, -0.5651028354962666, -0.5651028354962666, -0.5651028354962666, -0.5651028354962666, -0.3351337512334187, -0.3351337512334187, -0.3351337512334187, -0.5233766535917919, -0.5233766535917919, -0.5233766535917919, -0.5233766535917919, -0.7957450151443481, -0.7957450151443481, -1.041356086730957, -1.041356086730957, -0.17039740085601807, -0.17039740085601807, -0.17039740085601807, -0.17039740085601807, 0.0, 0.0, -0.5382274389266968, -0.5382274389266968, -0.6939306259155273, -0.6939306259155273, -0.6939306259155273, -0.6939306259155273, -0.6939306259155273, -0.6939306259155273, -0.16578817864259077, -0.16578817864259077, -0.16578817864259077, -0.16578817864259077, -0.4571666717529297, -0.4571666717529297, -0.8701218366622925, -0.8701218366622925, -0.8701218366622925, -0.6493813991546631, -0.6493813991546631, -0.6553358634312947, -0.6553358634312947, -0.6553358634312947, -0.7724928855895996, -0.7724928855895996, -0.7724928855895996, -0.7724928855895996, -0.4833857615788777, -0.4833857615788777, -0.4833857615788777, -0.3739889462788899, -0.3739889462788899, -0.3739889462788899, -0.3739889462788899, -1.4001669883728027, -1.4001669883728027, -0.4343254566192627, -0.4343254566192627, -0.4343254566192627, -0.4343254566192627, -0.4343254566192627, -0.1742922614018123, -0.1742922614018123, -0.1742922614018123, -0.1742922614018123, -0.6983116865158081, -0.6983116865158081, -0.6430822610855103, -0.6430822610855103, -0.7012458592653275, -0.7012458592653275, -0.7012458592653275, -0.7012458592653275, -0.09051024913787842, -0.09051024913787842, -0.09051024913787842, 0.16559283733367924, 0.16559283733367924, 0.16559283733367924, 0.16559283733367924, 0.16559283733367924, 0.0, 0.0, -1.3393120765686035, -1.3393120765686035, -0.557378101348877, -0.557378101348877, -0.557378101348877, -0.557378101348877, -0.557378101348877, -0.15474045276641846, -0.15474045276641846, -0.15474045276641846, -0.9152876536051433, -0.9152876536051433, -0.9152876536051433, -0.6529299020767212, -0.6529299020767212, -0.6529299020767212, 0.02487053275108342, 0.02487053275108342, 0.02487053275108342, 0.02487053275108342, 0.02487053275108342, -0.38382000923156734, -0.38382000923156734, -0.38382000923156734, -0.38382000923156734, -0.38382000923156734, 0.0, 0.0, -0.803709864616394, -0.803709864616394, -0.803709864616394, -0.6078358093897502, -0.6078358093897502, -0.6078358093897502, -0.6078358093897502, -0.1748753786087036, -0.1748753786087036, -0.1748753786087036, -0.1748753786087036, -0.14447440703709913, -0.14447440703709913, -0.14447440703709913, -0.14447440703709913, -0.296680857737859, -0.296680857737859, -0.296680857737859, -0.296680857737859, -0.31498217582702637, -0.31498217582702637, -0.3697231411933899, -0.3697231411933899, -0.3697231411933899, -0.3085826436678569, -0.3085826436678569, -0.3085826436678569, -0.3085826436678569, 0.0, 0.0, -0.2182844877243042, -0.2182844877243042, -0.38117833932240797, -0.38117833932240797, -0.38117833932240797, -0.38117833932240797, -0.11267148256301884, -0.11267148256301884, -0.11267148256301884, -0.11267148256301884, -0.11267148256301884, -0.2650756537914276, -0.2650756537914276, -0.2650756537914276, -0.2650756537914276, -0.5517729918162029, -0.5517729918162029, -0.5517729918162029, -0.5517729918162029, -0.12813082337379456, -0.12813082337379456, -0.12813082337379456, -0.12813082337379456, -0.12008610367774963, -0.12008610367774963, -0.12008610367774963, -0.12008610367774963, -0.12008610367774963, -0.2018701434135437, -0.2018701434135437, -0.2018701434135437, -0.2519499123096467, -0.2519499123096467, -0.2519499123096467, -0.2519499123096467, -0.2519499123096467, 0.09329968690872192, 0.09329968690872192, -0.15751194953918457, -0.15751194953918457, -0.15751194953918457, -0.15751194953918457, -0.4080152610937755, -0.4080152610937755, -0.4080152610937755, -0.4080152610937755, -0.38060579697291064, -0.38060579697291064, -0.38060579697291064, -0.17677192687988286, -0.17677192687988286, -0.17677192687988286, -0.17677192687988286, -0.17677192687988286, -0.32011919617652884, -0.32011919617652884, -0.32011919617652884, -0.32011919617652884, -0.32011919617652884, -0.19839288791020704, -0.19839288791020704, -0.19839288791020704, -0.19839288791020704, -0.8436547696590424, -0.8436547696590424, -0.8436547696590424, -0.8436547696590424, -0.8436547696590424, -0.5839412311712902, -0.5839412311712902, -0.5839412311712902, -0.5839412311712902, -0.1794390082359314, -0.1794390082359314, -0.1794390082359314, -0.18719015717506404, -0.18719015717506404, -0.18719015717506404, -0.18719015717506404, -0.18719015717506404, 0.006928682327270508, 0.006928682327270508, -0.18890579541524244, -0.18890579541524244, -0.18890579541524244, -0.27595226168632503, -0.27595226168632503, -0.27595226168632503, -0.27595226168632503, -0.27595226168632503, -0.28028929233551025, -0.28028929233551025, -0.6473224759101868, -0.6473224759101868, -0.6473224759101868, -0.6473224759101868, -0.6473224759101868, -0.5537169774373372, -0.5537169774373372, -0.5537169774373372, -0.32752174735069284, -0.32752174735069284, -0.32752174735069284, -0.32752174735069284, -0.32752174735069284, -0.39354363679885873, -0.39354363679885873, -0.39354363679885873, -0.39354363679885873, -0.39354363679885873, -0.1196795940399169, -0.1196795940399169, -0.1196795940399169, -0.1196795940399169, -0.1196795940399169, -0.6633942723274231, -0.6633942723274231, -0.6633942723274231, -0.6633942723274231, -0.3687024315198262, -0.3687024315198262, -0.3687024315198262, -0.3687024315198262, 0.06495672464370728, 0.06495672464370728, 0.06495672464370728, -0.37087355057398486, -0.37087355057398486, -0.37087355057398486, -0.37087355057398486, -0.5477015614509582, -0.5477015614509582, -0.5477015614509582, -0.5477015614509582, -0.5477015614509582, -0.5773506959279378, -0.5773506959279378, -0.5773506959279378, -0.9024514357248943, -0.9024514357248943, -0.9024514357248943, -0.18887901306152344, -0.18887901306152344, -0.18887901306152344, -0.18887901306152344, -0.820732831954956, -0.820732831954956, -0.1508649388949077, -0.1508649388949077, -0.1508649388949077, -0.1508649388949077, -0.5167351126670838, -0.5167351126670838, -0.5167351126670838, -0.5167351126670838, -0.5167351126670838, -0.2808822751045228, -0.2808822751045228, -0.2808822751045228, -0.2808822751045228, -0.2808822751045228, -0.16424516836802172, -0.16424516836802172, -0.16424516836802172, -0.38596908251444506, -0.38596908251444506, -0.38596908251444506, -0.8974623680114746, -0.8974623680114746, -0.8974623680114746, 0.08343835671742761, 0.08343835671742761, 0.08343835671742761, 0.3332880139350891, 0.3332880139350891, 0.24997729063034058, 0.24997729063034058, 0.3332880139350891, 0.3332880139350891, 0.36107329527537024, 0.36107329527537024, 0.36107329527537024, -0.6645521720250447, -0.6645521720250447, -0.6645521720250447, -0.5217475891113281, -0.5217475891113281, -0.321364164352417, -0.321364164352417, -0.321364164352417, -0.321364164352417, -0.321364164352417, -0.26495110988616943, -0.26495110988616943, -0.26495110988616943, -0.21547305583953857, -0.21547305583953857, -0.21547305583953857, -0.21547305583953857, -0.21547305583953857, -0.86236306031545, -0.86236306031545, -0.86236306031545, -1.5686066150665283, -1.5686066150665283, -1.008136510848999, -1.008136510848999, 0.0, 0.0, -0.7050030946731567, -0.7050030946731567, -0.7050030946731567, -0.7050030946731567, -0.7050030946731567, -0.16086522738138842, -0.16086522738138842, -0.16086522738138842, -0.5446614265441894, -0.5446614265441894, -0.5446614265441894, -0.5446614265441894, -0.5446614265441894, -0.5198571681976318, -0.5198571681976318, -0.5198571681976318, -0.6668287932872772, -0.6668287932872772, -0.6668287932872772, -0.6668287932872772, -0.6668287932872772, -0.35459931691487623, -0.35459931691487623, -0.35459931691487623, -0.3224140008290608, -0.3224140008290608, -0.3224140008290608, -0.3773643970489502, -0.3773643970489502, -0.3773643970489502, -0.2464038978020351, -0.2464038978020351, -0.2464038978020351, -0.2464038978020351, -0.333387275536855, -0.333387275536855, -0.333387275536855, -0.4655299584070842, -0.4655299584070842, -0.4655299584070842, -0.4655299584070842, -0.5312200387318928, -0.5312200387318928, -0.5312200387318928, -0.5312200387318928, -0.4784957587718963, -0.4784957587718963, -0.4784957587718963, -0.4784957587718963, -0.4784957587718963, -0.7162270943323772, -0.7162270943323772, -0.7162270943323772, -0.4598729610443115, -0.4598729610443115, -0.05142467021942143, -0.05142467021942143, -0.05142467021942143, -0.05142467021942143, -0.05142467021942143, -0.0804123729467392, -0.0804123729467392, -0.0804123729467392, -0.0804123729467392, 0.0, 0.0, -0.47739092111587533, -0.47739092111587533, -0.47739092111587533, -0.47739092111587533, -0.47739092111587533, -0.46876347064971924, -0.46876347064971924, 0.0, 0.0, 0.0, 0.0, -0.36455273628234863, -0.36455273628234863, -0.6817877093950908, -0.6817877093950908, -0.6817877093950908, 0.0, 0.0, -0.6560237288475037, -0.6560237288475037, -0.6560237288475037, -0.6560237288475037, -0.6560237288475037, -0.4123581647872925, -0.4123581647872925, -0.7354578574498494, -0.7354578574498494, -0.7354578574498494, -0.9921150207519531, -0.9921150207519531, -0.9528531233469646, -0.9528531233469646, -0.9528531233469646, -0.7617964744567871, -0.7617964744567871, -0.9995884895324707, -0.9995884895324707, -0.9995884895324707, -0.6036959687868755, -0.6036959687868755, -0.6036959687868755, -0.6417561968167622, -0.6417561968167622, -0.6417561968167622, -0.6417561968167622, -0.8432612021764119, -0.8432612021764119, -0.8432612021764119, -0.7108102639516194, -0.7108102639516194, -0.7108102639516194, -0.802912712097168, -0.802912712097168, -0.802912712097168, -1.092167615890503, -1.092167615890503, -1.2279548645019531, -1.2279548645019531, -0.6608352065086365, -0.6608352065086365, -0.6608352065086365, -0.6608352065086365, -0.737769365310669, -0.737769365310669, -0.3801444967587788, -0.3801444967587788, -0.3801444967587788, -0.3801444967587788, -1.1693999767303467, -1.1693999767303467, -0.9629629850387573, -0.9629629850387573, -1.2805607318878174, -1.2805607318878174, -0.309631069501241, -0.309631069501241, -0.309631069501241, -0.309631069501241, -0.484727144241333, -0.484727144241333, -0.484727144241333, -0.484727144241333, -0.4433626333872478, -0.4433626333872478, -0.4433626333872478, -0.303048700094223, -0.303048700094223, -0.303048700094223, -0.303048700094223, -0.7007021109263103, -0.7007021109263103, -0.7007021109263103, -0.16394555568695068, -0.16394555568695068, -0.0651931564013164, -0.0651931564013164, -0.0651931564013164, -0.0651931564013164, -0.5352089405059814, -0.5352089405059814, -0.5352089405059814, 0.05975903570652008, 0.05975903570652008, 0.05975903570652008, 0.05975903570652008, -0.016191283861796135, -0.016191283861796135, -0.016191283861796135, -0.016191283861796135, 0.6636858880519867, 0.6636858880519867, 0.6636858880519867, 0.6636858880519867, -0.08127863407135005, -0.08127863407135005, -0.08127863407135005, -0.08127863407135005, -0.08127863407135005, -0.06413202484448743, -0.06413202484448743, -0.06413202484448743, -0.06413202484448743, -0.25147769848505663, -0.25147769848505663, -0.25147769848505663, -0.8007549842198689, -0.8007549842198689, -0.8007549842198689, -0.8007549842198689, 0.0, 0.0, 0.0, 0.0, -0.41378031174341845, -0.41378031174341845, -0.41378031174341845, -0.11103046933809924, -0.11103046933809924, -0.11103046933809924, -0.11103046933809924, 0.3332425355911255, 0.3332425355911255, 0.3332425355911255, 0.0, 0.0, 0.15091111262639367, 0.15091111262639367, 0.15091111262639367, -0.5135129690170288, -0.5135129690170288, -0.5135129690170288, 0.15091111262639367, 0.15091111262639367, 0.15091111262639367, -0.406717340151469, -0.406717340151469, -0.406717340151469, -0.406717340151469, -0.09053888420263934, -0.09053888420263934, -0.09053888420263934, -0.09053888420263934, -0.3402250051498412, -0.3402250051498412, -0.3402250051498412, -0.3402250051498412, -0.3402250051498412, -0.45922751426696773, -0.45922751426696773, -0.45922751426696773, -0.45922751426696773, -0.45922751426696773, -0.9160400629043579, -0.9160400629043579, -0.9160400629043579, -0.27734241187572484, -0.27734241187572484, -0.27734241187572484, -0.27734241187572484, -0.27734241187572484, -0.8886896371841431, -0.8886896371841431, -0.8886896371841431, -0.4947009444236756, -0.4947009444236756, -0.4947009444236756, -0.4947009444236756, -0.4947009444236756, -0.9763351678848267, -0.9763351678848267, 0.0, 0.0, -0.708477000395457, -0.708477000395457, -0.708477000395457, -0.5054029226303101, -0.5054029226303101, -1.0304436683654785, -1.0304436683654785, -0.3818865299224854, -0.3818865299224854, -0.3818865299224854, -0.3818865299224854, -0.3818865299224854, -0.43281896909077955, -0.43281896909077955, -0.43281896909077955, -0.4759228229522705, -0.4759228229522705, -0.4759228229522705, -0.36305181185404467, -0.36305181185404467, -0.36305181185404467, -0.41445085406303406, -0.41445085406303406, -0.41445085406303406, -0.41445085406303406, -0.41445085406303406, -0.271582442522049, -0.271582442522049, -0.271582442522049, -0.271582442522049, -0.271582442522049, -0.8039600451787312, -0.8039600451787312, -0.8039600451787312, -0.9059752225875854, -0.9059752225875854, -0.9763351678848267, -0.9763351678848267, -0.605069359143575, -0.605069359143575, -0.605069359143575, -0.605069359143575, -0.6692613363265991, -0.6692613363265991, -0.6087858279546101, -0.6087858279546101, -0.6087858279546101, -0.261852890253067, -0.261852890253067, -0.261852890253067, -0.261852890253067, -0.261852890253067, -0.31733731031417856, -0.31733731031417856, -0.31733731031417856, -0.31733731031417856, -0.31733731031417856, -0.8151392141977947, -0.8151392141977947, -0.8151392141977947, -0.9763351678848267, -0.9763351678848267, -0.5772706667582195, -0.5772706667582195, -0.5772706667582195, -0.7686690092086792, -0.7686690092086792, -0.7686690092086792, -0.7686690092086792, -0.7814266681671143, -0.7814266681671143, -0.44676318764686584, -0.44676318764686584, -0.44676318764686584, -0.44676318764686584, -0.5974828998247783, -0.5974828998247783, -0.5974828998247783, -0.39288055896759033, -0.39288055896759033, -0.39288055896759033, -0.39288055896759033, -0.9695850610733032, -0.9695850610733032, -0.5758932828903198, -0.5758932828903198, -0.23674672842025757, -0.23674672842025757, -0.23674672842025757, 0.0, 0.0, -0.5048846364021302, -0.5048846364021302, -0.5048846364021302, -0.5048846364021302, -0.5048846364021302, 0.0, 0.0, -1.0451951026916504, -1.0451951026916504, -0.23674672842025757, -0.23674672842025757, -0.23674672842025757, -0.643896222114563, -0.643896222114563, -0.8146444161732991, -0.8146444161732991, -0.8146444161732991, -0.4479830265045166, -0.4479830265045166, -0.4479830265045166, -0.4479830265045166, -0.7428996562957764, -0.7428996562957764, -1.0872995853424072, -1.0872995853424072, -0.7428996562957764, -0.7428996562957764, -0.6482468366622924, -0.6482468366622924, -0.6482468366622924, -0.6482468366622924, -0.6482468366622924, -0.9030889272689819, -0.9030889272689819, -0.33778617779413866, -0.33778617779413866, -0.33778617779413866, -0.33778617779413866, -0.7428996562957764, -0.7428996562957764, -0.5761185487111409, -0.5761185487111409, -0.5761185487111409, -0.5761185487111409, -0.6662006378173828, -0.6662006378173828, -0.7428996562957764, -0.7428996562957764, -0.19493544101715088, -0.19493544101715088, -0.32680698235829664, -0.32680698235829664, -0.32680698235829664, -0.32680698235829664, -0.6751430829366047, -0.6751430829366047, -0.6751430829366047, -0.4479830265045166, -0.4479830265045166, -0.4479830265045166, -0.4479830265045166, -0.5862549940745037, -0.5862549940745037, -0.5862549940745037, -0.5036442279815674, -0.5036442279815674, -0.5036442279815674, -0.5036442279815674, -0.02878827850023913, -0.02878827850023913, -0.02878827850023913, -0.02878827850023913, -0.27676566441853834, -0.27676566441853834, -0.27676566441853834, 0.0, 0.0, -0.9152554273605347, -0.9152554273605347, -0.7396922906239827, -0.7396922906239827, -0.7396922906239827, -0.8298994302749634, -0.8298994302749634, -0.8298994302749634, -0.4479830265045166, -0.4479830265045166, -0.4479830265045166, -0.4479830265045166, -0.714685320854187, -0.714685320854187, -0.8080346584320068, -0.8080346584320068, -0.8080346584320068, -0.7544515629609425, -0.7544515629609425, -0.7544515629609425, -0.7544515629609425, -0.7995307644208272, -0.7995307644208272, -0.7995307644208272, -0.7995307644208272, -0.5662023226420085, -0.5662023226420085, -0.5662023226420085, -0.7396922906239827, -0.7396922906239827, -0.7396922906239827, -0.7428996562957764, -0.7428996562957764, -0.7396922906239827, -0.7396922906239827, -0.7396922906239827, -0.7428996562957764, -0.7428996562957764, -0.5758932828903198, -0.5758932828903198, 0.0, 0.0, 0.0, 0.0, -0.40719119707743334, -0.40719119707743334, -0.40719119707743334, -0.4698578000068665, -0.4698578000068665, -0.4698578000068665, -0.4698578000068665, -0.4698578000068665, -0.42606762647628793, -0.42606762647628793, -0.42606762647628793, -0.42606762647628793, -0.42606762647628793, -0.533038040002187, -0.533038040002187, -0.533038040002187, -0.533038040002187, -0.48596496880054474, -0.48596496880054474, -0.48596496880054474, -0.48596496880054474, -1.1848896344502768, -1.1848896344502768, -1.1848896344502768, -0.05846847593784332, -0.05846847593784332, -0.05846847593784332, -0.05846847593784332, -0.05846847593784332, -0.24689952035744978, -0.24689952035744978, -0.24689952035744978, -0.24689952035744978, -0.5401058077812195, -0.5401058077812195, -0.5401058077812195, -0.5401058077812195, -0.5401058077812195, -0.6573844313621522, -0.6573844313621522, -0.6573844313621522, -0.6573844313621522, -0.6573844313621522, 0.0, 0.0, -0.3888101577758789, -0.3888101577758789, -0.5666501522064209, -0.5666501522064209, 0.0, 0.0, -0.25635953744252515, -0.25635953744252515, -0.25635953744252515, -0.21956475575764967, -0.21956475575764967, -0.21956475575764967, 0.287336602807045, 0.287336602807045, 0.287336602807045, 0.287336602807045, -0.9827066659927368, -0.9827066659927368, -1.0673260291417441, -1.0673260291417441, -1.0673260291417441, -0.6511244773864746, -0.6511244773864746, -0.6511244773864746, -0.4694815079371135, -0.4694815079371135, -0.4694815079371135, -0.4694815079371135, -0.2147863626480102, -0.2147863626480102, -0.2147863626480102, -0.2147863626480102, -0.2147863626480102, -0.1822212874889373, -0.1822212874889373, -0.1822212874889373, -0.1822212874889373, -0.1822212874889373, 0.08193282286326087, 0.08193282286326087, 0.08193282286326087, 0.0, 0.0, -0.321032206217448, -0.321032206217448, -0.321032206217448, -0.31466751297314954, -0.31466751297314954, -0.31466751297314954, -0.31466751297314954, 0.21510648727416992, 0.21510648727416992, 0.21510648727416992, 0.21510648727416992, -0.03452713290850329, -0.03452713290850329, -0.03452713290850329, -0.03452713290850329, -0.12163143157958989, -0.12163143157958989, -0.12163143157958989, -0.12163143157958989, -0.12163143157958989, -0.47286132971445727, -0.47286132971445727, -0.47286132971445727, -0.522966742515564, -0.522966742515564, -0.522966742515564, -0.8885728120803833, -0.8885728120803833, -0.6486187378565471, -0.6486187378565471, -0.6486187378565471, -0.6486187378565471, -0.6486187378565471, -0.6486187378565471, -0.8885728120803833, -0.8885728120803833, -0.780597488085429, -0.780597488085429, -0.780597488085429, 0.013269782066345215, 0.013269782066345215, 0.013269782066345215, -0.00037274758021044185, -0.00037274758021044185, -0.00037274758021044185, -0.00037274758021044185, -0.10383085409800219, -0.10383085409800219, -0.10383085409800219, -0.4870429833730061, -0.4870429833730061, -0.4870429833730061, -0.6717890501022339, -0.6717890501022339, -1.3413116931915283, -1.3413116931915283, -0.5805362065633137, -0.5805362065633137, -0.5805362065633137, -0.2550853073596955, -0.2550853073596955, -0.2550853073596955, -0.2550853073596955, -0.2550853073596955, -0.09099733829498291, -0.09099733829498291, -0.3657241463661194, -0.3657241463661194, -0.3657241463661194, -0.3657241463661194, -1.3413116931915283, -1.3413116931915283, 0.03145911693573, 0.03145911693573, 0.03145911693573, 0.03145911693573, 0.03145911693573, -0.6993430852890015, -0.6993430852890015, -0.6993430852890015, -0.34535353332757945, -0.34535353332757945, -0.34535353332757945, -0.34535353332757945, -0.34535353332757945, -0.7524559855461121, -0.7524559855461121, -0.7524559855461121, -0.7524559855461121, -0.7524559855461121, 0.03700752258300777, 0.03700752258300777, 0.03700752258300777, 0.03700752258300777, 0.03700752258300777, -1.2768049240112305, -1.2768049240112305, 0.1733394066492716, 0.1733394066492716, 0.1733394066492716, -0.3987315833568572, -0.3987315833568572, -0.3987315833568572, -0.3987315833568572, -0.3987315833568572, -0.06250144044558215, -0.06250144044558215, -0.06250144044558215, -0.06250144044558215, -0.11331045627593994, -0.11331045627593994, -0.3468875288963318, -0.3468875288963318, -0.3468875288963318, -0.3468875288963318, -0.3468875288963318, -0.2930502355098725, -0.2930502355098725, -0.2930502355098725, -0.2930502355098725, -0.2930502355098725, -0.12080800533294678, -0.12080800533294678, -0.12080800533294678, -0.5821510155995686, -0.5821510155995686, -0.5821510155995686, -0.5821510155995686, -0.3191331624984741, -0.3191331624984741, -0.3191331624984741, -0.9149626890818279, -0.9149626890818279, -0.9149626890818279, -0.12315738201141357, -0.12315738201141357, -0.12315738201141357, 0.0, 0.0, -0.29427533745765677, -0.29427533745765677, -0.29427533745765677, -0.29427533745765677, -0.29427533745765677, -0.7059061129887898, -0.7059061129887898, -0.7059061129887898, -0.6977224349975586, -0.6977224349975586, -0.7951362530390422, -0.7951362530390422, -0.7951362530390422, -0.5533021688461304, -0.5533021688461304, -0.5533021688461304, -0.5533021688461304, -0.5533021688461304, -0.18865123391151428, -0.18865123391151428, -0.18865123391151428, -0.18865123391151428, -0.18865123391151428, -0.3308500349521637, -0.3308500349521637, -0.3308500349521637, -0.3308500349521637, 0.028862804174423218, 0.028862804174423218, 0.028862804174423218, 0.028862804174423218, 0.028862804174423218, -0.5388449728488922, -0.5388449728488922, -0.5388449728488922, -0.5388449728488922, -1.031745195388794, -1.031745195388794, -0.8488097190856934, -0.8488097190856934, -0.8488097190856934, -0.7483928203582764, -0.7483928203582764, -0.7483928203582764, -0.7483928203582764, -0.43981810808181754, -0.43981810808181754, -0.43981810808181754, -0.43981810808181754, -0.43981810808181754, -0.42160940170288086, -0.42160940170288086, -0.565419356028239, -0.565419356028239, -0.565419356028239, -0.565419356028239, -0.12134920060634613, -0.12134920060634613, -0.12134920060634613, -0.12134920060634613, -0.42160940170288086, -0.42160940170288086, -0.44183740615844735, -0.44183740615844735, -0.44183740615844735, -0.44183740615844735, -0.44183740615844735, -0.26291469732920336, -0.26291469732920336, -0.26291469732920336, -0.3244812389214833, -0.3244812389214833, -0.3244812389214833, -0.3244812389214833, -0.6502071619033813, -0.6502071619033813, -0.7033992211023967, -0.7033992211023967, -0.7033992211023967, -0.5420995404322941, -0.5420995404322941, -0.5420995404322941, -0.5420995404322941, -0.5137497186660767, -0.5137497186660767, -0.644167423248291, -0.644167423248291, -0.644167423248291, -0.644167423248291, -0.644167423248291, -0.012258370717366462, -0.012258370717366462, -0.012258370717366462, -0.012258370717366462, -0.06398773789405832, -0.06398773789405832, -0.06398773789405832, -0.06398773789405832, -0.06398773789405832, -0.8057190577189128, -0.8057190577189128, -0.8057190577189128, -0.5875904162724812, -0.5875904162724812, -0.5875904162724812, -0.5875904162724812, -0.07425494790077214, -0.07425494790077214, -0.07425494790077214, -0.07425494790077214, -0.07425494790077214, -1.0131912231445312, -1.0131912231445312, -0.33164942264556885, -0.33164942264556885, -0.33164942264556885, -0.33164942264556885, -0.46857462326685595, -0.46857462326685595, -0.46857462326685595, -0.46857462326685595, -0.14968453844388319, -0.14968453844388319, -0.14968453844388319, -0.14968453844388319, -0.04814732074737549, -0.04814732074737549, -0.04814732074737549, 0.0147924770911535, 0.0147924770911535, 0.0147924770911535, 0.0147924770911535, -0.14535021781921387, -0.14535021781921387, -0.14535021781921387, -0.08278012275695801, -0.08278012275695801, -0.08278012275695801, -0.14535021781921387, -0.14535021781921387, -0.14535021781921387, 0.1463007926940918, 0.1463007926940918, 0.1463007926940918, -0.08176529407501221, -0.08176529407501221, -0.14535021781921387, -0.14535021781921387, -0.14535021781921387, 0.20712310572465265, 0.20712310572465265, 0.20712310572465265, 0.20712310572465265, -0.1415649652481079, -0.1415649652481079, 0.10613354444503786, 0.10613354444503786, 0.10613354444503786, 0.10613354444503786, 0.10613354444503786, 0.08537721633911133, 0.08537721633911133, 0.08537721633911133, -0.03494457403818774, -0.03494457403818774, -0.03494457403818774, -0.03494457403818774, 0.059877505898475625, 0.059877505898475625, 0.059877505898475625, 0.059877505898475625, 0.059877505898475625, -0.09861385822296143, -0.09861385822296143, -0.09861385822296143, -0.09861385822296143, -0.021933654944102043, -0.021933654944102043, -0.021933654944102043, -0.44641547401746107, -0.44641547401746107, -0.44641547401746107, -0.7493576606114705, -0.7493576606114705, -0.7493576606114705, 0.0, 0.0, -0.004242147008578057, -0.004242147008578057, -0.004242147008578057, -0.004242147008578057, -0.21272039413452148, -0.21272039413452148, -0.8849285840988159, -0.8849285840988159, -0.05772004524866747, -0.05772004524866747, -0.05772004524866747, 0.001441200574239132, 0.001441200574239132, 0.001441200574239132, 0.09181081056594853, 0.09181081056594853, 0.09181081056594853, 0.09181081056594853, 0.09181081056594853, -0.0794419447580974, -0.0794419447580974, -0.0794419447580974, -0.0794419447580974, -1.0430352687835693, -1.0430352687835693, -0.4930813133716583, -0.4930813133716583, -0.4930813133716583, -0.4930813133716583, -0.4930813133716583, -0.9280003309249878, -0.9280003309249878, -0.9280003309249878, 0.071788489818573, 0.071788489818573, 0.071788489818573, -0.0644921064376831, -0.0644921064376831, 0.1082526445388794, 0.1082526445388794, 0.16763555010159814, 0.16763555010159814, 0.16763555010159814, 0.16763555010159814, 0.22278746962547302, 0.22278746962547302, 0.22278746962547302, 0.22278746962547302, 0.17817723751068115, 0.17817723751068115, 0.17817723751068115, 0.5302737355232239, 0.5302737355232239, -0.6920758485794067, -0.6920758485794067, 0.26117573181788123, 0.26117573181788123, 0.26117573181788123, 0.22278746962547302, 0.22278746962547302, 0.22278746962547302, 0.22278746962547302, 0.3211731115976969, 0.3211731115976969, 0.3211731115976969, 0.27237325310707095, 0.27237325310707095, 0.27237325310707095, 0.27237325310707095, 0.27237325310707095, 0.12482590476671851, 0.12482590476671851, 0.12482590476671851, 0.12482590476671851, -0.27834794918696093, -0.27834794918696093, -0.27834794918696093, -0.27834794918696093, 0.37479039529959357, 0.37479039529959357, 0.37479039529959357, 0.37479039529959357, 0.2503191828727722, 0.2503191828727722, 0.37478090822696686, 0.37478090822696686, 0.37478090822696686, 0.37478090822696686, -0.2861314217249553, -0.2861314217249553, -0.2861314217249553, -0.8648085594177246, -0.8648085594177246, -0.840774913628896, -0.840774913628896, -0.840774913628896, -0.840774913628896, -0.8648085594177246, -0.8648085594177246, 0.28710373739401496, 0.28710373739401496, 0.28710373739401496, 0.28710373739401496, -0.7777653932571411, -0.7777653932571411, 0.2306160728136698, 0.2306160728136698, 0.2306160728136698, 0.2306160728136698, -0.7498246431350708, -0.7498246431350708, 0.08154190778732295, 0.08154190778732295, 0.08154190778732295, 0.08154190778732295, 0.08154190778732295, -0.35959104696909594, -0.35959104696909594, -0.35959104696909594, -1.214294672012329, -1.214294672012329, -1.1415472030639648, -1.1415472030639648, -0.024320483207702637, -0.024320483207702637, -0.024320483207702637, -1.214294672012329, -1.214294672012329, -0.8014497756958008, -0.8014497756958008, -0.08406978845596313, -0.08406978845596313, -0.08406978845596313, -0.08406978845596313, -1.0029444694519043, -1.0029444694519043, 0.13577942053476966, 0.13577942053476966, 0.13577942053476966, 0.2686324715614319, 0.2686324715614319, -0.41179323196411133, -0.41179323196411133, -0.5210578441619873, -0.5210578441619873, -1.1616268157958984, -1.1616268157958984, -0.7983901898066204, -0.7983901898066204, -0.7983901898066204, -0.7001751661300659, -0.7001751661300659, -0.03850662708282471, -0.03850662708282471, -0.03850662708282471, -0.39164950450261427, -0.39164950450261427, -0.39164950450261427, -0.16902776559193922, -0.16902776559193922, -0.16902776559193922, -0.05910168091456103, -0.05910168091456103, -0.05910168091456103, -0.05910168091456103, -0.05910168091456103, -0.05910168091456103, -0.1813129305839538, -0.1813129305839538, -0.1813129305839538, -0.1813129305839538, -0.1813129305839538, -0.47623491287231445, -0.47623491287231445, -0.47623491287231445, -0.47623491287231445, -0.5966021180152894, -0.5966021180152894, -0.5966021180152894, -0.5966021180152894, -0.5966021180152894, -0.23756539821624756, -0.23756539821624756, 0.04136749505996706, 0.04136749505996706, 0.04136749505996706, 0.04136749505996706, 0.04136749505996706, -0.21396385629971815, -0.21396385629971815, -0.21396385629971815, -0.21396385629971815, -0.653651515642802, -0.653651515642802, -0.653651515642802, -0.2610518584648769, -0.2610518584648769, -0.2610518584648769, -0.2610518584648769, -0.2610518584648769, -0.2610518584648769, -0.2610518584648769, -0.2610518584648769, -0.6985868215560913, -0.6985868215560913, -0.7011806964874268, -0.7011806964874268, -0.5724854071935017, -0.5724854071935017, -0.5724854071935017, -0.6985868215560913, -0.6985868215560913, -0.5086529453595479, -0.5086529453595479, -0.5086529453595479, -0.5086529453595479, -0.4845472971598308, -0.4845472971598308, -0.4845472971598308, -0.9697881937026978, -0.9697881937026978, -0.37420177459716797, -0.37420177459716797, -0.37420177459716797, -0.8134387731552124, -0.8134387731552124, -0.06013673543930054, -0.06013673543930054, -0.06013673543930054, -0.06013673543930054, -0.06013673543930054, -0.69344957669576, -0.69344957669576, -0.69344957669576, -1.312455654144287, -1.312455654144287, 0.023675251007080123, 0.023675251007080123, 0.023675251007080123, 0.023675251007080123, 0.023675251007080123, -0.6132433414459229, -0.6132433414459229, -0.5140310923258464, -0.5140310923258464, -0.5140310923258464, -0.2903289000193279, -0.2903289000193279, -0.2903289000193279, -0.2903289000193279, -0.26668059825897217, -0.26668059825897217, -0.7921002904574077, -0.7921002904574077, -0.7921002904574077, -0.7921002904574077, -0.7201165358225505, -0.7201165358225505, -0.7201165358225505, 0.04499819874763489, 0.04499819874763489, 0.04499819874763489, 0.04499819874763489, 0.08594337105751038, 0.08594337105751038, 0.08594337105751038, 0.08594337105751038, -0.5113772948582966, -0.5113772948582966, -0.5113772948582966, -0.6918827493985493, -0.6918827493985493, -0.6918827493985493, -0.6918827493985493, -0.47987171014149976, -0.47987171014149976, -0.47987171014149976, -0.06698279529809947, -0.06698279529809947, -0.06698279529809947, -0.06698279529809947, -0.06698279529809947, -0.08320915699005127, -0.08320915699005127, -0.09379839897155762, -0.09379839897155762, -0.09379839897155762, -0.4353285630544026, -0.4353285630544026, -0.4353285630544026, -0.9714285135269165, -0.9714285135269165, 0.0, 0.0, -0.5504187345504761, -0.5504187345504761, -0.5504187345504761, 0.0, 0.0, -0.8999227285385132, -0.8999227285385132, -0.9917914867401123, -0.9917914867401123, -1.342937707901001, -1.342937707901001, 0.0, 0.0, -1.2744224071502686, -1.2744224071502686, -1.2744224071502686, -1.2744224071502686, -0.690436065196991, -0.690436065196991, -0.690436065196991, -0.690436065196991, 0.0, 0.0, -0.2371867299079895, -0.2371867299079895, -0.2371867299079895, -0.2371867299079895, -0.2371867299079895, -0.2371867299079895, -0.2371867299079895, -0.2371867299079895, -0.2371867299079895, -0.2371867299079895, -0.2371867299079895, -0.2371867299079895, -1.3287177085876465, -1.3287177085876465, -1.0500211715698242, -1.0500211715698242, -0.8818439245223999, -0.8818439245223999, -0.8818439245223999, -0.8818439245223999, -0.6183969974517822, -0.6183969974517822, -0.567140390475591, -0.567140390475591, -0.567140390475591, -0.567140390475591, -0.2546881238619487, -0.2546881238619487, -0.2546881238619487, -0.5773477554321289, -0.5773477554321289, -0.5773477554321289, -0.5773477554321289, -0.5773477554321289, -0.3709020415941875, -0.3709020415941875, -0.3709020415941875, -0.3709020415941875, -0.5795400540033977, -0.5795400540033977, -0.5795400540033977, -0.6012012163798015, -0.6012012163798015, -0.6012012163798015, -0.8842003345489502, -0.8842003345489502, -0.8842003345489502, -0.8842003345489502, -0.7364704608917236, -0.7364704608917236, -0.7364704608917236, -0.30791107416152963, -0.30791107416152963, -0.30791107416152963, -0.30791107416152963, -0.30791107416152963, 0.0, 0.0, -0.6568431258201599, -0.6568431258201599, -0.6568431258201599, -0.6568431258201599, 0.0, 0.0, -0.900283932685852, -0.900283932685852, -0.5002041260401409, -0.5002041260401409, -0.5002041260401409, -0.9714285135269165, -0.9714285135269165, -1.0127556324005127, -1.0127556324005127, -0.22034631669521332, -0.22034631669521332, -0.22034631669521332, -0.22034631669521332, -0.9291704893112183, -0.9291704893112183, -0.19434276819229135, -0.19434276819229135, -0.19434276819229135, -0.19434276819229135, -0.19434276819229135, -0.4851948618888855, -0.4851948618888855, -0.4851948618888855, -0.4851948618888855, -0.3469942410786946, -0.3469942410786946, -0.3469942410786946, -0.43939499060312914, -0.43939499060312914, -0.43939499060312914, -0.9539796511332195, -0.9539796511332195, -0.9539796511332195, -0.391957664489746, -0.391957664489746, -0.391957664489746, -0.391957664489746, -0.391957664489746, 0.0, 0.0, 0.0, 0.0, -0.4643988609313965, -0.4643988609313965, -0.17601728439331055, -0.17601728439331055, 0.0, 0.0, -0.2995817263921101, -0.2995817263921101, -0.2995817263921101, -0.2995817263921101, -0.12512648105621338, -0.12512648105621338, -0.29555476705233263, -0.29555476705233263, -0.29555476705233263, -0.29555476705233263, -1.0311729907989502, -1.0311729907989502, -0.521942635377248, -0.521942635377248, -0.521942635377248, -0.521942635377248, -0.5172749757766724, -0.5172749757766724, -0.5172749757766724, -0.5172749757766724, -0.08136789004007983, -0.08136789004007983, -0.08136789004007983, -0.30153261423110966, -0.30153261423110966, -0.30153261423110966, -0.30153261423110966, -0.30153261423110966, -0.3713983952999116, -0.3713983952999116, -0.3713983952999116, -0.3713983952999116, -0.3713983952999116, -0.5874367554982503, -0.5874367554982503, -0.5874367554982503, -0.7341340780258179, -0.7341340780258179, -0.7341340780258179, -0.13028244177500414, -0.13028244177500414, -0.13028244177500414, -0.857806921005249, -0.857806921005249, -0.40519253412882494, -0.40519253412882494, -0.40519253412882494, -0.44685792922973633, -0.44685792922973633, -0.19370908538500475, -0.19370908538500475, -0.19370908538500475, -0.19370908538500475, -1.1662802696228027, -1.1662802696228027, -0.26631476481755567, -0.26631476481755567, -0.26631476481755567, -0.26631476481755567, -1.2222223281860352, -1.2222223281860352, -0.49231728315353385, -0.49231728315353385, -0.49231728315353385, -0.49231728315353385, -0.49231728315353385, -0.444218357404073, -0.444218357404073, -0.444218357404073, -0.5959681669871013, -0.5959681669871013, -0.5959681669871013, -0.7860925197601318, -0.7860925197601318, 0.0, 0.0, -0.6332265138626099, -0.6332265138626099, -0.6332265138626099, -0.6332265138626099, -0.5994146466255188, -0.5994146466255188, -0.5994146466255188, -0.5994146466255188, -1.0500106811523438, -1.0500106811523438, -0.2636712888876598, -0.2636712888876598, -0.2636712888876598, -0.2636712888876598, -0.38891851902008057, -0.38891851902008057, -0.38891851902008057, -0.08997283379236864, -0.08997283379236864, -0.08997283379236864, 0.34879499673843384, 0.34879499673843384, -0.05003023147583008, -0.05003023147583008, -0.05003023147583008, -0.1510937511920929, -0.1510937511920929, -0.1510937511920929, -0.1510937511920929, -0.03687078356742868, -0.03687078356742868, -0.03687078356742868, -0.03687078356742868, -0.03687078356742868, 0.02536238233248389, 0.02536238233248389, 0.02536238233248389, 0.02536238233248389, -0.8903993368148804, -0.8903993368148804, -0.8903993368148804, -1.143107295036316, -1.143107295036316, -1.143107295036316, -0.7738251686096191, -0.7738251686096191, -0.7738251686096191, -0.9197878837585449, -0.9197878837585449, -0.9197878837585449, -1.2275171279907227, -1.2275171279907227, -0.5834355354309082, -0.5834355354309082, -0.09892634749412532, -0.09892634749412532, -0.09892634749412532, -0.09892634749412532, -0.09892634749412532, 0.0, 0.0, -0.874359130859375, -0.874359130859375, 0.04161207377910614, 0.04161207377910614, 0.04161207377910614, 0.04161207377910614, -0.9056934118270874, -0.9056934118270874, -0.9413446187973022, -0.9413446187973022, -0.554958063364029, -0.554958063364029, -0.554958063364029, -0.554958063364029, -0.554958063364029, -0.49200145403544115, -0.49200145403544115, -0.49200145403544115, -0.49200145403544115, -0.8952534993489583, -0.8952534993489583, -0.8952534993489583, -0.6912538011868794, -0.6912538011868794, -0.6912538011868794, -0.6912538011868794, -0.7522276639938354, -0.7522276639938354, -0.7522276639938354, -0.40425002574920654, -0.40425002574920654, -1.5655171871185303, -1.5655171871185303, -0.6366396347681682, -0.6366396347681682, -0.6366396347681682, -0.6366396347681682, 0.0, 0.0, -0.47878734270731615, -0.47878734270731615, -0.47878734270731615, -0.799747109413147, -0.799747109413147, -0.375667115052541, -0.375667115052541, -0.375667115052541, -0.375667115052541, -0.41673223177591967, -0.41673223177591967, -0.41673223177591967, -0.41673223177591967, -0.10252495110034943, -0.10252495110034943, -0.10252495110034943, -0.10252495110034943, -0.21605219443639112, -0.21605219443639112, -0.21605219443639112, -0.07955760757128405, -0.07955760757128405, -0.07955760757128405, -0.07955760757128405, -0.11961444218953443, -0.11961444218953443, -0.11961444218953443, -0.15353965163230887, -0.15353965163230887, -0.15353965163230887, -0.15353965163230887, -0.15353965163230887, -0.5697368383407593, -0.5697368383407593, -0.5316096544265747, -0.5316096544265747, -1.0417125225067139, -1.0417125225067139, 0.0, 0.0, -0.7088913321495056, -0.7088913321495056, -0.7088913321495056, -0.7088913321495056, -0.19120403130849195, -0.19120403130849195, -0.19120403130849195, -0.19120403130849195, -0.1929633617401123, -0.1929633617401123, -0.2371271967887878, -0.2371271967887878, -0.2371271967887878, -0.2371271967887878, -0.2371271967887878, -0.4991261959075928, -0.4991261959075928, 0.1412115573883057, 0.1412115573883057, 0.1412115573883057, 0.1412115573883057, 0.1412115573883057, -0.02252066135406494, -0.02252066135406494, -0.7093042532602947, -0.7093042532602947, -0.7093042532602947, -0.26202070713043213, -0.26202070713043213, -0.26202070713043213, -0.4956226944923401, -0.4956226944923401, -0.4956226944923401, -0.4956226944923401, -0.7950665553410847, -0.7950665553410847, -0.7950665553410847, -0.460918664932251, -0.460918664932251, -0.460918664932251, -0.460918664932251, -1.2395081520080566, -1.2395081520080566, -0.27279746532440186, -0.27279746532440186, -0.27279746532440186, -0.7459039092063904, -0.7459039092063904, -0.7459039092063904, -0.7459039092063904, -0.40814570585886645, -0.40814570585886645, -0.40814570585886645, -0.2483419974644978, -0.2483419974644978, -0.2483419974644978, -0.3706970612208049, -0.3706970612208049, -0.3706970612208049, -0.3706970612208049, -0.12288278341293335, -0.12288278341293335, -0.12288278341293335, -0.12288278341293335, -0.5455026229222615, -0.5455026229222615, -0.5455026229222615, -0.37585151195526123, -0.37585151195526123, -0.30952662229537964, -0.30952662229537964, -0.30952662229537964, -0.290693461894989, -0.290693461894989, -0.290693461894989, -0.290693461894989, 0.2259710431098938, 0.2259710431098938, 0.47090488175551093, 0.47090488175551093, 0.47090488175551093, 0.47090488175551093, 0.4462488770484925, 0.4462488770484925, 0.4462488770484925, 0.4462488770484925, 0.4462488770484925, 0.0, 0.0, 0.3056516647338867, 0.3056516647338867, 0.3056516647338867, 0.0, 0.0, -1.5181899070739746, -1.5181899070739746, -0.5146450638771056, -0.5146450638771056, -0.5146450638771056, -0.5146450638771056, -0.5146450638771056, 0.2147102952003479, 0.2147102952003479, 0.2147102952003479, -0.22598377863566088, -0.22598377863566088, -0.22598377863566088, 0.08993052442868554, 0.08993052442868554, 0.08993052442868554, 0.08993052442868554, -0.14285552501678467, -0.14285552501678467, 0.011026819547017452, 0.011026819547017452, 0.011026819547017452, -0.3749316930770874, -0.3749316930770874, 0.1288921584685644, 0.1288921584685644, 0.1288921584685644, 0.1288921584685644, 0.1171862930059433, 0.1171862930059433, 0.1171862930059433, 0.1171862930059433, 0.582409034172694, 0.582409034172694, 0.582409034172694, 0.4999319314956665, 0.4999319314956665, -0.826360821723938, -0.826360821723938, -0.043508609135945564, -0.043508609135945564, -0.043508609135945564, -0.043502887090047127, -0.043502887090047127, -0.043502887090047127, -0.8378374973932903, -0.8378374973932903, -0.8378374973932903, -0.34786224365234375, -0.34786224365234375, -0.2742723226547241, -0.2742723226547241, -0.2742723226547241, -0.45940113067626953, -0.45940113067626953, -0.2041733086109161, -0.2041733086109161, -0.2041733086109161, -0.2041733086109161, -0.2041733086109161, -0.2041733086109161, -0.2041733086109161, -0.2041733086109161, -0.2041733086109161, -0.2041733086109161, -0.09113895893096924, -0.09113895893096924, -0.5026245911916096, -0.5026245911916096, -0.5026245911916096, -0.4110714395840962, -0.4110714395840962, -0.4110714395840962, -0.4110714395840962, -0.4110714395840962, -0.4110714395840962, -0.4110714395840962, -0.4110714395840962, 0.09090405702590942, 0.09090405702590942, -0.3766875664393108, -0.3766875664393108, -0.3766875664393108, -0.5661372343699138, -0.5661372343699138, -0.5661372343699138, -0.5277491211891174, -0.5277491211891174, -0.5277491211891174, -0.5277491211891174, 0.08332574367523193, 0.08332574367523193, -1.0692810217539468, -1.0692810217539468, -1.0692810217539468, -0.2852971156438191, -0.2852971156438191, -0.2852971156438191, -0.05510156154632573, -0.05510156154632573, -0.05510156154632573, -0.05510156154632573, -0.05510156154632573, -0.6797896027565002, -0.6797896027565002, -0.6797896027565002, -0.6797896027565002, -0.42141245802243543, -0.42141245802243543, -0.42141245802243543, -0.42141245802243543, -0.6476871172587078, -0.6476871172587078, -0.6476871172587078, -0.23825454711914062, -0.23825454711914062, -0.5873777866363525, -0.5873777866363525, -0.5873777866363525, -0.32265399694442753, -0.32265399694442753, -0.32265399694442753, -0.32265399694442753, -0.32265399694442753, -0.2887493371963501, -0.2887493371963501, -0.2887493371963501, -0.6151448488235474, -0.6151448488235474, -0.6151448488235474, -0.3476744294166565, -0.3476744294166565, -0.3476744294166565, -0.3476744294166565, -0.5494341075420379, -0.5494341075420379, -0.5494341075420379, -0.5494341075420379, -0.5494341075420379, -0.1702918062607448, -0.1702918062607448, -0.1702918062607448, -0.1702918062607448, -0.23826713263988486, -0.23826713263988486, -0.23826713263988486, -0.23826713263988486, -0.23826713263988486, -0.5659207900365193, -0.5659207900365193, -0.5659207900365193, -0.362030029296875, -0.362030029296875, -0.29291224479675293, -0.29291224479675293, -0.29291224479675293, -0.017700552940368652, -0.017700552940368652, -0.017700552940368652, -0.017700552940368652, -0.017700552940368652, -0.29291224479675293, -0.29291224479675293, -0.29291224479675293, -0.266262412071228, -0.266262412071228, -0.266262412071228, -0.266262412071228, -0.4742088317871094, -0.4742088317871094, -0.3620641231536865, -0.3620641231536865, -0.36183273792266846, -0.36183273792266846, -0.5240490833918254, -0.5240490833918254, -0.5240490833918254, -0.14729420344034838, -0.14729420344034838, -0.14729420344034838, -0.11570858955383301, -0.11570858955383301, -0.3486176530520122, -0.3486176530520122, -0.3486176530520122, -0.3486176530520122, -0.38185346126556396, -0.38185346126556396, -0.305716872215271, -0.305716872215271, -0.6367921829223633, -0.6367921829223633, -0.6367921829223633, -0.03257552782694506, -0.03257552782694506, -0.03257552782694506, -0.03257552782694506, -0.06436268091201791, -0.06436268091201791, -0.06436268091201791, -0.06436268091201791, -0.06436268091201791, -0.305716872215271, -0.305716872215271, -0.1602144936720531, -0.1602144936720531, -0.1602144936720531, -0.1602144936720531, -1.3091325759887695, -1.3091325759887695, -0.26190757751464844, -0.26190757751464844, -0.26190757751464844, -0.2853538145621617, -0.2853538145621617, -0.2853538145621617, -0.2853538145621617, -0.918497125307719, -0.918497125307719, -0.918497125307719, -0.7768155733744304, -0.7768155733744304, -0.7768155733744304, -0.09552864730358124, -0.09552864730358124, -0.09552864730358124, -0.09552864730358124, -0.09552864730358124, -0.15981920560201002, -0.15981920560201002, -0.15981920560201002, 0.0, 0.0, -0.22648768623669935, -0.22648768623669935, -0.22648768623669935, -0.22648768623669935, 0.0, 0.0, -0.9332886934280396, -0.9332886934280396, -0.6999502182006836, -0.6999502182006836, -0.39978575706481934, -0.39978575706481934, -0.39978575706481934, -0.39978575706481934, -0.39978575706481934, -0.39978575706481934, -0.39978575706481934, -0.39978575706481934, -0.39978575706481934, -0.39978575706481934, -0.39978575706481934, -0.39978575706481934, -0.34456214904785165, -0.34456214904785165, -0.34456214904785165, -0.34456214904785165, -0.34456214904785165, 0.009031623601913452, 0.009031623601913452, 0.009031623601913452, 0.009031623601913452, -0.21414937575658155, -0.21414937575658155, -0.21414937575658155, -0.3104434013366699, -0.3104434013366699, -0.7856020927429199, -0.7856020927429199, -0.4984322786331177, -0.4984322786331177, 0.04468703269958496, 0.04468703269958496, 0.09320581456025445, 0.09320581456025445, 0.09320581456025445, 0.09320581456025445, 0.18356500864028935, 0.18356500864028935, 0.18356500864028935, 0.18356500864028935, 0.18356500864028935, 0.09390473365783691, 0.09390473365783691, 0.09390473365783691, 0.09390473365783691, 0.0, 0.0, -0.2067300478617351, -0.2067300478617351, -0.2067300478617351, 0.2709291378657023, 0.2709291378657023, 0.2709291378657023, 0.2709291378657023, -1.5166704654693604, -1.5166704654693604, -0.2774280905723572, -0.2774280905723572, -0.2774280905723572, -0.749672532081604, -0.749672532081604, -0.749672532081604, -0.612886627515157, -0.612886627515157, -0.612886627515157, -0.024322931965192085, -0.024322931965192085, -0.024322931965192085, -0.024322931965192085, 0.0, 0.0, 0.5625991880893707, 0.5625991880893707, 0.5625991880893707, 0.5625991880893707, 0.5625991880893707, -1.3777778148651123, -1.3777778148651123, -0.8734786510467529, -0.8734786510467529, -0.8734786510467529, -0.8734786510467529, -0.8734786510467529, -0.8734786510467529, 0.03397997220357263, 0.03397997220357263, 0.03397997220357263, 0.03397997220357263, -1.2835981845855713, -1.2835981845855713, -0.034996360540390015, -0.034996360540390015, -0.034996360540390015, -0.034996360540390015, -0.034996360540390015, -0.3423118392626445, -0.3423118392626445, -0.3423118392626445, -0.32218355536460885, -0.32218355536460885, -0.32218355536460885, -0.32218355536460885, -0.32218355536460885, 0.041975170373916626, 0.041975170373916626, 0.041975170373916626, 0.041975170373916626, 0.041975170373916626, 0.19637431701024377, 0.19637431701024377, 0.19637431701024377, -0.6332829395929973, -0.6332829395929973, -0.6332829395929973, -0.14574867486953735, -0.14574867486953735, -0.14574867486953735, -0.14574867486953735, -0.6544357140858967, -0.6544357140858967, -0.6544357140858967, -0.6544357140858967, -0.5969763994216919, -0.5969763994216919, -0.4641235272089641, -0.4641235272089641, -0.4641235272089641, 0.3136542618274689, 0.3136542618274689, 0.3136542618274689, 0.3136542618274689, 0.3136542618274689, -0.054363081852595085, -0.054363081852595085, -0.054363081852595085, -0.054363081852595085, 0.19035776853561404, 0.19035776853561404, 0.19035776853561404, 0.19035776853561404, 0.19035776853561404, 0.2818247675895691, 0.2818247675895691, 0.16350722312927246, 0.16350722312927246, 0.24146412809689843, 0.24146412809689843, 0.24146412809689843, -0.6228955189387004, -0.6228955189387004, -0.6228955189387004, 0.2539851019779841, 0.2539851019779841, 0.2539851019779841, 0.2539851019779841, 0.18178778886795044, 0.18178778886795044, 0.08651936054229736, 0.08651936054229736, 0.2937032183011373, 0.2937032183011373, 0.2937032183011373, 0.2937032183011373, -0.6565945148468018, -0.6565945148468018, -0.058858027060826545, -0.058858027060826545, -0.058858027060826545, -0.058858027060826545, 0.19932687282562256, 0.19932687282562256, 0.19932687282562256, -0.7579827308654785, -0.7579827308654785, -0.29670852422714233, -0.29670852422714233, -0.29670852422714233, 0.31140617529551184, 0.31140617529551184, 0.31140617529551184, -0.006950368483861213, -0.006950368483861213, -0.006950368483861213, -0.006950368483861213, 0.20931166410446167, 0.20931166410446167, -0.2718772888183594, -0.2718772888183594, -0.2718772888183594, -0.2718772888183594, -0.2718772888183594, 0.0, 0.0, 0.0, 0.0, -0.3468203445275624, -0.3468203445275624, -0.3468203445275624, -0.3468203445275624, -0.5609484910964966, -0.5609484910964966, -0.5609484910964966, -0.9328118960062664, -0.9328118960062664, -0.9328118960062664, 0.4285088777542114, 0.4285088777542114, 0.4285088777542114, 0.4285088777542114, 0.4285088777542114, 0.28882743418216705, 0.28882743418216705, 0.28882743418216705, 0.28882743418216705, -0.02792871495087934, -0.02792871495087934, -0.02792871495087934, -0.02792871495087934, 0.1894540240367254, 0.1894540240367254, 0.1894540240367254, 0.1894540240367254, -0.5713089307149251, -0.5713089307149251, -0.5713089307149251, 0.2078556617101034, 0.2078556617101034, 0.2078556617101034, 0.33938904106616974, 0.33938904106616974, 0.33938904106616974, 0.3278112545609474, 0.3278112545609474, 0.3278112545609474, 0.3278112545609474, 0.3278112545609474, 0.34406035890181863, 0.34406035890181863, 0.34406035890181863, 0.34406035890181863, 0.25011885166168213, 0.25011885166168213, 0.24090337753295898, 0.24090337753295898, 0.0, 0.0, -0.18664284547169996, -0.18664284547169996, -0.18664284547169996, -0.18664284547169996, -0.5779742797215779, -0.5779742797215779, -0.5779742797215779, -0.62594739596049, -0.62594739596049, -0.62594739596049, -0.09835445880889893, -0.09835445880889893, -0.09835445880889893, -0.996143102645874, -0.996143102645874, -0.9506487051645915, -0.9506487051645915, -0.9506487051645915, -0.5762863556543987, -0.5762863556543987, -0.5762863556543987, -0.39655327796936035, -0.39655327796936035, -0.2733243703842163, -0.2733243703842163, -0.5557982524236043, -0.5557982524236043, -0.5557982524236043, -0.626897931098938, -0.626897931098938, -0.39655327796936035, -0.39655327796936035, -0.4326275587081909, -0.4326275587081909, -0.62594739596049, -0.62594739596049, -0.62594739596049, 0.057275593280792236, 0.057275593280792236, 0.057275593280792236, 0.167181134223938, 0.167181134223938, 0.167181134223938, -0.4292004108428955, -0.4292004108428955, -0.6666666269302368, -0.6666666269302368, 0.3332425355911255, 0.3332425355911255, -0.6666666269302368, -0.6666666269302368, -0.6666666269302368, -0.6666666269302368, -1.3000013828277588, -1.3000013828277588, 0.29785364866256714, 0.29785364866256714, -0.05883445342381788, -0.05883445342381788, -0.05883445342381788, -0.05883445342381788, -0.40750702222188306, -0.40750702222188306, -0.40750702222188306, -0.06731975078582764, -0.06731975078582764, -0.06731975078582764, -0.3599252700805664, -0.3599252700805664, -0.3599252700805664, -0.3599252700805664, -0.29946064949035645, -0.29946064949035645, -0.8287492195765178, -0.8287492195765178, -0.8287492195765178, -0.24746525287628174, -0.24746525287628174, -0.349082350730896, -0.349082350730896, 0.0, 0.0, 0.0024390021959940222, 0.0024390021959940222, 0.0024390021959940222, 0.023799022038777706, 0.023799022038777706, 0.023799022038777706, 0.023799022038777706, 0.21286988258361816, 0.21286988258361816, 0.21286988258361816, 0.13325339406728742, 0.13325339406728742, 0.13325339406728742, 0.13325339406728742, 0.13325339406728742, 0.14880013217528665, 0.14880013217528665, 0.14880013217528665, 0.14880013217528665, -0.41895174980163574, -0.41895174980163574, -0.41895174980163574, -0.41895174980163574, -0.41895174980163574, -0.41895174980163574, 0.11486947089433674, 0.11486947089433674, 0.11486947089433674, 0.11486947089433674, 0.11486947089433674, -0.6182397206624348, -0.6182397206624348, -0.6182397206624348, -0.2675732374191284, -0.2675732374191284, 0.0, 0.0, -0.3358355164527893, -0.3358355164527893, -0.3358355164527893, -0.3358355164527893, 0.16924172639846802, 0.16924172639846802, 0.16924172639846802, 0.16924172639846802, 0.16924172639846802, 0.05555806557337439, 0.05555806557337439, 0.05555806557337439, -0.09998655319213867, -0.09998655319213867, 0.38410314321517947, 0.38410314321517947, 0.38410314321517947, 0.38410314321517947, 0.38410314321517947, 0.5300473421812057, 0.5300473421812057, 0.5300473421812057, 0.5300473421812057, 0.5300473421812057, -0.26626569529374433, -0.26626569529374433, -0.26626569529374433, -0.26626569529374433, -0.2998892664909363, -0.2998892664909363, -0.2998892664909363, -0.2998892664909363, -0.9959156513214111, -0.9959156513214111, 0.0, 0.0, -0.5001751979192097, -0.5001751979192097, -0.5001751979192097, -0.23925443490346265, -0.23925443490346265, -0.23925443490346265, -0.5263280868530273, -0.5263280868530273, -0.10675021012624097, -0.10675021012624097, -0.10675021012624097, -0.1609675188859303, -0.1609675188859303, -0.1609675188859303, -0.1609675188859303, 0.1868648171424866, 0.1868648171424866, 0.1868648171424866, 0.1868648171424866, 0.1868648171424866, -0.15093131860097242, -0.15093131860097242, -0.15093131860097242, -0.3748358885447185, -0.3748358885447185, -0.3748358885447185, -0.5263280868530273, -0.5263280868530273, 0.0, 0.0, -0.8666661977767944, -0.8666661977767944, -0.03380142152309418, -0.03380142152309418, -0.03380142152309418, -0.03380142152309418, -0.4822556177775066, -0.4822556177775066, -0.4822556177775066, -0.19724790255228686, -0.19724790255228686, -0.19724790255228686, -0.2398953676223754, -0.2398953676223754, -0.2398953676223754, -0.2398953676223754, -0.2398953676223754, 0.059732337792714474, 0.059732337792714474, 0.059732337792714474, 0.059732337792714474, -0.16130970915158582, -0.16130970915158582, -0.16130970915158582, -0.16130970915158582, -0.055634379386901855, -0.055634379386901855, 0.059732337792714474, 0.059732337792714474, 0.059732337792714474, 0.059732337792714474, -0.055634379386901855, -0.055634379386901855, 0.16856260299682613, 0.16856260299682613, 0.16856260299682613, 0.16856260299682613, 0.16856260299682613, -0.22591144839922594, -0.22591144839922594, -0.22591144839922594, -0.22591144839922594, -0.6545228958129883, -0.6545228958129883, -0.6853104035059612, -0.6853104035059612, -0.6853104035059612, -0.16701690355936694, -0.16701690355936694, -0.16701690355936694, 0.4759001011649767, 0.4759001011649767, 0.4759001011649767, 0.4759001011649767, 0.571226428449154, 0.571226428449154, 0.571226428449154, 0.571226428449154, 0.571226428449154, 0.5891873786846797, 0.5891873786846797, 0.5891873786846797, -0.055243611335754395, -0.055243611335754395, -0.055243611335754395, -0.8860492706298828, -0.8860492706298828, -1.1826777458190918, -1.1826777458190918, -0.22118908166885376, -0.22118908166885376, -0.22118908166885376, 0.0, 0.0, -0.08600360651810957, -0.08600360651810957, -0.08600360651810957, -0.08600360651810957, 0.05557221174240112, 0.05557221174240112, 0.05557221174240112, 0.05557221174240112, -0.18986231088638306, -0.18986231088638306, -0.18986231088638306, -0.18986231088638306, 0.053399811188379886, 0.053399811188379886, 0.053399811188379886, 0.053399811188379886, -0.25156235694885254, -0.25156235694885254, -0.23439661661783862, -0.23439661661783862, -0.23439661661783862, -0.23439661661783862, 0.0, 0.0, -0.2832245628039043, -0.2832245628039043, -0.2832245628039043, -0.2832245628039043, -0.28532306353251147, -0.28532306353251147, -0.28532306353251147, -0.2573916365702946, -0.2573916365702946, -0.2573916365702946, -0.2573916365702946, -0.4711722135543823, -0.4711722135543823, -0.6180650055408479, -0.6180650055408479, -0.6180650055408479, -0.6180650055408479, -0.6180650055408479, 0.033529917399088505, 0.033529917399088505, 0.033529917399088505, -0.4855440457661946, -0.4855440457661946, -0.4855440457661946, -0.7558386723200481, -0.7558386723200481, -0.7558386723200481, -0.2519817352294922, -0.2519817352294922, -0.4685007333755493, -0.4685007333755493, -0.4685007333755493, 0.2961571315924326, 0.2961571315924326, 0.2961571315924326, 0.2961571315924326, -0.09471754729747772, -0.09471754729747772, -0.09471754729747772, -0.09471754729747772, 0.09453518191973365, 0.09453518191973365, 0.09453518191973365, 0.0003075450658798218, 0.0003075450658798218, 0.0003075450658798218, 0.0003075450658798218, 0.0, 0.0, -0.31647189458211256, -0.31647189458211256, -0.31647189458211256, -0.31647189458211256, -0.31647189458211256, -0.31647189458211256, -0.4517922898133595, -0.4517922898133595, -0.4517922898133595, -0.4517922898133595, -0.3886089324951172, -0.3886089324951172, -0.614399254322052, -0.614399254322052, -0.614399254322052, -0.614399254322052, -0.8947434425354004, -0.8947434425354004, -0.35296404361724854, -0.35296404361724854, -0.35296404361724854, -0.4305816888809204, -0.4305816888809204, -0.2214432954788208, -0.2214432954788208, -0.3081023693084717, -0.3081023693084717, -0.3081023693084717, 0.060773104429244995, 0.060773104429244995, 0.060773104429244995, 0.060773104429244995, 0.060773104429244995, 0.0840482314427694, 0.0840482314427694, 0.0840482314427694, 0.009269336859385136, 0.009269336859385136, 0.009269336859385136, 0.009269336859385136, 0.12803654670715336, 0.12803654670715336, 0.12803654670715336, 0.12803654670715336, 0.12803654670715336, -0.12920993566513062, -0.12920993566513062, -0.12920993566513062, -0.12920993566513062, 0.01709669828414917, 0.01709669828414917, 0.04217161734898889, 0.04217161734898889, 0.04217161734898889, 0.04217161734898889, -0.18966341018676758, -0.18966341018676758, -0.18966341018676758, -0.09831748406092333, -0.09831748406092333, -0.09831748406092333, 0.060773104429244995, 0.060773104429244995, 0.060773104429244995, 0.060773104429244995, 0.060773104429244995, 0.18170255422592163, 0.18170255422592163, 0.18170255422592163, -0.3335449000199635, -0.3335449000199635, -0.3335449000199635, -0.3335449000199635, 0.0, 0.0, 0.01797419786453247, 0.01797419786453247, 0.01797419786453247, -0.0384058952331543, -0.0384058952331543, -0.482840359210968, -0.482840359210968, -0.482840359210968, -0.482840359210968, -0.482840359210968, -0.0133781830469768, -0.0133781830469768, -0.0133781830469768, 0.30984184741973875, 0.30984184741973875, 0.30984184741973875, 0.30984184741973875, 0.30984184741973875, 0.3332880139350891, 0.3332880139350891, 0.08908993005752563, 0.08908993005752563, 0.08908993005752563, 0.16451042890548706, 0.16451042890548706, 0.2797757784525553, 0.2797757784525553, 0.2797757784525553, 0.20483583211898804, 0.20483583211898804, 0.3066238959630331, 0.3066238959630331, 0.3066238959630331, 0.3066238959630331, 0.0, 0.0, 0.0, 0.0, 0.23757544755935667, 0.23757544755935667, 0.23757544755935667, 0.23757544755935667, 0.23757544755935667, -0.6364190578460693, -0.6364190578460693, 0.4102224111557007, 0.4102224111557007, 0.0, 0.0, -0.33336754639943433, -0.33336754639943433, -0.33336754639943433, 0.5979328081011772, 0.5979328081011772, 0.5979328081011772, 0.5979328081011772, 0.5979328081011772, 0.5979328081011772, 0.5979328081011772, 0.5979328081011772, 0.5319340179363887, 0.5319340179363887, 0.5319340179363887, 0.5979328081011772, 0.5979328081011772, 0.5979328081011772, 0.5979328081011772, 0.7959291785955429, 0.7959291785955429, 0.7959291785955429, 0.7959291785955429, 0.7959291785955429, 0.7959291785955429, 0.5319340179363887, 0.5319340179363887, 0.5319340179363887, -0.007638295491536384, -0.007638295491536384, -0.007638295491536384, -0.007638295491536384, -0.22287726402282715, -0.22287726402282715, -0.22287726402282715, -0.22287726402282715, -0.22287726402282715, -0.36459290981292725, -0.36459290981292725, -0.36459290981292725, -0.24431530634562182, -0.24431530634562182, -0.24431530634562182, -0.06239283084869385, -0.06239283084869385, -0.06239283084869385, -0.06239283084869385, -0.06239283084869385, 0.33347443739573157, 0.33347443739573157, 0.33347443739573157, -0.7180423736572266, -0.7180423736572266, 0.0, 0.0, -0.5821011066436768, -0.5821011066436768, 0.0, 0.0, -1.4811880588531494, -1.4811880588531494, -0.07143008708953857, -0.07143008708953857, 0.11839441458384192, 0.11839441458384192, 0.11839441458384192, 0.07311381896336877, 0.07311381896336877, 0.07311381896336877, 0.019585758447647095, 0.019585758447647095, 0.019585758447647095, 0.019585758447647095, 0.024902069568634055, 0.024902069568634055, 0.024902069568634055, 0.024902069568634055, 0.024902069568634055, 0.21944359938303626, 0.21944359938303626, 0.21944359938303626, 0.21944359938303626, -0.22436837355295824, -0.22436837355295824, -0.22436837355295824, 0.039659738540649414, 0.039659738540649414, 0.039659738540649414, -0.6596424182256062, -0.6596424182256062, -0.6596424182256062, 0.0, 0.0, 0.033529917399088505, 0.033529917399088505, 0.033529917399088505, -0.9754202365875244, -0.9754202365875244, 0.0, 0.0, -0.3803442716598511, -0.3803442716598511, -0.3803442716598511, -0.4077949702739716, -0.4077949702739716, -0.4077949702739716, -0.4077949702739716, -0.4077949702739716, -0.20620005329449964, -0.20620005329449964, -0.20620005329449964, -0.20620005329449964, -0.1664175589879353, -0.1664175589879353, -0.1664175589879353, -0.20620005329449964, -0.20620005329449964, -0.20620005329449964, -0.20620005329449964, 0.2863604426383972, 0.2863604426383972, 0.10796036720275881, 0.10796036720275881, 0.10796036720275881, 0.10796036720275881, 0.10796036720275881, -1.3333332538604736, -1.3333332538604736, 0.019616832335790035, 0.019616832335790035, 0.019616832335790035, 0.019616832335790035, 0.020811637242635128, 0.020811637242635128, 0.020811637242635128, 0.12202508449554439, 0.12202508449554439, 0.12202508449554439, 0.12202508449554439, 0.12202508449554439, 0.021293163299560547, 0.021293163299560547, 0.021293163299560547, -1.547619104385376, -1.547619104385376, 0.014640605449676491, 0.014640605449676491, 0.014640605449676491, 0.014640605449676491, 0.014640605449676491, 0.0, 0.0, 0.0, 0.0, -0.8849719762802124, -0.8849719762802124, -0.28310251235961914, -0.28310251235961914, -0.06683113177617384, -0.06683113177617384, -0.06683113177617384, -0.25375295678774523, -0.25375295678774523, -0.25375295678774523, -0.25375295678774523, 0.0, 0.0, -0.665665864944458, -0.665665864944458, 0.29951196908950806, 0.29951196908950806, -1.1000521183013916, -1.1000521183013916, 0.03728359937667847, 0.03728359937667847, 0.03728359937667847, 0.03728359937667847, 0.03728359937667847, 0.03728359937667847, -0.27613627910614014, -0.27613627910614014, -0.27613627910614014, -0.27613627910614014, -0.4817002018292744, -0.4817002018292744, -0.4817002018292744, -0.4817002018292744, -0.38135242462158203, -0.38135242462158203, -0.4068378806114197, -0.4068378806114197, -0.4068378806114197, -0.3041146993637085, -0.3041146993637085, -0.26196845372517896, -0.26196845372517896, -0.26196845372517896, -0.26196845372517896, -1.2393388748168945, -1.2393388748168945, -1.0923233032226562, -1.0923233032226562, 0.06705492734909058, 0.06705492734909058, 0.06705492734909058, -0.44445276260375977, -0.44445276260375977, 0.3332425355911255, 0.3332425355911255, -0.025515598058700517, -0.025515598058700517, -0.025515598058700517, -0.025515598058700517, -0.025515598058700517, 0.0, 0.0, -0.9556176662445068, -0.9556176662445068, 0.04161573946475983, 0.04161573946475983, 0.04161573946475983, 0.04161573946475983, -0.28046678503354383, -0.28046678503354383, -0.28046678503354383, -0.28046678503354383, 0.024373209476470925, 0.024373209476470925, 0.024373209476470925, 0.024373209476470925, 0.024373209476470925, -0.4817394018173218, -0.4817394018173218, -0.9787709315617878, -0.9787709315617878, -0.9787709315617878, -0.44260668754577637, -0.44260668754577637, -0.44260668754577637, -0.44260668754577637, -0.7917825778325398, -0.7917825778325398, -0.7917825778325398, -0.7917825778325398, -0.3803881506125133, -0.3803881506125133, -0.3803881506125133, -0.9512195587158203, -0.9512195587158203, -0.4208290179570515, -0.4208290179570515, -0.4208290179570515, -0.4208290179570515, 0.3899880448977152, 0.3899880448977152, 0.3899880448977152, 0.285697877407074, 0.285697877407074, -0.15979460378487897, -0.15979460378487897, -0.15979460378487897, -0.15979460378487897, 0.032353679339090946, 0.032353679339090946, 0.032353679339090946, 0.2899860044320425, 0.2899860044320425, 0.2899860044320425, 0.2899860044320425, 0.0, 0.0, 0.03220562934875493, 0.03220562934875493, 0.03220562934875493, 0.03220562934875493, 0.03220562934875493, -0.5483028888702393, -0.5483028888702393, -0.5483028888702393, -0.48751441637674975, -0.48751441637674975, -0.48751441637674975, -1.4637975692749023, -1.4637975692749023, -0.042310913403828865, -0.042310913403828865, -0.042310913403828865, -0.3124880790710449, -0.3124880790710449, -0.6253650188446045, -0.6253650188446045, -0.09465914964675903, -0.09465914964675903, -0.09465914964675903, -0.09465914964675903, -1.120976448059082, -1.120976448059082, -0.18079088926315312, -0.18079088926315312, -0.18079088926315312, -0.18079088926315312, -0.18079088926315312, -0.1642122666041057, -0.1642122666041057, -0.1642122666041057, 0.21582570672035217, 0.21582570672035217, 0.21582570672035217, 0.0012575984001159668, 0.0012575984001159668, -0.13734467923641214, -0.13734467923641214, -0.13734467923641214, -0.13734467923641214, -0.13734467923641214, 0.30501664678255713, 0.30501664678255713, 0.30501664678255713, -0.48004774252573656, -0.48004774252573656, -0.48004774252573656, -0.48004774252573656, -0.7431613564491273, -0.7431613564491273, -0.7431613564491273, -0.7431613564491273, -0.7431613564491273, -0.1923205852508545, -0.1923205852508545, -0.3573164939880371, -0.3573164939880371, -0.3573164939880371, -0.1923205852508545, -0.1923205852508545, -0.5895715951919556, -0.5895715951919556, -0.22939960161844897, -0.22939960161844897, -0.22939960161844897, -0.1923205852508545, -0.1923205852508545, 0.14017055432001746, 0.14017055432001746, 0.14017055432001746, -0.19446337223052979, -0.19446337223052979, -0.19446337223052979, -0.19446337223052979, 0.03776775797208154, 0.03776775797208154, 0.03776775797208154, -0.13796242078145338, -0.13796242078145338, -0.13796242078145338, -0.13796242078145338, -0.10148570934931445, -0.10148570934931445, -0.10148570934931445, -0.10148570934931445, -0.40233564376831055, -0.40233564376831055, -0.40233564376831055, -0.40438242753346754, -0.40438242753346754, -0.40438242753346754, -0.538001537322998, -0.538001537322998, 0.3610656062761942, 0.3610656062761942, 0.3610656062761942, 0.2768907994031906, 0.2768907994031906, 0.2768907994031906, 0.2768907994031906, 0.3335427939891815, 0.3335427939891815, 0.3335427939891815, 0.2530640423297882, 0.2530640423297882, 0.2530640423297882, 0.2530640423297882, 0.2530640423297882, 0.3580324649810791, 0.3580324649810791, 0.026483947038650535, 0.026483947038650535, 0.026483947038650535, 0.026483947038650535, 0.026483947038650535, 0.32001672089099886, 0.32001672089099886, 0.32001672089099886, 0.32001672089099886, 0.32001672089099886, 0.03566565116246545, 0.03566565116246545, 0.03566565116246545, 0.23198848962783813, 0.23198848962783813, -0.37862042586008715, -0.37862042586008715, -0.37862042586008715, 0.0, 0.0, -0.9845700263977051, -0.9845700263977051, -0.44361412525177, -0.44361412525177, -0.7761098146438599, -0.7761098146438599, -0.6659469604492188, -0.6659469604492188, -0.6659469604492188, -0.6659469604492188, -0.282630165417989, -0.282630165417989, -0.282630165417989, -0.5715478658676147, -0.5715478658676147, 0.022577941417694092, 0.022577941417694092, 0.022577941417694092, 0.24997729063034058, 0.24997729063034058, 0.0, 0.0, -1.549865484237671, -1.549865484237671, -0.35019017259279894, -0.35019017259279894, -0.35019017259279894, -0.46249163150787354, -0.46249163150787354, -1.011960744857788, -1.011960744857788, -0.7858856916427612, -0.7858856916427612, -0.35764421025911974, -0.35764421025911974, -0.35764421025911974, -0.35764421025911974, 0.0, 0.0, -0.6817004680633545, -0.6817004680633545, 0.0, 0.0, -2.568960189819336e-05, -2.568960189819336e-05, -2.568960189819336e-05, -0.3334534168243408, -0.3334534168243408, -0.3334534168243408, 0.08312944074471795, 0.08312944074471795, 0.08312944074471795, 0.08312944074471795, 0.19540365537007653, 0.19540365537007653, 0.19540365537007653, -0.32986998558044434, -0.32986998558044434, -0.9358872175216675, -0.9358872175216675, -0.9358872175216675, -0.8184804916381836, -0.8184804916381836, 0.0, 0.0, -1.0993521213531494, -1.0993521213531494, -0.6484332084655762, -0.6484332084655762, 0.24936425685882568, 0.24936425685882568, 0.24936425685882568, 0.24936425685882568, -0.14975110689798998, -0.14975110689798998, -0.14975110689798998, -0.09939408302307129, -0.09939408302307129, 0.10856509208679199, 0.10856509208679199, -0.7944689989089966, -0.7944689989089966, -0.26982033252716064, -0.26982033252716064, 0.0, 0.0, 0.1744890809059143, 0.1744890809059143, 0.1170291006565094, 0.1170291006565094, 0.1170291006565094, 0.1170291006565094, 0.1744890809059143, 0.1744890809059143, -0.24061099688212084, -0.24061099688212084, -0.24061099688212084, 0.2114048103491465, 0.2114048103491465, 0.2114048103491465, 0.2114048103491465, 0.0, 0.0, -0.3023720781008403, -0.3023720781008403, -0.3023720781008403, -0.3023720781008403, -0.6674433946609497, -0.6674433946609497, -0.6674433946609497, -0.4418614308039348, -0.4418614308039348, -0.4418614308039348, 0.3605241825183233, 0.3605241825183233, 0.3605241825183233, 0.3605241825183233, 0.0, 0.0, 0.0, 0.0, -0.21432161331176758, -0.21432161331176758, -0.10961566368738818, -0.10961566368738818, -0.10961566368738818, -0.10961566368738818, -1.2764999866485596, -1.2764999866485596, 0.0, 0.0, -0.884246826171875, -0.884246826171875, -0.884246826171875, -0.9284945726394653, -0.9284945726394653, -1.0833332538604736, -1.0833332538604736, -0.40694556633631396, -0.40694556633631396, -0.40694556633631396, -0.40694556633631396, -0.11865458885828661, -0.11865458885828661, -0.11865458885828661, -0.9166008234024048, -0.9166008234024048, -0.8806897401809692, -0.8806897401809692, -0.8806897401809692, -0.6945153872172039, -0.6945153872172039, -0.6945153872172039, -0.6945153872172039, -0.6945153872172039, -0.6945153872172039, -0.6945153872172039, -0.6945153872172039, -0.6945153872172039, -0.037236809730529785, -0.037236809730529785, -0.037236809730529785, -0.47179151574770617, -0.47179151574770617, -0.47179151574770617, -0.47179151574770617, -0.053946420550346375, -0.053946420550346375, -0.053946420550346375, -0.050003747145334954, -0.050003747145334954, -0.050003747145334954, -0.03347637255986524, -0.03347637255986524, -0.03347637255986524, -0.06662726402282715, -0.06662726402282715, -0.06662726402282715, -0.06662726402282715, -0.0033454298973083496, -0.0033454298973083496, -0.0033454298973083496, 0.0, 0.0, 0.3842035323381424, 0.3842035323381424, 0.3842035323381424, 0.3842035323381424, 0.3842035323381424, -0.5211109121640523, -0.5211109121640523, -0.5211109121640523, -0.5211109121640523, 0.49844085872173305, 0.49844085872173305, 0.49844085872173305, 0.49844085872173305, 0.49844085872173305, -0.875, -0.875, -1.349961519241333, -1.349961519241333, -0.7036508321762085, -0.7036508321762085, -0.7036508321762085, -0.7504633665084839, -0.7504633665084839, 0.04760265350341797, 0.04760265350341797, -0.2140580415725708, -0.2140580415725708, -0.5398441950480144, -0.5398441950480144, -0.5398441950480144, 0.26345010598500573, 0.26345010598500573, 0.26345010598500573, 0.1999766230583191, 0.1999766230583191, 0.3238251060247421, 0.3238251060247421, 0.3238251060247421, 0.3238251060247421, 0.14516856273015344, 0.14516856273015344, 0.14516856273015344, 0.25002448757489526, 0.25002448757489526, 0.25002448757489526, 0.0, 0.0, 0.0, 0.0, 0.08976580699284875, 0.08976580699284875, 0.08976580699284875, 0.25002448757489526, 0.25002448757489526, 0.25002448757489526, 0.39817462861537933, 0.39817462861537933, 0.39817462861537933, 0.39817462861537933, 0.39817462861537933, 0.37286674976348877, 0.37286674976348877, 0.37286674976348877, 0.37286674976348877, 0.2610901643832525, 0.2610901643832525, 0.2610901643832525, 0.2610901643832525, 0.08343774080276489, 0.08343774080276489, 0.2051342775424322, 0.2051342775424322, 0.2051342775424322, 0.2051342775424322, 0.24774508674939477, 0.24774508674939477, 0.24774508674939477, 0.08249409000078833, 0.08249409000078833, 0.08249409000078833, -0.6089199781417847, -0.6089199781417847, -0.19928932189941406, -0.19928932189941406, -0.19928932189941406, -0.47384527921676645, -0.47384527921676645, -0.47384527921676645, -0.47384527921676645, -0.47384527921676645, 0.0, 0.0, 0.22336674729983008, 0.22336674729983008, 0.22336674729983008, 0.22336674729983008, -0.10897569855054212, -0.10897569855054212, -0.10897569855054212, -0.10897569855054212, -1.200007677078247, -1.200007677078247, 0.15709655483563745, 0.15709655483563745, 0.15709655483563745, 0.15709655483563745, -0.04762885967890429, -0.04762885967890429, -0.04762885967890429, -0.1976778507232666, -0.1976778507232666, -0.1976778507232666, -0.015479803085327148, -0.015479803085327148, -0.04732954502105713, -0.04732954502105713, 0.0, 0.0, -0.20942566792170214, -0.20942566792170214, -0.20942566792170214, -0.43219188849131274, -0.43219188849131274, -0.43219188849131274, -0.43001508712768555, -0.43001508712768555, -1.3181817531585693, -1.3181817531585693, -0.5212219953536987, -0.5212219953536987, -0.2430290778477986, -0.2430290778477986, -0.2430290778477986, -0.2430290778477986, -0.2430290778477986, -0.2430290778477986, -0.19422060251235962, -0.19422060251235962, -0.19422060251235962, -0.32632305224736524, -0.32632305224736524, -0.32632305224736524, -0.32632305224736524, 0.10211294889450073, 0.10211294889450073, -0.6474946737289429, -0.6474946737289429, -0.6474946737289429, -0.022624289989471524, -0.022624289989471524, -0.022624289989471524, -0.022624289989471524, -0.022624289989471524, 0.05976079901059472, 0.05976079901059472, 0.05976079901059472, 0.05976079901059472, 0.10211294889450073, 0.10211294889450073, 0.10211294889450073, 0.10211294889450073, -0.7981253862380981, -0.7981253862380981, 0.0, 0.0, -0.24951322873433424, -0.24951322873433424, -0.24951322873433424, -0.24951322873433424, 0.05555295944213867, 0.05555295944213867, 0.05555295944213867, 0.21591649452845252, 0.21591649452845252, 0.21591649452845252, 0.21591649452845252, 0.12360227108001709, 0.12360227108001709, 0.12360227108001709, 0.12360227108001709, 0.12360227108001709, -0.23331163326899218, -0.23331163326899218, -0.23331163326899218, -0.3076610962549846, -0.3076610962549846, -0.3076610962549846, -0.3076610962549846, -0.3076610962549846, -0.3076610962549846, 0.0994618684053421, 0.0994618684053421, 0.0994618684053421, 0.0994618684053421, -0.1721569299697876, -0.1721569299697876, 0.0, 0.0, -0.09566378593444824, -0.09566378593444824, 0.28075761198997495, 0.28075761198997495, 0.28075761198997495, 0.28075761198997495, 0.28075761198997495, -0.16778349876403809, -0.16778349876403809, 0.05315859615802765, 0.05315859615802765, 0.05315859615802765, 0.05315859615802765, -0.08399281899134325, -0.08399281899134325, -0.08399281899134325, -0.17214963833491015, -0.17214963833491015, -0.17214963833491015, 0.03683907389640806, 0.03683907389640806, 0.03683907389640806, 0.03683907389640806, 0.03683907389640806, 0.0, 0.0, 0.3297106226285299, 0.3297106226285299, 0.3297106226285299, -0.7810373306274414, -0.7810373306274414, 0.14998269081115723, 0.14998269081115723, -0.3234790960947673, -0.3234790960947673, -0.3234790960947673, -0.2489527463912964, -0.2489527463912964, -0.11230069398880005, -0.11230069398880005, -0.11230069398880005, 0.0, 0.0, 0.20254921913146973, 0.20254921913146973, -0.06094833215077711, -0.06094833215077711, -0.06094833215077711, -0.06094833215077711, -0.40321099758148193, -0.40321099758148193, -0.40321099758148193, -0.40321099758148193, 0.0, 0.0, -0.33627049128214526, -0.33627049128214526, -0.33627049128214526, -1.0147149562835693, -1.0147149562835693, -0.25163586934407545, -0.25163586934407545, -0.25163586934407545, -0.9553514719009399, -0.9553514719009399, 0.19466912746429443, 0.19466912746429443, 0.19466912746429443, 0.5979238077998161, 0.5979238077998161, 0.5979238077998161, 0.5979238077998161, 0.0, 0.0, -0.30002379417419434, -0.30002379417419434, 0.10715281367301943, 0.10715281367301943, 0.10715281367301943, 0.10715281367301943, 0.10715281367301943, 0.0, 0.0, -0.2016986608505249, -0.2016986608505249, 0.38821653525034583, 0.38821653525034583, 0.38821653525034583, 0.0, 0.0, -0.11128419637680054, -0.11128419637680054, -0.11128419637680054, 0.027408987283706665, 0.027408987283706665, 0.027408987283706665, 0.027408987283706665, -0.07545510927836108, -0.07545510927836108, -0.07545510927836108, -0.07545510927836108, -1.2841665744781494, -1.2841665744781494, 0.0, 0.0, 0.0483269294102987, 0.0483269294102987, 0.0483269294102987, -0.13257379531860347, -0.13257379531860347, -0.13257379531860347, -0.13257379531860347, -0.13257379531860347, 0.0483269294102987, 0.0483269294102987, 0.0483269294102987, 0.07351831793785091, 0.07351831793785091, 0.07351831793785091, 0.07351831793785091, 0.07351831793785091, 0.0, 0.0, 0.14515708883603418, 0.14515708883603418, 0.14515708883603418, 0.14515708883603418, -0.27049195766448975, -0.27049195766448975, -0.27049195766448975, 0.14515708883603418, 0.14515708883603418, 0.14515708883603418, 0.14515708883603418, 0.15258492231369014, 0.15258492231369014, 0.15258492231369014, 0.15258492231369014, 0.15258492231369014, -0.38680148124694824, -0.38680148124694824, -0.38680148124694824, -0.24108399947484327, -0.24108399947484327, -0.24108399947484327, -0.15629948973655705, -0.15629948973655705, -0.15629948973655705, -0.15629948973655705, -0.15629948973655705, -0.26783172289530444, -0.26783172289530444, -0.26783172289530444, 0.12412762641906738, 0.12412762641906738, -0.5000226497650146, -0.5000226497650146, -0.9039314985275269, -0.9039314985275269, 0.0, 0.0, 0.5546192129453023, 0.5546192129453023, 0.5546192129453023, 0.5684522589047749, 0.5684522589047749, 0.5684522589047749, 0.5684522589047749, 0.0, 0.0, 0.0, 0.0, 0.3332829078038534, 0.3332829078038534, 0.3332829078038534, 0.0, 0.0, 0.0, 0.0, 0.00024233261744177614, 0.00024233261744177614, 0.00024233261744177614, 0.0, 0.0, -0.4999969005584717, -0.4999969005584717, -0.7500264644622803, -0.7500264644622803, -0.45643794536590576, -0.45643794536590576, -0.45643794536590576, -0.47605812549591064, -0.47605812549591064, -1.1428501605987549, -1.1428501605987549, 0.0, 0.0, 0.0, 0.0, -0.363412082195282, -0.363412082195282, -0.363412082195282, 0.12126104036966956, 0.12126104036966956, 0.12126104036966956, 0.16738885641098022, 0.16738885641098022, 0.16738885641098022, 0.16738885641098022, 0.16738885641098022, 0.16738885641098022, -0.05761953592300406, -0.05761953592300406, -0.05761953592300406, -0.05761953592300406, -0.05761953592300406, -0.18653269608815504, -0.18653269608815504, -0.18653269608815504, 0.37003741860389705, 0.37003741860389705, 0.37003741860389705, 0.37003741860389705, 0.37003741860389705, -0.22541735569636034, -0.22541735569636034, -0.22541735569636034, 0.0, 0.0, 0.0007724165916442871, 0.0007724165916442871, 0.0, 0.0, 0.16652941703796387, 0.16652941703796387, 0.16652941703796387, -0.33101630210876465, -0.33101630210876465, 0.0, 0.0, 0.0, 0.0, -0.664846658706665, -0.664846658706665, -0.25133395195007324, -0.25133395195007324, 0.49986380338668823, 0.49986380338668823, 0.0, 0.0, 0.2917480369408926, 0.2917480369408926, 0.2917480369408926, 0.2917480369408926, 0.7466391324996948, 0.7466391324996948, -1.0998635292053223, -1.0998635292053223, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.08139854669570923, -0.08139854669570923, -0.08139854669570923, 0.31445504228274024, 0.31445504228274024, 0.31445504228274024, 0.03174980481465661, 0.03174980481465661, 0.03174980481465661, -1.0714640617370605, -1.0714640617370605, 0.16652941703796387, 0.16652941703796387, 0.16652941703796387, 0.7466391324996948, 0.7466391324996948, 0.2770214776198069, 0.2770214776198069, 0.2770214776198069, 0.498043417930603, 0.498043417930603, 0.498043417930603, 0.0, 0.0, 0.34439904491106665, 0.34439904491106665, 0.34439904491106665, -0.023828089237213135, -0.023828089237213135, -0.023828089237213135, 0.0, 0.0, -0.2833470106124878, -0.2833470106124878, -0.05343915224075313, -0.05343915224075313, -0.05343915224075313, -0.05343915224075313, -0.05343915224075313, 0.0, 0.0, -0.8279567956924438, -0.8279567956924438, -0.8664689064025879, -0.8664689064025879, 0.498043417930603, 0.498043417930603, -0.43431293964385986, -0.43431293964385986, 0.23241210977236426, 0.23241210977236426, 0.23241210977236426, 0.0, 0.0, 0.0, 0.0, -0.6222509145736694, -0.6222509145736694, -0.19538748264312744, -0.19538748264312744, 0.0, 0.0, 0.0, 0.0, -0.582456390062968, -0.582456390062968, -0.582456390062968, 0.0380951464176178, 0.0380951464176178, 0.0380951464176178, 0.0380951464176178, -0.06346901754538226, -0.06346901754538226, -0.06346901754538226, -0.06346901754538226, -0.6711139678955078, -0.6711139678955078, -0.6711139678955078, 0.0, 0.0, 0.0, 0.0, -0.6415431499481201, -0.6415431499481201, -0.39987361431121826, -0.39987361431121826, 0.0, 0.0, -1.099998950958252, -1.099998950958252, 0.23360572258631385, 0.23360572258631385, 0.23360572258631385, 0.0007724165916442871, 0.0007724165916442871, 0.16663271188735962, 0.16663271188735962, 0.16652941703796387, 0.16652941703796387, 0.16652941703796387, 0.06770722568035126, 0.06770722568035126, 0.06770722568035126, 0.06770722568035126, 0.06770722568035126, -0.588110089302063, -0.588110089302063, -0.588110089302063, -0.3125501871109009, -0.3125501871109009, 0.021500269571940067, 0.021500269571940067, 0.021500269571940067, 0.021500269571940067, 0.0, 0.0, 0.18976362943649294, 0.18976362943649294, 0.18976362943649294, 0.18976362943649294, 0.18976362943649294, 0.19178104897340142, 0.19178104897340142, 0.19178104897340142, 0.19178104897340142, -0.11059983571370435, -0.11059983571370435, -0.11059983571370435, 0.09288944800694787, 0.09288944800694787, 0.09288944800694787, 0.1951285799344381, 0.1951285799344381, 0.1951285799344381, -0.05105769634246826, -0.05105769634246826, -0.01784137884775805, -0.01784137884775805, -0.01784137884775805, -0.3632686138153076, -0.3632686138153076, 0.018493548035621643, 0.018493548035621643, 0.018493548035621643, 0.018493548035621643, 0.0, 0.0, 0.215119194984436, 0.215119194984436, 0.215119194984436, 0.215119194984436, 0.215119194984436, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10836885571479793, 0.10836885571479793, 0.10836885571479793, 0.10836885571479793, 0.10836885571479793, 0.23657858967781065, 0.23657858967781065, 0.23657858967781065, 0.23657858967781065, 0.23657858967781065, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08749032020568848, 0.08749032020568848, 0.08749032020568848, -0.3348144292831421, -0.3348144292831421, -0.9641890525817871, -0.9641890525817871, -1.285665512084961, -1.285665512084961, -0.7536688248316448, -0.7536688248316448, -0.7536688248316448, -0.0990800062815349, -0.0990800062815349, -0.0990800062815349, 0.0, 0.0, 0.08267402648925781, 0.08267402648925781, 0.08267402648925781, 0.08267402648925781, 0.08267402648925781, 0.08267402648925781, 0.0, 0.0, -0.37502872943878174, -0.37502872943878174, 0.0, 0.0, -0.6440454721450806, -0.6440454721450806, 0.0, 0.0, -0.8284021615982056, -0.8284021615982056, -0.08863679567972826, -0.08863679567972826, -0.08863679567972826, -0.08863679567972826, 0.034383734067281124, 0.034383734067281124, 0.034383734067281124, -0.19744610786437988, -0.19744610786437988, -0.23292324940363573, -0.23292324940363573, -0.23292324940363573, 0.0, 0.0, 0.0, 0.0, 0.41555181642373407, 0.41555181642373407, 0.41555181642373407, 0.41555181642373407, 0.498043417930603, 0.498043417930603, 0.498043417930603, 0.498043417930603, 0.498043417930603, 0.498043417930603, 0.498043417930603, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.48917728662490845, 0.48917728662490845, 0.16642908255259192, 0.16642908255259192, 0.16642908255259192, -1.2666666507720947, -1.2666666507720947, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31939399242401123, 0.31939399242401123, 0.7959111779928207, 0.7959111779928207, 0.5998238325119019, 0.5998238325119019, 0.5723662078380585, 0.5723662078380585, 0.0, 0.0, -1.2000000476837158, -1.2000000476837158, -0.3333169221878052, -0.3333169221878052, 0.23326337337493896, 0.23326337337493896, 0.23326337337493896, -0.6001346111297607, -0.6001346111297607, 0.10087335109710693, 0.10087335109710693, -0.00454423824946093, -0.00454423824946093, -0.00454423824946093, -0.00454423824946093, 0.0, 0.0, 0.36107329527537024, 0.36107329527537024, 0.36107329527537024, 0.16743671894073486, 0.16743671894073486, 0.0, 0.0, 0.0, 0.0, -1.000030755996704, -1.000030755996704, 0.0, 0.0, 0.5002714693546295, 0.5002714693546295, 0.4155462483565012, 0.4155462483565012, 0.4155462483565012, 0.5002714693546295, 0.5002714693546295, 0.5002714693546295, 0.5002714693546295, -0.14422965049743652, -0.14422965049743652, -0.14422965049743652, 0.7466391324996948, 0.7466391324996948, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.014484167098999023, -0.014484167098999023, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21558036406834924, 0.21558036406834924, 0.21558036406834924, 0.2511914521455765, 0.2511914521455765, 0.2511914521455765, 0.2511914521455765, 0.0, 0.0, 0.032377878824869755, 0.032377878824869755, 0.032377878824869755, 0.0, 0.0, 0.1666553020477295, 0.1666553020477295, 0.248703733086586, 0.248703733086586, 0.248703733086586, 0.248703733086586, 0.248703733086586, 0.248703733086586, 0.248703733086586, 0.248703733086586, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16691186030705774, 0.16691186030705774, 0.16691186030705774, 0.06644868850708008, 0.06644868850708008, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.5000107288360596, -0.5000107288360596, -0.6246011257171631, -0.6246011257171631, -0.08493512868881226, -0.08493512868881226, -0.08493512868881226, 0.09788827101389563, 0.09788827101389563, 0.09788827101389563, 0.23328924179077148, 0.23328924179077148, 0.23328924179077148, 0.09408752123514807, 0.09408752123514807, 0.09408752123514807, 0.09408752123514807, 0.030023157596588135, 0.030023157596588135, 0.030023157596588135, 0.030023157596588135, 0.15224055449167884, 0.15224055449167884, 0.15224055449167884, 0.12457213799158728, 0.12457213799158728, 0.12457213799158728, 0.12457213799158728, 0.12457213799158728, 0.12457213799158728, 0.0, 0.0, 0.0, 0.0, 0.165063738822937, 0.165063738822937, 0.2943248152732849, 0.2943248152732849, 0.2943248152732849, 0.0, 0.0, 0.5818089097738266, 0.5818089097738266, 0.5818089097738266, 0.5818089097738266, 0.10091215372085571, 0.10091215372085571, -0.7002358436584473, -0.7002358436584473, 0.0, 0.0, 0.28566592931747437, 0.28566592931747437, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2755800485610962, 0.2755800485610962, 0.2755800485610962, 0.2755800485610962, 0.2857604821523031, 0.2857604821523031, 0.2857604821523031, 0.23327630758285522, 0.23327630758285522, 0.23327630758285522, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.19964802265167236, -0.19964802265167236, 0.08813996116320288, 0.08813996116320288, 0.08813996116320288, 0.44343065222104394, 0.44343065222104394, 0.44343065222104394, 0.7466391324996948, 0.7466391324996948, 0.7466391324996948, 0.7466391324996948, 0.7466391324996948, 0.7466391324996948, 0.0, 0.0, 0.44343065222104394, 0.44343065222104394, 0.44343065222104394, 0.44343065222104394, 0.44343065222104394, 0.44343065222104394]
the hyperedges are {538: [402, 1689, 2165, 2193, 1204, 2309, 1738, 487, 1036], 163: [402, 1696, 2295, 1274, 1286, 1544, 2600, 2363, 1905, 1611, 141, 1807, 1110, 174, 2521, 1792, 1675, 1334, 813, 1799, 1943, 2077, 769, 781, 940, 942, 1590, 1734, 1872, 2286, 390, 1717, 1030, 2274, 2518, 606, 800, 1575, 1070, 935, 1205, 1571, 1971, 1127, 530, 856, 2604, 910, 2173, 191, 1253, 1729, 1136, 1457, 1225, 2563, 1689, 1498, 563, 2396, 717, 1890, 188, 982, 55, 346, 2673, 1380, 1207, 2451, 961, 42, 624, 1573, 2232, 145, 2659, 743, 744, 1139, 1572, 1303, 1305, 129, 415, 2204, 1224, 219, 727, 448, 2557, 2200, 22, 1218, 2202, 1069, 2175, 966, 1219, 757, 1685, 1691, 1410, 380, 1536, 1650, 1558, 2564, 1362, 1784, 1065, 290, 1277, 395, 1098, 1404, 1099, 2196, 2554, 1715, 2248, 2251, 2280, 2296, 1113, 714, 2638, 602, 1530, 1257, 2259, 689, 1060, 2039, 1594, 2116, 1775, 422, 2316, 266, 658, 1333, 1515, 523, 1467], 219: [402, 2563, 1689, 563, 757, 1404, 1325, 712, 1738, 38], 1114: [1696, 602, 1117, 1588, 1261, 1603], 881: [2295, 112, 572, 1119, 816, 1411, 2702, 2203, 2405, 2454], 427: [2295, 1274, 658, 1103, 1488, 665, 494, 610, 526, 146, 202, 18, 205, 696, 561, 353, 1766, 1914, 2255, 1811, 2383, 2453, 1760, 2574, 2388, 1399, 1405, 587, 2258, 823, 1318, 1547, 1037, 2306, 312, 1283, 1284, 230, 2658, 399, 406, 1975, 250, 584, 304, 2448, 2683, 954, 2437, 916, 1999, 946, 1577, 1340, 391], 2667: [2295, 2381, 2504], 343: [1274, 1571, 530, 1303, 1098, 1577], 1305: [1274, 1575, 1065, 1098, 1740], 1303: [1274, 2347], 1136: [1286, 1030, 800, 1575, 935, 727, 1558, 1060, 1594, 1333, 1740, 1063, 1142, 1018, 1516], 566: [1544, 472, 16, 173, 688, 671, 466, 483, 2221, 633, 34, 1816, 2480, 449, 186, 699, 588, 2658, 477, 333, 1782, 2485, 1447, 858, 1643, 274, 321], 1206: [1544, 789], 1409: [2600, 2220, 789, 2703, 1797], 523: [2600, 910, 961, 415, 277, 1687, 478, 870, 2442, 1616, 882, 283, 1214, 1931, 2408, 54, 2642, 1676, 508, 1790, 465, 565, 2480, 1921], 2220: [2600, 2563, 290, 2478, 2139, 1958, 1409, 1940, 1950], 129: [2363, 188, 2451, 22, 2446, 1973, 2165, 1409, 538, 2220, 2193, 1204], 1969: [2363, 2200, 2309], 2165: [2363, 538, 2193, 1143], 1719: [2363], 1834: [1905], 55: [1905, 1832, 844], 1153: [1611, 1675, 942, 1070, 1205, 1127, 856, 1219, 1410, 1065, 1113, 1257, 1063, 1204, 1455], 100: [1611, 265, 758], 1380: [1611, 1717], 1379: [1611], 1184: [141, 781, 1362, 1784, 120, 2199, 844, 860], 188: [141, 717, 42, 415, 757, 380, 523, 139, 120, 163, 262, 755], 2490: [1807], 1728: [1110, 129, 1536, 1775, 1249, 1474], 1729: [1110], 1016: [1110, 1070, 1205, 856, 1729, 1305, 727, 943, 1670, 1466], 793: [1110, 1334, 1734, 1070, 1205, 1127, 856, 1380, 42, 1139, 1069, 1691, 1715, 1333, 1467, 943, 959, 1694, 1330, 163, 1455, 1249, 262], 2409: [174, 638, 2027, 2672], 501: [174], 503: [174, 2396, 501, 1813, 1952, 2029, 1851], 2529: [2521, 2294], 145: [2521, 2286, 390, 1224, 1218, 2248, 2251, 2280, 2296, 1530], 2164: [2521, 5], 582: [1792, 1498, 818, 259], 2067: [1792], 430: [1792, 1207, 2557, 89, 1698, 2137, 632, 1020, 1568, 1284, 2487, 1617, 2593, 1637, 683, 2033, 1165, 2197, 223, 524, 381, 651, 2343, 547, 208, 686, 172, 177, 1270, 119, 52, 236], 1696: [1334, 1943, 1872, 1971, 2175, 2250, 1771, 1769, 2330], 1130: [1334, 959], 1523: [813, 1136, 1060, 815], 1424: [1799], 2205: [1799], 727: [1799], 1936: [1943], 744: [2077, 743, 2316, 65], 2030: [2077, 2659, 743, 744, 2316], 743: [2077, 2316, 65], 764: [769, 636, 1311, 2174, 1581], 1533: [781, 1277, 900, 970, 1730], 846: [781, 2173, 2204, 219, 757], 934: [940, 1353, 1009, 844, 860, 1631], 965: [1590, 1253, 966, 1060, 177, 1673, 1432, 1140, 981, 1481], 910: [1590, 1253, 982, 1573, 966, 1117, 902, 1673, 1131, 1432, 1140, 1311, 964, 764, 811, 981, 1481], 1249: [1734, 1408], 1715: [1734, 1386], 1592: [1734, 1161, 1176, 1293, 405, 1298], 1257: [1872, 2175, 2259, 1719, 2087], 1771: [1872], 2280: [2286], 1441: [1717], 941: [1717], 1946: [1030], 1796: [1030, 1917, 833, 2001], 2237: [2274], 2298: [2518], 537: [606, 357, 1204], 1216: [1571, 1035, 1299], 1218: [1571, 714, 1530, 1516, 1670, 1035, 1299, 1424, 833], 2202: [1571, 1303, 2638], 2211: [1971, 854, 1232], 995: [1971, 2139, 956, 1690, 1230, 1231, 1232, 1985], 938: [1127, 2196], 728: [1127, 1305, 943, 1466], 168: [2604, 1756], 565: [910, 548, 2, 305, 239, 644, 173, 554, 85, 621, 738, 57, 60, 671, 92, 691, 367, 466, 575, 601, 306, 1540, 633, 566, 1859, 34, 478, 2442, 1616, 882, 283, 2642, 1676, 1790, 465, 2480, 502, 1975, 1438, 103, 477, 564, 128, 118, 2027, 2672, 1440, 82, 295, 333, 1401, 1782, 1585, 425, 1012, 1376, 1316, 773, 385, 842, 1957, 1213, 1741, 1546, 492, 410, 1727, 244, 1378, 2485, 2628, 2629, 1373], 902: [910, 966, 964, 981, 965], 636: [2173, 2204, 118, 2027, 2672, 764, 2174, 2233], 2204: [2173], 294: [191, 173, 554, 85, 636, 466, 483, 277, 633, 566, 2430, 465, 565, 1353, 83, 502, 665, 186, 699, 588, 153, 21, 252, 1438, 103, 638, 1672, 807, 2476, 477, 564, 128], 1462: [1457, 1657], 569: [1457, 456, 1181, 550, 1273, 1092, 1712, 255, 1522, 1986, 1430, 2099, 514, 710, 1269, 1000, 1025], 826: [1457, 1273, 1237, 1606, 1422], 828: [1457, 1422], 1117: [1225, 1588, 1261, 1603, 2332], 1333: [1225, 1207, 1224, 1467, 1514], 1583: [2563, 2396, 900], 2396: [2563], 1534: [1498, 1597, 1163, 1556, 1278, 1122, 289, 1148], 289: [1498, 1278, 2568, 64], 37: [563], 38: [563, 372], 247: [563, 982, 1117, 1261, 2257, 500, 2446, 1973], 1952: [2396, 2029], 1060: [1890, 2039], 2039: [1890], 42: [188], 144: [188, 2102], 189: [188, 42, 395, 2664, 2102, 163], 1784: [55, 2199, 844, 2688, 2705], 132: [346], 143: [346, 180, 337, 363], 1224: [2673], 638: [2673, 383, 2465, 2146, 701, 862, 722, 1808, 1189, 618, 1460, 1312, 1468, 1895, 2171, 227, 296, 2541, 1327, 264, 779, 348, 89, 1027, 521, 2426, 375, 1919, 449], 1113: [1380], 378: [1207, 227, 1027, 335, 1275, 2290, 2291, 1061, 692, 960, 2012, 2195, 2042, 1238, 218, 178, 585, 1644, 1290, 1963, 396, 2583], 659: [624, 695, 397, 11], 2058: [2232, 757, 755], 411: [2232, 2594, 512, 2582, 1891, 1470, 536], 1646: [145, 943], 2659: [743, 2316], 1139: [1572, 1018, 780, 1598], 1106: [1305, 1065], 15: [129, 701, 234, 322, 399, 254, 253, 538, 80], 2173: [2204], 191: [448, 22, 395], 686: [2557, 2541, 1698, 1273, 683, 223, 381, 119, 116, 225, 760, 44, 430], 1947: [2557], 1980: [2202, 2457, 262, 758], 1685: [1069, 422, 1455], 2361: [2175, 139], 964: [966, 902, 1673, 1432, 1140, 1481, 965], 1069: [1685, 422, 1455], 422: [1691], 207: [1536, 688, 1676], 1466: [1650, 943], 1558: [2564], 1159: [2564, 1485], 1178: [1362, 1099, 995, 1737, 854, 956, 1476, 1736, 1690, 1230, 1231, 1232], 2175: [1362, 2259, 1940, 1009, 1745, 1719, 2250, 2087, 2206, 120], 380: [1784, 2196, 1257, 1737, 1474], 1063: [1065, 1628], 1911: [290, 1098, 1719, 2087, 2206, 860], 1839: [290, 2688, 2705], 2152: [1277], 2470: [1277, 1230, 2643, 2646], 2267: [2196], 1850: [2554], 2248: [2296], 2251: [2296], 2089: [714, 1530, 1117, 1261, 1603, 357, 1965], 2334: [602, 54, 1790, 1588, 2332, 2103, 1832, 1836, 209, 2353, 1917], 2261: [2259, 1719, 2087, 2206], 967: [689, 57, 306, 322, 1032, 848, 1400, 847, 56], 237: [1060], 1890: [2039], 2254: [2039, 2347], 1775: [2116, 1771, 2082], 1793: [1775, 2178, 1795, 2082], 1795: [1775, 2545, 2178, 1788, 2082], 262: [266], 578: [1333, 1122, 1877, 482, 13], 1467: [1333], 961: [1515], 988: [1515], 717: [523, 1122, 2568, 1877, 2502], 745: [1756, 1763, 386, 1564, 2325, 600, 458, 771, 110, 72, 18, 124, 127, 205, 561, 91, 1932, 355, 2437, 477, 2162, 1157, 687, 2165, 1700, 1704, 36, 1886, 2336], 2603: [1756], 2613: [1756], 1663: [1756, 1266, 2504], 1859: [548, 1791], 1860: [548, 683, 906], 552: [548, 374, 451, 472, 2, 305, 239, 446, 644, 220, 311, 653, 16, 173, 554, 559, 85, 688, 621, 636, 738, 749, 57, 60, 210, 671, 167, 92, 691, 367, 466, 473, 483, 575, 298, 601, 370, 306, 277], 82: [548, 305, 239, 1440, 295, 410, 1553], 455: [374], 2537: [451, 1846, 2198, 1927], 2666: [451, 2453], 650: [472, 1557], 1006: [472, 1025, 876, 650, 2040, 242], 410: [2, 305, 298], 471: [2, 1859, 103, 1045, 1632, 295], 1378: [239, 559, 1540, 882, 21, 1440, 82, 970], 554: [446, 167, 306, 1859], 210: [644, 148], 490: [220, 45, 6, 492, 5, 734, 213], 45: [220], 295: [220, 210, 2078, 1782, 1012, 1316, 197], 406: [311], 2480: [653, 2485], 1697: [16, 804, 929, 1559, 1683, 1599, 1703, 1664, 1667], 466: [16], 162: [173, 775], 875: [554, 370, 775, 933, 1192], 2341: [559, 2425, 623, 788, 1757, 1842], 167: [559], 465: [85, 466, 566, 565, 103, 118, 2027, 2672, 2368], 60: [688, 252], 412: [688], 738: [621, 210, 575, 564, 1183, 1585, 1376, 1213, 1741, 1727, 2628, 2629], 1573: [636, 1673, 1432, 764, 1481, 2174], 2687: [636, 764, 2174], 1459: [749, 57, 210, 671, 167, 473, 2078, 2221, 1687, 775, 1540, 633, 45, 1137, 566, 804], 2485: [749, 1816, 1378], 483: [749, 367, 277, 463, 933, 650], 562: [60, 671, 370, 252, 1401, 2018, 440, 1889, 1117, 1183], 63: [210, 153, 1546], 559: [167, 34, 1816], 136: [92, 2487, 1370, 2172, 244, 1102, 2580, 2399, 1479, 2543, 1776, 1413, 159, 2231], 1259: [92, 521, 1944, 2213, 1081, 893, 2312, 136], 479: [92, 478, 449, 1627, 2300, 136, 418], 1649: [691, 1687, 1213, 1643, 868, 1228, 148, 1557], 2658: [691, 1559, 1860], 843: [575, 1101, 2658, 568], 173: [575, 1378, 2485], 611: [601, 633, 2230, 179, 133, 899, 2501, 1103, 2340, 2352, 718, 864, 2505, 1755, 1763, 2050, 386, 716, 1732, 511, 2540, 2385, 1668, 28, 1352, 579, 1493, 1116, 1564, 368, 1488, 938, 1265, 1935, 1499, 1315, 2325], 3: [601, 463, 197], 1741: [601, 492], 57: [306], 567: [277, 502, 21, 970, 385, 1828], 2406: [2078], 2474: [2078, 2425], 1983: [2078], 2404: [2221], 1527: [2221, 1086, 1240, 211, 2076, 1747, 260, 2166, 2561, 2420, 1543, 2524, 2525, 827, 909, 971, 419], 898: [1687], 1118: [1540, 134], 492: [45, 1186, 734, 213, 2323], 558: [1137, 1816, 1641, 1934, 905, 2034, 423, 2660, 376, 557, 1532, 1555], 1687: [1137, 870, 54, 1189, 1632, 773, 1228, 148, 1557], 621: [1859], 66: [34, 2359, 1248, 2294, 215, 2310, 1531, 2441, 2122, 183, 1698, 288, 2157], 35: [34], 1934: [1816], 682: [1791, 2430], 1559: [1791, 929, 1683, 1664], 2647: [2430, 2658], 87: [383, 2245], 631: [383, 1057, 1562], 2243: [2245, 831], 2471: [2245], 1994: [2097, 1995, 1800], 1995: [2097, 1800, 1907], 1800: [1995, 1907, 1901, 342], 2301: [1907, 1901], 929: [2442], 2118: [2442], 71: [2442, 929, 1559, 1923, 661, 1641], 882: [1616, 1012, 1316], 1529: [882], 575: [882, 1378, 2485], 747: [283, 680, 386, 579, 368, 779, 83, 600, 203, 112, 234, 458, 695, 198, 502, 724, 328, 397, 737, 665, 186, 494, 609, 610, 330, 229, 700, 526, 527, 763, 146, 104, 771, 572, 699, 110, 202, 204, 72, 26, 685, 18, 542, 588, 741, 310, 124, 127, 153, 205, 696, 561, 353, 91, 362], 583: [1214, 1278, 2568, 50, 2398, 1126, 64, 284, 643], 2624: [1931, 2277, 1204, 2702], 1975: [2408], 615: [54, 556, 98], 712: [54, 681, 1965, 2664, 1036], 2368: [2642], 74: [1676, 680, 975, 1103, 511, 310, 2266, 881, 949, 1220, 1928, 987, 1024, 547, 661, 906, 1017, 1634, 604, 2513, 154, 2654, 2284, 2335, 505, 252, 345, 787, 2390, 263, 377, 19, 867], 1146: [1676, 820], 417: [508, 317], 2326: [508, 2324, 317], 22: [1790, 2257], 415: [1790, 995, 789, 2664, 2103, 1737, 1832, 1836, 209, 2353, 712], 1408: [1921], 2402: [1921], 1931: [1921, 2277, 2402, 1408], 608: [394, 2230, 680, 975, 179, 133, 611], 125: [394, 1857], 133: [394], 157: [2230, 716, 2540, 897, 916, 1340, 1637, 1017, 181, 1550, 979, 78, 1375, 1075, 393, 2680, 1289, 2021, 1718, 560, 2095, 371, 2417], 2095: [2230], 90: [680], 76: [975, 250, 661, 345, 2390, 869, 1128, 694, 1934, 181, 2532, 2327, 518], 1079: [975, 611, 1103, 864, 1315, 1718, 1009], 668: [975, 1315, 310, 2683, 604, 263, 2413, 2414, 2562, 1508], 331: [179, 774, 101], 2378: [611, 2050, 1668, 2335], 1935: [611, 2340], 719: [611, 609, 520, 1838, 2561, 1133, 1112, 87], 1672: [899, 1352, 1564, 807, 903], 1564: [899], 1565: [899], 2260: [2501], 1579: [1103], 2055: [2352], 271: [718], 2499: [2505, 2385, 1935, 203, 863, 2147, 2500, 2322, 2376, 1787, 1871, 2357, 2422, 2498, 1418, 1068], 359: [2505, 203, 330, 2567, 2322, 2376, 1787, 1871, 2422, 2498, 1068, 1320, 808, 1342], 2432: [1755], 2611: [1763, 2606], 407: [1763, 1298, 512, 2439, 1875], 639: [1763, 584], 1644: [1732, 701, 322, 822, 318, 2409, 2484, 2616, 1964, 986, 1862, 1863, 2364, 1256, 1601], 2423: [1732, 1161, 862, 881, 550, 2225, 853, 1644, 1601, 1191], 160: [2540, 322, 2681, 469, 1857], 2676: [2385, 1787], 1488: [1668, 206], 1910: [1668, 705, 2159, 2366, 2416, 2506, 2242, 2375, 2575], 1606: [28, 1293, 2381], 1718: [28, 2413, 2414, 1373, 810, 2582, 201, 2138], 2674: [1352, 2066], 696: [579, 215, 1672], 1732: [1493, 2225, 853], 1593: [1116, 1437, 978, 1288], 687: [1564, 110, 2683, 2381, 1157, 1700, 1704, 626], 1490: [1488, 206], 980: [1488], 560: [938, 955, 78, 1375, 2287], 1725: [1265, 921], 1306: [1499, 607], 1050: [1499, 1591, 1389, 1306, 1492], 1310: [1499, 1306], 2452: [1315, 2595], 2538: [2325], 1138: [1597, 1420, 806], 1148: [1597, 1355], 486: [1597, 1278, 1148, 64, 284, 231, 285, 2276], 1497: [1163, 1212, 901, 580], 1498: [1163, 2398, 2158, 1049, 1071, 2450, 1483], 1537: [1556, 289, 1348, 629], 612: [289, 2568, 1046], 1535: [289, 1348, 629], 285: [1148, 231], 30: [1148, 231, 285, 2276, 679], 2544: [2568], 1345: [1046, 1047, 1344, 1640], 2706: [1046, 1138, 1640], 1121: [1046, 643, 1138, 1640, 806], 402: [403, 681], 2309: [403, 681, 1325], 2302: [403, 1353, 2111, 2104], 2317: [1353, 2624], 2047: [2111], 403: [2104], 2170: [1325], 1175: [1325, 1628], 2649: [1956, 249, 2106], 2527: [1956, 2100], 735: [1956, 2106, 216, 195], 1961: [2100], 1847: [1846, 1927], 1927: [1846], 1560: [2198, 535, 791, 1161, 456], 2695: [2198], 1846: [1927], 761: [535, 9], 109: [535, 488, 1296, 1074, 1888, 507, 2053, 635, 459, 2054, 1661, 672, 884, 778, 171, 955], 672: [535, 456, 1460, 549, 2653, 591, 924, 1363, 574, 130, 2004, 1386, 1343, 1293, 2329, 2360, 1003, 1004], 786: [791], 1003: [791, 2146, 2182, 1897, 1364, 1343, 786, 1469, 1004, 2028, 2483], 1004: [791, 2146, 1364, 1343, 786, 1469, 1003, 2483], 2657: [1161], 1702: [1161, 1199], 549: [456, 287, 453, 488, 507, 591, 574], 25: [456, 1181, 1897, 20, 778], 2057: [2465, 2695, 790, 1510], 228: [2465, 900, 259, 2617, 1692, 323, 119, 52], 2182: [2146, 1748, 2633, 1835], 1897: [2146, 2182], 1812: [862, 1513, 1155, 783, 785, 1407, 2364], 986: [862, 136, 2484, 1642, 2616], 1019: [862, 927, 906, 853, 1290, 1256, 926], 635: [722, 264, 89, 375, 632, 683, 381, 651, 208, 686, 172, 177, 430, 442, 474, 634], 2210: [1808], 1643: [1189, 773, 868], 478: [618, 227, 215, 303, 52, 497, 96, 274, 218, 396, 520, 568], 418: [1312, 1269, 135], 1428: [1312, 1622, 2617, 2229, 2539, 2493, 2424], 1269: [1312, 1260, 2618, 876], 344: [1312, 2171, 348, 521, 2426, 2441, 1671, 1267, 1622, 2617, 2609, 335, 1881, 2339, 2229, 963, 1442, 1445, 2589, 1627, 1275, 2290, 2291, 1260, 1244, 675], 848: [1468, 1607, 1223, 1032, 1739, 1388, 1400, 847], 847: [1468, 710, 1032, 848], 1893: [1895], 102: [1895, 2333, 2012, 2583, 1824, 1893, 1964], 2032: [2171], 142: [227, 823, 1318, 1547, 1283, 230, 391, 520], 438: [227, 296, 550, 710, 391, 79, 96, 21, 607, 616, 692], 227: [296, 823, 1318, 1547, 2319], 1236: [296, 861], 692: [296], 2673: [2541, 995], 1569: [1327, 1486, 208, 430, 1010, 1073, 1684, 1679, 1726, 1011, 877, 1182], 1203: [779, 542, 881, 1630, 1119, 816, 1411], 542: [779], 322: [1027], 1322: [1027, 1622, 1020, 2539, 323], 1336: [1027, 1478], 1081: [521], 1082: [521, 1081, 2312], 1007: [2426, 1919, 2217, 837], 2426: [1919, 2126, 2431, 1686, 2493], 1539: [449, 136, 924, 2091], 1052: [600, 832, 1066, 1051], 205: [600], 624: [112, 700, 572, 11], 572: [112], 1967: [695], 655: [198], 2536: [502, 1957, 2629], 2612: [724, 26], 2619: [328, 1774], 2346: [397], 2024: [737, 1932, 427, 1778], 588: [186, 699], 576: [330, 212, 2026, 1258, 1381, 1200, 1524, 1922, 1814, 59, 874, 1245, 1067, 605, 863, 293, 1526, 354, 1710, 570, 1801, 834, 839, 1014, 1932, 355], 354: [330, 839, 1072], 731: [330, 808, 1450, 576, 1778], 477: [229, 1766, 2658, 463, 2409, 427, 333], 625: [700], 879: [700, 979], 1656: [527, 1710, 2357, 731, 1822], 2662: [527, 2606, 1822], 1968: [527, 1774], 1879: [763, 1760, 2355], 771: [104, 72], 72: [771, 202], 202: [771, 204], 204: [202, 741, 1811, 274, 597, 2410, 2650], 1645: [204], 2410: [204, 1811], 217: [685, 2355], 779: [542, 684], 699: [588], 122: [124, 127, 1093, 2400], 127: [124, 2400], 18: [127, 1932, 890], 2191: [362], 2520: [2359], 688: [1248, 252], 835: [1248], 649: [2294, 2560, 96, 1643, 2581, 457], 2380: [2294, 215, 288, 2514, 1838, 2561], 1140: [2294, 1311, 811], 2061: [215, 96, 2062], 2583: [2310, 1423, 1173, 2333], 318: [2310, 1173, 686, 2333, 445, 991, 2427], 2607: [2310, 2369, 2074], 887: [1531, 983], 1154: [1531, 819], 983: [1531, 1735, 928], 2585: [2441, 1809, 1830, 1263], 1864: [2122, 1854, 1855], 75: [2122, 288, 2299, 1854, 1855], 1854: [2122, 2299, 1855, 2645, 2482], 1855: [2122, 1854, 2482], 539: [183, 541, 1209, 1665, 1210], 541: [183], 454: [183, 541], 2597: [1698], 1626: [1698, 1584, 1407, 1735, 1164], 1377: [288, 1642, 313], 2037: [288, 1584, 2151], 288: [2157], 1018: [1142], 1142: [1018], 833: [1516, 2001], 345: [117, 2389, 2266, 1924], 155: [117], 452: [117], 513: [117, 452], 2421: [2389, 2266, 2654, 345, 263, 2590, 74], 2040: [1924, 2004, 2360, 672], 2327: [1924, 2034, 2015], 1256: [881], 1351: [900], 1207: [900], 423: [2425, 1934, 557], 788: [2425, 945, 1414, 2323, 1268], 62: [2425, 945, 423, 324], 580: [1212], 1126: [1212, 580, 2489, 923], 1496: [1125], 166: [1125, 457], 1554: [901, 1552, 1401, 1553, 951], 1553: [901, 1552], 581: [580, 222, 49, 50], 1215: [580, 818, 819, 1699, 1209, 1707, 824], 643: [222, 49, 50, 2489, 1904], 150: [50, 643], 1125: [818, 1483, 817, 857], 817: [818, 819], 1212: [818, 817, 819, 1552], 320: [259], 2372: [2398, 1126, 2489], 2449: [2158, 2450, 1881], 1329: [1049, 1071, 1986, 1020, 2127], 1292: [1049, 1071, 960], 1123: [1483, 541, 1188, 1176, 75], 2371: [1126, 2489], 284: [64], 64: [284], 1512: [1235, 1671, 949, 999, 1463, 1335, 1267], 2508: [1235, 2577, 2137, 632, 322], 1608: [949, 1220, 987, 2225], 70: [999, 255, 1986, 1521, 1692, 931], 1463: [999, 931, 1399, 1405], 1240: [999, 998, 84, 912, 1201, 841, 897, 987, 1086, 742, 1141, 1241, 950, 1347, 1147], 347: [999, 713, 998, 452, 84], 1053: [1463, 931, 1197, 1568, 1024, 1607, 853], 1337: [1335, 1020, 2343, 944, 2300], 1339: [1335, 1111, 2677, 2443, 434, 1570], 1267: [1335, 1500], 2180: [2577, 1500], 2342: [2577, 2593, 2197, 381, 1270, 497, 2151], 1020: [2577], 2367: [2577], 431: [2137, 156], 1582: [632, 2677, 1545], 1909: [632, 2677, 1582, 1545], 1505: [2560, 932, 251], 865: [2560, 932, 1505, 251], 251: [2560, 6, 932, 1877, 482, 1505], 2555: [2560], 2215: [1877, 675, 2650], 414: [1877, 1442, 1445, 440, 908], 13: [482], 551: [482, 2473], 1312: [1622, 1260, 1269], 1578: [1622, 1010, 1651, 877, 1182], 2091: [2617, 303, 1944, 585], 1275: [2617, 2290, 2291], 2080: [2609], 2079: [2609], 585: [335, 1275, 2290, 2291, 1061, 911], 426: [335, 1275, 29, 2369, 1576, 836, 1187, 2126, 1486, 937, 303, 1061, 318, 2431], 2450: [1881], 675: [2339, 1442, 1445, 597, 1429], 2126: [2339, 2213, 2591], 2663: [2229], 1519: [963], 1444: [1442, 1135], 1445: [1442], 2329: [2589, 2080, 1918, 1428], 1177: [1627, 1180], 1234: [1275, 1187, 937, 790, 1510, 1263, 1226, 13], 2291: [2290], 2090: [1260], 654: [1260, 135], 314: [1244, 156, 1073, 1684, 1726, 1596], 348: [1244, 1596], 1061: [1244, 1010, 1651], 1431: [1181, 1412, 861], 1422: [550, 1273, 1237, 1223, 1657], 319: [1273, 29], 1484: [1092, 1522, 1430, 1521, 1057, 1692, 1220, 931, 822, 1197, 1500, 1237], 1290: [1712, 1025, 1197, 893, 1238, 218, 1644, 1226, 2329, 423, 2080, 1918, 1191], 453: [1712, 2099, 514, 1269, 1025, 2618, 876, 1358, 2455, 2156, 9, 2059, 2354, 1566, 1661, 1449, 2034], 1025: [1712, 876, 1449, 1388], 421: [255], 767: [255], 2338: [1986, 1024, 1478, 2677, 2127, 2443], 2462: [1986, 401], 1356: [1430], 456: [2099, 1296, 2455, 2653, 1748], 2098: [2099, 2156], 1185: [1000, 1521], 1238: [1025, 1500, 1550], 1237: [1521], 1359: [1057], 627: [1057, 1270, 1562, 1679], 1307: [1057, 1657], 1618: [1220, 2343, 1112], 1000: [931], 2634: [1197], 1457: [1237], 169: [212], 570: [212, 2026, 1258, 1381, 1200, 1243, 1246, 1524, 1709, 1922, 1814, 872, 59, 874, 1245, 1067], 2552: [2026], 1922: [2026], 1520: [1258, 1200, 1246, 1524, 1709, 872, 59, 874, 1245, 1526, 354, 570, 834, 839, 1072, 920, 1211], 1524: [1381, 1243, 1245, 1526], 1245: [1243], 59: [1709, 872, 874, 570], 839: [1709, 59, 874, 1067, 1526], 834: [1814], 1801: [1814], 1067: [1814, 1801, 834], 476: [605, 1091, 1056], 676: [605, 1014, 1091, 1056], 808: [863, 1320, 1342, 1450], 387: [293], 1821: [1801, 834], 1814: [834], 1072: [839], 1912: [1932, 304], 1562: [355, 1637], 2498: [2147, 2567, 2500, 2322], 2019: [2147], 2500: [2322, 2376, 2422, 2498, 1068], 2422: [2376], 2322: [2498], 989: [1418, 1068], 726: [1418, 1068], 257: [480, 36], 2570: [480, 1928], 667: [480, 1928, 867, 2072, 1974], 2270: [480], 2124: [1928, 2114], 2656: [799, 1699], 1706: [799], 1699: [799], 1122: [799, 1194, 594], 1621: [1194, 594, 824, 300], 593: [594, 300], 1194: [594], 1354: [1188, 1176, 75, 1047, 313, 1420, 806, 911, 1123, 1680], 1285: [1176, 405, 1298, 770], 818: [817, 819], 1367: [817], 1483: [857, 2060], 922: [923, 1416, 1233], 824: [1707, 1215], 825: [1707, 1215], 161: [713, 1147, 684, 95, 996, 2311], 1241: [998, 84, 912, 1141, 950, 1347, 1147, 1240], 84: [998, 950, 1347, 1240, 155, 1803, 1550], 325: [1201, 211], 2559: [841, 1241, 51, 2076, 1747, 260], 772: [742, 1241, 1147, 1240, 312, 1502, 51, 2076, 2166, 2167], 645: [742, 312, 211, 51], 1172: [1141, 623, 915], 1347: [1147], 1328: [1020, 1423, 1584, 1101, 1031, 1513, 927, 1412, 1155, 1109, 1111, 1568, 783, 785, 1173], 2560: [323], 2467: [323, 2155], 140: [323], 429: [29, 1514], 382: [29, 1837], 256: [2369, 430, 2225, 634, 928, 983], 1595: [1576, 836, 1776, 1824, 1964, 1862, 1863, 1657, 2346, 151], 1266: [1576, 836], 375: [1576, 836], 1008: [1486, 1011], 1174: [303, 893, 1074, 1135, 1102, 902], 1919: [2431], 2151: [1584], 1164: [1584], 1111: [1101, 1412], 258: [1031, 1111, 400, 544, 268, 0, 8, 751, 435], 785: [1513], 634: [1513, 2593, 686, 430], 862: [1513, 1155, 785, 1407, 2364], 1601: [1155, 224, 1428, 1429], 1570: [1109, 1111, 1164, 634], 1155: [783, 785, 1407], 1407: [783], 1335: [1024, 1478, 1270, 944, 1607, 1512, 1223, 885, 1032], 2017: [2677, 1479, 1582], 2343: [2443], 1810: [1766], 2176: [1914, 2535], 2412: [2255], 1820: [2383], 2107: [2574, 613, 2571, 2046], 2575: [2388], 88: [587, 208, 116, 225, 760, 44, 371], 214: [587, 78, 469], 2239: [587, 2513, 2413, 2414, 2507], 1887: [2258], 1283: [823, 1318, 1547, 1284, 230, 391], 1028: [1037, 1435, 206, 1042, 1610], 2049: [2306, 2410, 2218], 577: [312, 250, 1502, 2034, 2492, 401, 2660, 1532, 389], 230: [1284, 406, 391, 520], 1250: [399, 1251], 1251: [399], 2307: [406, 1772], 2655: [1975], 107: [250, 108, 498, 898, 1937], 250: [584, 304, 694, 499, 107, 108, 708, 31, 736, 170, 498, 33], 108: [304, 498, 898], 1461: [304, 1288, 1593, 1357], 2526: [2448], 2044: [2683, 80, 1149], 969: [954], 1038: [916], 2003: [1999, 2515], 1600: [946, 1293], 658: [1577], 1580: [1577], 52: [2487, 2580], 1823: [2487, 2580], 1107: [1617], 236: [1617, 156], 632: [683, 401], 2622: [2033], 1327: [1165], 520: [524], 208: [2343, 1270], 245: [172], 911: [177], 2192: [177], 1270: [236], 1271: [236], 1109: [2151], 1223: [1607], 1112: [1512, 1073, 926, 2415, 101], 1607: [1223], 177: [885, 1959], 591: [1032, 2475, 2155, 287, 1186, 453, 549, 1358, 2182, 848, 199, 488, 1105], 2636: [2300], 2473: [434, 1192], 597: [434, 224], 1678: [1570, 861], 2207: [79], 268: [616, 232, 2571, 2046], 613: [616, 268, 232, 2571], 2046: [616], 1191: [692], 2692: [1370, 2038, 2172], 1102: [1370, 1479, 1413], 985: [2038, 1102, 159, 2066], 159: [2172, 2543, 2674, 2066], 740: [929, 1559, 418, 1683, 560, 1703, 1664, 1667], 1703: [1559], 1928: [1923, 2114], 2114: [1923], 2227: [1923, 2390, 2072, 1974], 1086: [1641, 1076, 1078, 1582, 1545], 1574: [1017, 484], 1252: [1634, 820, 829, 1485, 1115], 2374: [2513, 2373, 2507, 1366], 310: [2513], 1366: [2513, 2413, 2414, 2562], 392: [154], 2283: [2284], 1293: [2284], 2278: [2284], 134: [505, 1782], 622: [345, 377], 1288: [787, 1128, 1091, 1414, 499, 1502, 107, 108], 1287: [787, 810, 945, 976, 1414, 1437, 1397, 499, 905], 36: [2390, 2072, 1974, 1701, 1066], 571: [19, 2495, 2331, 1762, 2694], 1493: [867, 774], 46: [867, 238], 1374: [869], 1393: [1128, 976, 1414, 1437, 1397, 905, 1502], 2463: [1934], 662: [181, 518], 766: [181, 2532, 518, 979, 78], 2417: [2532], 518: [2532], 2015: [2327, 2034], 1924: [2327], 181: [518], 807: [1438], 128: [103], 2472: [2476], 1841: [564, 1741], 640: [2475, 287, 549, 199, 591, 635, 459, 1363, 1897, 20, 574, 130, 2054], 2246: [2475], 1363: [2475, 2653, 2168, 1364, 1343, 786], 574: [2475, 1006, 2040, 2360, 242, 1962, 1827], 2539: [2213], 2253: [2213, 2591], 2290: [2213], 521: [2312, 156], 2509: [2618, 517, 1962], 1309: [1186, 924], 1268: [1186, 976, 1414, 2323, 788, 1757, 1842, 1898, 2140], 1124: [453, 1566, 1567], 1661: [1358, 1469], 1567: [1358, 1566], 2011: [2182, 2633, 1835], 1026: [1105, 1433], 2356: [1105, 1705], 798: [1105, 507, 1095, 1433], 2299: [1854, 1855], 2075: [1855], 2482: [2645], 2123: [2482], 12: [155], 99: [1803, 1852, 2578, 1613, 1619, 1722, 1255], 1419: [1550, 525], 1427: [1550, 525], 1042: [1435], 1043: [1435, 1042], 1037: [1042, 1610, 1470], 206: [1042, 1891, 536], 1300: [1320, 1342], 2454: [1342], 1014: [1091, 1056], 2696: [2695, 98], 1510: [790], 790: [1510], 1198: [1296, 418, 517, 1683], 2373: [2413, 2414, 2374, 2507, 1366], 85: [2368, 425], 39: [82, 24], 974: [970, 844], 1540: [1045, 842], 68: [1632, 1979, 1845, 979, 2007, 1825], 333: [463], 564: [463], 1869: [2409, 1754], 2543: [2409, 1479, 1776, 2674], 201: [427], 350: [427, 770, 180, 730, 201], 656: [650, 773, 148, 657], 2605: [1401, 2502, 951], 951: [1401], 1828: [2018], 2014: [2018, 1828], 367: [440], 2648: [1889], 730: [1889, 770], 2586: [1889, 2162, 1647], 1193: [1183], 1202: [1183], 2328: [1782], 804: [1585, 1664], 1546: [1376], 306: [1376], 1727: [1376], 2419: [492, 2323, 1268], 750: [244], 2628: [2629], 1515: [1447, 1711, 1581], 1581: [1447, 657, 1711], 1084: [858, 991, 2035], 2556: [858, 1904], 2303: [858, 2035], 2305: [321, 2183, 2150], 1954: [321, 2183, 98, 978, 2150], 2179: [321, 2183, 2150], 1192: [321], 2018: [1828], 1491: [903], 2135: [2333, 2012, 2583, 134], 2071: [1809], 2466: [1830, 831], 2428: [116, 225, 760, 44, 2042, 2021, 1860], 1002: [1735], 690: [445, 2457], 1688: [1228], 1693: [1228], 460: [148], 691: [1557], 924: [1480], 1532: [1480, 1487], 1010: [1651, 1011, 877, 1182], 877: [1651], 1442: [1135], 1368: [991], 858: [991, 2035], 2394: [2427], 1062: [1323], 1195: [1323, 1630], 1145: [1323], 1149: [1323, 1150], 1150: [1323], 2076: [211], 1414: [945], 327: [945, 1532, 324, 326, 1899], 787: [1437], 891: [1437], 905: [1397], 976: [905], 192: [10, 519, 439, 705, 510, 280, 420], 193: [10, 519, 2549, 280, 74, 420, 238], 420: [10, 2590, 519, 439], 519: [10, 2590, 2608, 190, 705, 510, 2223, 2549, 280], 280: [10], 388: [2590], 2448: [519, 2311], 2549: [519, 2608, 280], 356: [190, 510], 2223: [190, 510], 2214: [2223, 420], 2282: [1686, 2217], 837: [1686], 2427: [1888, 2548, 2591, 2391, 507], 507: [1888, 2548, 2391], 2391: [2548, 507], 2431: [2591], 2589: [2424], 1434: [1208], 1429: [1208], 2599: [2162], 1908: [1647, 442, 1893, 2514, 1838, 2561, 1582], 2615: [1647, 2074], 355: [1093, 1094, 2068], 2085: [1093, 1094], 2228: [1093, 1094, 2068], 1659: [1093, 1094], 124: [2400], 1916: [2400], 633: [197], 749: [197], 2150: [2183, 1192], 1272: [1604], 1281: [1604, 948, 1471, 1272, 1451], 1451: [1604, 1272], 1282: [948, 1471, 1272, 1281], 1446: [948], 1633: [1471], 693: [1471, 531, 1501], 447: [1471, 1501], 1346: [1272, 1451, 1281], 240: [2308], 2652: [2308], 1301: [960, 2346, 1266], 1963: [2195], 286: [2195, 178, 1963], 1550: [1238, 1555, 684, 1199, 1158, 1425], 1710: [1450, 576], 43: [576], 86: [249, 373, 652], 196: [216, 195, 707], 409: [1606, 1298, 753, 2582], 1819: [2580, 224], 2626: [2399], 2398: [2399], 2635: [2231], 1767: [2257, 500], 1768: [2257], 2444: [2257, 500], 563: [500], 2451: [2446, 1973], 803: [357, 53], 243: [357, 53], 1563: [357, 53], 757: [1965, 755], 2053: [2685], 2054: [2685, 591, 2053], 2690: [2053], 488: [2053], 2685: [2053], 2081: [2159, 2366], 2661: [2159, 2506, 2242, 2375], 1794: [2545, 2178, 1795], 2082: [2545, 2178], 2178: [2545, 2582, 1788, 1793, 1795], 1773: [2545], 2181: [2178, 1788, 1793, 1795], 2304: [908], 809: [908, 1190, 1229, 1639, 297, 1222, 1044, 812, 814, 919, 1080, 1365], 919: [908], 1460: [924], 698: [474], 660: [635], 2653: [1363, 2028], 732: [1897, 20, 267], 267: [20], 334: [20, 267], 459: [130, 1705, 267], 2403: [2581], 2616: [2484, 986], 2614: [2484, 2616, 986], 1475: [1642], 555: [400, 556, 2354, 615], 14: [400, 544, 268, 0, 8, 751, 435, 393, 232, 746, 242, 708, 31, 736, 613], 2354: [400, 556], 2360: [400, 556, 2354, 2004, 1386, 1006, 2040], 0: [544, 8, 435], 751: [544, 8, 435], 616: [268], 8: [435], 954: [1591, 450], 871: [1591], 432: [184, 598, 433, 302, 428], 433: [184], 428: [184, 598, 433, 302], 158: [184, 598, 433, 707], 302: [598, 428], 307: [598], 195: [707], 2350: [2168], 1343: [1364, 1456], 308: [1456, 9, 2028], 1469: [1456, 1705], 1095: [1456, 1433], 1364: [1705, 1469], 481: [9], 2651: [2059], 2455: [2354], 1358: [1661], 791: [1661], 1360: [1095], 792: [1095], 20: [267], 614: [615], 966: [1673, 1131, 1140, 1481], 1253: [1131, 1311, 811], 981: [1140], 489: [238, 2670, 301, 1815, 46], 1815: [238, 253, 2670], 652: [373], 1844: [1979, 1845], 1845: [1979, 979], 1779: [2007], 1780: [2007], 1966: [1825], 978: [1825], 979: [1825], 121: [1825, 731], 496: [2393, 501, 1896], 663: [2393, 501], 2393: [1896, 1813, 1952, 2029], 1818: [1813, 2029], 1896: [1851], 269: [24], 1029: [1090, 2456], 1023: [1090], 48: [2456, 1783, 2682], 41: [2456, 1733, 889, 94, 1982, 1713, 892, 352, 23, 115], 1504: [1108, 1276, 1507, 1525, 1308, 1415], 1506: [1108, 1276, 1504, 1507, 1525, 1308, 1415], 1507: [1108], 1503: [1108, 1276, 1504, 1507, 1525, 1308], 1525: [1276, 1415], 1308: [1504, 1507], 1636: [2074, 1073, 928, 983, 1684], 1647: [2074, 1684], 1182: [1073, 1684, 1726], 1389: [821], 1421: [821], 607: [821, 1055], 1658: [884], 114: [171], 883: [1665, 1210], 1453: [1210], 1324: [2076, 1747, 260, 1543], 419: [1747, 260], 1326: [1747, 971, 419, 2144, 1602, 776, 2161, 2434, 248, 105], 260: [2166, 2524, 419], 2088: [2167, 2524, 2525, 776, 248], 2166: [2167, 2524, 971, 776], 2306: [2594], 444: [753], 405: [753], 1875: [2439], 2553: [1875], 1891: [1875, 1149, 1774], 462: [2582], 200: [1891, 536], 58: [1891, 536, 990, 2495], 1464: [1470], 904: [1733, 889, 892, 352, 115], 2153: [889, 1982, 115, 1977, 1992, 1716, 2002, 2008, 2022], 1713: [889, 1982, 892, 115, 138, 416, 41, 1992, 1716, 2002, 1870, 1978, 1991, 2510], 892: [889], 126: [94, 2208], 1716: [1982, 1713, 892, 138, 41, 1991], 2002: [1713], 138: [892, 115, 748, 416], 1372: [352, 896], 759: [23, 93, 95, 99], 2348: [1783, 2682], 734: [213], 2186: [2321], 2187: [2321, 531], 2461: [2492, 1076, 1078], 1078: [1076], 1076: [1078], 442: [401], 557: [423], 704: [376], 641: [376, 1739, 484], 782: [1532], 642: [484], 2252: [2277, 2271], 2269: [2277], 1277: [2277, 1476], 1921: [2402], 1170: [517], 164: [517], 1700: [1157], 741: [1157, 687], 1704: [1157], 1933: [36, 748], 1591: [36, 33, 1701], 1884: [1886, 745, 1902], 1885: [1886, 745, 1902], 2579: [2336, 2279], 1344: [1138, 1121, 1047], 540: [1121, 1640, 603], 1348: [1047, 612], 1046: [1344], 1047: [1344, 1420, 603, 1355], 629: [612], 801: [313, 1475], 1642: [313, 1475], 534: [313], 1680: [1123], 1674: [1123, 1680], 2675: [1824, 1964, 1862, 1256], 2006: [1824], 1862: [1964, 1863], 1848: [2483], 485: [2514, 2380, 1838, 2561], 1838: [2514, 2380], 2558: [2514, 450], 2561: [2380], 2226: [2478, 2139, 1958, 1950], 860: [1940], 1964: [1862, 1863], 763: [2355], 2066: [2062], 2060: [2062], 2052: [2062], 2016: [467, 342], 495: [467, 270], 464: [467], 342: [467, 270, 276, 495, 729, 1945, 2546, 1993, 776], 729: [467, 1945, 1993], 2115: [2000, 276], 467: [2000, 270, 276, 495, 729], 339: [270, 495, 1549, 1602], 573: [270, 464], 1993: [276, 1945, 2546], 270: [276], 1452: [1375, 1289, 560], 2315: [2287], 661: [1191], 2707: [1389, 774], 1340: [1075], 2596: [2680, 2686], 101: [2680, 2686], 246: [560], 2056: [2095], 2532: [2417], 1857: [2681], 78: [2681, 469], 2540: [1857], 2020: [1857], 1059: [1055], 1284: [1055, 1169], 524: [1169, 1454], 2530: [2415], 671: [1599], 2430: [1860], 703: [2233], 1690: [995], 664: [978], 2222: [2150], 2092: [2015], 1566: [1567], 185: [389], 324: [389, 278, 326, 185], 332: [389, 324, 185, 1064], 351: [730], 1990: [201, 2093, 233], 2131: [2319, 2030], 299: [746, 2083, 1827, 2110], 1116: [708, 31, 736, 1593], 2496: [613, 2571], 2506: [2416, 2575, 2458], 2375: [2416], 2366: [2506, 2575, 2458], 399: [1251], 522: [525], 684: [525, 1425], 1425: [525, 684], 1878: [2571], 1176: [254, 2272], 1724: [1492, 1508], 1827: [1962, 2083, 2110], 2265: [2347], 1321: [1190, 297, 1222, 814, 1080], 1129: [1190, 1222, 814, 919, 1080], 1615: [1229, 1639], 441: [297], 812: [1044, 1080, 1365], 814: [1044, 812, 1365], 1365: [1044, 812], 1341: [1044, 812], 1080: [814], 2351: [1772], 1640: [603, 1355], 670: [603], 326: [324, 278], 1411: [2203], 2397: [2203, 2405], 2405: [2203], 1119: [2405], 2129: [2454], 1058: [2454, 1211], 4: [170], 1701: [33, 832], 2221: [1757], 1757: [1842], 2102: [2664], 1831: [1832, 1836], 1836: [1832], 1832: [1836], 725: [93], 358: [95, 1255], 1255: [95], 2578: [99], 2679: [99, 2578], 1648: [1355], 1679: [1726], 1684: [1726], 832: [1701], 33: [1701], 2169: [2420], 1894: [2524, 827, 909, 2144], 913: [827, 909], 909: [827], 398: [419, 776, 248], 248: [419, 105], 1867: [1754], 2242: [2458], 1689: [1738, 38], 2021: [2218, 87], 2337: [2218, 2606], 2132: [1837], 1586: [1388], 1027: [1388, 56], 291: [487], 1662: [1036], 1528: [151, 1487], 360: [151], 885: [831], 1439: [53], 866: [890], 389: [1937, 185], 1064: [1937], 623: [1937, 2140, 1064, 637], 2293: [1937], 930: [1062], 1254: [1062], 364: [590], 365: [590, 2572], 366: [590], 2112: [2572], 646: [450], 2598: [854, 956, 1230, 1232, 1631, 2330], 1231: [854, 956, 1230, 1232, 1985, 2086], 1737: [1736], 1736: [1690], 1495: [1231], 2641: [1745], 1940: [2250], 2379: [2206, 1984], 2133: [2206, 2199, 1925], 1161: [2324], 2194: [1955, 2188, 2190, 2189, 2149], 2149: [1955, 2188], 2190: [1955, 2188, 2365, 2189, 2149], 2189: [2188, 2190, 2365], 249: [652], 2128: [2479, 2030], 2319: [2479], 2130: [2030], 1265: [921], 880: [815], 120: [1959], 2259: [1959, 2271], 1620: [1180], 2429: [2281, 2624, 1143], 1134: [2281, 1264], 1404: [1143, 755], 2551: [2473], 1602: [1549], 971: [1602], 276: [2546, 1993], 778: [56], 2249: [2271], 2250: [2271], 2238: [2271], 2511: [2083, 2320, 2141, 2418, 2345], 1853: [2320], 2418: [2141, 2345], 2141: [2418], 2005: [2345], 689: [1826, 1580], 2630: [1826], 1625: [1580], 1654: [852, 855, 1338], 1015: [852, 1338, 996], 1338: [855], 784: [1338, 1613], 917: [1338], 1806: [2279], 1489: [837], 2167: [2161], 1747: [2434, 2160], 2069: [2434], 2434: [105, 2160], 275: [464], 279: [464], 916: [2686, 2373, 2374], 1770: [1938], 1764: [1938], 1765: [1938], 32: [679], 1471: [1501], 1244: [1596], 1668: [2535], 2108: [1898, 2140], 1898: [2140, 278], 2292: [2140, 1899], 1666: [1133], 2590: [388], 439: [388], 1555: [1158], 1455: [1694], 1167: [1330], 1474: [1249], 1473: [1249, 1474], 857: [2060], 2576: [1051], 2438: [2091], 253: [2670, 1815], 238: [301, 46], 301: [46], 2407: [1631, 2704], 2604: [1631, 2330], 2330: [1631, 1769], 912: [1149, 1150], 1631: [2330], 2660: [1933], 748: [1933, 138], 416: [138], 137: [138, 748, 416], 990: [1774], 1630: [1150], 2065: [1985, 2086], 952: [1064], 1937: [1064], 993: [973], 802: [973], 527: [2606], 2459: [2606, 553], 123: [553], 261: [626], 1804: [2289], 1805: [2289], 1926: [748], 1941: [748], 2510: [416, 1977, 2002, 1978, 1991], 1913: [416], 923: [1416, 1233], 758: [265], 265: [758], 2285: [2311, 2138], 1738: [1264], 2200: [2703, 1797], 1221: [2703, 1797], 379: [176], 756: [176], 2528: [1858], 2481: [1858], 2592: [1852], 1619: [1613, 1614], 1614: [1613], 1722: [1619], 1723: [1619, 1722], 894: [1255], 2236: [2542, 2435], 2262: [2542, 2435], 346: [337, 363], 363: [337], 77: [337], 341: [529, 709], 340: [529, 341, 709], 709: [529, 341], 283: [2093], 2275: [2445], 2268: [2445, 2275], 1120: [2275], 805: [990], 1774: [990], 1169: [1454], 273: [372], 1294: [915], 515: [637], 152: [1383], 1384: [1383], 962: [829], 820: [829, 1115], 1714: [1458], 838: [1458], 508: [317], 1034: [1730], 873: [920], 595: [233], 2668: [2365], 1536: [2504], 1511: [1357], 1598: [780], 1572: [1598], 272: [914], 1551: [914, 272], 1866: [2272], 1005: [1313], 1382: [1313], 209: [2704], 714: [1035, 1299, 833], 1392: [1299], 1530: [1424], 1613: [1614], 2216: [2688, 2705], 1840: [2688, 2705], 2646: [2643], 2643: [2646], 1260: [1104], 1262: [1104], 2392: [1925], 2206: [1925, 1984], 2697: [2700, 2288, 1759], 2698: [2700, 1759], 1759: [2288], 888: [896, 41], 468: [896, 2377, 182, 491], 2025: [2377], 491: [2377, 182], 2219: [1977, 2008, 2022], 2318: [1977, 491], 2022: [2008], 2008: [2022], 2247: [2701], 2263: [2701], 2362: [1870, 2510], 1761: [2208], 2234: [2208], 2235: [2208], 1906: [2421], 115: [2421], 592: [282, 1144, 589], 589: [282], 1509: [1144, 589], 1999: [2515], 371: [2507], 1373: [1366], 2562: [2595], 1315: [2595], 1777: [2064], 1802: [2064], 914: [272], 2694: [2331], 2495: [1762, 2694, 571], 1762: [2694, 571], 19: [571], 1477: [1252], 1485: [1252], 1886: [745, 1902]}
The hypergraph features for node 538, index 0 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.77733701  0.21510649 -0.32014836 -0.32680698  0.34136975]]
The hypergraph features for node 163, index 1 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.17888165  0.02033126 -0.49075605 -0.50625002  0.29207835]]
The hypergraph features for node 219, index 2 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.77733701  0.         -0.48051646 -0.50500486  0.25483317]]
The hypergraph features for node 1114, index 3 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.72739442  0.15091111 -0.21246583  0.          0.35792589]]
The hypergraph features for node 881, index 4 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.72739442 -0.22515059 -0.44737637 -0.31466751  0.19050864]]
The hypergraph features for node 427, index 5 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.29937696  0.06495672 -0.45230817 -0.39354364  0.22244691]]
The hypergraph features for node 2667, index 6 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.97633517 -0.33164942 -0.67845967 -0.72739442  0.26545669]]
The hypergraph features for node 343, index 7 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.72095954  0.         -0.36983003 -0.31355546  0.23936942]]
The hypergraph features for node 1305, index 8 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.72095954  0.         -0.47232826 -0.58518358  0.26796394]]
The hypergraph features for node 1303, index 9 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.72095954 -0.46948151 -0.59522053 -0.59522053  0.12573902]]
The hypergraph features for node 1136, index 10 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.76958609  0.         -0.48034165 -0.58518358  0.24809863]]
The hypergraph features for node 566, index 11 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.56860662 -0.05142467 -0.61877202 -0.56510284  0.31794073]]
The hypergraph features for node 1206, index 12 are 
 [[0. 0. 1. ... 0. 0. 0.]]
We add the encoding:
 [[-1.12121443 -0.12163143 -0.62142293 -0.62142293  0.4997915 ]]
The hypergraph features for node 1409, index 13 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.12121443  0.08193282 -0.40891757 -0.44183741  0.40816704]]
The hypergraph features for node 523, index 14 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.12121443 -0.18084474 -0.61821153 -0.64125472  0.32477208]]
The hypergraph features for node 2220, index 15 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.12121443 -0.36156756 -0.67708515 -0.77733701  0.2436693 ]]
The hypergraph features for node 129, index 16 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.12121443  0.08193282 -0.42123721 -0.480748    0.38594991]]
The hypergraph features for node 1969, index 17 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.12121443  0.21510649 -0.51233084 -0.63088457  0.55195389]]
The hypergraph features for node 2165, index 18 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.12121443  0.         -0.64681801 -0.73302881  0.46952086]]
The hypergraph features for node 1719, index 19 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.12121443 -1.12121443 -1.12121443 -1.12121443  0.        ]]
The hypergraph features for node 1834, index 20 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.12121443 -1.12121443 -1.12121443 -1.12121443  0.        ]]
The hypergraph features for node 55, index 21 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.12121443 -0.47286133 -0.78217725 -0.75245599  0.26552207]]
The hypergraph features for node 1153, index 22 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.87168217  0.         -0.3803108  -0.31790958  0.25444842]]
The hypergraph features for node 100, index 23 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.65873416 -0.1213492  -0.30047752 -0.1213492   0.2533257 ]]
The hypergraph features for node 1380, index 24 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.65873416 -0.28354715 -0.47114066 -0.47114066  0.1875935 ]]
The hypergraph features for node 1379, index 25 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.65873416 -0.65873416 -0.65873416 -0.65873416  0.        ]]
The hypergraph features for node 1184, index 26 are 
 [[0. 0. 0. ... 0. 1. 0.]]
We add the encoding:
 [[-0.75245599 -0.16185975 -0.51830631 -0.51015086  0.22087866]]
The hypergraph features for node 188, index 27 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.08917737 -0.18865123 -0.4994999  -0.48121879  0.23428301]]
The hypergraph features for node 2490, index 28 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.65873416 -0.65873416 -0.65873416 -0.65873416  0.        ]]
The hypergraph features for node 1728, index 29 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.08917737 -0.09568412 -0.4664004  -0.38307775  0.33171507]]
The hypergraph features for node 1729, index 30 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.65873416 -0.65873416 -0.65873416 -0.65873416  0.        ]]
The hypergraph features for node 1016, index 31 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.76958609  0.         -0.53730623 -0.58122712  0.22375022]]
The hypergraph features for node 793, index 32 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.87168217  0.0142325  -0.36709127 -0.33085003  0.22901975]]
The hypergraph features for node 2409, index 33 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.65873416 -0.21547306 -0.45779968 -0.47849576  0.15807713]]
The hypergraph features for node 501, index 34 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.65873416 -0.65873416 -0.65873416 -0.65873416  0.        ]]
The hypergraph features for node 503, index 35 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.97633517 -0.31733731 -0.71659813 -0.81513921  0.23434086]]
The hypergraph features for node 2529, index 36 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.69831169 -0.1703974  -0.43435454 -0.43435454  0.26395714]]
The hypergraph features for node 145, index 37 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.69831169 -0.31790958 -0.53181692 -0.54037686  0.11450549]]
The hypergraph features for node 2164, index 38 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.08729959 -0.69831169 -0.89280564 -0.89280564  0.19449395]]
The hypergraph features for node 582, index 39 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.77733701 -0.69831169 -0.75515862 -0.77249289  0.03288012]]
The hypergraph features for node 2067, index 40 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.69831169 -0.69831169 -0.69831169 -0.69831169  0.        ]]
The hypergraph features for node 430, index 41 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.90245144  0.06495672 -0.43630995 -0.46588554  0.26684644]]
The hypergraph features for node 1696, index 42 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.0317452  -0.25840848 -0.65843951 -0.79513625  0.29280341]]
The hypergraph features for node 1130, index 43 are 
 [[0. 1. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.87168217 -0.55330217 -0.71249217 -0.71249217  0.15919   ]]
The hypergraph features for node 1523, index 44 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.87168217  0.17333941 -0.39542268 -0.44167397  0.38250066]]
The hypergraph features for node 1424, index 45 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.80817133 -0.80817133 -0.80817133 -0.80817133  0.        ]]
The hypergraph features for node 2205, index 46 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.80817133 -0.80817133 -0.80817133 -0.80817133  0.        ]]
The hypergraph features for node 727, index 47 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.80817133 -0.80817133 -0.80817133 -0.80817133  0.        ]]
The hypergraph features for node 1936, index 48 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.80817133 -0.80817133 -0.80817133 -0.80817133  0.        ]]
The hypergraph features for node 744, index 49 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.17888165 -0.33164942 -0.72498618 -0.69470682  0.31157542]]
The hypergraph features for node 2030, index 50 are 
 [[1. 0. 1. ... 0. 0. 0.]]
We add the encoding:
 [[-1.17888165  0.02033126 -0.74536913 -0.80817133  0.4456557 ]]
The hypergraph features for node 743, index 51 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.80817133 -0.33164942 -0.57368769 -0.5812423   0.19461258]]
The hypergraph features for node 764, index 52 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.80817133 -0.05846848 -0.59875025 -0.75245599  0.27878732]]
The hypergraph features for node 1533, index 53 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.87012184  0.         -0.36449273 -0.16185975  0.35708008]]
The hypergraph features for node 846, index 54 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.76958609 -0.16185975 -0.44696821 -0.45716667  0.20021702]]
The hypergraph features for node 934, index 55 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.75245599 -0.16185975 -0.53717321 -0.56509995  0.20364857]]
The hypergraph features for node 965, index 56 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.97633517 -0.16185975 -0.49746185 -0.58525386  0.23407041]]
The hypergraph features for node 910, index 57 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.09216762  0.02033126 -0.5427579  -0.60506936  0.28184544]]
The hypergraph features for node 1249, index 58 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.74289966 -0.16185975 -0.45237971 -0.45237971  0.29051995]]
The hypergraph features for node 1715, index 59 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.36305181 -0.16185975 -0.26245578 -0.26245578  0.10059603]]
The hypergraph features for node 1592, index 60 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.80396005 -0.12813082 -0.37127768 -0.27158244  0.2443185 ]]
The hypergraph features for node 1257, index 61 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.4472272  -0.31790958 -0.35846601 -0.34535353  0.04549804]]
The hypergraph features for node 1771, index 62 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.4472272 -0.4472272 -0.4472272 -0.4472272  0.       ]]
The hypergraph features for node 2280, index 63 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.4472272 -0.4472272 -0.4472272 -0.4472272  0.       ]]
The hypergraph features for node 1441, index 64 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.28354715 -0.28354715 -0.28354715 -0.28354715  0.        ]]
The hypergraph features for node 941, index 65 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.28354715 -0.28354715 -0.28354715 -0.28354715  0.        ]]
The hypergraph features for node 1946, index 66 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.28354715 -0.28354715 -0.28354715 -0.28354715  0.        ]]
The hypergraph features for node 1796, index 67 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.28354715  0.08537722 -0.03659248  0.0259      0.14484525]]
The hypergraph features for node 2237, index 68 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.28354715 -0.28354715 -0.28354715 -0.28354715  0.        ]]
The hypergraph features for node 2298, index 69 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.28354715 -0.28354715 -0.28354715 -0.28354715  0.        ]]
The hypergraph features for node 537, index 70 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.51351297  0.         -0.26568671 -0.28354715  0.21002085]]
The hypergraph features for node 1216, index 71 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.25840848 -0.04814732 -0.11823437 -0.04814732  0.09911806]]
The hypergraph features for node 1218, index 72 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.69831169  0.01479248 -0.23859312 -0.16578818  0.23829044]]
The hypergraph features for node 2202, index 73 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.61250126 -0.25840848 -0.39627311 -0.31790958  0.15481399]]
The hypergraph features for node 2211, index 74 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.34131169 -0.25840848 -0.76635442 -0.69934309  0.44462548]]
The hypergraph features for node 995, index 75 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.82989943  0.03145912 -0.39262619 -0.47887578  0.36460385]]
The hypergraph features for node 938, index 76 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.98092735 -0.25840848 -0.61966791 -0.61966791  0.36125943]]
The hypergraph features for node 728, index 77 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.76866901 -0.25840848 -0.55421236 -0.59488597  0.18535161]]
The hypergraph features for node 168, index 78 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.50625002 -0.25718953 -0.38171978 -0.38171978  0.12453025]]
The hypergraph features for node 565, index 79 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.56860662  0.         -0.4993422  -0.47739092  0.32518441]]
The hypergraph features for node 902, index 80 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.75245599 -0.33648621 -0.57464788 -0.60878583  0.14361899]]
The hypergraph features for node 636, index 81 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.66926134 -0.05846848 -0.42169166 -0.47849576  0.17387597]]
The hypergraph features for node 2204, index 82 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.50625002 -0.50625002 -0.50625002 -0.50625002  0.        ]]
The hypergraph features for node 294, index 83 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.56860662  0.3610733  -0.57746927 -0.53599618  0.36894547]]
The hypergraph features for node 1462, index 84 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.56543837  0.         -0.28271918 -0.28271918  0.28271918]]
The hypergraph features for node 569, index 85 are 
 [[0. 1. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.91528765  0.02487053 -0.40078882 -0.38382001  0.34941602]]
The hypergraph features for node 826, index 86 are 
 [[0. 1. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.91528765  0.         -0.50046843 -0.56543837  0.29859917]]
The hypergraph features for node 828, index 87 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.56543837 -0.41378031 -0.48960934 -0.48960934  0.07582903]]
The hypergraph features for node 1117, index 88 are 
 [[0. 0. 0. ... 1. 0. 0.]]
We add the encoding:
 [[-0.77733701  0.15091111 -0.09510296  0.          0.34772929]]
The hypergraph features for node 1333, index 89 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.77733701  0.0142325  -0.36166215 -0.33085003  0.25886962]]
The hypergraph features for node 1583, index 90 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.87012184 -0.45716667 -0.70154184 -0.77733701  0.17690237]]
The hypergraph features for node 2396, index 91 are 
 [[1. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.77733701 -0.77733701 -0.77733701 -0.77733701  0.        ]]
The hypergraph features for node 1534, index 92 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.97939265 -0.17636123 -0.7338457  -0.90126407  0.32794952]]
The hypergraph features for node 289, index 93 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.97939265 -0.36275736 -0.62336899 -0.57566298  0.26484327]]
The hypergraph features for node 37, index 94 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.45716667 -0.45716667 -0.45716667 -0.45716667  0.        ]]
The hypergraph features for node 38, index 95 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.45716667 -0.01225837 -0.23471252 -0.23471252  0.22245415]]
The hypergraph features for node 247, index 96 are 
 [[0. 1. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.52977091  0.15091111 -0.21401766 -0.22858334  0.29499355]]
The hypergraph features for node 1952, index 97 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.97633517 -0.45716667 -0.71675092 -0.71675092  0.25958425]]
The hypergraph features for node 1060, index 98 are 
 [[0. 0. 1. ... 0. 0. 0.]]
We add the encoding:
 [[-0.52977091 -0.3243181  -0.42704451 -0.42704451  0.1027264 ]]
The hypergraph features for node 2039, index 99 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.52977091 -0.52977091 -0.52977091 -0.52977091  0.        ]]
The hypergraph features for node 42, index 100 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.52977091 -0.52977091 -0.52977091 -0.52977091  0.        ]]
The hypergraph features for node 144, index 101 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.67178905 -0.52977091 -0.60077998 -0.60077998  0.07100907]]
The hypergraph features for node 189, index 102 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.67178905  0.         -0.32408488 -0.31065895  0.23782193]]
The hypergraph features for node 1784, index 103 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.86152291  0.01479248 -0.49939852 -0.75245599  0.36026457]]
The hypergraph features for node 132, index 104 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.86152291 -0.86152291 -0.86152291 -0.86152291  0.        ]]
The hypergraph features for node 143, index 105 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.86152291 -0.07425495 -0.53381546 -0.59974199  0.28893163]]
The hypergraph features for node 1224, index 106 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.0142325 0.0142325 0.0142325 0.0142325 0.       ]]
The hypergraph features for node 638, index 107 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.90653467  0.0142325  -0.46427286 -0.40027718  0.1863697 ]]
The hypergraph features for node 1113, index 108 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.0142325 0.0142325 0.0142325 0.0142325 0.       ]]
The hypergraph features for node 378, index 109 are 
 [[0. 0. 1. ... 0. 0. 0.]]
We add the encoding:
 [[-0.84365477  0.66368589 -0.18493264 -0.08127863  0.29866647]]
The hypergraph features for node 659, index 110 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.90210851 -0.43266666 -0.5694767  -0.47156582  0.19296934]]
The hypergraph features for node 2058, index 111 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.58759042  0.02033126 -0.30241255 -0.3399785   0.24960047]]
The hypergraph features for node 411, index 112 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.64389622  0.02033126 -0.26266997 -0.23674673  0.22523088]]
The hypergraph features for node 1646, index 113 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.57727067  0.02033126 -0.2784697  -0.2784697   0.29880097]]
The hypergraph features for node 2659, index 114 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.17888165 -0.5812423  -0.88006197 -0.88006197  0.29881967]]
The hypergraph features for node 1139, index 115 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.69393063 -0.46857462 -0.56089528 -0.54053794  0.09670547]]
The hypergraph features for node 1106, index 116 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.61250126 -0.36156756 -0.48703441 -0.48703441  0.12546685]]
The hypergraph features for node 15, index 117 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.08917737 -0.28004307 -0.78073607 -0.98270667  0.32483028]]
The hypergraph features for node 2173, index 118 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.45716667 -0.45716667 -0.45716667 -0.45716667  0.        ]]
The hypergraph features for node 191, index 119 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.76958609  0.         -0.46682355 -0.63088457  0.33491561]]
The hypergraph features for node 686, index 120 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.99958849 -0.18887901 -0.65086825 -0.60369597  0.25743567]]
The hypergraph features for node 1947, index 121 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.63088457 -0.63088457 -0.63088457 -0.63088457  0.        ]]
The hypergraph features for node 1980, index 122 are 
 [[0. 0. 0. ... 0. 0. 1.]]
We add the encoding:
 [[-0.6417562  -0.1213492  -0.43121    -0.4808673   0.21810862]]
The hypergraph features for node 1685, index 123 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.33648621 -0.18865123 -0.28315185 -0.3243181   0.06700642]]
The hypergraph features for node 2361, index 124 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.54010581 -0.33648621 -0.43829601 -0.43829601  0.1018098 ]]
The hypergraph features for node 964, index 125 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.09216762 -0.33648621 -0.71090993 -0.60878583  0.23620843]]
The hypergraph features for node 1069, index 126 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.3399785  -0.18865123 -0.28431594 -0.3243181   0.06794662]]
The hypergraph features for node 422, index 127 are 
 [[0. 0. 0. ... 0. 0. 1.]]
We add the encoding:
 [[-0.3399785 -0.3399785 -0.3399785 -0.3399785  0.       ]]
The hypergraph features for node 207, index 128 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.19073963 -0.09568412 -0.64255949 -0.64125472  0.44705549]]
The hypergraph features for node 1466, index 129 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.57727067 -0.09568412 -0.33647739 -0.33647739  0.24079328]]
The hypergraph features for node 1558, index 130 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.09568412 -0.09568412 -0.09568412 -0.09568412  0.        ]]
The hypergraph features for node 1159, index 131 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.80571906 -0.09568412 -0.45070159 -0.45070159  0.35501747]]
The hypergraph features for node 1178, index 132 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.34131169  0.03145912 -0.38707984 -0.30423354  0.44363687]]
The hypergraph features for node 2175, index 133 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.69934309 -0.31790958 -0.39796385 -0.34535353  0.10756105]]
The hypergraph features for node 380, index 134 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.98092735 -0.31790958 -0.51502064 -0.44183741  0.23941716]]
The hypergraph features for node 1063, index 135 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.36156756 -0.32448124 -0.3430244  -0.3430244   0.01854316]]
The hypergraph features for node 1911, index 136 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.75245599  0.         -0.35834736 -0.34535353  0.2176585 ]]
The hypergraph features for node 1839, index 137 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.36156756  0.01479248 -0.16404177 -0.14535022  0.15421576]]
The hypergraph features for node 2152, index 138 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 2470, index 139 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.14535022  0.03145912 -0.06481033 -0.07267511  0.08130426]]
The hypergraph features for node 2267, index 140 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.98092735 -0.98092735 -0.98092735 -0.98092735  0.        ]]
The hypergraph features for node 1850, index 141 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.69831169 -0.69831169 -0.69831169 -0.69831169  0.        ]]
The hypergraph features for node 2248, index 142 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.31790958 -0.31790958 -0.31790958 -0.31790958  0.        ]]
The hypergraph features for node 2251, index 143 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.31790958 -0.31790958 -0.31790958 -0.31790958  0.        ]]
The hypergraph features for node 2089, index 144 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.69831169  0.15091111 -0.175416    0.          0.3122623 ]]
The hypergraph features for node 2334, index 145 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.11325788  0.15091111 -0.4046371  -0.47286133  0.36543333]]
The hypergraph features for node 2261, index 146 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.34535353 -0.31790958 -0.33849254 -0.34535353  0.01188358]]
The hypergraph features for node 967, index 147 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.34131169 -0.31790958 -0.67354875 -0.51985717  0.37175424]]
The hypergraph features for node 237, index 148 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.31790958 -0.31790958 -0.31790958 -0.31790958  0.        ]]
The hypergraph features for node 1890, index 149 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.3243181 -0.3243181 -0.3243181 -0.3243181  0.       ]]
The hypergraph features for node 2254, index 150 are 
 [[0. 0. 1. ... 0. 0. 0.]]
We add the encoding:
 [[-0.46948151 -0.3243181  -0.3968998  -0.3968998   0.0725817 ]]
The hypergraph features for node 1775, index 151 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.79513625 -0.3243181  -0.55758551 -0.55330217  0.19223457]]
The hypergraph features for node 1793, index 152 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.55330217 -0.09053888 -0.35941017 -0.3968998   0.17552281]]
The hypergraph features for node 1795, index 153 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.55330217 -0.09053888 -0.30563591 -0.3243181   0.19029789]]
The hypergraph features for node 262, index 154 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.5812423 -0.5812423 -0.5812423 -0.5812423  0.       ]]
The hypergraph features for node 578, index 155 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.97939265 -0.5812423  -0.73432073 -0.70124586  0.13141464]]
The hypergraph features for node 1467, index 156 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.5812423 -0.5812423 -0.5812423 -0.5812423  0.       ]]
The hypergraph features for node 961, index 157 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.25718953 -0.25718953 -0.25718953 -0.25718953  0.        ]]
The hypergraph features for node 988, index 158 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.25718953 -0.25718953 -0.25718953 -0.25718953  0.        ]]
The hypergraph features for node 717, index 159 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.97939265 -0.10383085 -0.48088325 -0.36275736  0.3172483 ]]
The hypergraph features for node 745, index 160 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.56860662 -0.06519316 -0.52544325 -0.50908494  0.31791348]]
The hypergraph features for node 2603, index 161 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.25718953 -0.25718953 -0.25718953 -0.25718953  0.        ]]
The hypergraph features for node 2613, index 162 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.25718953 -0.25718953 -0.25718953 -0.25718953  0.        ]]
The hypergraph features for node 1663, index 163 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.4216094  -0.25718953 -0.33681612 -0.33164942  0.06722348]]
The hypergraph features for node 1859, index 164 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.7142034  -0.25718953 -0.48569646 -0.48569646  0.22850693]]
The hypergraph features for node 1860, index 165 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.37087355  0.08343836 -0.18154157 -0.25718953  0.19303158]]
The hypergraph features for node 552, index 166 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.32464695 -0.25718953 -0.62336715 -0.52489781  0.28243013]]
The hypergraph features for node 82, index 167 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.68178771 -0.02878828 -0.3631362  -0.31790958  0.18947625]]
The hypergraph features for node 455, index 168 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.85444927 -0.85444927 -0.85444927 -0.85444927  0.        ]]
The hypergraph features for node 2537, index 169 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.85444927 -0.48893948 -0.60943567 -0.54717696  0.14344275]]
The hypergraph features for node 2666, index 170 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.85444927 -0.28028929 -0.56736928 -0.56736928  0.28707999]]
The hypergraph features for node 650, index 171 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.8432612  -0.31790958 -0.58058539 -0.58058539  0.26267581]]
The hypergraph features for node 1006, index 172 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.41445085 -0.08041237 -0.2910447  -0.35086479  0.12611424]]
The hypergraph features for node 410, index 173 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.45471647 -0.31790958 -0.36351188 -0.31790958  0.06449139]]
The hypergraph features for node 471, index 174 are 
 [[0. 1. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.71622709 -0.21547306 -0.43239183 -0.44243413  0.15399542]]
The hypergraph features for node 1378, index 175 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.89986599 -0.18084474 -0.49415124 -0.42062656  0.23860402]]
The hypergraph features for node 554, index 176 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.32464695 -0.4249953  -0.67260066 -0.4703802   0.37707748]]
The hypergraph features for node 210, index 177 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.32464695 -0.8432612  -1.08395408 -1.08395408  0.24069287]]
The hypergraph features for node 490, index 178 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.08729959 -0.64308226 -0.80075426 -0.74289966  0.14024669]]
The hypergraph features for node 45, index 179 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.80744497 -0.80744497 -0.80744497 -0.80744497  0.        ]]
The hypergraph features for node 295, index 180 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.86152291  0.05975904 -0.53716408 -0.47739092  0.28849103]]
The hypergraph features for node 406, index 181 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.80744497 -0.80744497 -0.80744497 -0.80744497  0.        ]]
The hypergraph features for node 2480, index 182 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.80744497 -0.65602373 -0.73173435 -0.73173435  0.07571062]]
The hypergraph features for node 1697, index 183 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.18488963 -0.05846848 -0.45234469 -0.38596908  0.39407793]]
The hypergraph features for node 466, index 184 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.80744497 -0.80744497 -0.80744497 -0.80744497  0.        ]]
The hypergraph features for node 162, index 185 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.89986599 -0.36275736 -0.63131168 -0.63131168  0.26855431]]
The hypergraph features for node 875, index 186 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.89986599  0.05975904 -0.34180109 -0.36275736  0.33769646]]
The hypergraph features for node 2341, index 187 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.87012184 -0.03452713 -0.25753272 -0.12163143  0.29516632]]
The hypergraph features for node 167, index 188 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.36275736 -0.36275736 -0.36275736 -0.36275736  0.        ]]
The hypergraph features for node 465, index 189 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.71406957 -0.21547306 -0.45748025 -0.47849576  0.12338546]]
The hypergraph features for node 60, index 190 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.19073963  0.3610733  -0.41483317 -0.41483317  0.77590646]]
The hypergraph features for node 412, index 191 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.19073963 -1.19073963 -1.19073963 -1.19073963  0.        ]]
The hypergraph features for node 738, index 192 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.56860662  0.         -0.63425863 -0.50146353  0.39491186]]
The hypergraph features for node 1573, index 193 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.97633517 -0.05846848 -0.61458438 -0.63902358  0.27906624]]
The hypergraph features for node 2687, index 194 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.76958609 -0.05846848 -0.4991053  -0.66926134  0.31425772]]
The hypergraph features for node 1459, index 195 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.89986599 -0.4249953  -0.69285893 -0.72739442  0.18574192]]
The hypergraph features for node 2485, index 196 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.76958609 -0.65602373 -0.71327107 -0.7142034   0.04636633]]
The hypergraph features for node 483, index 197 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.76958609 -0.05142467 -0.31560137 -0.26756442  0.27367813]]
The hypergraph features for node 562, index 198 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.86152291  0.3610733  -0.21986919 -0.08041237  0.33447815]]
The hypergraph features for node 63, index 199 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.86152291 -0.33513375 -0.62614812 -0.68178771  0.21846923]]
The hypergraph features for node 559, index 200 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.7142034  -0.4249953  -0.54174754 -0.48604393  0.12446552]]
The hypergraph features for node 136, index 201 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.80291271  0.33324254 -0.1165992  -0.11103047  0.31897349]]
The hypergraph features for node 1259, index 202 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.70500309  0.         -0.47040287 -0.59552351  0.28790767]]
The hypergraph features for node 479, index 203 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.33931208 -0.18084474 -0.59153976 -0.49470094  0.33740155]]
The hypergraph features for node 1649, index 204 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.8432612  -0.36455274 -0.68562359 -0.73142614  0.16719001]]
The hypergraph features for node 2658, index 205 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.89746237 -0.05846848 -0.48065826 -0.48604393  0.34253899]]
The hypergraph features for node 843, index 206 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.58394123 -0.39354364 -0.49943049 -0.51011854  0.06902529]]
The hypergraph features for node 173, index 207 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.65602373 -0.52553614 -0.61252787 -0.65602373  0.06151244]]
The hypergraph features for node 611, index 208 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.32478762  0.02033126 -0.70797678 -0.60070217  0.40315917]]
The hypergraph features for node 3, index 209 are 
 [[0. 0. 1. ... 0. 0. 0.]]
We add the encoding:
 [[-0.45471647  0.05975904 -0.14879404 -0.05142467  0.2210307 ]]
The hypergraph features for node 1741, index 210 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.68178771 -0.45471647 -0.56825209 -0.56825209  0.11353562]]
The hypergraph features for node 57, index 211 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.45471647 -0.45471647 -0.45471647 -0.45471647  0.        ]]
The hypergraph features for node 567, index 212 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.99211502  0.         -0.49839951 -0.50058635  0.31388356]]
The hypergraph features for node 2406, index 213 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.72739442 -0.72739442 -0.72739442 -0.72739442  0.        ]]
The hypergraph features for node 2474, index 214 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.87012184 -0.72739442 -0.79875813 -0.79875813  0.07136371]]
The hypergraph features for node 1983, index 215 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.72739442 -0.72739442 -0.72739442 -0.72739442  0.        ]]
The hypergraph features for node 2404, index 216 are 
 [[0. 0. 0. ... 0. 1. 0.]]
We add the encoding:
 [[-0.72739442 -0.72739442 -0.72739442 -0.72739442  0.        ]]
The hypergraph features for node 1527, index 217 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.78059749  0.01326978 -0.35894948 -0.40801526  0.34009153]]
The hypergraph features for node 898, index 218 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.72739442 -0.72739442 -0.72739442 -0.72739442  0.        ]]
The hypergraph features for node 1118, index 219 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.89986599 -0.53303804 -0.71645202 -0.71645202  0.18341398]]
The hypergraph features for node 492, index 220 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.89986599 -0.54466143 -0.71571471 -0.74289966  0.11764428]]
The hypergraph features for node 558, index 221 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.96296299  0.08343836 -0.48623947 -0.45045906  0.26810039]]
The hypergraph features for node 1687, index 222 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.11325788  0.         -0.60029835 -0.69393063  0.33923725]]
The hypergraph features for node 621, index 223 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.4249953 -0.4249953 -0.4249953 -0.4249953  0.       ]]
The hypergraph features for node 66, index 224 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.04135609  0.         -0.43489332 -0.4249953   0.34522517]]
The hypergraph features for node 35, index 225 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.4249953 -0.4249953 -0.4249953 -0.4249953  0.       ]]
The hypergraph features for node 1934, index 226 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.7142034 -0.7142034 -0.7142034 -0.7142034  0.       ]]
The hypergraph features for node 682, index 227 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.7142034 -0.7142034 -0.7142034 -0.7142034  0.       ]]
The hypergraph features for node 1559, index 228 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.7142034  -0.05846848 -0.3383941  -0.29045226  0.24618832]]
The hypergraph features for node 2647, index 229 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.7142034  -0.39354364 -0.55387352 -0.55387352  0.16032988]]
The hypergraph features for node 87, index 230 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.7142034  -0.54645622 -0.63032981 -0.63032981  0.08387359]]
The hypergraph features for node 631, index 231 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.7142034  -0.38382001 -0.52799613 -0.48596497  0.13811411]]
The hypergraph features for node 2243, index 232 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.54645622 -0.25508531 -0.40077076 -0.40077076  0.14568546]]
The hypergraph features for node 2471, index 233 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.54645622 -0.54645622 -0.54645622 -0.54645622  0.        ]]
The hypergraph features for node 1994, index 234 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.54645622 -0.30608807 -0.38621079 -0.30608807  0.11331063]]
The hypergraph features for node 1995, index 235 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.54645622 -0.30608807 -0.38621079 -0.30608807  0.11331063]]
The hypergraph features for node 1800, index 236 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.57589328  0.0288628  -0.28980165 -0.30608807  0.21443295]]
The hypergraph features for node 2301, index 237 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.57589328 -0.30608807 -0.44099068 -0.44099068  0.13490261]]
The hypergraph features for node 929, index 238 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.18084474 -0.18084474 -0.18084474 -0.18084474  0.        ]]
The hypergraph features for node 2118, index 239 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.18084474 -0.18084474 -0.18084474 -0.18084474  0.        ]]
The hypergraph features for node 71, index 240 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.89746237  0.08343836 -0.52929376 -0.64171573  0.39243762]]
The hypergraph features for node 882, index 241 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.47739092 -0.18084474 -0.37566638 -0.46876347  0.13780472]]
The hypergraph features for node 1529, index 242 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.18084474 -0.18084474 -0.18084474 -0.18084474  0.        ]]
The hypergraph features for node 575, index 243 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.65602373 -0.18084474 -0.49763073 -0.65602373  0.22400153]]
The hypergraph features for node 747, index 244 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.21355339  0.02033126 -0.54106354 -0.54645622  0.22735772]]
The hypergraph features for node 583, index 245 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.40016699 -0.36275736 -0.81685933 -0.77249289  0.38285654]]
The hypergraph features for node 2624, index 246 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.91628631  0.         -0.47428862 -0.49043408  0.34735456]]
The hypergraph features for node 1975, index 247 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.11325788 -1.11325788 -1.11325788 -1.11325788  0.        ]]
The hypergraph features for node 615, index 248 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.11325788 -0.24689952 -0.58068131 -0.38188653  0.38059929]]
The hypergraph features for node 712, index 249 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.11325788  0.15091111 -0.45532496 -0.58053621  0.4363505 ]]
The hypergraph features for node 2368, index 250 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.64125472 -0.64125472 -0.64125472 -0.64125472  0.        ]]
The hypergraph features for node 74, index 251 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.02597912  0.3610733  -0.25825502 -0.40801526  0.44104758]]
The hypergraph features for node 1146, index 252 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.64125472  0.0288628  -0.30619596 -0.30619596  0.33505876]]
The hypergraph features for node 417, index 253 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.64125472 -0.58759042 -0.61442257 -0.61442257  0.02683215]]
The hypergraph features for node 2326, index 254 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.64125472  0.03700752 -0.39727921 -0.58759042  0.3078676 ]]
The hypergraph features for node 22, index 255 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.71406957  0.15091111 -0.28157923 -0.28157923  0.43249034]]
The hypergraph features for node 415, index 256 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.71406957 -0.12163143 -0.39212251 -0.47286133  0.19427085]]
The hypergraph features for node 1408, index 257 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.28004307 -0.28004307 -0.28004307 -0.28004307  0.        ]]
The hypergraph features for node 2402, index 258 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.28004307 -0.28004307 -0.28004307 -0.28004307  0.        ]]
The hypergraph features for node 1931, index 259 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.74289966 -0.28004307 -0.60801075 -0.70455015  0.19192375]]
The hypergraph features for node 608, index 260 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.02597912 -0.28004307 -0.79699479 -1.02597912  0.28197496]]
The hypergraph features for node 125, index 261 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.42606763 -0.28004307 -0.35305535 -0.35305535  0.07301228]]
The hypergraph features for node 133, index 262 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.28004307 -0.28004307 -0.28004307 -0.28004307  0.        ]]
The hypergraph features for node 157, index 263 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.21355339  0.08343836 -0.39765555 -0.4071912   0.31722378]]
The hypergraph features for node 2095, index 264 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.02597912 -1.02597912 -1.02597912 -1.02597912  0.        ]]
The hypergraph features for node 90, index 265 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.02597912 -1.02597912 -1.02597912 -1.02597912  0.        ]]
The hypergraph features for node 76, index 266 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.02597912 -0.11967959 -0.42785518 -0.32136416  0.27584177]]
The hypergraph features for node 1079, index 267 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.32478762 -0.17636123 -0.64852239 -0.58176792  0.36596848]]
The hypergraph features for node 668, index 268 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.02597912  0.33328801 -0.44758719 -0.53122004  0.33746652]]
The hypergraph features for node 331, index 269 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.02597912 -0.53303804 -0.71163681 -0.57589328  0.22296107]]
The hypergraph features for node 2378, index 270 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.21355339  0.3610733  -0.35741271 -0.28858536  0.60198264]]
The hypergraph features for node 1935, index 271 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.59750199 -0.58176792 -0.58963495 -0.58963495  0.00786704]]
The hypergraph features for node 719, index 272 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.79513625 -0.49470094 -0.68256004 -0.69801706  0.08834267]]
The hypergraph features for node 1672, index 273 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.03943157  0.02033126 -0.68236117 -0.86236306  0.39252989]]
The hypergraph features for node 1564, index 274 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.03943157 -1.03943157 -1.03943157 -1.03943157  0.        ]]
The hypergraph features for node 1565, index 275 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.03943157 -1.03943157 -1.03943157 -1.03943157  0.        ]]
The hypergraph features for node 2260, index 276 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.03943157 -1.03943157 -1.03943157 -1.03943157  0.        ]]
The hypergraph features for node 1579, index 277 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.58176792 -0.58176792 -0.58176792 -0.58176792  0.        ]]
The hypergraph features for node 2055, index 278 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.58176792 -0.58176792 -0.58176792 -0.58176792  0.        ]]
The hypergraph features for node 271, index 279 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.58176792 -0.58176792 -0.58176792 -0.58176792  0.        ]]
The hypergraph features for node 2499, index 280 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.32478762 -0.11267148 -0.36021996 -0.26507565  0.29867212]]
The hypergraph features for node 359, index 281 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.32478762 -0.11267148 -0.33519878 -0.26507565  0.29793437]]
The hypergraph features for node 2432, index 282 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.05961013 -1.05961013 -1.05961013 -1.05961013  0.        ]]
The hypergraph features for node 2611, index 283 are 
 [[0. 0. 1. ... 0. 0. 0.]]
We add the encoding:
 [[-1.05961013 -0.43981811 -0.74971412 -0.74971412  0.30989601]]
The hypergraph features for node 407, index 284 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.05961013  0.         -0.79079208 -1.0451951   0.40675065]]
The hypergraph features for node 639, index 285 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.05961013 -0.11967959 -0.58964486 -0.58964486  0.46996527]]
The hypergraph features for node 1644, index 286 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.84365477 -0.05142467 -0.59317176 -0.75445156  0.25467423]]
The hypergraph features for node 2423, index 287 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.75445156  0.66368589 -0.31470145 -0.41901539  0.40830939]]
The hypergraph features for node 160, index 288 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.70597661 -0.42606763 -0.5540143  -0.4698578   0.12202398]]
The hypergraph features for node 2676, index 289 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.70597661 -0.11267148 -0.40932404 -0.40932404  0.29665256]]
The hypergraph features for node 1488, index 290 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.322414    0.02033126 -0.15104137 -0.15104137  0.17137263]]
The hypergraph features for node 1910, index 291 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.48472714  0.2873366  -0.21172753 -0.21956476  0.22500949]]
The hypergraph features for node 1606, index 292 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.97633517  0.02033126 -0.40919545 -0.27158244  0.41836109]]
The hypergraph features for node 1718, index 293 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.73776937  0.02033126 -0.4242526  -0.53122004  0.27199475]]
The hypergraph features for node 2674, index 294 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.02033126 0.08193282 0.05113204 0.05113204 0.03080078]]
The hypergraph features for node 696, index 295 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.86236306  0.02033126 -0.3374764  -0.1703974   0.37923068]]
The hypergraph features for node 1732, index 296 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.70597661  0.66368589 -0.03582796 -0.06519316  0.55954778]]
The hypergraph features for node 1593, index 297 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.70597661 -0.24689952 -0.49760126 -0.51876446  0.19078679]]
The hypergraph features for node 687, index 298 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.97633517 -0.22515059 -0.51918424 -0.47991842  0.23034321]]
The hypergraph features for node 1490, index 299 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.29937696 -0.322414   -0.81089548 -0.81089548  0.48848148]]
The hypergraph features for node 980, index 300 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.29937696 -1.29937696 -1.29937696 -1.29937696  0.        ]]
The hypergraph features for node 560, index 301 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.29937696 -0.44798303 -0.76052757 -0.73969229  0.29086107]]
The hypergraph features for node 1725, index 302 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.4679303   0.17333941 -0.14729545 -0.14729545  0.32063485]]
The hypergraph features for node 1306, index 303 are 
 [[0. 0. 0. ... 0. 1. 0.]]
We add the encoding:
 [[-0.4679303  -0.28088228 -0.37440629 -0.37440629  0.09352401]]
The hypergraph features for node 1050, index 304 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.97633517 -0.4679303  -0.69788282 -0.65112448  0.16541649]]
The hypergraph features for node 1310, index 305 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.65112448 -0.4679303  -0.55952739 -0.55952739  0.09159709]]
The hypergraph features for node 2452, index 306 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.17636123  0.05987751 -0.05824186 -0.05824186  0.11811937]]
The hypergraph features for node 2538, index 307 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.17636123 -0.17636123 -0.17636123 -0.17636123  0.        ]]
The hypergraph features for node 1138, index 308 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.50364423 -0.17636123 -0.3945499  -0.50364423  0.15428268]]
The hypergraph features for node 1148, index 309 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.64861874 -0.17636123 -0.41248999 -0.41248999  0.23612875]]
The hypergraph features for node 486, index 310 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.40016699 -0.17636123 -0.71356594 -0.71468532  0.36258683]]
The hypergraph features for node 1497, index 311 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.87012184 -0.17636123 -0.58780008 -0.65235863  0.25364386]]
The hypergraph features for node 1498, index 312 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.77249289 -0.17636123 -0.4495699  -0.48338576  0.16638069]]
The hypergraph features for node 1537, index 313 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.97939265 -0.36275736 -0.78616954 -0.90126407  0.24652918]]
The hypergraph features for node 612, index 314 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.90126407 -0.36275736 -0.5422596  -0.36275736  0.2538545 ]]
The hypergraph features for node 1535, index 315 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.90126407 -0.36275736 -0.72176184 -0.90126407  0.2538545 ]]
The hypergraph features for node 285, index 316 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.90126407 -0.44798303 -0.67462355 -0.67462355  0.22664052]]
The hypergraph features for node 30, index 317 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.90126407 -0.29427534 -0.61457862 -0.71468532  0.21567547]]
The hypergraph features for node 2544, index 318 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.36275736 -0.36275736 -0.36275736 -0.36275736  0.        ]]
The hypergraph features for node 1345, index 319 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.58625499 -0.36275736 -0.53038059 -0.58625499  0.09677731]]
The hypergraph features for node 2706, index 320 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.58625499 -0.36275736 -0.46566513 -0.44798303  0.0920952 ]]
The hypergraph features for node 1121, index 321 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.40016699 -0.36275736 -0.66016132 -0.50364423  0.37710393]]
The hypergraph features for node 402, index 322 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.61211038 -0.36275736 -0.48743387 -0.48743387  0.12467651]]
The hypergraph features for node 2309, index 323 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.61211038 -0.36275736 -0.52899271 -0.61211038  0.11754614]]
The hypergraph features for node 2302, index 324 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.59135493 -0.36275736 -0.53420554 -0.59135493  0.09898565]]
The hypergraph features for node 2317, index 325 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.59135493 -0.39873158 -0.49504325 -0.49504325  0.09631167]]
The hypergraph features for node 2047, index 326 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.59135493 -0.59135493 -0.59135493 -0.59135493  0.        ]]
The hypergraph features for node 403, index 327 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.59135493 -0.59135493 -0.59135493 -0.59135493  0.        ]]
The hypergraph features for node 2170, index 328 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.61211038 -0.61211038 -0.61211038 -0.61211038  0.        ]]
The hypergraph features for node 1175, index 329 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.61211038 -0.32448124 -0.46829581 -0.46829581  0.14381457]]
The hypergraph features for node 2649, index 330 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.80075498  0.         -0.42989816 -0.48893948  0.32956188]]
The hypergraph features for node 2527, index 331 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.48893948 -0.48893948 -0.48893948 -0.48893948  0.        ]]
The hypergraph features for node 735, index 332 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.48893948  0.         -0.12223487  0.          0.21171701]]
The hypergraph features for node 1961, index 333 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.48893948 -0.48893948 -0.48893948 -0.48893948  0.        ]]
The hypergraph features for node 1847, index 334 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.54717696 -0.48893948 -0.51805822 -0.51805822  0.02911874]]
The hypergraph features for node 1927, index 335 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.48893948 -0.48893948 -0.48893948 -0.48893948  0.        ]]
The hypergraph features for node 1560, index 336 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.5905506  -0.54717696 -0.58187587 -0.5905506   0.01734946]]
The hypergraph features for node 2695, index 337 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.54717696 -0.54717696 -0.54717696 -0.54717696  0.        ]]
The hypergraph features for node 1846, index 338 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.54717696 -0.54717696 -0.54717696 -0.54717696  0.        ]]
The hypergraph features for node 761, index 339 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.5905506  -0.38188653 -0.48621857 -0.48621857  0.10433204]]
The hypergraph features for node 109, index 340 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.96958506 -0.16394556 -0.57271011 -0.54787523  0.23339058]]
The hypergraph features for node 672, index 341 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.90653467 -0.09053888 -0.54470964 -0.46237874  0.22872062]]
The hypergraph features for node 786, index 342 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.5905506 -0.5905506 -0.5905506 -0.5905506  0.       ]]
The hypergraph features for node 1003, index 343 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.03044367  0.         -0.49531175 -0.45922751  0.30303151]]
The hypergraph features for node 1004, index 344 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.03044367  0.         -0.55556754 -0.50250073  0.30248796]]
The hypergraph features for node 2657, index 345 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.5905506 -0.5905506 -0.5905506 -0.5905506  0.       ]]
The hypergraph features for node 1702, index 346 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.5905506  -0.25508531 -0.42281795 -0.42281795  0.16773265]]
The hypergraph features for node 549, index 347 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.5905506  -0.16086523 -0.4065464  -0.45922751  0.16377789]]
The hypergraph features for node 25, index 348 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.96958506 -0.15474045 -0.52666623 -0.45922751  0.26369795]]
The hypergraph features for node 2057, index 349 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.6742183  -0.33338728 -0.4516307  -0.39945862  0.13937489]]
The hypergraph features for node 228, index 350 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.87012184 -0.09051025 -0.41821165 -0.28634951  0.28881838]]
The hypergraph features for node 2182, index 351 are 
 [[0. 0. 1. ... 0. 0. 0.]]
We add the encoding:
 [[-0.6742183  -0.48596497 -0.55611034 -0.53212904  0.07074906]]
The hypergraph features for node 1897, index 352 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.6742183  -0.54466143 -0.60943986 -0.60943986  0.06477844]]
The hypergraph features for node 1812, index 353 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.75445156 -0.17943901 -0.41096746 -0.3808641   0.22029517]]
The hypergraph features for node 986, index 354 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.70500309 -0.27734241 -0.38357889 -0.27734241  0.16563783]]
The hypergraph features for node 1019, index 355 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.75445156  0.66368589 -0.20890461 -0.3808641   0.4444646 ]]
The hypergraph features for node 635, index 356 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.90245144 -0.18887901 -0.51597446 -0.39950126  0.22142345]]
The hypergraph features for node 2210, index 357 are 
 [[0. 1. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.69393063 -0.69393063 -0.69393063 -0.69393063  0.        ]]
The hypergraph features for node 1643, index 358 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.69393063  0.         -0.44522894 -0.6417562   0.31554413]]
The hypergraph features for node 478, index 359 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.90653467 -0.08127863 -0.41507576 -0.31917815  0.26440329]]
The hypergraph features for node 418, index 360 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.64824684  0.02487053 -0.34121783 -0.40027718  0.27795414]]
The hypergraph features for node 1428, index 361 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.40027718  0.16559284 -0.13724552 -0.09051025  0.16882352]]
The hypergraph features for node 1269, index 362 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.70500309 -0.16086523 -0.4558809  -0.47882764  0.20154998]]
The hypergraph features for node 344, index 363 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.33931208  0.16559284 -0.29524497 -0.26588314  0.39654741]]
The hypergraph features for node 848, index 364 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.34131169 -0.15086494 -0.68939292 -0.54642683  0.40254796]]
The hypergraph features for node 847, index 365 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.51985717  0.02487053 -0.35299973 -0.45850615  0.22342324]]
The hypergraph features for node 1893, index 366 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.40027718 -0.40027718 -0.40027718 -0.40027718  0.        ]]
The hypergraph features for node 102, index 367 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.95285312 -0.08127863 -0.3594548  -0.27676566  0.25727738]]
The hypergraph features for node 2032, index 368 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.35747402 -0.35747402 -0.35747402 -0.35747402  0.        ]]
The hypergraph features for node 142, index 369 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.55371698  0.06495672 -0.36790492 -0.37550883  0.18591881]]
The hypergraph features for node 438, index 370 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.35747402  0.06495672 -0.20289824 -0.28088228  0.13519197]]
The hypergraph features for node 227, index 371 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.65738443 -0.32752175 -0.48996283 -0.55371698  0.1265698 ]]
The hypergraph features for node 1236, index 372 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.50540292 -0.35747402 -0.43143847 -0.43143847  0.07396445]]
The hypergraph features for node 692, index 373 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.35747402 -0.35747402 -0.35747402 -0.35747402  0.        ]]
The hypergraph features for node 2673, index 374 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.35747402 -0.24689952 -0.30218677 -0.30218677  0.05528725]]
The hypergraph features for node 1569, index 375 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.18488963 -0.19839289 -0.62966268 -0.64861874  0.24177365]]
The hypergraph features for node 1203, index 376 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.66083521 -0.37087356 -0.57664056 -0.66083521  0.11026082]]
The hypergraph features for node 542, index 377 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.37087356 -0.37087356 -0.37087356 -0.37087356  0.        ]]
The hypergraph features for node 322, index 378 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.45716667 -0.45716667 -0.45716667 -0.45716667  0.        ]]
The hypergraph features for node 1322, index 379 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.45716667 -0.09051025 -0.21559854 -0.17677193  0.1253194 ]]
The hypergraph features for node 1336, index 380 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.45716667  0.00692868 -0.22511899 -0.22511899  0.23204768]]
The hypergraph features for node 1081, index 381 are 
 [[0. 0. 0. ... 1. 0. 0.]]
We add the encoding:
 [[-0.45716667 -0.45716667 -0.45716667 -0.45716667  0.        ]]
The hypergraph features for node 1082, index 382 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.70500309 -0.45716667 -0.62239095 -0.70500309  0.11683121]]
The hypergraph features for node 1007, index 383 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.91525543 -0.41813841 -0.66662374 -0.66655055  0.24848534]]
The hypergraph features for node 2426, index 384 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.84365477 -0.19839289 -0.41325669 -0.3030487   0.22615234]]
The hypergraph features for node 1539, index 385 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.70500309  0.0288628  -0.2962044  -0.25433865  0.28720692]]
The hypergraph features for node 1052, index 386 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.88857281 -0.12080801 -0.46232895 -0.41996749  0.28186575]]
The hypergraph features for node 205, index 387 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.50908494 -0.50908494 -0.50908494 -0.50908494  0.        ]]
The hypergraph features for node 624, index 388 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.70626587 -0.22515059 -0.42435612 -0.38300402  0.18959025]]
The hypergraph features for node 572, index 389 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.28004307 -0.28004307 -0.28004307 -0.28004307  0.        ]]
The hypergraph features for node 1967, index 390 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.45716667 -0.45716667 -0.45716667 -0.45716667  0.        ]]
The hypergraph features for node 655, index 391 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.45716667 -0.45716667 -0.45716667 -0.45716667  0.        ]]
The hypergraph features for node 2536, index 392 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.65602373  0.         -0.40082665 -0.54645622  0.28693526]]
The hypergraph features for node 2612, index 393 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.61320474 -0.54645622 -0.57983048 -0.57983048  0.03337426]]
The hypergraph features for node 2619, index 394 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.90210851 -0.84880972 -0.87545911 -0.87545911  0.0266494 ]]
The hypergraph features for node 2346, index 395 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.90210851 -0.90210851 -0.90210851 -0.90210851  0.        ]]
The hypergraph features for node 2024, index 396 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.90210851 -0.05142467 -0.49314316 -0.50951974  0.36487676]]
The hypergraph features for node 588, index 397 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.60070217 -0.22515059 -0.41292638 -0.41292638  0.18777579]]
The hypergraph features for node 576, index 398 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.60783581  0.         -0.29421471 -0.30858264  0.15271909]]
The hypergraph features for node 354, index 399 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.80571906  0.         -0.38912198 -0.36164689  0.32950666]]
The hypergraph features for node 731, index 400 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.80075498 -0.2464039  -0.60206315 -0.80075498  0.24606049]]
The hypergraph features for node 477, index 401 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.39354364 -0.05142467 -0.17669164 -0.05142467  0.14825324]]
The hypergraph features for node 625, index 402 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.70626587 -0.70626587 -0.70626587 -0.70626587  0.        ]]
The hypergraph features for node 879, index 403 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.70626587 -0.31733731 -0.51180159 -0.51180159  0.19446428]]
The hypergraph features for node 1656, index 404 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.70626587 -0.11267148 -0.31360047 -0.30858264  0.21520916]]
The hypergraph features for node 2662, index 405 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.70626587 -0.1213492  -0.42247773 -0.43981811  0.23910582]]
The hypergraph features for node 1968, index 406 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.84880972 -0.70626587 -0.77753779 -0.77753779  0.07127193]]
The hypergraph features for node 1879, index 407 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.75445156 -0.28028929 -0.58033557 -0.70626587  0.21307478]]
The hypergraph features for node 771, index 408 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.61320474 -0.56216149 -0.58768311 -0.58768311  0.02552163]]
The hypergraph features for node 72, index 409 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.56216149 -0.22515059 -0.39365604 -0.39365604  0.16850545]]
The hypergraph features for node 202, index 410 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.56216149 -0.22515059 -0.39365604 -0.39365604  0.16850545]]
The hypergraph features for node 204, index 411 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.74289966 -0.22515059 -0.57530788 -0.73545786  0.21420028]]
The hypergraph features for node 1645, index 412 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.22515059 -0.22515059 -0.22515059 -0.22515059  0.        ]]
The hypergraph features for node 2410, index 413 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.27595226 -0.22515059 -0.25055143 -0.25055143  0.02540084]]
The hypergraph features for node 217, index 414 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.75445156 -0.61320474 -0.68382815 -0.68382815  0.07062341]]
The hypergraph features for node 779, index 415 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.56510284  0.2873366  -0.13888312 -0.13888312  0.42621972]]
The hypergraph features for node 699, index 416 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.56510284 -0.56510284 -0.56510284 -0.56510284  0.        ]]
The hypergraph features for node 122, index 417 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.53520894 -0.33513375 -0.43517135 -0.43517135  0.10003759]]
The hypergraph features for node 127, index 418 are 
 [[0. 0. 1. ... 0. 0. 0.]]
We add the encoding:
 [[-0.53520894 -0.33513375 -0.43517135 -0.43517135  0.10003759]]
The hypergraph features for node 18, index 419 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.33513375 -0.21828449 -0.26950118 -0.25508531  0.04878047]]
The hypergraph features for node 2191, index 420 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.79574502 -0.79574502 -0.79574502 -0.79574502  0.        ]]
The hypergraph features for node 2520, index 421 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.04135609 -1.04135609 -1.04135609 -1.04135609  0.        ]]
The hypergraph features for node 688, index 422 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.04135609  0.3610733  -0.3401414  -0.3401414   0.70121469]]
The hypergraph features for node 835, index 423 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.04135609 -1.04135609 -1.04135609 -1.04135609  0.        ]]
The hypergraph features for node 649, index 424 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.91604006 -0.1703974  -0.61031665 -0.68927006  0.29032266]]
The hypergraph features for node 2380, index 425 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.91525543 -0.1703974  -0.57156091 -0.71681146  0.29197629]]
The hypergraph features for node 1140, index 426 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.60878583 -0.1703974  -0.46141753 -0.60506936  0.2057879 ]]
The hypergraph features for node 2061, index 427 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.79953076 -0.1703974  -0.41693681 -0.28088228  0.27426912]]
The hypergraph features for node 2583, index 428 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.95285312 -0.1703974  -0.53852386 -0.51542246  0.3618446 ]]
The hypergraph features for node 318, index 429 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.22795486 -0.1703974  -0.73925297 -0.90245144  0.39161072]]
The hypergraph features for node 2607, index 430 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.5974829  -0.1703974  -0.3626665  -0.3201192   0.17693353]]
The hypergraph features for node 887, index 431 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.39288056 -0.1703974  -0.28163898 -0.28163898  0.11124158]]
The hypergraph features for node 1154, index 432 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.20187014 -0.1703974  -0.18613377 -0.18613377  0.01573637]]
The hypergraph features for node 983, index 433 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.60369597 -0.1703974  -0.45719209 -0.5974829   0.20281033]]
The hypergraph features for node 2585, index 434 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.95285312  0.         -0.66687568 -0.8573248   0.39284199]]
The hypergraph features for node 1864, index 435 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.66682879  0.         -0.44455253 -0.66682879  0.31434611]]
The hypergraph features for node 75, index 436 are 
 [[0. 0. 0. ... 1. 1. 0.]]
We add the encoding:
 [[-0.69393063  0.         -0.5388834  -0.66682879  0.26964608]]
The hypergraph features for node 1854, index 437 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.66682879  0.         -0.47101714 -0.66682879  0.2647402 ]]
The hypergraph features for node 1855, index 438 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.66682879  0.         -0.34047604 -0.35459932  0.27241483]]
The hypergraph features for node 539, index 439 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.53822744 -0.12813082 -0.27836033 -0.23674673  0.1373031 ]]
The hypergraph features for node 541, index 440 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.53822744 -0.53822744 -0.53822744 -0.53822744  0.        ]]
The hypergraph features for node 454, index 441 are 
 [[0. 1. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.53822744 -0.12813082 -0.33317913 -0.33317913  0.20504831]]
The hypergraph features for node 2597, index 442 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.53822744 -0.53822744 -0.53822744 -0.53822744  0.        ]]
The hypergraph features for node 1626, index 443 are 
 [[0. 0. 0. ... 0. 1. 0.]]
We add the encoding:
 [[-0.84365477 -0.53822744 -0.64620607 -0.60369597  0.10419155]]
The hypergraph features for node 1377, index 444 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.69393063 -0.27734241 -0.49163909 -0.50364423  0.17028315]]
The hypergraph features for node 2037, index 445 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.84365477 -0.15086494 -0.56281678 -0.69393063  0.297638  ]]
The hypergraph features for node 288, index 446 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.69393063 -0.69393063 -0.69393063 -0.69393063  0.        ]]
The hypergraph features for node 1018, index 447 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.69393063 -0.69393063 -0.69393063 -0.69393063  0.        ]]
The hypergraph features for node 1142, index 448 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.69393063 -0.69393063 -0.69393063 -0.69393063  0.        ]]
The hypergraph features for node 833, index 449 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.16578818  0.08537722 -0.04020548 -0.04020548  0.1255827 ]]
The hypergraph features for node 345, index 450 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.45716667 -0.16578818 -0.2386328  -0.16578818  0.12617059]]
The hypergraph features for node 155, index 451 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.16578818 -0.16578818 -0.16578818 -0.16578818  0.        ]]
The hypergraph features for node 452, index 452 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.16578818 -0.16578818 -0.16578818 -0.16578818  0.        ]]
The hypergraph features for node 513, index 453 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.16578818  0.09329969 -0.03624425 -0.03624425  0.12954393]]
The hypergraph features for node 2421, index 454 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.66455217  0.33328801 -0.25675255 -0.3030487   0.29426889]]
The hypergraph features for node 2040, index 455 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.80396005 -0.36305181 -0.50426477 -0.42502362  0.17633989]]
The hypergraph features for node 2327, index 456 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.54010581 -0.45716667 -0.4910651  -0.47592282  0.03551236]]
The hypergraph features for node 1256, index 457 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.45716667 -0.45716667 -0.45716667 -0.45716667  0.        ]]
The hypergraph features for node 1351, index 458 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.87012184 -0.87012184 -0.87012184 -0.87012184  0.        ]]
The hypergraph features for node 1207, index 459 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.87012184 -0.87012184 -0.87012184 -0.87012184  0.        ]]
The hypergraph features for node 423, index 460 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.87012184 -0.26495111 -0.62599087 -0.74289966  0.26052338]]
The hypergraph features for node 788, index 461 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.87012184 -0.03452713 -0.46263696 -0.3801445   0.28191594]]
The hypergraph features for node 62, index 462 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.87012184 -0.32103221 -0.47727118 -0.35896534  0.22783323]]
The hypergraph features for node 580, index 463 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.87012184 -0.87012184 -0.87012184 -0.87012184  0.        ]]
The hypergraph features for node 1126, index 464 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.87012184 -0.1200861  -0.44140748 -0.38771098  0.33017276]]
The hypergraph features for node 1496, index 465 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.6493814 -0.6493814 -0.6493814 -0.6493814  0.       ]]
The hypergraph features for node 166, index 466 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.91604006 -0.6493814  -0.78271073 -0.78271073  0.13332933]]
The hypergraph features for node 1554, index 467 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.6493814  -0.02878828 -0.21285661 -0.10383085  0.22538138]]
The hypergraph features for node 1553, index 468 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.6493814  -0.20187014 -0.42562577 -0.42562577  0.22375563]]
The hypergraph features for node 581, index 469 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.77249289 -0.65533586 -0.68462512 -0.65533586  0.05073048]]
The hypergraph features for node 1215, index 470 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.77249289 -0.20187014 -0.36963125 -0.25194991  0.22095859]]
The hypergraph features for node 643, index 471 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.77249289 -0.1200861  -0.46528162 -0.65533586  0.28384113]]
The hypergraph features for node 150, index 472 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.40016699 -0.77249289 -1.08632994 -1.08632994  0.31383705]]
The hypergraph features for node 1125, index 473 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.77249289 -0.1200861  -0.34666351 -0.24703752  0.26681078]]
The hypergraph features for node 817, index 474 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.77249289 -0.20187014 -0.48718151 -0.48718151  0.28531137]]
The hypergraph features for node 1212, index 475 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.77249289 -0.1200861  -0.32407982 -0.20187014  0.2610355 ]]
The hypergraph features for node 320, index 476 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.77249289 -0.77249289 -0.77249289 -0.77249289  0.        ]]
The hypergraph features for node 2372, index 477 are 
 [[0. 1. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.77249289 -0.1200861  -0.42218931 -0.37398895  0.26851581]]
The hypergraph features for node 2449, index 478 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.48338576  0.16559284 -0.23059396 -0.37398895  0.28368398]]
The hypergraph features for node 1329, index 479 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.6529299  -0.17677193 -0.39707583 -0.48338576  0.18559563]]
The hypergraph features for node 1292, index 480 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.48338576  0.66368589 -0.10102855 -0.48338576  0.54073476]]
The hypergraph features for node 1123, index 481 are 
 [[0. 0. 0. ... 0. 1. 0.]]
We add the encoding:
 [[-0.37398895 -0.1200861  -0.1756935  -0.12813082  0.09919666]]
The hypergraph features for node 2371, index 482 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.37398895 -0.1200861  -0.24703752 -0.24703752  0.12695142]]
The hypergraph features for node 284, index 483 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.37398895 -0.37398895 -0.37398895 -0.37398895  0.        ]]
The hypergraph features for node 64, index 484 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.40016699 -1.40016699 -1.40016699 -1.40016699  0.        ]]
The hypergraph features for node 1512, index 485 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.43432546 -0.17429226 -0.36003026 -0.43432546  0.11747102]]
The hypergraph features for node 2508, index 486 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.69831169 -0.17429226 -0.43590667 -0.43432546  0.23434994]]
The hypergraph features for node 1608, index 487 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.43432546  0.         -0.22688347 -0.23660421  0.19587035]]
The hypergraph features for node 70, index 488 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.6529299   0.         -0.41797088 -0.40907273  0.21897914]]
The hypergraph features for node 1463, index 489 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.64732248  0.         -0.4322426  -0.54082397  0.26427103]]
The hypergraph features for node 1240, index 490 are 
 [[0. 0. 1. ... 0. 0. 0.]]
We add the encoding:
 [[-0.43432546  0.09329969 -0.27824492 -0.3806058   0.1484352 ]]
The hypergraph features for node 347, index 491 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.43432546  0.09329969 -0.15032518 -0.25194991  0.2097701 ]]
The hypergraph features for node 1053, index 492 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.80370986  0.66368589 -0.12935369 -0.15086494  0.41675427]]
The hypergraph features for node 1337, index 493 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.5773507  -0.15086494 -0.31920299 -0.17677193  0.18723387]]
The hypergraph features for node 1339, index 494 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.51673511 -0.17429226 -0.2561519  -0.1889058   0.12180312]]
The hypergraph features for node 1267, index 495 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.80370986 -0.17429226 -0.48900106 -0.48900106  0.3147088 ]]
The hypergraph features for node 2180, index 496 are 
 [[0. 0. 1. ... 0. 0. 0.]]
We add the encoding:
 [[-0.80370986 -0.17429226 -0.48900106 -0.48900106  0.3147088 ]]
The hypergraph features for node 2342, index 497 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.82073283 -0.15086494 -0.40014939 -0.37087355  0.23271078]]
The hypergraph features for node 1020, index 498 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.17429226 -0.17429226 -0.17429226 -0.17429226  0.        ]]
The hypergraph features for node 2367, index 499 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.17429226 -0.17429226 -0.17429226 -0.17429226  0.        ]]
The hypergraph features for node 431, index 500 are 
 [[0. 0. 1. ... 0. 0. 0.]]
We add the encoding:
 [[-0.3030487  -0.17429226 -0.23867048 -0.23867048  0.06437822]]
The hypergraph features for node 1582, index 501 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.69831169 -0.1889058  -0.38119234 -0.25635954  0.22592183]]
The hypergraph features for node 1909, index 502 are 
 [[0. 0. 1. ... 0. 0. 0.]]
We add the encoding:
 [[-0.69831169 -0.1889058  -0.34998414 -0.25635954  0.20298365]]
The hypergraph features for node 1505, index 503 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.70124586  0.         -0.44810937 -0.64308226  0.31774965]]
The hypergraph features for node 865, index 504 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.70124586  0.         -0.51139349 -0.67216406  0.29620646]]
The hypergraph features for node 251, index 505 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.70124586 -0.64308226 -0.68185799 -0.70124586  0.02741858]]
The hypergraph features for node 2555, index 506 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.64308226 -0.64308226 -0.64308226 -0.64308226  0.        ]]
The hypergraph features for node 2215, index 507 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.73969229 -0.15474045 -0.53189287 -0.70124586  0.26714851]]
The hypergraph features for node 414, index 508 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.70124586  0.         -0.17443942 -0.08041237  0.26618175]]
The hypergraph features for node 13, index 509 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.70124586 -0.70124586 -0.70124586 -0.70124586  0.        ]]
The hypergraph features for node 551, index 510 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.70124586 -0.06250144 -0.38187365 -0.38187365  0.31937221]]
The hypergraph features for node 1312, index 511 are 
 [[0. 0. 0. ... 0. 1. 0.]]
We add the encoding:
 [[-0.5573781   0.02487053 -0.20767261 -0.09051025  0.25172555]]
The hypergraph features for node 1578, index 512 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.71081026 -0.09051025 -0.56187365 -0.64861874  0.23731713]]
The hypergraph features for node 2091, index 513 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.19839289  0.         -0.08825879 -0.07732114  0.07160011]]
The hypergraph features for node 1275, index 514 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.5573781  -0.09051025 -0.40175548 -0.5573781   0.22008362]]
The hypergraph features for node 2080, index 515 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.09051025 -0.09051025 -0.09051025 -0.09051025  0.        ]]
The hypergraph features for node 2079, index 516 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.09051025 -0.09051025 -0.09051025 -0.09051025  0.        ]]
The hypergraph features for node 585, index 517 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.84365477  0.16559284 -0.39649742 -0.5573781   0.34850943]]
The hypergraph features for node 426, index 518 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.84365477  0.16559284 -0.37977979 -0.3201192   0.28333653]]
The hypergraph features for node 2450, index 519 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.16559284 0.16559284 0.16559284 0.16559284 0.        ]]
The hypergraph features for node 675, index 520 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.74289966  0.16559284 -0.12796165  0.          0.31665641]]
The hypergraph features for node 2126, index 521 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.70070211  0.16559284 -0.17836976  0.          0.37548064]]
The hypergraph features for node 2663, index 522 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.16559284 0.16559284 0.16559284 0.16559284 0.        ]]
The hypergraph features for node 1519, index 523 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.16559284 0.16559284 0.16559284 0.16559284 0.        ]]
The hypergraph features for node 1444, index 524 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.80291271  0.         -0.40145636 -0.40145636  0.40145636]]
The hypergraph features for node 1445, index 525 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 2329, index 526 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.33931208 -0.39873158 -0.80435706 -0.73969229  0.33877425]]
The hypergraph features for node 1177, index 527 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.33931208 -0.39873158 -0.86902183 -0.86902183  0.47029025]]
The hypergraph features for node 1234, index 528 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.76179647 -0.19839289 -0.50669474 -0.51145403  0.19706228]]
The hypergraph features for node 2291, index 529 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.5573781 -0.5573781 -0.5573781 -0.5573781  0.       ]]
The hypergraph features for node 2090, index 530 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.5573781 -0.5573781 -0.5573781 -0.5573781  0.       ]]
The hypergraph features for node 654, index 531 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.64824684 -0.5573781  -0.60281247 -0.60281247  0.04543437]]
The hypergraph features for node 314, index 532 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.64861874 -0.29427534 -0.46561406 -0.47512933  0.14164237]]
The hypergraph features for node 348, index 533 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.5573781  -0.29427534 -0.42582672 -0.42582672  0.13155138]]
The hypergraph features for node 1061, index 534 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.71081026 -0.5573781  -0.65966621 -0.71081026  0.07232862]]
The hypergraph features for node 1431, index 535 are 
 [[0. 0. 1. ... 0. 0. 0.]]
We add the encoding:
 [[-0.50540292 -0.15474045 -0.27986079 -0.17943901  0.1598008 ]]
The hypergraph features for node 1422, index 536 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.91528765  0.         -0.43891981 -0.51673511  0.3270849 ]]
The hypergraph features for node 319, index 537 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.91528765 -0.3201192  -0.61770342 -0.61770342  0.29758423]]
The hypergraph features for node 1484, index 538 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.91528765  0.02487053 -0.4761477  -0.49582791  0.32829792]]
The hypergraph features for node 1290, index 539 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.91528765 -0.06413202 -0.54669364 -0.708477    0.2998195 ]]
The hypergraph features for node 453, index 540 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.91528765  0.02487053 -0.38214811 -0.43281897  0.24169595]]
The hypergraph features for node 1025, index 541 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.91528765 -0.16086523 -0.55596619 -0.57385594  0.27615148]]
The hypergraph features for node 421, index 542 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.6529299 -0.6529299 -0.6529299 -0.6529299  0.       ]]
The hypergraph features for node 767, index 543 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.6529299 -0.6529299 -0.6529299 -0.6529299  0.       ]]
The hypergraph features for node 2338, index 544 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.6529299   0.00692868 -0.20096499 -0.1889058   0.22028319]]
The hypergraph features for node 2462, index 545 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.6529299  -0.33778618 -0.49535804 -0.49535804  0.15757186]]
The hypergraph features for node 1356, index 546 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.02487053 0.02487053 0.02487053 0.02487053 0.        ]]
The hypergraph features for node 456, index 547 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.53122004  0.02487053 -0.38058788 -0.46552996  0.20431938]]
The hypergraph features for node 2098, index 548 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.38188653  0.02487053 -0.178508   -0.178508    0.20337853]]
The hypergraph features for node 1185, index 549 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.38382001 -0.38382001 -0.38382001 -0.38382001  0.        ]]
The hypergraph features for node 1238, index 550 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.80370986 -0.322414   -0.50331462 -0.38382001  0.21388572]]
The hypergraph features for node 1237, index 551 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.38382001 -0.38382001 -0.38382001 -0.38382001  0.        ]]
The hypergraph features for node 1359, index 552 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.38382001 -0.38382001 -0.38382001 -0.38382001  0.        ]]
The hypergraph features for node 627, index 553 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.18488963 -0.18887901 -0.56088841 -0.43489249  0.37574435]]
The hypergraph features for node 1307, index 554 are 
 [[0. 1. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.38382001  0.         -0.19191    -0.19191     0.19191   ]]
The hypergraph features for node 1618, index 555 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.69772243  0.         -0.42502438 -0.5773507   0.30452874]]
The hypergraph features for node 1000, index 556 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 2634, index 557 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.80370986 -0.80370986 -0.80370986 -0.80370986  0.        ]]
The hypergraph features for node 1457, index 558 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.60783581 -0.60783581 -0.60783581 -0.60783581  0.        ]]
The hypergraph features for node 169, index 559 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.60783581 -0.60783581 -0.60783581 -0.60783581  0.        ]]
The hypergraph features for node 570, index 560 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.60783581 -0.14447441 -0.28766326 -0.23577812  0.16611359]]
The hypergraph features for node 2552, index 561 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.60783581 -0.60783581 -0.60783581 -0.60783581  0.        ]]
The hypergraph features for node 1922, index 562 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.60783581 -0.60783581 -0.60783581 -0.60783581  0.        ]]
The hypergraph features for node 1520, index 563 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.80571906  0.         -0.25731625 -0.29668086  0.20091049]]
The hypergraph features for node 1524, index 564 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.36972314 -0.17487538 -0.25403869 -0.23577812  0.083269  ]]
The hypergraph features for node 1245, index 565 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.17487538 -0.17487538 -0.17487538 -0.17487538  0.        ]]
The hypergraph features for node 59, index 566 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.30858264 -0.14447441 -0.26160469 -0.29668086  0.06779953]]
The hypergraph features for node 839, index 567 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.36972314 -0.14447441 -0.28450829 -0.29668086  0.0749614 ]]
The hypergraph features for node 834, index 568 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.14447441 -0.14447441 -0.14447441 -0.14447441  0.        ]]
The hypergraph features for node 1801, index 569 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.14447441 -0.14447441 -0.14447441 -0.14447441  0.        ]]
The hypergraph features for node 1067, index 570 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.30858264  0.         -0.15101902 -0.14447441  0.12606331]]
The hypergraph features for node 476, index 571 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.31498218 -0.2464039  -0.26926332 -0.2464039   0.03232811]]
The hypergraph features for node 676, index 572 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.31498218 -0.21828449 -0.25651861 -0.2464039   0.03565267]]
The hypergraph features for node 808, index 573 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.80075498 -0.2464039  -0.44856161 -0.37354377  0.20987643]]
The hypergraph features for node 387, index 574 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.36972314 -0.36972314 -0.36972314 -0.36972314  0.        ]]
The hypergraph features for node 1821, index 575 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.30858264  0.         -0.15429132 -0.15429132  0.15429132]]
The hypergraph features for node 1814, index 576 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 1072, index 577 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 1912, index 578 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.21828449 -0.11967959 -0.16898204 -0.16898204  0.04930245]]
The hypergraph features for node 1562, index 579 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.38117834 -0.37087355 -0.37602594 -0.37602594  0.00515239]]
The hypergraph features for node 2498, index 580 are 
 [[0. 0. 0. ... 0. 1. 0.]]
We add the encoding:
 [[-0.38117834 -0.11267148 -0.31405163 -0.38117834  0.11626688]]
The hypergraph features for node 2019, index 581 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.38117834 -0.38117834 -0.38117834 -0.38117834  0.        ]]
The hypergraph features for node 2500, index 582 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.26507565 -0.11267148 -0.20411399 -0.26507565  0.07466249]]
The hypergraph features for node 2422, index 583 are 
 [[0. 0. 1. ... 0. 1. 0.]]
We add the encoding:
 [[-0.11267148 -0.11267148 -0.11267148 -0.11267148  0.        ]]
The hypergraph features for node 2322, index 584 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.26507565 -0.26507565 -0.26507565 -0.26507565  0.        ]]
The hypergraph features for node 989, index 585 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.26507565 -0.26507565 -0.26507565 -0.26507565  0.        ]]
The hypergraph features for node 726, index 586 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.26507565 -0.26507565 -0.26507565 -0.26507565  0.        ]]
The hypergraph features for node 257, index 587 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.67514308 -0.55177299 -0.61345804 -0.61345804  0.06168505]]
The hypergraph features for node 2570, index 588 are 
 [[0. 1. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.55177299 -0.55177299 -0.55177299 -0.55177299  0.        ]]
The hypergraph features for node 667, index 589 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.55177299 -0.27158244 -0.39361501 -0.32136416  0.13040857]]
The hypergraph features for node 2270, index 590 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.55177299 -0.55177299 -0.55177299 -0.55177299  0.        ]]
The hypergraph features for node 2124, index 591 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.55177299 -0.14968454 -0.35072877 -0.35072877  0.20104423]]
The hypergraph features for node 2656, index 592 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.55177299 -0.20187014 -0.37682157 -0.37682157  0.17495142]]
The hypergraph features for node 1706, index 593 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.55177299 -0.55177299 -0.55177299 -0.55177299  0.        ]]
The hypergraph features for node 1699, index 594 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.55177299 -0.55177299 -0.55177299 -0.55177299  0.        ]]
The hypergraph features for node 1122, index 595 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.55177299 -0.12813082 -0.41055894 -0.55177299  0.19970683]]
The hypergraph features for node 1621, index 596 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.55177299 -0.12813082 -0.31631025 -0.29266859  0.15434383]]
The hypergraph features for node 593, index 597 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.33338728 -0.12813082 -0.23075905 -0.23075905  0.10262823]]
The hypergraph features for node 1194, index 598 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.12813082 -0.12813082 -0.12813082 -0.12813082  0.        ]]
The hypergraph features for node 1354, index 599 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.58625499 -0.02878828 -0.25599003 -0.12813082  0.22340557]]
The hypergraph features for node 1285, index 600 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.80396005 -0.12813082 -0.42713949 -0.38823354  0.25575487]]
The hypergraph features for node 818, index 601 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.20187014 -0.1200861  -0.16097812 -0.16097812  0.04089202]]
The hypergraph features for node 1367, index 602 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.1200861 -0.1200861 -0.1200861 -0.1200861  0.       ]]
The hypergraph features for node 1483, index 603 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.33085003 -0.1200861  -0.22546807 -0.22546807  0.10538197]]
The hypergraph features for node 922, index 604 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.56541936 -0.1200861  -0.26895155 -0.1213492   0.20963503]]
The hypergraph features for node 824, index 605 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.79513625 -0.25194991 -0.52354308 -0.52354308  0.27159317]]
The hypergraph features for node 825, index 606 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.79513625 -0.25194991 -0.52354308 -0.52354308  0.27159317]]
The hypergraph features for node 161, index 607 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.88857281  0.2873366  -0.31556748 -0.33677966  0.35174063]]
The hypergraph features for node 1241, index 608 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.40801526  0.09329969 -0.22986661 -0.21436092  0.15531378]]
The hypergraph features for node 84, index 609 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.3806058  -0.17677193 -0.31736372 -0.35459932  0.07055153]]
The hypergraph features for node 325, index 610 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.73776937 -0.15751195 -0.44764066 -0.44764066  0.29012871]]
The hypergraph features for node 2559, index 611 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.50488464  0.         -0.21329152 -0.19712934  0.18604151]]
The hypergraph features for node 772, index 612 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.96296299  0.         -0.36791656 -0.35406377  0.24912689]]
The hypergraph features for node 645, index 613 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.73776937 -0.23674673 -0.42751328 -0.3677685   0.189096  ]]
The hypergraph features for node 1172, index 614 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.40801526 -0.01225837 -0.15160025 -0.03452713  0.18154057]]
The hypergraph features for node 1347, index 615 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.17677193 -0.17677193 -0.17677193 -0.17677193  0.        ]]
The hypergraph features for node 1328, index 616 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.84365477 -0.17677193 -0.37827428 -0.18719016  0.25088229]]
The hypergraph features for node 2560, index 617 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.17677193 -0.17677193 -0.17677193 -0.17677193  0.        ]]
The hypergraph features for node 2467, index 618 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.17677193 -0.16086523 -0.16881858 -0.16881858  0.00795335]]
The hypergraph features for node 140, index 619 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.17677193 -0.17677193 -0.17677193 -0.17677193  0.        ]]
The hypergraph features for node 429, index 620 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.33085003 -0.3201192  -0.32548462 -0.32548462  0.00536542]]
The hypergraph features for node 382, index 621 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.48704298 -0.3201192  -0.40358109 -0.40358109  0.08346189]]
The hypergraph features for node 256, index 622 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.60369597 -0.06519316 -0.38659946 -0.36655278  0.18326411]]
The hypergraph features for node 1595, index 623 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.80803466  0.         -0.3756491  -0.29844243  0.25831902]]
The hypergraph features for node 1266, index 624 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.3201192 -0.3201192 -0.3201192 -0.3201192  0.       ]]
The hypergraph features for node 375, index 625 are 
 [[0. 0. 1. ... 0. 0. 0.]]
We add the encoding:
 [[-0.3201192 -0.3201192 -0.3201192 -0.3201192  0.       ]]
The hypergraph features for node 1008, index 626 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.64861874 -0.19839289 -0.42350581 -0.42350581  0.22511292]]
The hypergraph features for node 1174, index 627 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.09216762 -0.19839289 -0.73405029 -0.80291271  0.26778309]]
The hypergraph features for node 1919, index 628 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.84365477 -0.84365477 -0.84365477 -0.84365477  0.        ]]
The hypergraph features for node 2151, index 629 are 
 [[0. 0. 1. ... 0. 1. 0.]]
We add the encoding:
 [[-0.84365477 -0.84365477 -0.84365477 -0.84365477  0.        ]]
The hypergraph features for node 1164, index 630 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.84365477 -0.84365477 -0.84365477 -0.84365477  0.        ]]
The hypergraph features for node 1111, index 631 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.58394123 -0.17943901 -0.38169012 -0.38169012  0.20225111]]
The hypergraph features for node 258, index 632 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.88868964 -0.18719016 -0.55347633 -0.49470094  0.26491226]]
The hypergraph features for node 785, index 633 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.58394123 -0.58394123 -0.58394123 -0.58394123  0.        ]]
The hypergraph features for node 634, index 634 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.90245144 -0.37087355 -0.61524055 -0.5938186   0.18928698]]
The hypergraph features for node 862, index 635 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.75445156 -0.17943901 -0.46174359 -0.58394123  0.23487706]]
The hypergraph features for node 1601, index 636 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.41378031 -0.06250144 -0.26361309 -0.2890853   0.14860886]]
The hypergraph features for node 1570, index 637 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.6417562  -0.17943901 -0.33715259 -0.26370758  0.18718647]]
The hypergraph features for node 1155, index 638 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.60369597 -0.18719016 -0.32602543 -0.18719016  0.19634272]]
The hypergraph features for node 1407, index 639 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.18719016 -0.18719016 -0.18719016 -0.18719016  0.        ]]
The hypergraph features for node 1335, index 640 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.51673511  0.00692868 -0.2419802  -0.15086494  0.20496822]]
The hypergraph features for node 2017, index 641 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.25635954 -0.11103047 -0.18543193 -0.1889058   0.05938117]]
The hypergraph features for node 2343, index 642 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.1889058 -0.1889058 -0.1889058 -0.1889058  0.       ]]
The hypergraph features for node 1810, index 643 are 
 [[0. 0. 1. ... 0. 0. 0.]]
We add the encoding:
 [[-0.27595226 -0.27595226 -0.27595226 -0.27595226  0.        ]]
The hypergraph features for node 2176, index 644 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.70590611 -0.27595226 -0.49092919 -0.49092919  0.21497693]]
The hypergraph features for node 2412, index 645 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.27595226 -0.27595226 -0.27595226 -0.27595226  0.        ]]
The hypergraph features for node 1820, index 646 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.27595226 -0.27595226 -0.27595226 -0.27595226  0.        ]]
The hypergraph features for node 2107, index 647 are 
 [[1. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.98270667  0.         -0.42350422 -0.35565511  0.40969803]]
The hypergraph features for node 2575, index 648 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.64732248 -0.64732248 -0.64732248 -0.64732248  0.        ]]
The hypergraph features for node 88, index 649 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.99958849 -0.4698578  -0.82574195 -0.90245144  0.19240146]]
The hypergraph features for node 214, index 650 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.64732248 -0.44798303 -0.5217211  -0.4698578   0.08926143]]
The hypergraph features for node 2239, index 651 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.64732248  0.24997729 -0.29894597 -0.53122004  0.3464916 ]]
The hypergraph features for node 1887, index 652 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.55371698 -0.55371698 -0.55371698 -0.55371698  0.        ]]
The hypergraph features for node 1283, index 653 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.55371698  0.06495672 -0.35951438 -0.39354364  0.20774364]]
The hypergraph features for node 1028, index 654 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.3773644  -0.322414   -0.34541571 -0.32752175  0.02615258]]
The hypergraph features for node 2049, index 655 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.74289966 -0.32752175 -0.5191548  -0.48704298  0.17109077]]
The hypergraph features for node 577, index 656 are 
 [[0. 0. 0. ... 0. 1. 0.]]
We add the encoding:
 [[-0.96296299 -0.11967959 -0.49921242 -0.47592282  0.24141106]]
The hypergraph features for node 230, index 657 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.49470094  0.06495672 -0.30420787 -0.39354364  0.21710128]]
The hypergraph features for node 1250, index 658 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.39354364  0.2873366  -0.05310352 -0.05310352  0.34044012]]
The hypergraph features for node 1251, index 659 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.39354364 -0.39354364 -0.39354364 -0.39354364  0.        ]]
The hypergraph features for node 2307, index 660 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.39354364 -0.32103221 -0.35728792 -0.35728792  0.03625572]]
The hypergraph features for node 2655, index 661 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.11967959 -0.11967959 -0.11967959 -0.11967959  0.        ]]
The hypergraph features for node 107, index 662 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.28056073  0.21510649 -0.2734257  -0.09099734  0.5183016 ]]
The hypergraph features for node 250, index 663 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.28056073  0.21510649 -0.4190716  -0.22052188  0.53288176]]
The hypergraph features for node 108, index 664 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.11967959  0.21510649  0.00147652 -0.09099734  0.15151236]]
The hypergraph features for node 1461, index 665 are 
 [[0. 0. 1. ... 0. 0. 0.]]
We add the encoding:
 [[-0.65738443  0.21510649 -0.25763304 -0.29412711  0.33421974]]
The hypergraph features for node 2526, index 666 are 
 [[0. 0. 1. ... 0. 0. 0.]]
We add the encoding:
 [[-0.11967959 -0.11967959 -0.11967959 -0.11967959  0.        ]]
The hypergraph features for node 2044, index 667 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.06732603 -0.53884497 -0.75652176 -0.66339427  0.22557719]]
The hypergraph features for node 969, index 668 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.66339427 -0.66339427 -0.66339427 -0.66339427  0.        ]]
The hypergraph features for node 1038, index 669 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.66339427 -0.66339427 -0.66339427 -0.66339427  0.        ]]
The hypergraph features for node 2003, index 670 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.36870243 -0.03494457 -0.2018235  -0.2018235   0.16687893]]
The hypergraph features for node 1600, index 671 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.36870243 -0.27158244 -0.32014244 -0.32014244  0.04855999]]
The hypergraph features for node 658, index 672 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.36870243 -0.36870243 -0.36870243 -0.36870243  0.        ]]
The hypergraph features for node 1580, index 673 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.36870243 -0.36870243 -0.36870243 -0.36870243  0.        ]]
The hypergraph features for node 52, index 674 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.41378031  0.06495672 -0.17441179 -0.17441179  0.23936852]]
The hypergraph features for node 1823, index 675 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.41378031  0.06495672 -0.17441179 -0.17441179  0.23936852]]
The hypergraph features for node 1107, index 676 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.06495672 0.06495672 0.06495672 0.06495672 0.        ]]
The hypergraph features for node 236, index 677 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.3030487   0.06495672 -0.11904599 -0.11904599  0.18400271]]
The hypergraph features for node 632, index 678 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.37087355 -0.33778618 -0.35432986 -0.35432986  0.01654369]]
The hypergraph features for node 2622, index 679 are 
 [[0. 0. 0. ... 0. 1. 0.]]
We add the encoding:
 [[-0.37087355 -0.37087355 -0.37087355 -0.37087355  0.        ]]
The hypergraph features for node 1327, index 680 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.54770156 -0.54770156 -0.54770156 -0.54770156  0.        ]]
The hypergraph features for node 520, index 681 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.54770156 -0.54770156 -0.54770156 -0.54770156  0.        ]]
The hypergraph features for node 208, index 682 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.5773507  -0.18887901 -0.38311485 -0.38311485  0.19423584]]
The hypergraph features for node 245, index 683 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.90245144 -0.90245144 -0.90245144 -0.90245144  0.        ]]
The hypergraph features for node 911, index 684 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.18887901 -0.18887901 -0.18887901 -0.18887901  0.        ]]
The hypergraph features for node 2192, index 685 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.18887901 -0.18887901 -0.18887901 -0.18887901  0.        ]]
The hypergraph features for node 1270, index 686 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.82073283 -0.82073283 -0.82073283 -0.82073283  0.        ]]
The hypergraph features for node 1271, index 687 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.82073283 -0.82073283 -0.82073283 -0.82073283  0.        ]]
The hypergraph features for node 1109, index 688 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.15086494 -0.15086494 -0.15086494 -0.15086494  0.        ]]
The hypergraph features for node 1223, index 689 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.15086494 -0.15086494 -0.15086494 -0.15086494  0.        ]]
The hypergraph features for node 1112, index 690 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.5974829  -0.15086494 -0.44809831 -0.53303804  0.15847921]]
The hypergraph features for node 1607, index 691 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.51673511 -0.51673511 -0.51673511 -0.51673511  0.        ]]
The hypergraph features for node 177, index 692 are 
 [[0. 0. 0. ... 0. 1. 0.]]
We add the encoding:
 [[-0.51673511  0.17333941 -0.17169785 -0.17169785  0.34503726]]
The hypergraph features for node 591, index 693 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.00813651 -0.16086523 -0.52279304 -0.54466143  0.20014259]]
The hypergraph features for node 2636, index 694 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.51673511 -0.51673511 -0.51673511 -0.51673511  0.        ]]
The hypergraph features for node 2473, index 695 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.51673511  0.05975904 -0.22848804 -0.22848804  0.28824707]]
The hypergraph features for node 597, index 696 are 
 [[0. 0. 1. ... 0. 0. 0.]]
We add the encoding:
 [[-0.51673511 -0.41378031 -0.46525771 -0.46525771  0.0514774 ]]
The hypergraph features for node 1678, index 697 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.50540292 -0.28088228 -0.3931426  -0.3931426   0.11226032]]
The hypergraph features for node 2207, index 698 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.28088228 -0.28088228 -0.28088228 -0.28088228  0.        ]]
The hypergraph features for node 268, index 699 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.98270667  0.         -0.30273489 -0.11411645  0.39692242]]
The hypergraph features for node 613, index 700 are 
 [[0. 0. 0. ... 0. 1. 0.]]
We add the encoding:
 [[-0.98270667  0.         -0.50891037 -0.5264674   0.43200089]]
The hypergraph features for node 2046, index 701 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.16424517 -0.16424517 -0.16424517 -0.16424517  0.        ]]
The hypergraph features for node 1191, index 702 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.16424517 -0.16424517 -0.16424517 -0.16424517  0.        ]]
The hypergraph features for node 2692, index 703 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.38596908 -0.16424517 -0.31206111 -0.38596908  0.10452166]]
The hypergraph features for node 1102, index 704 are 
 [[0. 0. 1. ... 0. 0. 0.]]
We add the encoding:
 [[-0.16424517  0.33324254  0.0193223  -0.11103047  0.2230357 ]]
The hypergraph features for node 985, index 705 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.80291271  0.33324254 -0.19342661 -0.15201813  0.4363912 ]]
The hypergraph features for node 159, index 706 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.18488963  0.08193282 -0.39998909 -0.24849978  0.48270365]]
The hypergraph features for node 740, index 707 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.89746237 -0.05846848 -0.31945806 -0.29045226  0.2731237 ]]
The hypergraph features for node 1703, index 708 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.89746237 -0.89746237 -0.89746237 -0.89746237  0.        ]]
The hypergraph features for node 1928, index 709 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.89746237 -0.14968454 -0.52357345 -0.52357345  0.37388891]]
The hypergraph features for node 2114, index 710 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.89746237 -0.89746237 -0.89746237 -0.89746237  0.        ]]
The hypergraph features for node 2227, index 711 are 
 [[0. 0. 0. ... 0. 1. 0.]]
We add the encoding:
 [[-0.89746237 -0.27158244 -0.52629486 -0.46806731  0.26769256]]
The hypergraph features for node 1086, index 712 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.90308893  0.08343836 -0.44709171 -0.25635954  0.3924504 ]]
The hypergraph features for node 1574, index 713 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.57611855  0.08343836 -0.2463401  -0.2463401   0.32977845]]
The hypergraph features for node 1252, index 714 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.80571906  0.33328801 -0.36737554 -0.58759042  0.46491266]]
The hypergraph features for node 2374, index 715 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.03494457  0.24997729  0.10530275  0.10308915  0.10529046]]
The hypergraph features for node 310, index 716 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.24997729 0.24997729 0.24997729 0.24997729 0.        ]]
The hypergraph features for node 1366, index 717 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.53122004  0.24997729 -0.33592071 -0.53122004  0.33826837]]
The hypergraph features for node 392, index 718 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.24997729 0.24997729 0.24997729 0.24997729 0.        ]]
The hypergraph features for node 2283, index 719 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.33328801 0.33328801 0.33328801 0.33328801 0.        ]]
The hypergraph features for node 1293, index 720 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.33328801 0.33328801 0.33328801 0.33328801 0.        ]]
The hypergraph features for node 2278, index 721 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.33328801 0.33328801 0.33328801 0.33328801 0.        ]]
The hypergraph features for node 134, index 722 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.47739092  0.3610733  -0.05815881 -0.05815881  0.41923211]]
The hypergraph features for node 622, index 723 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.66455217 -0.52174759 -0.59314988 -0.59314988  0.07140229]]
The hypergraph features for node 1288, index 724 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.28056073 -0.2464039  -0.78824364 -0.81375758  0.41139556]]
The hypergraph features for node 1287, index 725 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.16939998 -0.3801445  -0.69162916 -0.66455217  0.31991402]]
The hypergraph features for node 36, index 726 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.88857281 -0.12080801 -0.44341957 -0.27158244  0.2864246 ]]
The hypergraph features for node 571, index 727 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.64416742  0.05987751 -0.22057636 -0.09861386  0.24412769]]
The hypergraph features for node 1493, index 728 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.57589328 -0.32136416 -0.44862872 -0.44862872  0.12726456]]
The hypergraph features for node 46, index 729 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.32136416 -0.26185289 -0.29160853 -0.29160853  0.02975564]]
The hypergraph features for node 1374, index 730 are 
 [[0. 0. 1. ... 0. 0. 0.]]
We add the encoding:
 [[-0.32136416 -0.32136416 -0.32136416 -0.32136416  0.        ]]
The hypergraph features for node 1393, index 731 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.16939998 -0.32136416 -0.65101766 -0.3801445   0.33639438]]
The hypergraph features for node 2463, index 732 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.26495111 -0.26495111 -0.26495111 -0.26495111  0.        ]]
The hypergraph features for node 662, index 733 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.26495111 -0.21547306 -0.24021208 -0.24021208  0.02473903]]
The hypergraph features for node 766, index 734 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.44798303 -0.21547306 -0.30213912 -0.26495111  0.07972243]]
The hypergraph features for node 2417, index 735 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.26495111 -0.26495111 -0.26495111 -0.26495111  0.        ]]
The hypergraph features for node 518, index 736 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.26495111 -0.26495111 -0.26495111 -0.26495111  0.        ]]
The hypergraph features for node 2015, index 737 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.47592282 -0.21547306 -0.34569794 -0.34569794  0.13022488]]
The hypergraph features for node 1924, index 738 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.21547306 -0.21547306 -0.21547306 -0.21547306  0.        ]]
The hypergraph features for node 181, index 739 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.21547306 -0.21547306 -0.21547306 -0.21547306  0.        ]]
The hypergraph features for node 807, index 740 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.21547306 -0.21547306 -0.21547306 -0.21547306  0.        ]]
The hypergraph features for node 128, index 741 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.21547306 -0.21547306 -0.21547306 -0.21547306  0.        ]]
The hypergraph features for node 2472, index 742 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.86236306 -0.86236306 -0.86236306 -0.86236306  0.        ]]
The hypergraph features for node 1841, index 743 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.56860662 -0.36455274 -0.96657968 -0.96657968  0.60202694]]
The hypergraph features for node 640, index 744 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.00813651 -0.16086523 -0.5025281  -0.45922751  0.2178002 ]]
The hypergraph features for node 2246, index 745 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.00813651 -1.00813651 -1.00813651 -1.00813651  0.        ]]
The hypergraph features for node 1363, index 746 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.03044367 -0.41445085 -0.63973579 -0.48546644  0.2702688 ]]
The hypergraph features for node 574, index 747 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.00813651 -0.36305181 -0.60166927 -0.58215102  0.22240953]]
The hypergraph features for node 2539, index 748 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 2253, index 749 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.70070211  0.         -0.35035106 -0.35035106  0.35035106]]
The hypergraph features for node 2290, index 750 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 521, index 751 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.70500309 -0.3030487  -0.5040259  -0.5040259   0.2009772 ]]
The hypergraph features for node 2509, index 752 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.70500309 -0.19493544 -0.517021   -0.65112448  0.22880859]]
The hypergraph features for node 1309, index 753 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.54466143 -0.09053888 -0.31760016 -0.31760016  0.22706127]]
The hypergraph features for node 1268, index 754 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.70590611 -0.03452713 -0.4047555  -0.3801445   0.24918623]]
The hypergraph features for node 1124, index 755 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.54466143 -0.43281897 -0.50586207 -0.54010581  0.05168274]]
The hypergraph features for node 1661, index 756 are 
 [[0. 0. 1. ... 0. 0. 0.]]
We add the encoding:
 [[-0.54466143 -0.41445085 -0.47955614 -0.47955614  0.06510529]]
The hypergraph features for node 1567, index 757 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.54466143 -0.43281897 -0.4887402  -0.4887402   0.05592123]]
The hypergraph features for node 2011, index 758 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.54466143 -0.48596497 -0.52122148 -0.53303804  0.02537771]]
The hypergraph features for node 1026, index 759 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.66682879 -0.48596497 -0.57639688 -0.57639688  0.09043191]]
The hypergraph features for node 2356, index 760 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.66682879 -0.38188653 -0.52435766 -0.52435766  0.14247113]]
The hypergraph features for node 798, index 761 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.66682879 -0.16394556 -0.44816554 -0.4809439   0.18082718]]
The hypergraph features for node 2299, index 762 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.66682879 -0.66682879 -0.66682879 -0.66682879  0.        ]]
The hypergraph features for node 2075, index 763 are 
 [[0. 0. 0. ... 0. 1. 0.]]
We add the encoding:
 [[-0.66682879 -0.66682879 -0.66682879 -0.66682879  0.        ]]
The hypergraph features for node 2482, index 764 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.66682879 -0.66682879 -0.66682879 -0.66682879  0.        ]]
The hypergraph features for node 2123, index 765 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.35459932 -0.35459932 -0.35459932 -0.35459932  0.        ]]
The hypergraph features for node 12, index 766 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.35459932 -0.35459932 -0.35459932 -0.35459932  0.        ]]
The hypergraph features for node 99, index 767 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.70339922 -0.32448124 -0.47597951 -0.35459932  0.16733995]]
The hypergraph features for node 1419, index 768 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.322414   0.2873366 -0.0175387 -0.0175387  0.3048753]]
The hypergraph features for node 1427, index 769 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.322414   0.2873366 -0.0175387 -0.0175387  0.3048753]]
The hypergraph features for node 1042, index 770 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.322414 -0.322414 -0.322414 -0.322414  0.      ]]
The hypergraph features for node 1043, index 771 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.3773644 -0.322414  -0.3498892 -0.3498892  0.0274752]]
The hypergraph features for node 1037, index 772 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.3773644  -0.23674673 -0.33049184 -0.3773644   0.0662878 ]]
The hypergraph features for node 206, index 773 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.64389622 -0.23674673 -0.41933578 -0.3773644   0.16884682]]
The hypergraph features for node 1300, index 774 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.3773644  -0.2464039  -0.31188415 -0.31188415  0.06548025]]
The hypergraph features for node 2454, index 775 are 
 [[0. 0. 1. ... 0. 0. 0.]]
We add the encoding:
 [[-0.2464039 -0.2464039 -0.2464039 -0.2464039  0.       ]]
The hypergraph features for node 1014, index 776 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.2464039 -0.2464039 -0.2464039 -0.2464039  0.       ]]
The hypergraph features for node 2696, index 777 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.33338728 -0.24689952 -0.2901434  -0.2901434   0.04324388]]
The hypergraph features for node 1510, index 778 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.33338728 -0.33338728 -0.33338728 -0.33338728  0.        ]]
The hypergraph features for node 790, index 779 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.46552996 -0.46552996 -0.46552996 -0.46552996  0.        ]]
The hypergraph features for node 1198, index 780 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.49470094 -0.19493544 -0.33752545 -0.3302327   0.14296251]]
The hypergraph features for node 2373, index 781 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.53122004  0.14630079 -0.17824127 -0.03494457  0.29385373]]
The hypergraph features for node 85, index 782 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.47849576 -0.47739092 -0.47794334 -0.47794334  0.00055242]]
The hypergraph features for node 39, index 783 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.71622709 -0.57727067 -0.64674888 -0.64674888  0.06947821]]
The hypergraph features for node 974, index 784 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.75245599 -0.71622709 -0.73434154 -0.73434154  0.01811445]]
The hypergraph features for node 1540, index 785 are 
 [[0. 0. 1. ... 0. 0. 0.]]
We add the encoding:
 [[-0.71622709  0.         -0.35811355 -0.35811355  0.35811355]]
The hypergraph features for node 68, index 786 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.45987296 -0.26185289 -0.32259845 -0.31733731  0.06621603]]
The hypergraph features for node 333, index 787 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.05142467 -0.05142467 -0.05142467 -0.05142467  0.        ]]
The hypergraph features for node 564, index 788 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.05142467 -0.05142467 -0.05142467 -0.05142467  0.        ]]
The hypergraph features for node 1869, index 789 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.05142467 -0.00037275 -0.02589871 -0.02589871  0.02552596]]
The hypergraph features for node 2543, index 790 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.18488963 -0.05142467 -0.36459381 -0.11103047  0.47422275]]
The hypergraph features for node 201, index 791 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.05142467 -0.05142467 -0.05142467 -0.05142467  0.        ]]
The hypergraph features for node 350, index 792 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.65738443 -0.05142467 -0.50569252 -0.65738443  0.23468755]]
The hypergraph features for node 656, index 793 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.8432612   0.         -0.37494803 -0.32826546  0.34897138]]
The hypergraph features for node 2605, index 794 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.10383085 -0.08041237 -0.09602469 -0.10383085  0.01103958]]
The hypergraph features for node 951, index 795 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.08041237 -0.08041237 -0.08041237 -0.08041237  0.        ]]
The hypergraph features for node 1828, index 796 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.08041237 -0.08041237 -0.08041237 -0.08041237  0.        ]]
The hypergraph features for node 2014, index 797 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.99211502 -0.08041237 -0.5362637  -0.5362637   0.45585132]]
The hypergraph features for node 367, index 798 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.08041237 -0.08041237 -0.08041237 -0.08041237  0.        ]]
The hypergraph features for node 2648, index 799 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 730, index 800 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.50488464  0.         -0.25244232 -0.25244232  0.25244232]]
The hypergraph features for node 2586, index 801 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.53520894  0.         -0.20013403 -0.06519316  0.2384239 ]]
The hypergraph features for node 1193, index 802 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.47739092 -0.47739092 -0.47739092 -0.47739092  0.        ]]
The hypergraph features for node 1202, index 803 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.47739092 -0.47739092 -0.47739092 -0.47739092  0.        ]]
The hypergraph features for node 2328, index 804 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.47739092 -0.47739092 -0.47739092 -0.47739092  0.        ]]
The hypergraph features for node 804, index 805 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.47739092 -0.05846848 -0.2679297  -0.2679297   0.20946122]]
The hypergraph features for node 1546, index 806 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.46876347 -0.46876347 -0.46876347 -0.46876347  0.        ]]
The hypergraph features for node 306, index 807 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.46876347 -0.46876347 -0.46876347 -0.46876347  0.        ]]
The hypergraph features for node 1727, index 808 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.46876347 -0.46876347 -0.46876347 -0.46876347  0.        ]]
The hypergraph features for node 2419, index 809 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.68178771 -0.03452713 -0.45485389 -0.64824684  0.29753116]]
The hypergraph features for node 750, index 810 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 2628, index 811 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.65602373 -0.65602373 -0.65602373 -0.65602373  0.        ]]
The hypergraph features for node 1515, index 812 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.75245599 -0.41235816 -0.61033826 -0.66620064  0.14435393]]
The hypergraph features for node 1581, index 813 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.66620064 -0.41235816 -0.55155912 -0.57611855  0.10507576]]
The hypergraph features for node 1084, index 814 are 
 [[0. 0. 0. ... 0. 1. 0.]]
We add the encoding:
 [[-1.09216762 -0.41235816 -0.76801885 -0.79953076  0.2784241 ]]
The hypergraph features for node 2556, index 815 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.41235816 -0.12315738 -0.26775777 -0.26775777  0.14460039]]
The hypergraph features for node 2303, index 816 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.79953076 -0.41235816 -0.60594446 -0.60594446  0.1935863 ]]
The hypergraph features for node 2305, index 817 are 
 [[0. 0. 0. ... 0. 1. 0.]]
We add the encoding:
 [[-0.73545786  0.05975904 -0.40526821 -0.54010581  0.33835717]]
The hypergraph features for node 1954, index 818 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.73545786  0.05975904 -0.34192073 -0.24689952  0.27333258]]
The hypergraph features for node 2179, index 819 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.73545786  0.05975904 -0.40526821 -0.54010581  0.33835717]]
The hypergraph features for node 1192, index 820 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.73545786 -0.73545786 -0.73545786 -0.73545786  0.        ]]
The hypergraph features for node 2018, index 821 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.99211502 -0.99211502 -0.99211502 -0.99211502  0.        ]]
The hypergraph features for node 1491, index 822 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.99211502 -0.99211502 -0.99211502 -0.99211502  0.        ]]
The hypergraph features for node 2135, index 823 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.95285312 -0.08127863 -0.45466187 -0.39225787  0.32978637]]
The hypergraph features for node 2071, index 824 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.95285312 -0.95285312 -0.95285312 -0.95285312  0.        ]]
The hypergraph features for node 2466, index 825 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.95285312 -0.25508531 -0.60396922 -0.60396922  0.34888391]]
The hypergraph features for node 2428, index 826 are 
 [[0. 0. 0. ... 0. 0. 1.]]
We add the encoding:
 [[-0.99958849 -0.05846848 -0.61535718 -0.76179647  0.39696052]]
The hypergraph features for node 1002, index 827 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.60369597 -0.60369597 -0.60369597 -0.60369597  0.        ]]
The hypergraph features for node 690, index 828 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.6417562 -0.6417562 -0.6417562 -0.6417562  0.       ]]
The hypergraph features for node 1688, index 829 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.8432612 -0.8432612 -0.8432612 -0.8432612  0.       ]]
The hypergraph features for node 1693, index 830 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.8432612 -0.8432612 -0.8432612 -0.8432612  0.       ]]
The hypergraph features for node 460, index 831 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.8432612 -0.8432612 -0.8432612 -0.8432612  0.       ]]
The hypergraph features for node 691, index 832 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.8432612 -0.8432612 -0.8432612 -0.8432612  0.       ]]
The hypergraph features for node 924, index 833 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.71081026 -0.71081026 -0.71081026 -0.71081026  0.        ]]
The hypergraph features for node 1532, index 834 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.71081026 -0.29305024 -0.50193025 -0.50193025  0.20888001]]
The hypergraph features for node 1010, index 835 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.71081026 -0.64861874 -0.66416662 -0.64861874  0.02692972]]
The hypergraph features for node 877, index 836 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.71081026 -0.71081026 -0.71081026 -0.71081026  0.        ]]
The hypergraph features for node 1442, index 837 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.80291271 -0.80291271 -0.80291271 -0.80291271  0.        ]]
The hypergraph features for node 1368, index 838 are 
 [[0. 0. 1. ... 0. 0. 0.]]
We add the encoding:
 [[-1.09216762 -1.09216762 -1.09216762 -1.09216762  0.        ]]
The hypergraph features for node 858, index 839 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.09216762 -0.79953076 -0.94584919 -0.94584919  0.14631843]]
The hypergraph features for node 2394, index 840 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.22795486 -1.22795486 -1.22795486 -1.22795486  0.        ]]
The hypergraph features for node 1062, index 841 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.22795486 -1.22795486 -1.22795486 -1.22795486  0.        ]]
The hypergraph features for node 1195, index 842 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.22795486 -0.66083521 -0.94439504 -0.94439504  0.28355983]]
The hypergraph features for node 1145, index 843 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.22795486 -1.22795486 -1.22795486 -1.22795486  0.        ]]
The hypergraph features for node 1149, index 844 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.22795486 -0.74839282 -0.98817384 -0.98817384  0.23978102]]
The hypergraph features for node 1150, index 845 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.22795486 -1.22795486 -1.22795486 -1.22795486  0.        ]]
The hypergraph features for node 2076, index 846 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.73776937 -0.73776937 -0.73776937 -0.73776937  0.        ]]
The hypergraph features for node 1414, index 847 are 
 [[0. 0. 0. ... 1. 0. 0.]]
We add the encoding:
 [[-0.3801445 -0.3801445 -0.3801445 -0.3801445  0.       ]]
The hypergraph features for node 327, index 848 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.74839282 -0.32103221 -0.52645746 -0.43981811  0.18287478]]
The hypergraph features for node 787, index 849 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.3801445 -0.3801445 -0.3801445 -0.3801445  0.       ]]
The hypergraph features for node 891, index 850 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.3801445 -0.3801445 -0.3801445 -0.3801445  0.       ]]
The hypergraph features for node 905, index 851 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.16939998 -1.16939998 -1.16939998 -1.16939998  0.        ]]
The hypergraph features for node 976, index 852 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.96296299 -0.96296299 -0.96296299 -0.96296299  0.        ]]
The hypergraph features for node 192, index 853 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.48472714 -0.26185289 -0.37193757 -0.30963107  0.08808231]]
The hypergraph features for node 193, index 854 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.44336263 -0.26185289 -0.33324884 -0.30963107  0.07218281]]
The hypergraph features for node 420, index 855 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.30963107 -0.30963107 -0.30963107 -0.30963107  0.        ]]
The hypergraph features for node 519, index 856 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.48472714 -0.30963107 -0.43202874 -0.44336263  0.06786935]]
The hypergraph features for node 280, index 857 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.30963107 -0.30963107 -0.30963107 -0.30963107  0.        ]]
The hypergraph features for node 388, index 858 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.30963107 -0.30963107 -0.30963107 -0.30963107  0.        ]]
The hypergraph features for node 2448, index 859 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.44183741 -0.30963107 -0.37573424 -0.37573424  0.06610317]]
The hypergraph features for node 2549, index 860 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.48472714 -0.30963107 -0.41257362 -0.44336263  0.07472452]]
The hypergraph features for node 356, index 861 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.48472714 -0.48472714 -0.48472714 -0.48472714  0.        ]]
The hypergraph features for node 2223, index 862 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.48472714 -0.48472714 -0.48472714 -0.48472714  0.        ]]
The hypergraph features for node 2214, index 863 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.44336263 -0.26185289 -0.35260776 -0.35260776  0.09075487]]
The hypergraph features for node 2282, index 864 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.91525543 -0.3030487  -0.60915206 -0.60915206  0.30610336]]
The hypergraph features for node 837, index 865 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.3030487 -0.3030487 -0.3030487 -0.3030487  0.       ]]
The hypergraph features for node 2427, index 866 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.70070211 -0.16394556 -0.48599949 -0.70070211  0.26295594]]
The hypergraph features for node 507, index 867 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.70070211 -0.16394556 -0.52178326 -0.70070211  0.25302947]]
The hypergraph features for node 2391, index 868 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.70070211 -0.16394556 -0.43232383 -0.43232383  0.26837828]]
The hypergraph features for node 2431, index 869 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.70070211 -0.70070211 -0.70070211 -0.70070211  0.        ]]
The hypergraph features for node 2589, index 870 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.06519316 -0.06519316 -0.06519316 -0.06519316  0.        ]]
The hypergraph features for node 1434, index 871 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.06519316 -0.06519316 -0.06519316 -0.06519316  0.        ]]
The hypergraph features for node 1429, index 872 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.06519316 -0.06519316 -0.06519316 -0.06519316  0.        ]]
The hypergraph features for node 2599, index 873 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.06519316 -0.06519316 -0.06519316 -0.06519316  0.        ]]
The hypergraph features for node 1908, index 874 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.91525543 -0.25635954 -0.54331417 -0.53520894  0.24206581]]
The hypergraph features for node 2615, index 875 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.5974829  -0.53520894 -0.56634592 -0.56634592  0.03113698]]
The hypergraph features for node 355, index 876 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.53520894 -0.02193365 -0.21858548 -0.09861386  0.22606455]]
The hypergraph features for node 2085, index 877 are 
 [[0. 0. 0. ... 0. 1. 0.]]
We add the encoding:
 [[-0.53520894 -0.09861386 -0.3169114  -0.3169114   0.21829754]]
The hypergraph features for node 2228, index 878 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.53520894 -0.02193365 -0.21858548 -0.09861386  0.22606455]]
The hypergraph features for node 1659, index 879 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.53520894 -0.09861386 -0.3169114  -0.3169114   0.21829754]]
The hypergraph features for node 124, index 880 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.53520894 -0.53520894 -0.53520894 -0.53520894  0.        ]]
The hypergraph features for node 1916, index 881 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.53520894 -0.53520894 -0.53520894 -0.53520894  0.        ]]
The hypergraph features for node 633, index 882 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.05975904 0.05975904 0.05975904 0.05975904 0.        ]]
The hypergraph features for node 749, index 883 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.05975904 0.05975904 0.05975904 0.05975904 0.        ]]
The hypergraph features for node 2150, index 884 are 
 [[0. 0. 0. ... 0. 1. 0.]]
We add the encoding:
 [[0.05975904 0.05975904 0.05975904 0.05975904 0.        ]]
The hypergraph features for node 1272, index 885 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.05975904 0.05975904 0.05975904 0.05975904 0.        ]]
The hypergraph features for node 1281, index 886 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.01619128  0.05975904 -0.00100122 -0.01619128  0.03038013]]
The hypergraph features for node 1451, index 887 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.01619128  0.05975904  0.02178388  0.02178388  0.03797516]]
The hypergraph features for node 1282, index 888 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.01619128  0.66368589  0.15377801 -0.01619128  0.29439545]]
The hypergraph features for node 1446, index 889 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.01619128 -0.01619128 -0.01619128 -0.01619128  0.        ]]
The hypergraph features for node 1633, index 890 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.01619128 -0.01619128 -0.01619128 -0.01619128  0.        ]]
The hypergraph features for node 693, index 891 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.64824684 -0.01619128 -0.31957115 -0.29427534  0.25865481]]
The hypergraph features for node 447, index 892 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.29427534 -0.01619128 -0.15523331 -0.15523331  0.13904203]]
The hypergraph features for node 1346, index 893 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.01619128  0.66368589  0.21043444 -0.01619128  0.32049717]]
The hypergraph features for node 240, index 894 are 
 [[0. 1. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.66368589 0.66368589 0.66368589 0.66368589 0.        ]]
The hypergraph features for node 2652, index 895 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.66368589 0.66368589 0.66368589 0.66368589 0.        ]]
The hypergraph features for node 1301, index 896 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.58053621  0.66368589 -0.11281991 -0.4216094   0.55289262]]
The hypergraph features for node 1963, index 897 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.08127863 -0.08127863 -0.08127863 -0.08127863  0.        ]]
The hypergraph features for node 286, index 898 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.2514777  -0.06413202 -0.13229612 -0.08127863  0.08456433]]
The hypergraph features for node 1550, index 899 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.57611855  0.2873366  -0.28862504 -0.40419374  0.31586119]]
The hypergraph features for node 1710, index 900 are 
 [[1. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.80075498 -0.80075498 -0.80075498 -0.80075498  0.        ]]
The hypergraph features for node 43, index 901 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.80075498 -0.80075498 -0.80075498 -0.80075498  0.        ]]
The hypergraph features for node 86, index 902 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.80075498  0.03700752 -0.34186678 -0.26185289  0.34666331]]
The hypergraph features for node 196, index 903 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.708477    0.         -0.236159    0.          0.33397926]]
The hypergraph features for node 409, index 904 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.80396005  0.         -0.26017669 -0.11837336  0.32849396]]
The hypergraph features for node 1819, index 905 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.41378031 -0.41378031 -0.41378031 -0.41378031  0.        ]]
The hypergraph features for node 2626, index 906 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.11103047 -0.11103047 -0.11103047 -0.11103047  0.        ]]
The hypergraph features for node 2398, index 907 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.11103047 -0.11103047 -0.11103047 -0.11103047  0.        ]]
The hypergraph features for node 2635, index 908 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.33324254 0.33324254 0.33324254 0.33324254 0.        ]]
The hypergraph features for node 1767, index 909 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.15091111 0.15091111 0.15091111 0.15091111 0.        ]]
The hypergraph features for node 1768, index 910 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.15091111 0.15091111 0.15091111 0.15091111 0.        ]]
The hypergraph features for node 2444, index 911 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.15091111 0.15091111 0.15091111 0.15091111 0.        ]]
The hypergraph features for node 563, index 912 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.15091111 0.15091111 0.15091111 0.15091111 0.        ]]
The hypergraph features for node 2451, index 913 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.51351297 -0.51351297 -0.51351297 -0.51351297  0.        ]]
The hypergraph features for node 803, index 914 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.51351297 -0.25508531 -0.38429914 -0.38429914  0.12921383]]
The hypergraph features for node 243, index 915 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.51351297 -0.25508531 -0.38429914 -0.38429914  0.12921383]]
The hypergraph features for node 1563, index 916 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.51351297 -0.25508531 -0.38429914 -0.38429914  0.12921383]]
The hypergraph features for node 757, index 917 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.58759042  0.15091111 -0.21833965 -0.21833965  0.36925076]]
The hypergraph features for node 2053, index 918 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.15091111 0.15091111 0.15091111 0.15091111 0.        ]]
The hypergraph features for node 2054, index 919 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.40671734  0.15091111 -0.22084119 -0.40671734  0.26286857]]
The hypergraph features for node 2690, index 920 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.40671734 -0.40671734 -0.40671734 -0.40671734  0.        ]]
The hypergraph features for node 488, index 921 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.40671734 -0.40671734 -0.40671734 -0.40671734  0.        ]]
The hypergraph features for node 2685, index 922 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.40671734 -0.40671734 -0.40671734 -0.40671734  0.        ]]
The hypergraph features for node 2081, index 923 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.40671734 -0.40671734 -0.40671734 -0.40671734  0.        ]]
The hypergraph features for node 2661, index 924 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.40671734 -0.21956476 -0.2663529  -0.21956476  0.08103945]]
The hypergraph features for node 1794, index 925 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.46948151 -0.09053888 -0.21685309 -0.09053888  0.17863527]]
The hypergraph features for node 2082, index 926 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.09053888 -0.09053888 -0.09053888 -0.09053888  0.        ]]
The hypergraph features for node 2178, index 927 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.46948151 -0.09053888 -0.34714603 -0.46948151  0.15680124]]
The hypergraph features for node 1773, index 928 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.09053888 -0.09053888 -0.09053888 -0.09053888  0.        ]]
The hypergraph features for node 2181, index 929 are 
 [[0. 0. 1. ... 0. 0. 0.]]
We add the encoding:
 [[-0.46948151 -0.09053888 -0.37474585 -0.46948151  0.16408697]]
The hypergraph features for node 2304, index 930 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.09053888 -0.09053888 -0.09053888 -0.09053888  0.        ]]
The hypergraph features for node 809, index 931 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.21478636  0.08193282 -0.16613703 -0.18222129  0.08184387]]
The hypergraph features for node 919, index 932 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.09053888 -0.09053888 -0.09053888 -0.09053888  0.        ]]
The hypergraph features for node 1460, index 933 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.09053888 -0.09053888 -0.09053888 -0.09053888  0.        ]]
The hypergraph features for node 698, index 934 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.34022501 -0.34022501 -0.34022501 -0.34022501  0.        ]]
The hypergraph features for node 660, index 935 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.34022501 -0.34022501 -0.34022501 -0.34022501  0.        ]]
The hypergraph features for node 2653, index 936 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.45922751  0.         -0.22961376 -0.22961376  0.22961376]]
The hypergraph features for node 732, index 937 are 
 [[0. 0. 0. ... 0. 1. 0.]]
We add the encoding:
 [[-0.45922751 -0.41445085 -0.44430196 -0.45922751  0.02110792]]
The hypergraph features for node 267, index 938 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.45922751 -0.45922751 -0.45922751 -0.45922751  0.        ]]
The hypergraph features for node 334, index 939 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.45922751 -0.41445085 -0.43683918 -0.43683918  0.02238833]]
The hypergraph features for node 459, index 940 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.45922751 -0.38188653 -0.41852163 -0.41445085  0.03170526]]
The hypergraph features for node 2403, index 941 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.91604006 -0.91604006 -0.91604006 -0.91604006  0.        ]]
The hypergraph features for node 2616, index 942 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.80803466 -0.27734241 -0.54268854 -0.54268854  0.26534612]]
The hypergraph features for node 2614, index 943 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.80803466 -0.27734241 -0.45423983 -0.27734241  0.25017072]]
The hypergraph features for node 1475, index 944 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.27734241 -0.27734241 -0.27734241 -0.27734241  0.        ]]
The hypergraph features for node 555, index 945 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.43281897 -0.27158244 -0.34090759 -0.32961447  0.06887213]]
The hypergraph features for node 14, index 946 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.88868964  0.         -0.40807175 -0.38881016  0.31190775]]
The hypergraph features for node 2354, index 947 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.38188653 -0.27734241 -0.32961447 -0.32961447  0.05227206]]
The hypergraph features for node 2360, index 948 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.43281897 -0.27734241 -0.37080774 -0.36305181  0.0459644 ]]
The hypergraph features for node 0, index 949 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.88868964 -0.27734241 -0.55357766 -0.49470094  0.25302992]]
The hypergraph features for node 751, index 950 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.88868964 -0.27734241 -0.55357766 -0.49470094  0.25302992]]
The hypergraph features for node 616, index 951 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.88868964 -0.88868964 -0.88868964 -0.88868964  0.        ]]
The hypergraph features for node 8, index 952 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.49470094 -0.49470094 -0.49470094 -0.49470094  0.        ]]
The hypergraph features for node 954, index 953 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.34131169 -0.97633517 -1.15882343 -1.15882343  0.18248826]]
The hypergraph features for node 871, index 954 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.97633517 -0.97633517 -0.97633517 -0.97633517  0.        ]]
The hypergraph features for node 432, index 955 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.97633517  0.         -0.22086213 -0.06398774  0.37881891]]
The hypergraph features for node 433, index 956 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.97633517 -0.97633517 -0.97633517 -0.97633517  0.        ]]
The hypergraph features for node 428, index 957 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.97633517  0.         -0.26008073 -0.03199387  0.41435397]]
The hypergraph features for node 158, index 958 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.97633517  0.         -0.42120304 -0.3542385   0.43171808]]
The hypergraph features for node 302, index 959 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.06398774  0.         -0.03199387 -0.03199387  0.03199387]]
The hypergraph features for node 307, index 960 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 195, index 961 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.708477 -0.708477 -0.708477 -0.708477  0.      ]]
The hypergraph features for node 2350, index 962 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.50540292 -0.50540292 -0.50540292 -0.50540292  0.        ]]
The hypergraph features for node 1343, index 963 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.03044367 -1.03044367 -1.03044367 -1.03044367  0.        ]]
The hypergraph features for node 308, index 964 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.03044367  0.         -0.47077673 -0.38188653  0.42534663]]
The hypergraph features for node 1469, index 965 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.03044367 -0.38188653 -0.7061651  -0.7061651   0.32427857]]
The hypergraph features for node 1095, index 966 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.03044367 -0.48596497 -0.75820432 -0.75820432  0.27223935]]
The hypergraph features for node 1364, index 967 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.41445085 -0.38188653 -0.39816869 -0.39816869  0.01628216]]
The hypergraph features for node 481, index 968 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.38188653 -0.38188653 -0.38188653 -0.38188653  0.        ]]
The hypergraph features for node 2651, index 969 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.38188653 -0.38188653 -0.38188653 -0.38188653  0.        ]]
The hypergraph features for node 2455, index 970 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.43281897 -0.43281897 -0.43281897 -0.43281897  0.        ]]
The hypergraph features for node 1358, index 971 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.43281897 -0.43281897 -0.43281897 -0.43281897  0.        ]]
The hypergraph features for node 791, index 972 are 
 [[0. 1. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.43281897 -0.43281897 -0.43281897 -0.43281897  0.        ]]
The hypergraph features for node 1360, index 973 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.47592282 -0.47592282 -0.47592282 -0.47592282  0.        ]]
The hypergraph features for node 792, index 974 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.47592282 -0.47592282 -0.47592282 -0.47592282  0.        ]]
The hypergraph features for node 20, index 975 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.41445085 -0.41445085 -0.41445085 -0.41445085  0.        ]]
The hypergraph features for node 614, index 976 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.27158244 -0.27158244 -0.27158244 -0.27158244  0.        ]]
The hypergraph features for node 966, index 977 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.97633517 -0.60506936 -0.69881493 -0.60692759  0.16023357]]
The hypergraph features for node 1253, index 978 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.60878583 -0.60506936 -0.60630818 -0.60506936  0.00175196]]
The hypergraph features for node 981, index 979 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.60506936 -0.60506936 -0.60506936 -0.60506936  0.        ]]
The hypergraph features for node 489, index 980 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.53884497  0.0288628  -0.25636345 -0.26185289  0.25390147]]
The hypergraph features for node 1815, index 981 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.06732603  0.0288628  -0.43343871 -0.26185289  0.46367284]]
The hypergraph features for node 652, index 982 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.26185289 -0.26185289 -0.26185289 -0.26185289  0.        ]]
The hypergraph features for node 1844, index 983 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.26185289 -0.26185289 -0.26185289 -0.26185289  0.        ]]
The hypergraph features for node 1845, index 984 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.31733731 -0.26185289 -0.2895951  -0.2895951   0.02774221]]
The hypergraph features for node 1779, index 985 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.31733731 -0.31733731 -0.31733731 -0.31733731  0.        ]]
The hypergraph features for node 1780, index 986 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.31733731 -0.31733731 -0.31733731 -0.31733731  0.        ]]
The hypergraph features for node 1966, index 987 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.31733731 -0.31733731 -0.31733731 -0.31733731  0.        ]]
The hypergraph features for node 978, index 988 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.31733731 -0.31733731 -0.31733731 -0.31733731  0.        ]]
The hypergraph features for node 979, index 989 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.31733731 -0.31733731 -0.31733731 -0.31733731  0.        ]]
The hypergraph features for node 121, index 990 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.31913316 -0.31733731 -0.31823524 -0.31823524  0.00089793]]
The hypergraph features for node 496, index 991 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.81513921 -0.31733731 -0.48327128 -0.31733731  0.23466607]]
The hypergraph features for node 663, index 992 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.31733731 -0.31733731 -0.31733731 -0.31733731  0.        ]]
The hypergraph features for node 2393, index 993 are 
 [[0. 0. 0. ... 0. 1. 0.]]
We add the encoding:
 [[-0.97633517 -0.81513921 -0.8554382  -0.81513921  0.0697999 ]]
The hypergraph features for node 1818, index 994 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.97633517 -0.81513921 -0.89573719 -0.89573719  0.08059798]]
The hypergraph features for node 1896, index 995 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.97633517 -0.97633517 -0.97633517 -0.97633517  0.        ]]
The hypergraph features for node 269, index 996 are 
 [[0. 0. 1. ... 0. 0. 0.]]
We add the encoding:
 [[-0.57727067 -0.57727067 -0.57727067 -0.57727067  0.        ]]
The hypergraph features for node 1029, index 997 are 
 [[1. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.76866901 -0.76866901 -0.76866901 -0.76866901  0.        ]]
The hypergraph features for node 1023, index 998 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.76866901 -0.76866901 -0.76866901 -0.76866901  0.        ]]
The hypergraph features for node 48, index 999 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.08729959 -0.74289966 -0.86628942 -0.76866901  0.15663149]]
The hypergraph features for node 41, index 1000 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.81464442 -0.44798303 -0.63913302 -0.69339794  0.16313669]]
The hypergraph features for node 1504, index 1001 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.78142667 -0.44676319 -0.5561914  -0.44676319  0.15479868]]
The hypergraph features for node 1506, index 1002 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.78142667 -0.44676319 -0.58836787 -0.44676319  0.1635584 ]]
The hypergraph features for node 1507, index 1003 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.76866901 -0.76866901 -0.76866901 -0.76866901  0.        ]]
The hypergraph features for node 1503, index 1004 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.78142667 -0.44676319 -0.61196865 -0.6077161   0.16526019]]
The hypergraph features for node 1525, index 1005 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.78142667 -0.44676319 -0.61409493 -0.61409493  0.16733174]]
The hypergraph features for node 1308, index 1006 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.78142667 -0.44676319 -0.61409493 -0.61409493  0.16733174]]
The hypergraph features for node 1636, index 1007 are 
 [[0. 1. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.5974829  -0.39288056 -0.51564196 -0.5974829   0.10023427]]
The hypergraph features for node 1647, index 1008 are 
 [[0. 0. 0. ... 0. 1. 0.]]
We add the encoding:
 [[-0.5974829  -0.39288056 -0.49518173 -0.49518173  0.10230117]]
The hypergraph features for node 1182, index 1009 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.64861874 -0.39288056 -0.5463274  -0.5974829   0.11049334]]
The hypergraph features for node 1389, index 1010 are 
 [[0. 0. 0. ... 0. 1. 0.]]
We add the encoding:
 [[-0.39288056 -0.39288056 -0.39288056 -0.39288056  0.        ]]
The hypergraph features for node 1421, index 1011 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.39288056 -0.39288056 -0.39288056 -0.39288056  0.        ]]
The hypergraph features for node 607, index 1012 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.42606763 -0.39288056 -0.40947409 -0.40947409  0.01659353]]
The hypergraph features for node 1658, index 1013 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.96958506 -0.96958506 -0.96958506 -0.96958506  0.        ]]
The hypergraph features for node 114, index 1014 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.57589328 -0.57589328 -0.57589328 -0.57589328  0.        ]]
The hypergraph features for node 883, index 1015 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.23674673 -0.23674673 -0.23674673 -0.23674673  0.        ]]
The hypergraph features for node 1453, index 1016 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.23674673 -0.23674673 -0.23674673 -0.23674673  0.        ]]
The hypergraph features for node 1324, index 1017 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.78059749  0.         -0.32137053 -0.25244232  0.33582915]]
The hypergraph features for node 419, index 1018 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.50488464  0.         -0.25244232 -0.25244232  0.25244232]]
The hypergraph features for node 1326, index 1019 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.91496269  0.         -0.28520988 -0.12315738  0.33899764]]
The hypergraph features for node 260, index 1020 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-7.80597488e-01 -3.72747580e-04 -4.28618291e-01 -5.04884636e-01
   3.23058383e-01]]
The hypergraph features for node 2088, index 1021 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.78059749  0.01326978 -0.34845145 -0.34688753  0.28029401]]
The hypergraph features for node 2166, index 1022 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-7.80597488e-01 -3.72747580e-04 -4.08185600e-01 -4.25886083e-01
   2.82006247e-01]]
The hypergraph features for node 2306, index 1023 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.50488464 -0.50488464 -0.50488464 -0.50488464  0.        ]]
The hypergraph features for node 444, index 1024 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 405, index 1025 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 1875, index 1026 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.0451951 -1.0451951 -1.0451951 -1.0451951  0.       ]]
The hypergraph features for node 2553, index 1027 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.0451951 -1.0451951 -1.0451951 -1.0451951  0.       ]]
The hypergraph features for node 1891, index 1028 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.0451951  -0.53884497 -0.81094993 -0.84880972  0.20844286]]
The hypergraph features for node 462, index 1029 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.23674673 -0.23674673 -0.23674673 -0.23674673  0.        ]]
The hypergraph features for node 200, index 1030 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.64389622 -0.23674673 -0.44032148 -0.44032148  0.20357475]]
The hypergraph features for node 58, index 1031 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.64416742 -0.23674673 -0.54224445 -0.64403182  0.17637923]]
The hypergraph features for node 1464, index 1032 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.23674673 -0.23674673 -0.23674673 -0.23674673  0.        ]]
The hypergraph features for node 904, index 1033 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.81464442 -0.44798303 -0.61948127 -0.64389622  0.15016001]]
The hypergraph features for node 2153, index 1034 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.81464442  0.20712311 -0.24298554 -0.14535022  0.41364145]]
The hypergraph features for node 1713, index 1035 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.84880972  0.20712311 -0.319329   -0.14535022  0.36321102]]
The hypergraph features for node 892, index 1036 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.81464442 -0.81464442 -0.81464442 -0.81464442  0.        ]]
The hypergraph features for node 126, index 1037 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.81464442  0.10613354 -0.35425544 -0.35425544  0.46038898]]
The hypergraph features for node 1716, index 1038 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.84880972  0.10613354 -0.42250866 -0.44798303  0.34937314]]
The hypergraph features for node 2002, index 1039 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.44798303 -0.44798303 -0.44798303 -0.44798303  0.        ]]
The hypergraph features for node 138, index 1040 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.74289966 -0.44798303 -0.58043035 -0.56541936  0.10534376]]
The hypergraph features for node 1372, index 1041 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.44798303  0.14630079 -0.15084112 -0.15084112  0.29714191]]
The hypergraph features for node 759, index 1042 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.88857281 -0.44798303 -0.71843685 -0.76859577  0.18433145]]
The hypergraph features for node 2348, index 1043 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.08729959 -0.74289966 -0.91509962 -0.91509962  0.17219996]]
The hypergraph features for node 734, index 1044 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.74289966 -0.74289966 -0.74289966 -0.74289966  0.        ]]
The hypergraph features for node 2186, index 1045 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.64824684 -0.64824684 -0.64824684 -0.64824684  0.        ]]
The hypergraph features for node 2187, index 1046 are 
 [[0. 0. 1. ... 0. 0. 0.]]
We add the encoding:
 [[-0.64824684 -0.64824684 -0.64824684 -0.64824684  0.        ]]
The hypergraph features for node 2461, index 1047 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.90308893 -0.64824684 -0.81814156 -0.90308893  0.12013371]]
The hypergraph features for node 1078, index 1048 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.90308893 -0.90308893 -0.90308893 -0.90308893  0.        ]]
The hypergraph features for node 1076, index 1049 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.90308893 -0.90308893 -0.90308893 -0.90308893  0.        ]]
The hypergraph features for node 442, index 1050 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.33778618 -0.33778618 -0.33778618 -0.33778618  0.        ]]
The hypergraph features for node 557, index 1051 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.33778618 -0.33778618 -0.33778618 -0.33778618  0.        ]]
The hypergraph features for node 704, index 1052 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.33778618 -0.33778618 -0.33778618 -0.33778618  0.        ]]
The hypergraph features for node 641, index 1053 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.57611855 -0.33778618 -0.49667443 -0.57611855  0.11235096]]
The hypergraph features for node 782, index 1054 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.74289966 -0.74289966 -0.74289966 -0.74289966  0.        ]]
The hypergraph features for node 642, index 1055 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.57611855 -0.57611855 -0.57611855 -0.57611855  0.        ]]
The hypergraph features for node 2252, index 1056 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.66620064 -0.34688753 -0.50654408 -0.50654408  0.15965655]]
The hypergraph features for node 2269, index 1057 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.66620064 -0.66620064 -0.66620064 -0.66620064  0.        ]]
The hypergraph features for node 1277, index 1058 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.66620064  0.03145912 -0.31737076 -0.31737076  0.34882988]]
The hypergraph features for node 1921, index 1059 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.74289966 -0.74289966 -0.74289966 -0.74289966  0.        ]]
The hypergraph features for node 1170, index 1060 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.19493544 -0.19493544 -0.19493544 -0.19493544  0.        ]]
The hypergraph features for node 164, index 1061 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.19493544 -0.19493544 -0.19493544 -0.19493544  0.        ]]
The hypergraph features for node 1700, index 1062 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.32680698 -0.32680698 -0.32680698 -0.32680698  0.        ]]
The hypergraph features for node 741, index 1063 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.32680698 -0.32680698 -0.32680698 -0.32680698  0.        ]]
The hypergraph features for node 1704, index 1064 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.32680698 -0.32680698 -0.32680698 -0.32680698  0.        ]]
The hypergraph features for node 1933, index 1065 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.67514308 -0.56541936 -0.62028122 -0.62028122  0.05486186]]
The hypergraph features for node 1591, index 1066 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.88857281 -0.03452713 -0.53274768 -0.67514308  0.36291031]]
The hypergraph features for node 1884, index 1067 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.67514308 -0.02193365 -0.38116407 -0.44641547  0.27063376]]
The hypergraph features for node 1885, index 1068 are 
 [[0. 0. 1. ... 0. 0. 0.]]
We add the encoding:
 [[-0.67514308 -0.02193365 -0.38116407 -0.44641547  0.27063376]]
The hypergraph features for node 2579, index 1069 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.44798303 -0.31913316 -0.38355809 -0.38355809  0.06442493]]
The hypergraph features for node 1344, index 1070 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.58625499 -0.44798303 -0.49407368 -0.44798303  0.06518203]]
The hypergraph features for node 540, index 1071 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.58625499 -0.32103221 -0.45175674 -0.44798303  0.10830963]]
The hypergraph features for node 1348, index 1072 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.58625499 -0.50364423 -0.54494961 -0.54494961  0.04130538]]
The hypergraph features for node 1046, index 1073 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.58625499 -0.58625499 -0.58625499 -0.58625499  0.        ]]
The hypergraph features for node 1047, index 1074 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.64861874 -0.32103221 -0.51488754 -0.54494961  0.12317026]]
The hypergraph features for node 629, index 1075 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.50364423 -0.50364423 -0.50364423 -0.50364423  0.        ]]
The hypergraph features for node 801, index 1076 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.50364423 -0.31913316 -0.4113887  -0.4113887   0.09225553]]
The hypergraph features for node 1642, index 1077 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.50364423 -0.31913316 -0.4113887  -0.4113887   0.09225553]]
The hypergraph features for node 534, index 1078 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.50364423 -0.50364423 -0.50364423 -0.50364423  0.        ]]
The hypergraph features for node 1680, index 1079 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.02878828 -0.02878828 -0.02878828 -0.02878828  0.        ]]
The hypergraph features for node 1674, index 1080 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.02878828 -0.02878828 -0.02878828 -0.02878828  0.        ]]
The hypergraph features for node 2675, index 1081 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.80803466 -0.27676566 -0.52900439 -0.51560861  0.25294914]]
The hypergraph features for node 2006, index 1082 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.27676566 -0.27676566 -0.27676566 -0.27676566  0.        ]]
The hypergraph features for node 1862, index 1083 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.80803466 -0.27676566 -0.54240016 -0.54240016  0.2656345 ]]
The hypergraph features for node 1848, index 1084 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 485, index 1085 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.91525543 -0.73969229 -0.78358307 -0.73969229  0.07602107]]
The hypergraph features for node 1838, index 1086 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.91525543 -0.73969229 -0.82747386 -0.82747386  0.08778157]]
The hypergraph features for node 2558, index 1087 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.34131169 -0.91525543 -1.12828356 -1.12828356  0.21302813]]
The hypergraph features for node 2561, index 1088 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.73969229 -0.73969229 -0.73969229 -0.73969229  0.        ]]
The hypergraph features for node 2226, index 1089 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.82989943 -0.44798303 -0.73442033 -0.82989943  0.16537465]]
The hypergraph features for node 860, index 1090 are 
 [[0. 0. 1. ... 0. 0. 0.]]
We add the encoding:
 [[-0.44798303 -0.44798303 -0.44798303 -0.44798303  0.        ]]
The hypergraph features for node 1964, index 1091 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.80803466 -0.80803466 -0.80803466 -0.80803466  0.        ]]
The hypergraph features for node 763, index 1092 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.75445156 -0.75445156 -0.75445156 -0.75445156  0.        ]]
The hypergraph features for node 2066, index 1093 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.79953076 -0.79953076 -0.79953076 -0.79953076  0.        ]]
The hypergraph features for node 2060, index 1094 are 
 [[0. 0. 0. ... 0. 0. 1.]]
We add the encoding:
 [[-0.79953076 -0.79953076 -0.79953076 -0.79953076  0.        ]]
The hypergraph features for node 2052, index 1095 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.79953076 -0.79953076 -0.79953076 -0.79953076  0.        ]]
The hypergraph features for node 2016, index 1096 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.79953076  0.0288628  -0.38533398 -0.38533398  0.41419678]]
The hypergraph features for node 495, index 1097 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.79953076 -0.56620232 -0.68286654 -0.68286654  0.11666422]]
The hypergraph features for node 464, index 1098 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.79953076 -0.79953076 -0.79953076 -0.79953076  0.        ]]
The hypergraph features for node 342, index 1099 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.79953076 -0.11331046 -0.46202511 -0.56620232  0.23411416]]
The hypergraph features for node 729, index 1100 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.79953076 -0.11331046 -0.41990958 -0.34688753  0.28486692]]
The hypergraph features for node 2115, index 1101 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.79953076 -0.56620232 -0.68286654 -0.68286654  0.11666422]]
The hypergraph features for node 467, index 1102 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.79953076 -0.56620232 -0.647566   -0.56620232  0.10143046]]
The hypergraph features for node 339, index 1103 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.56620232 -0.06250144 -0.31435188 -0.31435188  0.25185044]]
The hypergraph features for node 573, index 1104 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.56620232  0.         -0.28310116 -0.28310116  0.28310116]]
The hypergraph features for node 1993, index 1105 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.56620232 -0.11331046 -0.26427441 -0.11331046  0.21349527]]
The hypergraph features for node 270, index 1106 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.56620232 -0.56620232 -0.56620232 -0.56620232  0.        ]]
The hypergraph features for node 1452, index 1107 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.73969229  0.         -0.3822945  -0.4071912   0.30249083]]
The hypergraph features for node 2315, index 1108 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.73969229 -0.73969229 -0.73969229 -0.73969229  0.        ]]
The hypergraph features for node 661, index 1109 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.74289966 -0.74289966 -0.74289966 -0.74289966  0.        ]]
The hypergraph features for node 2707, index 1110 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.74289966 -0.57589328 -0.65939647 -0.65939647  0.08350319]]
The hypergraph features for node 1340, index 1111 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 2596, index 1112 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.29427534  0.         -0.14713767 -0.14713767  0.14713767]]
The hypergraph features for node 101, index 1113 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.29427534  0.         -0.14713767 -0.14713767  0.14713767]]
The hypergraph features for node 246, index 1114 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.4071912 -0.4071912 -0.4071912 -0.4071912  0.       ]]
The hypergraph features for node 2056, index 1115 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.4698578 -0.4698578 -0.4698578 -0.4698578  0.       ]]
The hypergraph features for node 2532, index 1116 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.4698578 -0.4698578 -0.4698578 -0.4698578  0.       ]]
The hypergraph features for node 1857, index 1117 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.4698578 -0.4698578 -0.4698578 -0.4698578  0.       ]]
The hypergraph features for node 78, index 1118 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.4698578 -0.4698578 -0.4698578 -0.4698578  0.       ]]
The hypergraph features for node 2540, index 1119 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.42606763 -0.42606763 -0.42606763 -0.42606763  0.        ]]
The hypergraph features for node 2020, index 1120 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.42606763 -0.42606763 -0.42606763 -0.42606763  0.        ]]
The hypergraph features for node 1059, index 1121 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.42606763 -0.42606763 -0.42606763 -0.42606763  0.        ]]
The hypergraph features for node 1284, index 1122 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.42606763 -0.42606763 -0.42606763 -0.42606763  0.        ]]
The hypergraph features for node 524, index 1123 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.64416742 -0.42606763 -0.53511752 -0.53511752  0.1090499 ]]
The hypergraph features for node 2530, index 1124 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.53303804 -0.53303804 -0.53303804 -0.53303804  0.        ]]
The hypergraph features for node 671, index 1125 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.18488963 -1.18488963 -1.18488963 -1.18488963  0.        ]]
The hypergraph features for node 2430, index 1126 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.05846848 -0.05846848 -0.05846848 -0.05846848  0.        ]]
The hypergraph features for node 703, index 1127 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.24689952 -0.24689952 -0.24689952 -0.24689952  0.        ]]
The hypergraph features for node 1690, index 1128 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.24689952 -0.24689952 -0.24689952 -0.24689952  0.        ]]
The hypergraph features for node 664, index 1129 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.24689952 -0.24689952 -0.24689952 -0.24689952  0.        ]]
The hypergraph features for node 2222, index 1130 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.54010581 -0.54010581 -0.54010581 -0.54010581  0.        ]]
The hypergraph features for node 2092, index 1131 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.54010581 -0.54010581 -0.54010581 -0.54010581  0.        ]]
The hypergraph features for node 1566, index 1132 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.54010581 -0.54010581 -0.54010581 -0.54010581  0.        ]]
The hypergraph features for node 185, index 1133 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.54010581 -0.54010581 -0.54010581 -0.54010581  0.        ]]
The hypergraph features for node 324, index 1134 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.74839282 -0.43981811 -0.61917739 -0.64424931  0.1339919 ]]
The hypergraph features for node 332, index 1135 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.54010581 -0.32103221 -0.43519356 -0.43981811  0.07759215]]
The hypergraph features for node 351, index 1136 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.65738443 -0.65738443 -0.65738443 -0.65738443  0.        ]]
The hypergraph features for node 1990, index 1137 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.65738443 -0.07425495 -0.4151297  -0.51374972  0.24806509]]
The hypergraph features for node 2131, index 1138 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.27680492 -0.65738443 -0.96709468 -0.96709468  0.30971025]]
The hypergraph features for node 299, index 1139 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.64416742 -0.34688753 -0.49050403 -0.48548059  0.12547804]]
The hypergraph features for node 1116, index 1140 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.56665015  0.21510649 -0.22954845 -0.28332508  0.34557402]]
The hypergraph features for node 2496, index 1141 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.98270667  0.         -0.49135333 -0.49135333  0.49135333]]
The hypergraph features for node 2506, index 1142 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.25635954  0.2873366   0.01020144 -0.00037275  0.22208892]]
The hypergraph features for node 2375, index 1143 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.25635954 -0.25635954 -0.25635954 -0.25635954  0.        ]]
The hypergraph features for node 2366, index 1144 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.21956476  0.2873366   0.02246637 -0.00037275  0.20757082]]
The hypergraph features for node 399, index 1145 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.2873366 0.2873366 0.2873366 0.2873366 0.       ]]
The hypergraph features for node 522, index 1146 are 
 [[0. 0. 1. ... 0. 0. 0.]]
We add the encoding:
 [[0.2873366 0.2873366 0.2873366 0.2873366 0.       ]]
The hypergraph features for node 684, index 1147 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.55330217  0.2873366  -0.13298278 -0.13298278  0.42031939]]
The hypergraph features for node 1425, index 1148 are 
 [[1. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.2873366 0.2873366 0.2873366 0.2873366 0.       ]]
The hypergraph features for node 1878, index 1149 are 
 [[0. 1. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.98270667 -0.98270667 -0.98270667 -0.98270667  0.        ]]
The hypergraph features for node 1176, index 1150 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.98270667 -0.14968454 -0.5661956  -0.5661956   0.41651106]]
The hypergraph features for node 1724, index 1151 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.65112448 -0.2629147  -0.45701959 -0.45701959  0.19410489]]
The hypergraph features for node 1827, index 1152 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.65112448 -0.34688753 -0.54739314 -0.64416742  0.14180733]]
The hypergraph features for node 2265, index 1153 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.46948151 -0.46948151 -0.46948151 -0.46948151  0.        ]]
The hypergraph features for node 1321, index 1154 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.21478636 -0.18222129 -0.20176033 -0.21478636  0.01595356]]
The hypergraph features for node 1129, index 1155 are 
 [[0. 0. 0. ... 0. 1. 0.]]
We add the encoding:
 [[-0.21478636 -0.18222129 -0.19524732 -0.18222129  0.01595356]]
The hypergraph features for node 1615, index 1156 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.21478636 -0.21478636 -0.21478636 -0.21478636  0.        ]]
The hypergraph features for node 441, index 1157 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.21478636 -0.21478636 -0.21478636 -0.21478636  0.        ]]
The hypergraph features for node 812, index 1158 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.18222129  0.08193282 -0.09416992 -0.18222129  0.12452344]]
The hypergraph features for node 814, index 1159 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.18222129  0.08193282 -0.09416992 -0.18222129  0.12452344]]
The hypergraph features for node 1365, index 1160 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.18222129 -0.18222129 -0.18222129 -0.18222129  0.        ]]
The hypergraph features for node 1341, index 1161 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.18222129 -0.18222129 -0.18222129 -0.18222129  0.        ]]
The hypergraph features for node 1080, index 1162 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.18222129 -0.18222129 -0.18222129 -0.18222129  0.        ]]
The hypergraph features for node 2351, index 1163 are 
 [[0. 0. 1. ... 0. 0. 0.]]
We add the encoding:
 [[-0.32103221 -0.32103221 -0.32103221 -0.32103221  0.        ]]
The hypergraph features for node 1640, index 1164 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.64861874 -0.32103221 -0.48482547 -0.48482547  0.16379327]]
The hypergraph features for node 670, index 1165 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.32103221 -0.32103221 -0.32103221 -0.32103221  0.        ]]
The hypergraph features for node 326, index 1166 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.74839282 -0.32103221 -0.53471251 -0.53471251  0.21368031]]
The hypergraph features for node 1411, index 1167 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.31466751 -0.31466751 -0.31466751 -0.31466751  0.        ]]
The hypergraph features for node 2397, index 1168 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.31466751 -0.31466751 -0.31466751 -0.31466751  0.        ]]
The hypergraph features for node 2405, index 1169 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.31466751 -0.31466751 -0.31466751 -0.31466751  0.        ]]
The hypergraph features for node 1119, index 1170 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.31466751 -0.31466751 -0.31466751 -0.31466751  0.        ]]
The hypergraph features for node 2129, index 1171 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.31466751 -0.31466751 -0.31466751 -0.31466751  0.        ]]
The hypergraph features for node 1058, index 1172 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.31466751 -0.07425495 -0.19446123 -0.19446123  0.12020628]]
The hypergraph features for node 4, index 1173 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.21510649 0.21510649 0.21510649 0.21510649 0.        ]]
The hypergraph features for node 1701, index 1174 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.88857281 -0.03452713 -0.46154997 -0.46154997  0.42702284]]
The hypergraph features for node 2221, index 1175 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.12163143 -0.12163143 -0.12163143 -0.12163143  0.        ]]
The hypergraph features for node 1757, index 1176 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.12163143 -0.12163143 -0.12163143 -0.12163143  0.        ]]
The hypergraph features for node 2102, index 1177 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.12163143 -0.12163143 -0.12163143 -0.12163143  0.        ]]
The hypergraph features for node 1831, index 1178 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.47286133 -0.47286133 -0.47286133 -0.47286133  0.        ]]
The hypergraph features for node 1836, index 1179 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.47286133 -0.47286133 -0.47286133 -0.47286133  0.        ]]
The hypergraph features for node 1832, index 1180 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.47286133 -0.47286133 -0.47286133 -0.47286133  0.        ]]
The hypergraph features for node 725, index 1181 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.88857281 -0.88857281 -0.88857281 -0.88857281  0.        ]]
The hypergraph features for node 358, index 1182 are 
 [[0. 0. 0. ... 0. 1. 0.]]
We add the encoding:
 [[-0.88857281 -0.70339922 -0.79598602 -0.79598602  0.0925868 ]]
The hypergraph features for node 1255, index 1183 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.88857281 -0.88857281 -0.88857281 -0.88857281  0.        ]]
The hypergraph features for node 2578, index 1184 are 
 [[0. 1. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.64861874 -0.64861874 -0.64861874 -0.64861874  0.        ]]
The hypergraph features for node 2679, index 1185 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.64861874 -0.32448124 -0.48654999 -0.48654999  0.16206875]]
The hypergraph features for node 1648, index 1186 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.64861874 -0.64861874 -0.64861874 -0.64861874  0.        ]]
The hypergraph features for node 1679, index 1187 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.64861874 -0.64861874 -0.64861874 -0.64861874  0.        ]]
The hypergraph features for node 1684, index 1188 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.64861874 -0.64861874 -0.64861874 -0.64861874  0.        ]]
The hypergraph features for node 832, index 1189 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.88857281 -0.88857281 -0.88857281 -0.88857281  0.        ]]
The hypergraph features for node 33, index 1190 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.88857281 -0.88857281 -0.88857281 -0.88857281  0.        ]]
The hypergraph features for node 2169, index 1191 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.78059749 -0.78059749 -0.78059749 -0.78059749  0.        ]]
The hypergraph features for node 1894, index 1192 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.78059749  0.01326978 -0.27994552 -0.17622718  0.32785698]]
The hypergraph features for node 913, index 1193 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.01326978 0.01326978 0.01326978 0.01326978 0.        ]]
The hypergraph features for node 909, index 1194 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.01326978 0.01326978 0.01326978 0.01326978 0.        ]]
The hypergraph features for node 398, index 1195 are 
 [[0. 0. 0. ... 0. 1. 0.]]
We add the encoding:
 [[-0.34688753 -0.00037275 -0.15680589 -0.12315738  0.14345101]]
The hypergraph features for node 248, index 1196 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.12315738 -0.00037275 -0.06176506 -0.06176506  0.06139232]]
The hypergraph features for node 1867, index 1197 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.00037275 -0.00037275 -0.00037275 -0.00037275  0.        ]]
The hypergraph features for node 2242, index 1198 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.00037275 -0.00037275 -0.00037275 -0.00037275  0.        ]]
The hypergraph features for node 1689, index 1199 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.48704298 -0.10383085 -0.29543692 -0.29543692  0.19160606]]
The hypergraph features for node 2021, index 1200 are 
 [[0. 0. 0. ... 0. 0. 1.]]
We add the encoding:
 [[-0.79513625 -0.48704298 -0.64108962 -0.64108962  0.15404663]]
The hypergraph features for node 2337, index 1201 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.48704298 -0.43981811 -0.46343055 -0.46343055  0.02361244]]
The hypergraph features for node 2132, index 1202 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.48704298 -0.48704298 -0.48704298 -0.48704298  0.        ]]
The hypergraph features for node 1586, index 1203 are 
 [[0. 0. 0. ... 0. 1. 0.]]
We add the encoding:
 [[-0.67178905 -0.67178905 -0.67178905 -0.67178905  0.        ]]
The hypergraph features for node 1027, index 1204 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.67178905 -0.34688753 -0.50933829 -0.50933829  0.16245076]]
The hypergraph features for node 291, index 1205 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.58053621 -0.58053621 -0.58053621 -0.58053621  0.        ]]
The hypergraph features for node 1662, index 1206 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.58053621 -0.58053621 -0.58053621 -0.58053621  0.        ]]
The hypergraph features for node 1528, index 1207 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.29305024 -0.25508531 -0.27406777 -0.27406777  0.01898246]]
The hypergraph features for node 360, index 1208 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.25508531 -0.25508531 -0.25508531 -0.25508531  0.        ]]
The hypergraph features for node 885, index 1209 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.25508531 -0.25508531 -0.25508531 -0.25508531  0.        ]]
The hypergraph features for node 1439, index 1210 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.25508531 -0.25508531 -0.25508531 -0.25508531  0.        ]]
The hypergraph features for node 866, index 1211 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.25508531 -0.25508531 -0.25508531 -0.25508531  0.        ]]
The hypergraph features for node 389, index 1212 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.43981811 -0.09099734 -0.26540772 -0.26540772  0.17441038]]
The hypergraph features for node 1064, index 1213 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.09099734 -0.09099734 -0.09099734 -0.09099734  0.        ]]
The hypergraph features for node 623, index 1214 are 
 [[0. 0. 1. ... 0. 0. 0.]]
We add the encoding:
 [[-0.70590611 -0.01225837 -0.31224498 -0.26540772  0.27847194]]
The hypergraph features for node 2293, index 1215 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.09099734 -0.09099734 -0.09099734 -0.09099734  0.        ]]
The hypergraph features for node 930, index 1216 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.36572415 -0.36572415 -0.36572415 -0.36572415  0.        ]]
The hypergraph features for node 1254, index 1217 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.36572415 -0.36572415 -0.36572415 -0.36572415  0.        ]]
The hypergraph features for node 364, index 1218 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.36572415 -0.36572415 -0.36572415 -0.36572415  0.        ]]
The hypergraph features for node 365, index 1219 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.36572415 -0.36572415 -0.36572415 -0.36572415  0.        ]]
The hypergraph features for node 366, index 1220 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.36572415 -0.36572415 -0.36572415 -0.36572415  0.        ]]
The hypergraph features for node 2112, index 1221 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.36572415 -0.36572415 -0.36572415 -0.36572415  0.        ]]
The hypergraph features for node 646, index 1222 are 
 [[0. 0. 1. ... 0. 0. 0.]]
We add the encoding:
 [[-1.34131169 -1.34131169 -1.34131169 -1.34131169  0.        ]]
The hypergraph features for node 2598, index 1223 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.34131169  0.03145912 -0.59138779 -0.61909403  0.50770597]]
The hypergraph features for node 1231, index 1224 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.34131169  0.03145912 -0.45188948 -0.3422753   0.52045864]]
The hypergraph features for node 1737, index 1225 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.03145912 0.03145912 0.03145912 0.03145912 0.        ]]
The hypergraph features for node 1736, index 1226 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.03145912 0.03145912 0.03145912 0.03145912 0.        ]]
The hypergraph features for node 1495, index 1227 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.69934309 -0.69934309 -0.69934309 -0.69934309  0.        ]]
The hypergraph features for node 2641, index 1228 are 
 [[0. 1. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.69934309 -0.69934309 -0.69934309 -0.69934309  0.        ]]
The hypergraph features for node 1940, index 1229 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.34535353 -0.34535353 -0.34535353 -0.34535353  0.        ]]
The hypergraph features for node 2379, index 1230 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.34535353 -0.08278012 -0.21406683 -0.21406683  0.13128671]]
The hypergraph features for node 2133, index 1231 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.75245599 -0.08278012 -0.39352988 -0.34535353  0.27550821]]
The hypergraph features for node 1161, index 1232 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.03700752 0.03700752 0.03700752 0.03700752 0.        ]]
The hypergraph features for node 2194, index 1233 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.01319122  0.03700752 -0.320495   -0.33164942  0.3835865 ]]
The hypergraph features for node 2149, index 1234 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.03700752 0.03700752 0.03700752 0.03700752 0.        ]]
The hypergraph features for node 2190, index 1235 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.01319122  0.03700752 -0.320495   -0.33164942  0.3835865 ]]
The hypergraph features for node 2189, index 1236 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.01319122  0.03700752 -0.66312497 -1.01319122  0.49506844]]
The hypergraph features for node 249, index 1237 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.03700752 0.03700752 0.03700752 0.03700752 0.        ]]
The hypergraph features for node 2128, index 1238 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.27680492 -1.27680492 -1.27680492 -1.27680492  0.        ]]
The hypergraph features for node 2319, index 1239 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.27680492 -1.27680492 -1.27680492 -1.27680492  0.        ]]
The hypergraph features for node 2130, index 1240 are 
 [[0. 0. 0. ... 0. 1. 0.]]
We add the encoding:
 [[-1.27680492 -1.27680492 -1.27680492 -1.27680492  0.        ]]
The hypergraph features for node 1265, index 1241 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.17333941 0.17333941 0.17333941 0.17333941 0.        ]]
The hypergraph features for node 880, index 1242 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.17333941 0.17333941 0.17333941 0.17333941 0.        ]]
The hypergraph features for node 120, index 1243 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.17333941 0.17333941 0.17333941 0.17333941 0.        ]]
The hypergraph features for node 2259, index 1244 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.34688753  0.17333941 -0.08677406 -0.08677406  0.26011347]]
The hypergraph features for node 1620, index 1245 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.39873158 -0.39873158 -0.39873158 -0.39873158  0.        ]]
The hypergraph features for node 2429, index 1246 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.39873158 -0.39873158 -0.39873158 -0.39873158  0.        ]]
The hypergraph features for node 1134, index 1247 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.44183741 -0.39873158 -0.42028449 -0.42028449  0.02155291]]
The hypergraph features for node 1404, index 1248 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.58759042 -0.39873158 -0.493161   -0.493161    0.09442942]]
The hypergraph features for node 2551, index 1249 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.06250144 -0.06250144 -0.06250144 -0.06250144  0.        ]]
The hypergraph features for node 1602, index 1250 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.06250144 -0.06250144 -0.06250144 -0.06250144  0.        ]]
The hypergraph features for node 971, index 1251 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.06250144 -0.06250144 -0.06250144 -0.06250144  0.        ]]
The hypergraph features for node 276, index 1252 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.34688753 -0.11331046 -0.23009899 -0.23009899  0.11678854]]
The hypergraph features for node 778, index 1253 are 
 [[0. 0. 1. ... 0. 0. 0.]]
We add the encoding:
 [[-0.34688753 -0.34688753 -0.34688753 -0.34688753  0.        ]]
The hypergraph features for node 2249, index 1254 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.34688753 -0.34688753 -0.34688753 -0.34688753  0.        ]]
The hypergraph features for node 2250, index 1255 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.34688753 -0.34688753 -0.34688753 -0.34688753  0.        ]]
The hypergraph features for node 2238, index 1256 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.34688753 -0.34688753 -0.34688753 -0.34688753  0.        ]]
The hypergraph features for node 2511, index 1257 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.34688753 -0.29305024 -0.30381769 -0.29305024  0.02153492]]
The hypergraph features for node 1853, index 1258 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.29305024 -0.29305024 -0.29305024 -0.29305024  0.        ]]
The hypergraph features for node 2418, index 1259 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.29305024 -0.29305024 -0.29305024 -0.29305024  0.        ]]
The hypergraph features for node 2141, index 1260 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.29305024 -0.29305024 -0.29305024 -0.29305024  0.        ]]
The hypergraph features for node 2005, index 1261 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.29305024 -0.29305024 -0.29305024 -0.29305024  0.        ]]
The hypergraph features for node 689, index 1262 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.12080801 -0.12080801 -0.12080801 -0.12080801  0.        ]]
The hypergraph features for node 2630, index 1263 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.12080801 -0.12080801 -0.12080801 -0.12080801  0.        ]]
The hypergraph features for node 1625, index 1264 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.12080801 -0.12080801 -0.12080801 -0.12080801  0.        ]]
The hypergraph features for node 1654, index 1265 are 
 [[0. 1. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.58215102 -0.58215102 -0.58215102 -0.58215102  0.        ]]
The hypergraph features for node 1015, index 1266 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.58215102 -0.4216094  -0.52863714 -0.58215102  0.07568004]]
The hypergraph features for node 1338, index 1267 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.58215102 -0.58215102 -0.58215102 -0.58215102  0.        ]]
The hypergraph features for node 784, index 1268 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.58215102 -0.32448124 -0.45331613 -0.45331613  0.12883489]]
The hypergraph features for node 917, index 1269 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.58215102 -0.58215102 -0.58215102 -0.58215102  0.        ]]
The hypergraph features for node 1806, index 1270 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.31913316 -0.31913316 -0.31913316 -0.31913316  0.        ]]
The hypergraph features for node 1489, index 1271 are 
 [[0. 0. 0. ... 0. 1. 0.]]
We add the encoding:
 [[-0.91496269 -0.91496269 -0.91496269 -0.91496269  0.        ]]
The hypergraph features for node 2167, index 1272 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.91496269 -0.91496269 -0.91496269 -0.91496269  0.        ]]
The hypergraph features for node 1747, index 1273 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.91496269 -0.01225837 -0.46361053 -0.46361053  0.45135216]]
The hypergraph features for node 2069, index 1274 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.91496269 -0.91496269 -0.91496269 -0.91496269  0.        ]]
The hypergraph features for node 2434, index 1275 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.12315738 -0.01225837 -0.06770788 -0.06770788  0.05544951]]
The hypergraph features for node 275, index 1276 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 279, index 1277 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 916, index 1278 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.29427534  0.14630079 -0.00055792  0.14630079  0.20768958]]
The hypergraph features for node 1770, index 1279 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.29427534 -0.29427534 -0.29427534 -0.29427534  0.        ]]
The hypergraph features for node 1764, index 1280 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.29427534 -0.29427534 -0.29427534 -0.29427534  0.        ]]
The hypergraph features for node 1765, index 1281 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.29427534 -0.29427534 -0.29427534 -0.29427534  0.        ]]
The hypergraph features for node 32, index 1282 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.29427534 -0.29427534 -0.29427534 -0.29427534  0.        ]]
The hypergraph features for node 1471, index 1283 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.29427534 -0.29427534 -0.29427534 -0.29427534  0.        ]]
The hypergraph features for node 1244, index 1284 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.29427534 -0.29427534 -0.29427534 -0.29427534  0.        ]]
The hypergraph features for node 1668, index 1285 are 
 [[0. 1. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.70590611 -0.70590611 -0.70590611 -0.70590611  0.        ]]
The hypergraph features for node 2108, index 1286 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.70590611 -0.70590611 -0.70590611 -0.70590611  0.        ]]
The hypergraph features for node 1898, index 1287 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.74839282 -0.70590611 -0.72714947 -0.72714947  0.02124335]]
The hypergraph features for node 2292, index 1288 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.70590611 -0.43981811 -0.57286211 -0.57286211  0.133044  ]]
The hypergraph features for node 1666, index 1289 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.69772243 -0.69772243 -0.69772243 -0.69772243  0.        ]]
The hypergraph features for node 2590, index 1290 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.55330217 -0.55330217 -0.55330217 -0.55330217  0.        ]]
The hypergraph features for node 439, index 1291 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.55330217 -0.55330217 -0.55330217 -0.55330217  0.        ]]
The hypergraph features for node 1555, index 1292 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.55330217 -0.55330217 -0.55330217 -0.55330217  0.        ]]
The hypergraph features for node 1455, index 1293 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.18865123 -0.18865123 -0.18865123 -0.18865123  0.        ]]
The hypergraph features for node 1167, index 1294 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.18865123 -0.18865123 -0.18865123 -0.18865123  0.        ]]
The hypergraph features for node 1474, index 1295 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.18865123 -0.18865123 -0.18865123 -0.18865123  0.        ]]
The hypergraph features for node 1473, index 1296 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.44183741 -0.18865123 -0.31524432 -0.31524432  0.12659309]]
The hypergraph features for node 857, index 1297 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.33085003 -0.33085003 -0.33085003 -0.33085003  0.        ]]
The hypergraph features for node 2576, index 1298 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.33085003 -0.33085003 -0.33085003 -0.33085003  0.        ]]
The hypergraph features for node 2438, index 1299 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.0288628 0.0288628 0.0288628 0.0288628 0.       ]]
The hypergraph features for node 253, index 1300 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.53884497  0.0288628  -0.25499108 -0.25499108  0.28385389]]
The hypergraph features for node 238, index 1301 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.53884497  0.0288628  -0.25499108 -0.25499108  0.28385389]]
The hypergraph features for node 301, index 1302 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.53884497 -0.53884497 -0.53884497 -0.53884497  0.        ]]
The hypergraph features for node 2407, index 1303 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.53884497 -0.14968454 -0.34426476 -0.34426476  0.19458022]]
The hypergraph features for node 2604, index 1304 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.0317452  -0.53884497 -0.78529508 -0.78529508  0.24645011]]
The hypergraph features for node 2330, index 1305 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.0317452  -0.53884497 -0.78529508 -0.78529508  0.24645011]]
The hypergraph features for node 912, index 1306 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.74839282 -0.53884497 -0.6436189  -0.6436189   0.10477392]]
The hypergraph features for node 1631, index 1307 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.0317452 -1.0317452 -1.0317452 -1.0317452  0.       ]]
The hypergraph features for node 2660, index 1308 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.84880972 -0.84880972 -0.84880972 -0.84880972  0.        ]]
The hypergraph features for node 748, index 1309 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.84880972 -0.84880972 -0.84880972 -0.84880972  0.        ]]
The hypergraph features for node 416, index 1310 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.84880972 -0.84880972 -0.84880972 -0.84880972  0.        ]]
The hypergraph features for node 137, index 1311 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.84880972 -0.56541936 -0.65988281 -0.56541936  0.1335915 ]]
The hypergraph features for node 990, index 1312 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.84880972 -0.84880972 -0.84880972 -0.84880972  0.        ]]
The hypergraph features for node 1630, index 1313 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.74839282 -0.74839282 -0.74839282 -0.74839282  0.        ]]
The hypergraph features for node 2065, index 1314 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.74839282  0.01479248 -0.36680017 -0.36680017  0.38159265]]
The hypergraph features for node 952, index 1315 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.43981811 -0.43981811 -0.43981811 -0.43981811  0.        ]]
The hypergraph features for node 1937, index 1316 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.43981811 -0.43981811 -0.43981811 -0.43981811  0.        ]]
The hypergraph features for node 993, index 1317 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.43981811 -0.43981811 -0.43981811 -0.43981811  0.        ]]
The hypergraph features for node 802, index 1318 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.43981811 -0.43981811 -0.43981811 -0.43981811  0.        ]]
The hypergraph features for node 527, index 1319 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.43981811 -0.43981811 -0.43981811 -0.43981811  0.        ]]
The hypergraph features for node 2459, index 1320 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.43981811 -0.4216094  -0.43071375 -0.43071375  0.00910435]]
The hypergraph features for node 123, index 1321 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.4216094 -0.4216094 -0.4216094 -0.4216094  0.       ]]
The hypergraph features for node 261, index 1322 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.4216094 -0.4216094 -0.4216094 -0.4216094  0.       ]]
The hypergraph features for node 1804, index 1323 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.56541936 -0.56541936 -0.56541936 -0.56541936  0.        ]]
The hypergraph features for node 1805, index 1324 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.56541936 -0.56541936 -0.56541936 -0.56541936  0.        ]]
The hypergraph features for node 1926, index 1325 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.56541936 -0.56541936 -0.56541936 -0.56541936  0.        ]]
The hypergraph features for node 1941, index 1326 are 
 [[0. 0. 0. ... 0. 0. 1.]]
We add the encoding:
 [[-0.56541936 -0.56541936 -0.56541936 -0.56541936  0.        ]]
The hypergraph features for node 2510, index 1327 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.56541936  0.20712311 -0.10781558 -0.14156497  0.26717398]]
The hypergraph features for node 1913, index 1328 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.56541936 -0.56541936 -0.56541936 -0.56541936  0.        ]]
The hypergraph features for node 923, index 1329 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.56541936 -0.1213492  -0.34338428 -0.34338428  0.22203508]]
The hypergraph features for node 758, index 1330 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.1213492 -0.1213492 -0.1213492 -0.1213492  0.       ]]
The hypergraph features for node 265, index 1331 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.1213492 -0.1213492 -0.1213492 -0.1213492  0.       ]]
The hypergraph features for node 2285, index 1332 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.44183741 -0.06398774 -0.25291257 -0.25291257  0.18892483]]
The hypergraph features for node 1738, index 1333 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.44183741 -0.44183741 -0.44183741 -0.44183741  0.        ]]
The hypergraph features for node 2200, index 1334 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.44183741 -0.44183741 -0.44183741 -0.44183741  0.        ]]
The hypergraph features for node 1221, index 1335 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.44183741 -0.44183741 -0.44183741 -0.44183741  0.        ]]
The hypergraph features for node 379, index 1336 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.2629147 -0.2629147 -0.2629147 -0.2629147  0.       ]]
The hypergraph features for node 756, index 1337 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.2629147 -0.2629147 -0.2629147 -0.2629147  0.       ]]
The hypergraph features for node 2528, index 1338 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.2629147 -0.2629147 -0.2629147 -0.2629147  0.       ]]
The hypergraph features for node 2481, index 1339 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.2629147 -0.2629147 -0.2629147 -0.2629147  0.       ]]
The hypergraph features for node 2592, index 1340 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.32448124 -0.32448124 -0.32448124 -0.32448124  0.        ]]
The hypergraph features for node 1619, index 1341 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.32448124  0.01479248 -0.15484438 -0.15484438  0.16963686]]
The hypergraph features for node 1614, index 1342 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.32448124 -0.32448124 -0.32448124 -0.32448124  0.        ]]
The hypergraph features for node 1722, index 1343 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.65020716 -0.65020716 -0.65020716 -0.65020716  0.        ]]
The hypergraph features for node 1723, index 1344 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.65020716 -0.65020716 -0.65020716 -0.65020716  0.        ]]
The hypergraph features for node 894, index 1345 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.70339922 -0.70339922 -0.70339922 -0.70339922  0.        ]]
The hypergraph features for node 2236, index 1346 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.70339922 -0.70339922 -0.70339922 -0.70339922  0.        ]]
The hypergraph features for node 2262, index 1347 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.70339922 -0.70339922 -0.70339922 -0.70339922  0.        ]]
The hypergraph features for node 346, index 1348 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.54209954 -0.07425495 -0.30817724 -0.30817724  0.2339223 ]]
The hypergraph features for node 363, index 1349 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.54209954 -0.54209954 -0.54209954 -0.54209954  0.        ]]
The hypergraph features for node 77, index 1350 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.54209954 -0.54209954 -0.54209954 -0.54209954  0.        ]]
The hypergraph features for node 341, index 1351 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.54209954 -0.54209954 -0.54209954 -0.54209954  0.        ]]
The hypergraph features for node 340, index 1352 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.54209954 -0.54209954 -0.54209954 -0.54209954  0.        ]]
The hypergraph features for node 709, index 1353 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.54209954 -0.54209954 -0.54209954 -0.54209954  0.        ]]
The hypergraph features for node 283, index 1354 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.51374972 -0.51374972 -0.51374972 -0.51374972  0.        ]]
The hypergraph features for node 2275, index 1355 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.51374972 -0.51374972 -0.51374972 -0.51374972  0.        ]]
The hypergraph features for node 2268, index 1356 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.64416742 -0.51374972 -0.57895857 -0.57895857  0.06520885]]
The hypergraph features for node 1120, index 1357 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.64416742 -0.64416742 -0.64416742 -0.64416742  0.        ]]
The hypergraph features for node 805, index 1358 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.64416742 -0.64416742 -0.64416742 -0.64416742  0.        ]]
The hypergraph features for node 1774, index 1359 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.64416742 -0.64416742 -0.64416742 -0.64416742  0.        ]]
The hypergraph features for node 1169, index 1360 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.64416742 -0.64416742 -0.64416742 -0.64416742  0.        ]]
The hypergraph features for node 273, index 1361 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.01225837 -0.01225837 -0.01225837 -0.01225837  0.        ]]
The hypergraph features for node 1294, index 1362 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.01225837 -0.01225837 -0.01225837 -0.01225837  0.        ]]
The hypergraph features for node 515, index 1363 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.01225837 -0.01225837 -0.01225837 -0.01225837  0.        ]]
The hypergraph features for node 152, index 1364 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.06398774 -0.06398774 -0.06398774 -0.06398774  0.        ]]
The hypergraph features for node 1384, index 1365 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.06398774 -0.06398774 -0.06398774 -0.06398774  0.        ]]
The hypergraph features for node 962, index 1366 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.80571906 -0.80571906 -0.80571906 -0.80571906  0.        ]]
The hypergraph features for node 820, index 1367 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.80571906 -0.58759042 -0.69665474 -0.69665474  0.10906432]]
The hypergraph features for node 1714, index 1368 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.58759042 -0.58759042 -0.58759042 -0.58759042  0.        ]]
The hypergraph features for node 838, index 1369 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.58759042 -0.58759042 -0.58759042 -0.58759042  0.        ]]
The hypergraph features for node 508, index 1370 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.58759042 -0.58759042 -0.58759042 -0.58759042  0.        ]]
The hypergraph features for node 1034, index 1371 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.07425495 -0.07425495 -0.07425495 -0.07425495  0.        ]]
The hypergraph features for node 873, index 1372 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.07425495 -0.07425495 -0.07425495 -0.07425495  0.        ]]
The hypergraph features for node 595, index 1373 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.07425495 -0.07425495 -0.07425495 -0.07425495  0.        ]]
The hypergraph features for node 2668, index 1374 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.01319122 -1.01319122 -1.01319122 -1.01319122  0.        ]]
The hypergraph features for node 1536, index 1375 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.33164942 -0.33164942 -0.33164942 -0.33164942  0.        ]]
The hypergraph features for node 1511, index 1376 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.46857462 -0.46857462 -0.46857462 -0.46857462  0.        ]]
The hypergraph features for node 1598, index 1377 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.46857462 -0.46857462 -0.46857462 -0.46857462  0.        ]]
The hypergraph features for node 1572, index 1378 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.46857462 -0.46857462 -0.46857462 -0.46857462  0.        ]]
The hypergraph features for node 272, index 1379 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.46857462 -0.46857462 -0.46857462 -0.46857462  0.        ]]
The hypergraph features for node 1551, index 1380 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.46857462  0.05987751 -0.20434856 -0.20434856  0.26422606]]
The hypergraph features for node 1866, index 1381 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.14968454 -0.14968454 -0.14968454 -0.14968454  0.        ]]
The hypergraph features for node 1005, index 1382 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.14968454 -0.14968454 -0.14968454 -0.14968454  0.        ]]
The hypergraph features for node 1382, index 1383 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.14968454 -0.14968454 -0.14968454 -0.14968454  0.        ]]
The hypergraph features for node 209, index 1384 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.14968454 -0.14968454 -0.14968454 -0.14968454  0.        ]]
The hypergraph features for node 714, index 1385 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.04814732  0.01479248 -0.02716739 -0.04814732  0.02967011]]
The hypergraph features for node 1392, index 1386 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.04814732 -0.04814732 -0.04814732 -0.04814732  0.        ]]
The hypergraph features for node 1530, index 1387 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.04814732 -0.04814732 -0.04814732 -0.04814732  0.        ]]
The hypergraph features for node 1613, index 1388 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.01479248 0.01479248 0.01479248 0.01479248 0.        ]]
The hypergraph features for node 2216, index 1389 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.14535022  0.01479248 -0.06527887 -0.06527887  0.08007135]]
The hypergraph features for node 1840, index 1390 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.14535022  0.01479248 -0.06527887 -0.06527887  0.08007135]]
The hypergraph features for node 2646, index 1391 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.14535022 -0.14535022 -0.14535022 -0.14535022  0.        ]]
The hypergraph features for node 2643, index 1392 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.14535022 -0.14535022 -0.14535022 -0.14535022  0.        ]]
The hypergraph features for node 1260, index 1393 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.08278012 -0.08278012 -0.08278012 -0.08278012  0.        ]]
The hypergraph features for node 1262, index 1394 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.08278012 -0.08278012 -0.08278012 -0.08278012  0.        ]]
The hypergraph features for node 2392, index 1395 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.08278012 -0.08278012 -0.08278012 -0.08278012  0.        ]]
The hypergraph features for node 2206, index 1396 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.08278012 -0.08278012 -0.08278012 -0.08278012  0.        ]]
The hypergraph features for node 2697, index 1397 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.14535022 -0.14535022 -0.14535022 -0.14535022  0.        ]]
The hypergraph features for node 2698, index 1398 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.14535022 -0.14535022 -0.14535022 -0.14535022  0.        ]]
The hypergraph features for node 1759, index 1399 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.14535022 -0.14535022 -0.14535022 -0.14535022  0.        ]]
The hypergraph features for node 888, index 1400 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.08176529  0.14630079  0.03226775  0.03226775  0.11403304]]
The hypergraph features for node 468, index 1401 are 
 [[0. 1. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.08176529  0.14630079  0.06920065  0.10613354  0.08868938]]
The hypergraph features for node 2025, index 1402 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.08176529 -0.08176529 -0.08176529 -0.08176529  0.        ]]
The hypergraph features for node 491, index 1403 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.08176529  0.10613354  0.01218413  0.01218413  0.09394942]]
The hypergraph features for node 2219, index 1404 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.14535022  0.20712311  0.089632    0.20712311  0.16615752]]
The hypergraph features for node 2318, index 1405 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.14535022  0.10613354 -0.01960834 -0.01960834  0.12574188]]
The hypergraph features for node 2022, index 1406 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.20712311 0.20712311 0.20712311 0.20712311 0.        ]]
The hypergraph features for node 2008, index 1407 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.20712311 0.20712311 0.20712311 0.20712311 0.        ]]
The hypergraph features for node 2247, index 1408 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.20712311 0.20712311 0.20712311 0.20712311 0.        ]]
The hypergraph features for node 2263, index 1409 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.20712311 0.20712311 0.20712311 0.20712311 0.        ]]
The hypergraph features for node 2362, index 1410 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.14156497  0.10613354 -0.01771571 -0.01771571  0.12384925]]
The hypergraph features for node 1761, index 1411 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.10613354 0.10613354 0.10613354 0.10613354 0.        ]]
The hypergraph features for node 2234, index 1412 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.10613354 0.10613354 0.10613354 0.10613354 0.        ]]
The hypergraph features for node 2235, index 1413 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.10613354 0.10613354 0.10613354 0.10613354 0.        ]]
The hypergraph features for node 1906, index 1414 are 
 [[0. 0. 0. ... 0. 1. 0.]]
We add the encoding:
 [[0.08537722 0.08537722 0.08537722 0.08537722 0.        ]]
The hypergraph features for node 115, index 1415 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.08537722 0.08537722 0.08537722 0.08537722 0.        ]]
The hypergraph features for node 592, index 1416 are 
 [[0. 1. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.03494457  0.08537722  0.00516269 -0.03494457  0.05672024]]
The hypergraph features for node 589, index 1417 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.08537722 0.08537722 0.08537722 0.08537722 0.        ]]
The hypergraph features for node 1509, index 1418 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.03494457 -0.03494457 -0.03494457 -0.03494457  0.        ]]
The hypergraph features for node 1999, index 1419 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.03494457 -0.03494457 -0.03494457 -0.03494457  0.        ]]
The hypergraph features for node 371, index 1420 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.03494457 -0.03494457 -0.03494457 -0.03494457  0.        ]]
The hypergraph features for node 1373, index 1421 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.05987751 0.05987751 0.05987751 0.05987751 0.        ]]
The hypergraph features for node 2562, index 1422 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.05987751 0.05987751 0.05987751 0.05987751 0.        ]]
The hypergraph features for node 1315, index 1423 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.05987751 0.05987751 0.05987751 0.05987751 0.        ]]
The hypergraph features for node 1777, index 1424 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.05987751 0.05987751 0.05987751 0.05987751 0.        ]]
The hypergraph features for node 1802, index 1425 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.05987751 0.05987751 0.05987751 0.05987751 0.        ]]
The hypergraph features for node 914, index 1426 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.05987751 0.05987751 0.05987751 0.05987751 0.        ]]
The hypergraph features for node 2694, index 1427 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.05987751 0.05987751 0.05987751 0.05987751 0.        ]]
The hypergraph features for node 2495, index 1428 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.09861386 -0.09861386 -0.09861386 -0.09861386  0.        ]]
The hypergraph features for node 1762, index 1429 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.09861386 -0.09861386 -0.09861386 -0.09861386  0.        ]]
The hypergraph features for node 19, index 1430 are 
 [[0. 0. 1. ... 0. 0. 0.]]
We add the encoding:
 [[-0.09861386 -0.09861386 -0.09861386 -0.09861386  0.        ]]
The hypergraph features for node 1477, index 1431 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.02193365 -0.02193365 -0.02193365 -0.02193365  0.        ]]
The hypergraph features for node 1485, index 1432 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.02193365 -0.02193365 -0.02193365 -0.02193365  0.        ]]
The hypergraph features for node 1886, index 1433 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.44641547 -0.02193365 -0.23417456 -0.23417456  0.21224091]]
X are the features 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]]) 
 with shape torch.Size([2708, 1433])
Y are the labels 
 tensor([3, 2, 6,  ..., 0, 4, 3])
G is the hg
Namespace(add_encodings=True, encodings='LCP', data='cocitation', dataset='cora', model_name='UniGCN', first_aggregate='mean', second_aggregate='sum', add_self_loop=True, use_norm=True, activation='relu', nlayer=2, nhid=8, nhead=8, dropout=0.6, input_drop=0.6, attn_drop=0.6, lr=0.01, wd=0.0005, epochs=200, n_runs=10, gpu=0, seed=1, patience=200, nostdout=False, split=1, out_dir='runs/test', dataset_dict={'hypergraph': {402: {538, 163, 219}, 1696: {1114, 163}, 2295: {881, 427, 2667, 163}, 1274: {163, 427, 343, 1305, 1303}, 1286: {1136, 163}, 1544: {566, 163, 1206}, 2600: {1409, 523, 2220, 163}, 2363: {129, 163, 1969, 2165, 1719}, 1905: {1834, 163, 55}, 1611: {1153, 163, 100, 1380, 1379}, 141: {1184, 163, 188}, 1807: {2490, 163}, 1110: {1728, 1729, 163, 1016, 793}, 174: {2409, 163, 501, 503}, 2521: {2529, 145, 163, 2164}, 1792: {582, 2067, 163, 430}, 1675: {1153, 163}, 1334: {1696, 793, 1130, 163}, 813: {163, 1523}, 1799: {1424, 163, 2205, 727}, 1943: {1696, 163, 1936}, 2077: {744, 163, 2030, 743}, 769: {163, 764}, 781: {1184, 163, 1533, 846}, 940: {163, 934}, 942: {1153, 163}, 1590: {163, 965, 910}, 1734: {1249, 163, 1715, 1592, 793}, 1872: {1696, 1257, 163, 1771}, 2286: {2280, 145, 163}, 390: {145, 163}, 1717: {1441, 163, 1380, 941}, 1030: {1136, 1946, 163, 1796}, 2274: {163, 2237}, 2518: {2298, 163}, 606: {537, 163}, 800: {1136, 163}, 1575: {1136, 1305, 163}, 1070: {1016, 793, 1153, 163}, 935: {1136, 163}, 1205: {1016, 793, 1153, 163}, 1571: {1216, 1218, 163, 343, 2202}, 1971: {1696, 163, 2211, 995}, 1127: {1153, 163, 938, 728, 793}, 530: {163, 343}, 856: {1016, 793, 1153, 163}, 2604: {168, 163}, 910: {565, 523, 163, 902}, 2173: {636, 163, 2204, 846}, 191: {163, 294}, 1253: {163, 965, 910}, 1729: {1016, 163}, 1136: {163, 1523}, 1457: {163, 1462, 569, 826, 828}, 1225: {163, 1117, 1333}, 2563: {163, 2220, 1583, 219, 2396}, 1689: {538, 163, 219}, 1498: {1534, 289, 163, 582}, 563: {163, 37, 38, 247, 219}, 2396: {1952, 503, 163, 1583}, 717: {163, 188}, 1890: {163, 1060, 2039}, 188: {129, 163, 42, 144, 189}, 982: {163, 910, 247}, 55: {1784, 163}, 346: {163, 132, 143}, 2673: {1224, 163, 638}, 1380: {1113, 163, 793}, 1207: {378, 163, 1333, 430}, 2451: {129, 163}, 961: {523, 163}, 42: {793, 163, 188, 189}, 624: {163, 659}, 1573: {163, 910}, 2232: {2058, 163, 411}, 145: {163, 1646}, 2659: {163, 2030}, 743: {744, 2659, 163, 2030}, 744: {163, 2030}, 1139: {793, 163}, 1572: {1139, 163}, 1303: {2202, 163, 343}, 1305: {1016, 1106, 163, 728}, 129: {1728, 163, 15}, 415: {523, 188, 163}, 2204: {163, 636, 2173, 846}, 1224: {145, 163, 1333}, 219: {163, 846}, 727: {1136, 1016, 163}, 448: {163, 191}, 2557: {163, 430, 686, 1947}, 2200: {1969, 163}, 22: {129, 163, 191}, 1218: {145, 163}, 2202: {163, 1980}, 1069: {793, 163, 1685}, 2175: {1696, 2361, 163, 1257}, 966: {163, 964, 965, 902, 910}, 1219: {1153, 163}, 757: {163, 2058, 846, 219, 188}, 1685: {163, 1069}, 1691: {793, 163, 422}, 1410: {1153, 163}, 380: {163, 188}, 1536: {1728, 163, 207}, 1650: {1466, 163}, 1558: {1136, 163}, 2564: {163, 1558, 1159}, 1362: {1184, 1178, 163, 2175}, 1784: {1184, 163, 380}, 1065: {1153, 163, 1063, 1106, 1305}, 290: {163, 2220, 1911, 1839}, 1277: {2152, 163, 1533, 2470}, 395: {163, 189, 191}, 1098: {1305, 163, 343, 1911}, 1404: {163, 219}, 1099: {1178, 163}, 2196: {938, 163, 380, 2267}, 2554: {1850, 163}, 1715: {793, 163}, 2248: {145, 163}, 2251: {145, 163}, 2280: {145, 163}, 2296: {2248, 145, 163, 2251}, 1113: {1153, 163}, 714: {2089, 1218, 163}, 2638: {2202, 163}, 602: {1114, 163, 2334}, 1530: {2089, 145, 163, 1218}, 1257: {1153, 163, 380}, 2259: {1257, 163, 2261, 2175}, 689: {163, 967}, 1060: {163, 965, 237, 1136, 1523}, 2039: {1890, 163, 1060, 2254}, 1594: {1136, 163}, 2116: {163, 1775}, 1775: {1728, 1793, 1795, 163}, 422: {163, 1069, 1685}, 2316: {2659, 163, 743, 744, 2030}, 266: {163, 262}, 658: {163, 427}, 1333: {578, 163, 1136, 793, 1467}, 1515: {961, 163, 988}, 523: {163, 188, 717}, 1467: {793, 163, 1333}, 1756: {168, 745, 2603, 2613, 1663}, 548: {1859, 1860, 552, 82, 565}, 374: {552, 455}, 451: {552, 2537, 2666}, 472: {552, 650, 566, 1006}, 2: {552, 410, 565, 471}, 305: {552, 82, 565, 410}, 239: {552, 1378, 565, 82}, 446: {552, 554}, 644: {552, 210, 565}, 220: {552, 490, 45, 295}, 311: {552, 406}, 653: {552, 2480}, 16: {552, 1697, 466, 566}, 173: {162, 294, 552, 565, 566}, 554: {552, 875, 565, 294}, 559: {552, 1378, 2341, 167}, 85: {552, 465, 565, 294}, 688: {552, 60, 207, 566, 412}, 621: {552, 738, 565}, 636: {1573, 294, 552, 764, 2687}, 738: {552, 565}, 749: {552, 1459, 2485, 483}, 57: {552, 1459, 565, 967}, 60: {552, 562, 565}, 210: {738, 295, 552, 1459, 63}, 671: {552, 562, 1459, 565, 566}, 167: {552, 554, 1459, 559}, 92: {552, 136, 1259, 565, 479}, 691: {552, 1649, 2658, 565}, 367: {552, 483, 565}, 466: {294, 552, 465, 565, 566}, 473: {552, 1459}, 483: {552, 294, 566}, 575: {738, 552, 843, 173, 565}, 298: {552, 410}, 601: {611, 3, 552, 1741, 565}, 370: {552, 562, 875}, 306: {967, 552, 554, 565, 57}, 277: {483, 294, 552, 523, 567}, 2078: {2406, 295, 2474, 1459, 1983}, 2221: {1459, 2404, 566, 1527}, 1687: {1649, 898, 523, 1459}, 775: {162, 875, 1459}, 1540: {1378, 1459, 565, 1118}, 633: {611, 294, 1459, 565, 566}, 45: {490, 1459, 492}, 1137: {1459, 558, 1687}, 566: {465, 1459, 565, 294}, 804: {1697, 1459}, 1859: {554, 565, 621, 471}, 34: {66, 35, 559, 565, 566}, 1816: {1934, 558, 559, 2485, 566}, 1791: {682, 1859, 1559}, 2430: {682, 294, 2647}, 383: {87, 638, 631}, 2245: {2243, 87, 2471}, 2097: {1994, 1995}, 1995: {1800, 1994}, 1800: {1994, 1995}, 1907: {1800, 1995, 2301}, 478: {523, 565, 479}, 870: {523, 1687}, 2442: {929, 2118, 71, 523, 565}, 1616: {882, 523, 565}, 882: {1378, 523, 565, 1529, 575}, 283: {523, 565, 747}, 1214: {523, 583}, 1931: {2624, 523}, 2408: {523, 1975}, 54: {615, 712, 523, 1687, 2334}, 2642: {2368, 523, 565}, 1676: {74, 523, 207, 565, 1146}, 508: {417, 523, 2326}, 1790: {523, 565, 22, 2334, 415}, 465: {523, 565, 294}, 565: {465, 523, 294}, 2480: {523, 565, 566}, 1921: {1408, 2402, 523, 1931}, 394: {608, 125, 133}, 2230: {608, 611, 157, 2095}, 680: {608, 74, 747, 90}, 975: {608, 74, 76, 1079, 668}, 179: {608, 611, 331}, 133: {608, 611}, 611: {608, 2378, 1935, 719, 1079}, 899: {1672, 611, 1564, 1565}, 2501: {611, 2260}, 1103: {611, 74, 1579, 427, 1079}, 2340: {611, 1935}, 2352: {611, 2055}, 718: {611, 271}, 864: {611, 1079}, 2505: {611, 2499, 359}, 1755: {2432, 611}, 1763: {611, 745, 2611, 407, 639}, 2050: {2378, 611}, 386: {745, 611, 747}, 716: {611, 157}, 1732: {611, 1644, 2423}, 511: {74, 611}, 2540: {160, 611, 157}, 2385: {611, 2676, 2499}, 1668: {1488, 2378, 611, 1910}, 28: {1606, 611, 1718}, 1352: {1672, 2674, 611}, 579: {696, 611, 747}, 1493: {611, 1732}, 1116: {1593, 611}, 1564: {1672, 745, 611, 687}, 368: {611, 747}, 1488: {1490, 611, 980, 427}, 938: {560, 611}, 1265: {611, 1725}, 1935: {611, 2499}, 1499: {611, 1306, 1050, 1310}, 1315: {668, 611, 2452, 1079}, 2325: {745, 2538, 611}, 1597: {1138, 1148, 1534, 486}, 1163: {1497, 1498, 1534}, 1556: {1537, 1534}, 1278: {1534, 289, 486, 583}, 1122: {578, 717, 1534}, 289: {1537, 612, 1534, 1535}, 1148: {1534, 285, 30, 486}, 1348: {1537, 1535}, 629: {1537, 1535}, 2568: {289, 612, 583, 717, 2544}, 1046: {1345, 2706, 612, 1121}, 403: {402, 2309, 2302}, 1353: {2302, 934, 2317, 294}, 2111: {2302, 2047}, 2104: {403, 2302}, 681: {712, 402, 2309}, 1325: {2170, 219, 2309, 1175}, 1956: {2649, 2527, 735}, 2100: {1961, 2527}, 1846: {2537, 1847, 1927}, 2198: {1560, 2537, 2695}, 1927: {2537, 1846, 1847}, 535: {1560, 761, 109, 672}, 791: {1560, 786, 1003, 1004}, 1161: {2657, 1702, 1560, 2423, 1592}, 456: {672, 549, 569, 1560, 25}, 2465: {2057, 228, 638}, 2146: {2182, 1897, 1003, 1004, 638}, 701: {1644, 638, 15}, 862: {1812, 2423, 986, 1019, 638}, 722: {635, 638}, 1808: {2210, 638}, 1189: {1643, 638, 1687}, 618: {638, 478}, 1460: {672, 638}, 1312: {418, 1428, 1269, 344, 638}, 1468: {848, 638, 847}, 1895: {638, 1893, 102}, 2171: {344, 2032, 638}, 227: {638, 142, 438, 378, 478}, 296: {227, 1236, 692, 438, 638}, 2541: {2673, 638, 686}, 1327: {1569, 638}, 264: {635, 638}, 779: {747, 1203, 638, 542}, 348: {344, 638}, 89: {638, 635, 430}, 1027: {322, 1322, 1336, 378, 638}, 521: {1259, 344, 1081, 1082, 638}, 2426: {344, 638, 1007}, 375: {635, 638}, 1919: {2426, 638, 1007}, 449: {638, 1539, 566, 479}, 83: {747, 294}, 600: {745, 747, 1052, 205}, 203: {747, 2499, 359}, 112: {624, 881, 747, 572}, 234: {747, 15}, 458: {745, 747}, 695: {747, 659, 1967}, 198: {747, 655}, 502: {294, 2536, 747, 565, 567}, 724: {747, 2612}, 328: {2619, 747}, 397: {2346, 747, 659}, 737: {2024, 747}, 665: {747, 427, 294}, 186: {294, 747, 588, 566}, 494: {747, 427}, 609: {747, 719}, 610: {747, 427}, 330: {576, 354, 359, 747, 731}, 229: {747, 477}, 700: {624, 625, 747, 879}, 526: {747, 427}, 527: {1656, 2662, 747, 1968}, 763: {747, 1879}, 146: {747, 427}, 104: {771, 747}, 771: {72, 745, 202, 747}, 572: {624, 881, 747}, 699: {294, 747, 588, 566}, 110: {745, 747, 687}, 202: {72, 747, 204, 427}, 204: {202, 747, 1645, 2410}, 72: {745, 771, 747}, 26: {747, 2612}, 685: {217, 747}, 18: {745, 747, 427}, 542: {779, 1203, 747}, 588: {294, 747, 699, 566}, 741: {747, 204}, 310: {74, 747, 668}, 124: {745, 122, 747, 127}, 127: {745, 18, 747, 122}, 153: {747, 294, 63}, 205: {745, 747, 427}, 696: {747, 427}, 561: {745, 747, 427}, 353: {747, 427}, 91: {745, 747}, 362: {747, 2191}, 2359: {2520, 66}, 1248: {688, 66, 835}, 2294: {2529, 66, 649, 2380, 1140}, 215: {66, 2380, 2061, 696, 478}, 2310: {66, 2583, 318, 2607}, 1531: {66, 887, 1154, 983}, 2441: {344, 2585, 66}, 2122: {66, 1864, 75, 1854, 1855}, 183: {66, 539, 541, 454}, 1698: {66, 2597, 430, 686, 1626}, 288: {1377, 66, 75, 2380, 2037}, 2157: {288, 66}, 1740: {1136, 1305}, 1063: {1136, 1153}, 1142: {1136, 1018}, 1018: {1136, 1139, 1142}, 1516: {1136, 833, 1218}, 117: {345, 155, 452, 513}, 2389: {345, 2421}, 2266: {345, 74, 2421}, 1924: {2040, 345, 2327}, 881: {1256, 74, 1203, 2423}, 900: {228, 1351, 1583, 1207, 1533}, 2425: {2341, 423, 2474, 788, 62}, 1212: {1497, 580, 1126}, 1125: {1496, 166}, 901: {1497, 1554, 1553}, 580: {1497, 581, 1126, 1215}, 222: {643, 581}, 49: {643, 581}, 50: {643, 581, 150, 583}, 818: {1125, 582, 817, 1212, 1215}, 259: {320, 228, 582}, 2398: {1498, 2372, 583}, 2158: {2449, 1498}, 1049: {1329, 1498, 1292}, 1071: {1329, 1498, 1292}, 2450: {2449, 1498}, 1483: {1498, 1123, 1125}, 1126: {2371, 2372, 583}, 64: {289, 284, 486, 583}, 284: {64, 486, 583}, 643: {1121, 150, 583}, 1235: {1512, 2508}, 1671: {1512, 344}, 949: {1512, 74, 1608}, 999: {70, 1512, 1463, 1240, 347}, 1463: {1512, 1053}, 1335: {1512, 1337, 1339, 1267}, 1267: {1512, 344}, 2577: {2180, 2342, 2508, 1020, 2367}, 2137: {2508, 430, 431}, 632: {2508, 430, 1582, 1909, 635}, 322: {160, 967, 2508, 1644, 15}, 2560: {1505, 865, 649, 251, 2555}, 6: {490, 251}, 932: {1505, 251, 865}, 1877: {578, 2215, 717, 251, 414}, 482: {578, 251, 13, 551}, 1505: {865, 251}, 1622: {1312, 1322, 1578, 1428, 344}, 2617: {228, 2091, 1428, 344, 1275}, 2609: {344, 2080, 2079}, 335: {344, 585, 426, 378}, 1881: {344, 2449, 2450}, 2339: {344, 675, 2126}, 2229: {344, 1428, 2663}, 963: {344, 1519}, 1442: {675, 1444, 1445, 344, 414}, 1445: {344, 675, 414}, 2589: {344, 2329}, 1627: {344, 1177, 479}, 1275: {585, 426, 1234, 344, 378}, 2290: {585, 2291, 344, 378, 1275}, 2291: {344, 585, 378, 1275}, 1260: {1312, 2090, 654, 1269, 344}, 1244: {344, 314, 348, 1061}, 675: {344, 2215}, 1181: {569, 25, 1431}, 550: {569, 438, 1422, 2423}, 1273: {686, 1422, 569, 826, 319}, 1092: {569, 1484}, 1712: {569, 1290, 453, 1025}, 255: {569, 421, 70, 767}, 1522: {569, 1484}, 1986: {2338, 70, 1329, 569, 2462}, 1430: {569, 1484, 1356}, 2099: {456, 569, 2098, 453}, 514: {569, 453}, 710: {569, 438, 847}, 1269: {1312, 569, 418, 453}, 1000: {569, 1185}, 1025: {453, 1290, 1006, 1238, 569}, 1521: {1185, 1484, 1237, 70}, 1057: {1484, 1359, 627, 631, 1307}, 1692: {1484, 228, 70}, 1220: {1608, 74, 1484, 1618}, 931: {70, 1000, 1484, 1463, 1053}, 822: {1484, 1644}, 1197: {1290, 1053, 1484, 2634}, 1500: {1484, 1267, 2180, 1238}, 1237: {1457, 826, 1484, 1422}, 212: {576, 169, 570}, 2026: {576, 570, 2552, 1922}, 1258: {576, 570, 1520}, 1381: {576, 570, 1524}, 1200: {576, 570, 1520}, 1243: {570, 1524, 1245}, 1246: {1520, 570}, 1524: {576, 570, 1520}, 1709: {1520, 570, 59, 839}, 1922: {576, 570}, 1814: {576, 834, 1801, 1067, 570}, 872: {1520, 570, 59}, 59: {576, 570, 1520, 839}, 874: {576, 839, 1520, 570, 59}, 1245: {576, 570, 1520, 1524}, 1067: {576, 570, 839}, 605: {576, 476, 676}, 863: {576, 808, 2499}, 293: {576, 387}, 1526: {576, 1520, 1524, 839}, 354: {576, 1520}, 1710: {576, 1656}, 570: {576, 1520, 59}, 1801: {576, 1067, 1821}, 834: {576, 1067, 1520, 1814, 1821}, 839: {576, 1072, 1520, 354}, 1014: {576, 676}, 1932: {576, 2024, 745, 18, 1912}, 355: {576, 745, 1562}, 2147: {2498, 2499, 2019}, 2567: {2498, 359}, 2500: {2498, 2499}, 2322: {2498, 2499, 2500, 359}, 2376: {2499, 2500, 2422, 359}, 1787: {2499, 2676, 359}, 1871: {2499, 359}, 2357: {1656, 2499}, 2422: {2499, 2500, 359}, 2498: {2322, 2499, 2500, 359}, 1418: {2499, 989, 726}, 1068: {2499, 2500, 359, 726, 989}, 480: {257, 2570, 667, 2270}, 1928: {2570, 667, 2124, 74}, 799: {2656, 1706, 1699, 1122}, 1194: {1122, 1621}, 594: {593, 1194, 1621, 1122}, 541: {1123, 539, 454}, 1188: {1354, 1123}, 1176: {1592, 1354, 1123, 1285}, 75: {1354, 1123}, 817: {818, 1212, 1125, 1367}, 857: {1483, 1125}, 2489: {2371, 643, 1126, 2372}, 923: {922, 1126}, 819: {1154, 817, 818, 1212, 1215}, 1552: {1553, 1554, 1212}, 1699: {2656, 1215}, 1209: {539, 1215}, 1707: {824, 825, 1215}, 824: {1621, 1215}, 713: {161, 347}, 998: {1240, 1241, 347, 84}, 452: {513, 347}, 84: {1240, 1241, 347}, 912: {1240, 1241}, 1201: {1240, 325}, 841: {1240, 2559}, 897: {1240, 157}, 987: {1240, 74, 1608}, 1086: {1240, 1527}, 742: {1240, 772, 645}, 1141: {1240, 1241, 1172}, 1241: {1240, 772, 2559}, 950: {1240, 1241, 84}, 1347: {1240, 1241, 84}, 1147: {161, 1347, 772, 1240, 1241}, 1240: {1241, 84, 772, 1527}, 1020: {1322, 430, 1328, 1329, 1337}, 2539: {1322, 1428}, 323: {2560, 2467, 228, 1322, 140}, 29: {426, 429, 382, 319}, 2369: {256, 426, 2607}, 1576: {426, 1595, 1266, 375}, 836: {426, 1595, 1266, 375}, 1187: {426, 1234}, 2126: {426, 2426}, 1486: {1008, 1569, 426}, 937: {426, 1234}, 303: {1174, 426, 2091, 478}, 1061: {585, 426, 378}, 318: {426, 1644}, 2431: {426, 2426, 1919}, 1423: {1328, 2583}, 1584: {2151, 1164, 1328, 2037, 1626}, 1101: {1328, 843, 1111}, 1031: {1328, 258}, 1513: {1328, 785, 1812, 634, 862}, 927: {1328, 1019}, 1412: {1328, 1431, 1111}, 1155: {1328, 1601, 1812, 862}, 1109: {1328, 1570}, 1111: {1328, 258, 1339, 1570}, 1568: {1328, 1053, 430}, 783: {1328, 1155, 1812, 1407}, 785: {1328, 1155, 1812, 862}, 1173: {1328, 318, 2583}, 1024: {2338, 74, 1053, 1335}, 1478: {1336, 2338, 1335}, 2677: {2017, 2338, 1582, 1909, 1339}, 2127: {1329, 2338}, 2443: {2338, 1339, 2343}, 1766: {1810, 427, 477}, 1914: {2176, 427}, 2255: {427, 2412}, 1811: {2410, 427, 204}, 2383: {427, 1820}, 2453: {2666, 427}, 1760: {427, 1879}, 2574: {2107, 427}, 2388: {427, 2575}, 1399: {427, 1463}, 1405: {427, 1463}, 587: {88, 427, 214, 2239}, 2258: {427, 1887}, 823: {1283, 427, 142, 227}, 1318: {1283, 427, 142, 227}, 1547: {1283, 427, 142, 227}, 1037: {427, 1028}, 2306: {2049, 427}, 312: {577, 427, 772, 645}, 1283: {427, 142}, 1284: {1283, 427, 430, 230}, 230: {1283, 427, 142}, 2658: {843, 427, 566, 2647, 477}, 399: {1250, 427, 15, 1251}, 406: {2307, 427, 230}, 1975: {427, 565, 2655}, 250: {577, 107, 76, 427}, 584: {250, 427, 639}, 304: {427, 108, 1461, 1912, 250}, 2448: {427, 2526}, 2683: {427, 668, 2044, 687}, 954: {969, 427}, 2437: {745, 427}, 916: {427, 157, 1038}, 1999: {427, 2003}, 946: {1600, 427}, 1577: {658, 427, 1580, 343}, 1340: {427, 157}, 391: {1283, 230, 427, 142, 438}, 2487: {136, 52, 430, 1823}, 1617: {1107, 236, 430}, 2593: {634, 430, 2342}, 1637: {1562, 157, 430}, 683: {1860, 430, 686, 632, 635}, 2033: {430, 2622}, 1165: {430, 1327}, 2197: {430, 2342}, 223: {430, 686}, 524: {520, 430}, 381: {635, 686, 430, 2342}, 651: {635, 430}, 2343: {208, 1337, 1618, 430}, 547: {74, 430}, 208: {88, 1569, 635, 430}, 686: {634, 635, 430, 318}, 172: {635, 245, 430}, 177: {965, 430, 911, 2192, 635}, 1270: {2342, 430, 208, 627, 1335}, 119: {228, 430, 686}, 52: {228, 430, 478}, 236: {430, 1270, 1271}, 497: {2342, 478}, 2151: {1109, 2037, 2342}, 944: {1337, 1335}, 1607: {848, 1335, 1053, 1223}, 1512: {1112, 1335}, 1223: {848, 1335, 1422, 1607}, 885: {177, 1335}, 1032: {967, 591, 848, 847, 1335}, 2300: {1337, 2636, 479}, 434: {2473, 1339, 597}, 1570: {1339, 1678}, 79: {438, 2207}, 96: {649, 438, 2061, 478}, 21: {1378, 438, 294, 567}, 607: {1306, 438}, 616: {438, 268, 613, 2046}, 692: {378, 438, 1191}, 1370: {136, 2692, 1102}, 2038: {985, 2692}, 2172: {136, 2692, 159}, 929: {1697, 740, 71, 1559}, 1559: {1697, 2658, 740, 71, 1703}, 1923: {1928, 2114, 2227, 71}, 661: {74, 76, 71}, 1641: {558, 1086, 71}, 906: {74, 1019, 1860}, 1017: {74, 157, 1574}, 1634: {74, 1252}, 604: {74, 668}, 2513: {2374, 74, 310, 1366, 2239}, 154: {392, 74}, 2654: {74, 2421}, 2284: {74, 2283, 1293, 2278}, 2335: {74, 2378}, 505: {74, 134}, 252: {294, 74, 688, 562, 60}, 345: {74, 76, 2421, 622}, 787: {1288, 74, 1287}, 2390: {74, 2227, 76, 36}, 263: {74, 668, 2421}, 377: {74, 622}, 19: {74, 571}, 867: {74, 667, 1493, 46}, 869: {76, 1374}, 1128: {1288, 1393, 76}, 694: {250, 76}, 1934: {423, 76, 558, 2463}, 181: {76, 157, 662, 766}, 2532: {2417, 76, 766, 518}, 2327: {2015, 1924, 76}, 518: {76, 181, 662, 766}, 1438: {565, 294, 807}, 103: {128, 294, 465, 565, 471}, 638: {2409, 294}, 1672: {696, 294}, 807: {1672, 294}, 2476: {2472, 294}, 477: {294, 745, 565, 566}, 564: {1841, 738, 565, 294}, 128: {565, 294}, 2475: {640, 2246, 591, 1363, 574}, 1944: {2091, 1259}, 2213: {1259, 2539, 2253, 2126, 2290}, 1081: {1082, 1259}, 893: {1290, 1259, 1174}, 2312: {521, 1082, 1259}, 136: {479, 986, 1539, 1259}, 2618: {2509, 1269, 453}, 876: {1025, 453, 1006, 1269}, 2155: {2467, 591}, 287: {640, 549, 591}, 1186: {492, 1309, 1268, 591}, 453: {1124, 549, 591}, 549: {640, 672, 591}, 1358: {453, 1661, 591, 1567}, 2182: {1897, 1003, 2011, 591}, 848: {591, 847, 967}, 199: {640, 591}, 488: {549, 109, 591}, 1105: {1026, 2356, 798, 591}, 2299: {75, 1854}, 1854: {1864, 75, 1855, 2299}, 1855: {1864, 75, 2299, 2075, 1854}, 2645: {2482, 1854}, 2482: {2123, 1854, 1855}, 155: {84, 12}, 1803: {99, 84}, 1550: {1419, 1427, 84, 1238, 157}, 1435: {1042, 1043, 1028}, 206: {1488, 1490, 1028}, 1042: {1043, 1028, 1037, 206}, 1610: {1028, 1037}, 1320: {808, 1300, 359}, 808: {731, 359}, 1342: {808, 1300, 2454, 359}, 1091: {1288, 476, 676, 1014}, 1056: {476, 676, 1014}, 300: {593, 1621}, 2695: {2696, 2057}, 790: {2057, 1234, 1510}, 1510: {2057, 1234, 790}, 1296: {456, 109, 1198}, 2455: {456, 453}, 2653: {456, 1363, 672}, 1748: {456, 2182}, 2413: {2373, 1366, 1718, 668, 2239}, 2414: {2373, 1366, 1718, 668, 2239}, 2562: {668, 1366}, 118: {465, 636, 565}, 2027: {636, 2409, 465, 565}, 2672: {636, 2409, 465, 565}, 2368: {465, 85}, 1440: {1378, 565, 82}, 82: {1378, 565, 39}, 970: {1378, 1533, 974, 567}, 1045: {1540, 471}, 1632: {471, 68, 1687}, 295: {82, 565, 471}, 463: {3, 483, 333, 564, 477}, 2409: {1644, 477, 1869, 2543}, 427: {2024, 201, 477, 350}, 333: {477, 565, 566}, 933: {875, 483}, 650: {656, 483, 1006}, 1401: {2605, 1554, 562, 565, 951}, 2018: {562, 1828, 2014}, 440: {562, 414, 367}, 1889: {2648, 730, 2586, 562}, 1117: {2089, 910, 562, 247, 1114}, 1183: {1193, 1202, 738, 562}, 1782: {134, 295, 565, 566, 2328}, 1585: {738, 804, 565}, 425: {565, 85}, 1012: {882, 565, 295}, 1376: {738, 1546, 306, 565, 1727}, 1316: {882, 565, 295}, 773: {656, 1643, 565, 1687}, 385: {565, 567}, 842: {1540, 565}, 1957: {2536, 565}, 1213: {1649, 738, 565}, 1741: {1841, 738, 565}, 1546: {565, 63}, 492: {565, 490, 2419, 1741}, 410: {82, 565}, 1727: {738, 565}, 244: {136, 565, 750}, 1378: {575, 565, 173, 2485}, 2485: {173, 2480, 565, 566, 575}, 2628: {738, 565}, 2629: {2536, 738, 2628, 565}, 1373: {565, 1718}, 1447: {1515, 1581, 566}, 858: {1084, 2556, 566, 2303}, 1643: {1649, 649, 566}, 274: {566, 204, 478}, 321: {2305, 1954, 2179, 1192, 566}, 1828: {2018, 2014, 567}, 903: {1672, 1491}, 2333: {318, 2583, 102, 2135}, 1809: {2585, 2071}, 1830: {2585, 2466}, 1263: {2585, 1234}, 116: {88, 2428, 686}, 225: {88, 2428, 686}, 760: {88, 2428, 686}, 44: {88, 2428, 686}, 430: {256, 1569, 686, 634, 635}, 1407: {1626, 1155, 1812, 862}, 1735: {1626, 1002, 983}, 1164: {1626, 1570}, 445: {690, 318}, 2457: {690, 1980}, 868: {1649, 1643}, 1228: {1688, 1649, 1693, 1687}, 148: {460, 656, 1649, 210, 1687}, 1557: {1649, 650, 691, 1687}, 1480: {924, 1532}, 1010: {1569, 1578, 1061}, 1651: {1010, 1061, 1578, 877}, 1074: {109, 1174}, 1135: {1442, 1444, 1174}, 1102: {136, 985, 1174}, 902: {1174, 964, 910}, 991: {1368, 858, 1084, 318}, 2427: {2394, 318}, 1323: {1062, 1195, 1145, 1149, 1150}, 1630: {1195, 1203}, 1119: {881, 1203}, 816: {881, 1203}, 1411: {881, 1203}, 211: {2076, 325, 645, 1527}, 810: {1718, 1287}, 945: {1414, 1287, 327, 788, 62}, 976: {1393, 1268, 1287}, 1414: {1287, 1288, 1393, 788, 1268}, 1437: {1287, 1393, 787, 1593, 891}, 1397: {1393, 905, 1287}, 499: {1288, 250, 1287}, 905: {976, 1393, 558, 1287}, 1502: {1288, 577, 772, 1393}, 107: {1288, 250}, 108: {1288, 250, 107}, 10: {192, 193, 420, 519, 280}, 2590: {420, 388, 2421, 519}, 519: {192, 193, 420, 2448, 2549}, 439: {192, 420}, 2608: {2549, 519}, 190: {356, 519, 2223}, 705: {192, 1910, 519}, 510: {192, 356, 519, 2223}, 2223: {2214, 519}, 2549: {193, 519}, 280: {192, 193, 2549, 519}, 74: {193, 2421}, 156: {521, 314, 236, 431}, 1686: {2426, 2282, 837}, 2493: {2426, 1428}, 1888: {2427, 109, 507}, 2548: {2427, 2391, 507}, 2591: {2427, 2253, 2126, 2431}, 2391: {2427, 507}, 507: {549, 109, 2391, 2427, 798}, 2424: {1428, 2589}, 1208: {1434, 1429}, 2225: {1608, 256, 1732, 2423}, 2162: {745, 2586, 2599}, 1647: {2586, 1908, 2615}, 1093: {355, 2085, 2228, 122, 1659}, 2400: {124, 122, 1916, 127}, 197: {633, 3, 749, 295}, 2183: {2305, 1954, 2179, 2150}, 1192: {2473, 875, 2150}, 1604: {1272, 1281, 1451}, 948: {1281, 1282, 1446}, 1471: {1281, 1282, 1633, 693, 447}, 1272: {1281, 1282, 1451, 1346}, 1451: {1281, 1346}, 1281: {1282, 1346}, 2308: {240, 2652}, 853: {1019, 1732, 1053, 2423}, 960: {378, 1292, 1301}, 2012: {378, 102, 2135}, 2195: {378, 1963, 286}, 2042: {378, 2428}, 1238: {378, 1290, 1550}, 218: {378, 1290, 478}, 178: {378, 286}, 585: {378, 2091}, 1644: {378, 1290, 2423}, 1290: {378, 1019}, 1963: {378, 286}, 396: {378, 478}, 2583: {378, 102, 2135}, 1450: {808, 731, 1710}, 576: {43, 1710, 731}, 1778: {2024, 731}, 249: {2649, 86}, 2106: {2649, 735}, 216: {196, 735}, 195: {196, 735}, 1606: {409, 826}, 1422: {826, 828}, 2580: {136, 1819, 52, 1823}, 224: {1601, 1819, 597}, 2399: {136, 2626, 2398}, 1479: {136, 2017, 1102, 2543}, 2543: {136, 159}, 1776: {136, 1595, 2543}, 1413: {136, 1102}, 159: {136, 985}, 2231: {136, 2635}, 1588: {1114, 1117, 2334}, 1261: {2089, 1114, 1117, 247}, 1603: {2089, 1114, 1117}, 2257: {1767, 1768, 2444, 22, 247}, 500: {247, 563, 2444, 1767}, 2446: {129, 2451, 247}, 1973: {129, 2451, 247}, 357: {803, 2089, 243, 537, 1563}, 1965: {712, 2089, 757}, 2332: {1117, 2334}, 2685: {2053, 2054}, 591: {640, 549, 2054, 672}, 2053: {2690, 2054, 488, 109, 2685}, 2159: {2081, 2661, 1910}, 2366: {2081, 1910}, 2545: {1794, 2082, 1795, 2178, 1773}, 2178: {1793, 2082, 1795, 1794, 2181}, 908: {2304, 809, 414, 919}, 924: {672, 1539, 1460, 1309}, 442: {635, 1908}, 474: {698, 635}, 634: {256, 1570, 635}, 635: {640, 660, 109}, 459: {640, 109}, 1363: {640, 2653, 672}, 1897: {640, 25, 1003, 732}, 20: {640, 267, 334, 25, 732}, 574: {640, 549, 672}, 130: {640, 459, 672}, 2054: {640, 109}, 2581: {649, 2403}, 457: {649, 166}, 2484: {2616, 986, 1644, 2614}, 1642: {1377, 986, 1475}, 2616: {986, 1644, 2614}, 400: {258, 555, 14, 2354, 2360}, 544: {0, 258, 14, 751}, 268: {616, 258, 613, 14}, 0: {258, 14}, 8: {0, 258, 14, 751}, 751: {258, 14}, 435: {0, 258, 8, 14, 751}, 520: {230, 478, 142, 719}, 568: {843, 478}, 418: {740, 1198, 479}, 1591: {954, 1050, 871}, 184: {432, 433, 428, 158}, 598: {428, 302, 432, 307, 158}, 433: {432, 428, 158}, 707: {195, 196, 158}, 1226: {1234, 1290}, 13: {1234, 578}, 861: {1236, 1678, 1431}, 2168: {1363, 2350}, 1364: {1363, 1004, 1343, 1003}, 1456: {1343, 308, 1469, 1095}, 556: {2360, 2354, 555, 615}, 1705: {459, 2356, 1469, 1364}, 2156: {2098, 453}, 9: {453, 761, 308, 481}, 2059: {2651, 453}, 2354: {2360, 555, 453, 2455}, 1566: {1124, 453, 1567}, 1661: {453, 109, 1358, 791}, 1449: {1025, 453}, 2034: {577, 453, 558, 2327, 2015}, 1095: {1360, 792, 798}, 2004: {2360, 2040, 672}, 1386: {2360, 1715, 672}, 1006: {2360, 574}, 2040: {2360, 574, 1006}, 267: {459, 732, 20, 334}, 1343: {672, 1363, 1004, 1003}, 786: {1363, 1004, 1003}, 1469: {1004, 1003, 1364, 1661}, 615: {555, 614}, 2072: {667, 36, 2227}, 1974: {667, 36, 2227}, 1293: {1592, 1600, 1606, 672}, 405: {1592, 1285}, 1298: {1592, 409, 1285, 407}, 2329: {672, 1290}, 2360: {672, 2040, 574}, 1003: {672, 1004}, 1004: {672, 1003}, 2381: {2667, 1606, 687}, 1673: {964, 1573, 965, 966, 910}, 1131: {910, 1253, 966}, 1432: {1573, 964, 965, 910}, 1140: {964, 965, 966, 910, 981}, 1311: {1253, 1140, 764, 910}, 964: {910, 902}, 764: {636, 1573, 910, 2687}, 811: {1140, 1253, 910}, 981: {910, 965, 902}, 1481: {964, 1573, 965, 966, 910}, 420: {192, 193, 2214}, 238: {193, 489, 46, 1815}, 373: {652, 86}, 1979: {1844, 1845, 68}, 1845: {1844, 68}, 979: {68, 879, 1845, 157, 766}, 2007: {1779, 68, 1780}, 1825: {68, 1966, 978, 979, 121}, 2393: {496, 663}, 501: {496, 503, 663}, 1896: {496, 2393}, 1813: {2393, 1818, 503}, 1952: {2393, 503}, 2029: {1952, 2393, 1818, 503}, 1851: {1896, 503}, 24: {269, 39}, 943: {1646, 793, 728, 1016, 1466}, 1670: {1016, 1218}, 1466: {1016, 728}, 1090: {1029, 1023}, 2456: {48, 41, 1029}, 1108: {1504, 1506, 1507, 1503}, 1276: {1504, 1506, 1525, 1503}, 1504: {1506, 1308, 1503}, 1507: {1504, 1506, 1308, 1503}, 1525: {1504, 1506, 1503}, 1308: {1504, 1506, 1503}, 1415: {1504, 1506, 1525}, 2074: {2607, 1636, 2615, 1647}, 1073: {1569, 1636, 1112, 314, 1182}, 928: {256, 1636, 983}, 983: {256, 1636, 887}, 1684: {1569, 1636, 1647, 314, 1182}, 821: {1389, 1421, 607}, 672: {2040, 109}, 884: {1658, 109}, 778: {25, 109}, 171: {114, 109}, 955: {560, 109}, 1665: {539, 883}, 1210: {539, 883, 1453}, 51: {772, 645, 2559}, 2076: {2559, 772, 1324, 1527}, 1747: {419, 1324, 1326, 1527, 2559}, 260: {2559, 419, 1324, 1527}, 2166: {772, 260, 1527}, 2167: {2088, 772, 2166}, 770: {730, 1285, 350}, 2594: {2306, 411}, 753: {409, 444, 405}, 512: {411, 407}, 2439: {1875, 407}, 1875: {2553, 1891, 407}, 2582: {2178, 462, 1718, 409, 411}, 1891: {200, 58, 411, 206}, 1470: {1464, 411, 1037}, 536: {200, 58, 411, 206}, 1733: {904, 41}, 889: {904, 41, 2153, 1713, 892}, 94: {41, 126}, 1982: {2153, 1713, 1716, 41}, 1713: {41, 2002, 1716}, 892: {904, 41, 138, 1713, 1716}, 352: {904, 41, 1372}, 23: {41, 759}, 115: {904, 41, 138, 2153, 1713}, 1783: {48, 2348}, 2682: {48, 2348}, 5: {490, 2164}, 734: {490, 492}, 213: {490, 492, 734}, 2323: {2419, 492, 788, 1268}, 2321: {2186, 2187}, 531: {2187, 693}, 135: {418, 654}, 2492: {577, 2461}, 1076: {1086, 2461, 1078}, 1078: {1076, 2461, 1086}, 401: {632, 577, 442, 2462}, 423: {558, 1290, 557, 62}, 2660: {577, 558}, 376: {704, 641, 558}, 557: {558, 423}, 1532: {558, 577, 782, 327}, 1555: {558, 1550}, 1739: {848, 641}, 484: {641, 642, 1574}, 657: {656, 1581}, 1711: {1515, 1581}, 2277: {2624, 1931, 2252, 2269, 1277}, 2402: {1921, 1931}, 1408: {1249, 1931}, 517: {1170, 164, 2509, 1198}, 1683: {1697, 740, 1198, 1559}, 1157: {1700, 741, 1704, 745, 687}, 687: {745, 741}, 2165: {745, 129, 538}, 1700: {745, 687}, 1704: {745, 687}, 36: {745, 1933, 257, 1591}, 1886: {745, 1884, 1885}, 2336: {745, 2579}, 78: {560, 157, 214, 766}, 1138: {1344, 1121, 2706}, 1121: {1344, 540}, 1047: {1344, 1345, 1354, 1348}, 1344: {1345, 1046, 1047}, 1640: {1345, 2706, 540, 1121}, 612: {1348, 629}, 313: {1377, 801, 1642, 1354, 534}, 1420: {1354, 1138, 1047}, 806: {1121, 1354, 1138}, 911: {585, 1354}, 1123: {1680, 1354, 1674}, 1680: {1354, 1674}, 1553: {82, 1554}, 1824: {1595, 102, 2675, 2006}, 1893: {1908, 102}, 1964: {102, 1862, 1644, 2675, 1595}, 2028: {1003, 308, 2653}, 2483: {1848, 1003, 1004}, 2217: {2282, 1007}, 2514: {485, 2380, 1838, 1908, 2558}, 2380: {2561, 485, 1838}, 1838: {719, 1908, 485, 2380}, 2561: {485, 2380, 719, 1908, 1527}, 2478: {2226, 2220}, 2139: {2226, 995, 2220}, 1958: {2226, 2220}, 1409: {129, 2220}, 1940: {2220, 2175, 860}, 1950: {2226, 2220}, 231: {285, 486, 30}, 285: {486, 30}, 2276: {486, 30}, 986: {2616, 1644, 2614}, 1862: {1595, 1964, 2675, 1644}, 1863: {1644, 1595, 1964, 1862}, 2364: {1644, 1812, 862}, 1256: {2675, 1644, 1019}, 1601: {1644, 2423}, 2355: {217, 763, 1879}, 2062: {2066, 2060, 2052, 2061}, 2035: {858, 1084, 2303}, 467: {2016, 495, 464, 342, 729}, 2000: {2115, 467}, 270: {495, 339, 467, 342, 573}, 276: {2115, 1993, 270, 467, 342}, 495: {342, 339, 467}, 729: {467, 342}, 1375: {560, 1452, 157}, 2287: {560, 2315}, 597: {675, 204}, 2410: {2049, 204}, 2650: {204, 2215}, 2080: {2329, 1290}, 1918: {2329, 1290}, 1191: {1290, 661, 2423}, 1389: {1050, 2707}, 774: {331, 2707, 1493}, 1901: {1800, 2301}, 1075: {1340, 157}, 393: {157, 14}, 2680: {2596, 101, 157}, 1289: {1452, 157}, 2021: {2428, 157}, 1718: {157, 1079}, 560: {1452, 740, 157, 246}, 2095: {2056, 157}, 371: {88, 157}, 2417: {2532, 157}, 2681: {160, 1857, 78}, 469: {160, 78, 214}, 1857: {160, 2540, 125, 2020}, 1055: {1059, 1284, 607}, 1169: {1284, 524}, 1009: {1079, 934, 2175}, 926: {1112, 1019}, 2415: {1112, 2530}, 101: {1112, 331}, 134: {1118, 2135}, 2633: {2011, 2182}, 1835: {2011, 2182}, 1433: {1026, 798, 1095}, 11: {624, 659}, 1562: {627, 631}, 1679: {1569, 627}, 2674: {2543, 159}, 1599: {1697, 671}, 1703: {1697, 740}, 1664: {1697, 740, 804, 1559}, 1667: {1697, 740}, 1860: {2658, 2428, 2430}, 2174: {636, 764, 1573, 2687}, 2233: {636, 703}, 995: {415, 2673, 1690, 1178}, 98: {2696, 1954, 615}, 978: {664, 1593, 1954}, 2150: {2305, 1954, 2179, 2222}, 2015: {2092, 2327}, 1567: {1124, 1566}, 139: {2361, 188}, 389: {577, 185, 324, 332}, 180: {350, 143}, 730: {350, 351}, 201: {350, 1990, 1718}, 1288: {1593, 1461}, 2319: {2131, 227}, 1657: {1595, 1307, 1462, 1422}, 232: {268, 613, 14}, 746: {299, 14}, 242: {1006, 574, 14}, 708: {250, 1116, 14}, 31: {250, 1116, 14}, 736: {250, 1116, 14}, 613: {2496, 2107, 14}, 1582: {2017, 1908, 1909, 1086}, 1545: {1582, 1909, 1086}, 2416: {2506, 1910, 2375}, 2506: {1910, 2661, 2366}, 2242: {2661, 1910}, 2375: {2661, 1910}, 2575: {2506, 1910, 2366}, 1251: {1250, 399}, 525: {522, 1419, 684, 1425, 1427}, 684: {161, 1425, 779, 1550}, 2571: {2496, 613, 268, 1878, 2107}, 254: {1176, 15}, 253: {15, 1815}, 538: {129, 2165, 15}, 80: {2044, 15}, 1306: {1050, 1310}, 1492: {1050, 1724}, 1962: {1827, 2509, 574}, 1788: {2178, 1795, 2181}, 1793: {2178, 2181}, 1795: {1793, 2178, 1794, 2181}, 2347: {2265, 2254, 1303}, 1190: {809, 1321, 1129}, 1229: {809, 1615}, 1639: {809, 1615}, 297: {809, 441, 1321}, 1222: {809, 1321, 1129}, 1044: {809, 812, 814, 1365, 1341}, 812: {809, 1365, 1341, 814}, 814: {1080, 809, 1321, 1129}, 919: {809, 1129}, 1080: {809, 812, 1321, 1129}, 1365: {809, 812, 814}, 2066: {985, 2674, 159}, 2220: {129, 1409}, 2193: {129, 538, 2165}, 1204: {2624, 129, 1153, 537, 538}, 1772: {2307, 2351}, 603: {1640, 540, 670, 1047}, 324: {62, 332, 326, 327}, 2702: {2624, 881}, 2203: {881, 1411, 2397, 2405}, 2405: {881, 2397, 1119}, 2454: {2129, 1058, 881}, 2309: {1969, 538}, 1593: {1116, 1461}, 170: {250, 4}, 498: {250, 107, 108}, 33: {250, 1701, 1591}, 1268: {2419, 788}, 623: {1172, 2341}, 788: {1268, 2341}, 1757: {1268, 2341, 2221}, 1842: {2341, 1268, 1757}, 789: {1409, 1206, 415}, 2664: {712, 189, 2102, 415}, 2103: {2334, 415}, 1737: {1178, 380, 415}, 1832: {1831, 1836, 55, 2334, 415}, 1836: {1832, 415, 2334, 1831}, 209: {2334, 415}, 2353: {2334, 415}, 712: {219, 415}, 93: {725, 759}, 95: {161, 759, 358, 1255}, 99: {2578, 759, 2679}, 1355: {1640, 1148, 1648, 1047}, 1726: {1569, 1679, 1684, 314, 1182}, 1011: {1008, 1569, 1010}, 877: {1569, 1010, 1578}, 1182: {1569, 1010, 1578}, 1701: {832, 33, 36, 1591}, 832: {1052, 1701}, 2420: {2169, 1527}, 1543: {1324, 1527}, 2524: {260, 1894, 2088, 2166, 1527}, 2525: {2088, 1527}, 827: {913, 909, 1894, 1527}, 909: {913, 1894, 1527}, 971: {1326, 2166, 1527}, 419: {260, 1326, 398, 1527, 248}, 1754: {1867, 1869}, 2458: {2506, 2242, 2366}, 2502: {2605, 717}, 951: {1554, 2605}, 1738: {1689, 538, 219}, 38: {1689, 219}, 2218: {2049, 2021, 2337}, 1837: {2132, 382}, 2102: {144, 189}, 1388: {848, 1025, 1586, 1027}, 1400: {848, 967}, 847: {848, 967}, 487: {538, 291}, 1036: {712, 538, 1662}, 2346: {1595, 1301}, 151: {1528, 1595, 360}, 1199: {1702, 1550}, 831: {2466, 2243, 885}, 53: {803, 1563, 243, 1439}, 890: {18, 866}, 898: {107, 108}, 1937: {389, 1064, 107, 623, 2293}, 1062: {930, 1254}, 2144: {1894, 1326}, 590: {364, 365, 366}, 2572: {2112, 365}, 450: {2558, 954, 646}, 854: {1178, 2211, 2598, 1231}, 956: {1178, 995, 2598, 1231}, 1476: {1178, 1277}, 1736: {1737, 1178}, 1690: {1736, 1178, 995}, 1230: {995, 2598, 2470, 1231, 1178}, 1231: {1178, 995, 1495}, 1232: {995, 2211, 2598, 1231, 1178}, 1745: {2641, 2175}, 1719: {1257, 2261, 2175, 1911}, 2250: {1696, 1940, 2175}, 2087: {1257, 2261, 2175, 1911}, 2206: {2379, 2261, 2133, 1911, 2175}, 120: {1184, 188, 2175}, 2199: {1184, 1784, 2133}, 844: {1184, 934, 974, 55, 1784}, 860: {1184, 934, 1911}, 965: {964, 902}, 1581: {1515, 764}, 1917: {1796, 2334}, 2324: {1161, 2326}, 1955: {2194, 2149, 2190}, 2188: {2189, 2194, 2149, 2190}, 652: {249, 86}, 2479: {2128, 2319}, 2030: {2128, 2130, 2131}, 921: {1265, 1725}, 815: {880, 1523}, 1959: {120, 177, 2259}, 1180: {1177, 1620}, 2281: {2429, 1134}, 2624: {2429, 2317}, 1143: {2429, 1404, 2165}, 1428: {1601, 2329}, 1429: {1601, 675}, 2473: {551, 2551}, 1549: {1602, 339}, 1602: {339, 1326, 971}, 1945: {729, 1993, 342}, 2546: {1993, 276, 342}, 1993: {729, 276, 342}, 776: {2088, 398, 1326, 342, 2166}, 56: {778, 1027, 967}, 2271: {2249, 2250, 2252, 2259, 2238}, 2083: {1827, 299, 2511}, 2320: {1853, 2511}, 2141: {2418, 2511}, 2418: {2141, 2511}, 2345: {2418, 2005, 2511}, 1487: {1528, 1532}, 1826: {689, 2630}, 1580: {1625, 689}, 1066: {36, 1052}, 1827: {299, 574}, 852: {1654, 1015}, 855: {1338, 1654}, 1338: {784, 917, 1654, 1015}, 1475: {801, 1642}, 731: {1656, 121}, 2279: {2579, 1806}, 837: {1489, 1007}, 2161: {1326, 2167}, 2434: {1747, 2069, 1326}, 248: {2088, 1326, 398}, 105: {248, 2434, 1326}, 1904: {643, 2556}, 464: {275, 573, 279}, 251: {1505, 865}, 2686: {916, 101, 2596}, 1938: {1770, 1764, 1765}, 679: {32, 30}, 1501: {447, 693, 1471}, 1596: {314, 1244, 348}, 2535: {2176, 1668}, 1898: {1268, 2108}, 2140: {1898, 623, 1268, 2292, 2108}, 1133: {1666, 719}, 1112: {1618, 719}, 87: {2021, 719}, 1215: {824, 825}, 1771: {1696, 1775}, 2082: {1793, 1795, 1775}, 388: {2590, 439}, 1158: {1555, 1550}, 1425: {684, 1550}, 959: {793, 1130}, 1694: {793, 1455}, 1330: {793, 1167}, 163: {793, 188, 189}, 1455: {793, 1153, 1069, 1685}, 1249: {1728, 793, 1474, 1473}, 262: {793, 1980, 188}, 2060: {857, 1483}, 1051: {2576, 1052}, 1514: {1333, 429}, 820: {1146, 1252}, 2091: {1539, 2438}, 342: {1800, 2016}, 2670: {489, 253, 1815}, 301: {489, 238}, 1815: {489, 253}, 46: {489, 301, 238}, 1631: {934, 2598, 2407, 2604, 2330}, 1149: {912, 1891, 2044}, 1769: {1696, 2330}, 2330: {1696, 2604, 2598, 1631}, 1933: {2660, 748}, 138: {416, 137, 748, 1713, 1716}, 1774: {1968, 1891, 2619, 990}, 1150: {912, 1149, 1630}, 278: {1898, 324, 326}, 1985: {2065, 995, 1231}, 326: {324, 327}, 185: {324, 389, 332}, 1899: {2292, 327}, 1064: {952, 1937, 332, 623}, 973: {993, 802}, 2606: {2337, 2662, 527, 2611, 2459}, 553: {123, 2459}, 626: {261, 687}, 2289: {1804, 1805}, 748: {1926, 137, 138, 1933, 1941}, 416: {137, 138, 2510, 1713, 1913}, 1416: {922, 923}, 1233: {922, 923}, 265: {100, 758}, 758: {100, 265, 1980}, 1822: {1656, 2662}, 1266: {1301, 1663}, 996: {161, 1015}, 2311: {2448, 161, 2285}, 1474: {1728, 1473, 380}, 1264: {1738, 1134}, 2703: {2200, 1409, 1221}, 1797: {2200, 1409, 1221}, 176: {379, 756}, 1508: {1724, 668}, 1858: {2528, 2481}, 1628: {1063, 1175}, 1852: {2592, 99}, 2578: {99, 2679}, 1613: {784, 99, 1619, 1614}, 1619: {1722, 99, 1723}, 1722: {99, 1723}, 1255: {894, 99, 358}, 2542: {2236, 2262}, 2435: {2236, 2262}, 337: {346, 363, 77, 143}, 529: {341, 340, 709}, 341: {340, 709}, 709: {340, 341}, 2093: {283, 1990}, 2445: {2275, 2268}, 2275: {1120, 2268}, 2110: {1827, 299}, 990: {58, 805, 1774}, 2495: {58, 571}, 1454: {1169, 524}, 372: {273, 38}, 915: {1172, 1294}, 637: {515, 623}, 2160: {2434, 1747}, 302: {432, 428}, 428: {432, 302}, 2046: {2107, 268}, 1383: {152, 1384}, 2138: {2285, 1718}, 1072: {1520, 354}, 829: {962, 1252, 820}, 1485: {1252, 1159}, 1115: {1252, 820}, 755: {2058, 188, 1404, 757}, 1458: {1714, 838}, 317: {417, 508, 2326}, 363: {346, 143}, 1730: {1034, 1533}, 920: {1520, 873}, 1211: {1520, 1058}, 233: {595, 1990}, 2190: {2194, 2189}, 2365: {2668, 2189, 2190}, 2189: {2194, 2190}, 2149: {2194, 2190}, 2504: {1536, 2667, 1663}, 65: {744, 743}, 1357: {1461, 1511}, 780: {1139, 1598}, 1598: {1139, 1572}, 914: {272, 1551}, 2114: {1928, 2124}, 2272: {1176, 1866}, 1313: {1005, 1382}, 2704: {209, 2407}, 1035: {1216, 1218, 714}, 1299: {1216, 1218, 714, 1392}, 1424: {1218, 1530}, 833: {1218, 714, 1796}, 1614: {1619, 1613}, 2086: {2065, 1231}, 2688: {2216, 1784, 1840, 1839}, 2705: {2216, 1784, 1840, 1839}, 2643: {2470, 2646}, 2646: {2643, 2470}, 1104: {1260, 1262}, 1925: {2392, 2133, 2206}, 1984: {2379, 2206}, 2700: {2697, 2698}, 2288: {2697, 1759}, 1759: {2697, 2698}, 2373: {916, 2374}, 2374: {916, 2373}, 896: {888, 468, 1372}, 41: {888, 1713, 1716}, 2377: {2025, 491, 468}, 1977: {2153, 2219, 2510, 2318}, 1992: {2153, 1713}, 1716: {2153, 1713}, 2002: {2153, 1713, 2510}, 2008: {2153, 2219, 2022}, 2022: {2008, 2153, 2219}, 2701: {2247, 2263}, 1870: {1713, 2362}, 1978: {1713, 2510}, 1991: {1713, 1716, 2510}, 2510: {1713, 2362}, 2208: {1761, 2234, 2235, 126}, 182: {491, 468}, 491: {468, 2318}, 2421: {1906, 115}, 2001: {833, 1796}, 282: {592, 589}, 1144: {592, 1509}, 589: {592, 1509}, 2515: {2003, 1999}, 2507: {371, 2373, 2374, 2239}, 1366: {1373, 2373, 2374}, 2595: {2562, 1315, 2452}, 2064: {1777, 1802}, 272: {914, 1551}, 2331: {571, 2694}, 1762: {571, 2495}, 2694: {1762, 571, 2495}, 571: {1762, 19, 2495}, 1094: {1659, 355, 2228, 2085}, 2068: {355, 2228}, 1252: {1477, 1485}, 745: {1884, 1885, 1886}, 1902: {1884, 1885, 1886}}, 'features': matrix([[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), 'labels': array([[0, 0, 0, ..., 0, 0, 0],
       [0, 0, 1, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 1],
       ...,
       [1, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 1, 0, 0],
       [0, 0, 0, ..., 0, 0, 0]], dtype=int32), 'n': 2708, 'feature': array([[[ 0.        ,  0.        ,  0.        , ..., -0.55357766,
         -0.49470094,  0.25302992]],

       [[ 0.        ,  0.        ,  0.        , ..., -0.14879404,
         -0.05142467,  0.2210307 ]],

       [[ 0.        ,  0.        ,  0.        , ...,  0.21510649,
          0.21510649,  0.        ]],

       ...,

       [[ 0.        ,  0.        ,  0.        , ..., -0.14535022,
         -0.14535022,  0.        ]],

       [[ 0.        ,  0.        ,  0.        , ..., -0.46566513,
         -0.44798303,  0.0920952 ]],

       [[ 0.        ,  0.        ,  0.        , ..., -0.65939647,
         -0.65939647,  0.08350319]]])})
Namespace(add_encodings=True, encodings='LCP', data='cocitation', dataset='cora', model_name='UniGCN', first_aggregate='mean', second_aggregate='sum', add_self_loop=True, use_norm=True, activation='relu', nlayer=2, nhid=8, nhead=8, dropout=0.6, input_drop=0.6, attn_drop=0.6, lr=0.01, wd=0.0005, epochs=200, n_runs=10, gpu=0, seed=1, patience=200, nostdout=False, split=1, out_dir='runs/test', dataset_dict={'hypergraph': {402: {538, 163, 219}, 1696: {1114, 163}, 2295: {881, 427, 2667, 163}, 1274: {163, 427, 343, 1305, 1303}, 1286: {1136, 163}, 1544: {566, 163, 1206}, 2600: {1409, 523, 2220, 163}, 2363: {129, 163, 1969, 2165, 1719}, 1905: {1834, 163, 55}, 1611: {1153, 163, 100, 1380, 1379}, 141: {1184, 163, 188}, 1807: {2490, 163}, 1110: {1728, 1729, 163, 1016, 793}, 174: {2409, 163, 501, 503}, 2521: {2529, 145, 163, 2164}, 1792: {582, 2067, 163, 430}, 1675: {1153, 163}, 1334: {1696, 793, 1130, 163}, 813: {163, 1523}, 1799: {1424, 163, 2205, 727}, 1943: {1696, 163, 1936}, 2077: {744, 163, 2030, 743}, 769: {163, 764}, 781: {1184, 163, 1533, 846}, 940: {163, 934}, 942: {1153, 163}, 1590: {163, 965, 910}, 1734: {1249, 163, 1715, 1592, 793}, 1872: {1696, 1257, 163, 1771}, 2286: {2280, 145, 163}, 390: {145, 163}, 1717: {1441, 163, 1380, 941}, 1030: {1136, 1946, 163, 1796}, 2274: {163, 2237}, 2518: {2298, 163}, 606: {537, 163}, 800: {1136, 163}, 1575: {1136, 1305, 163}, 1070: {1016, 793, 1153, 163}, 935: {1136, 163}, 1205: {1016, 793, 1153, 163}, 1571: {1216, 1218, 163, 343, 2202}, 1971: {1696, 163, 2211, 995}, 1127: {1153, 163, 938, 728, 793}, 530: {163, 343}, 856: {1016, 793, 1153, 163}, 2604: {168, 163}, 910: {565, 523, 163, 902}, 2173: {636, 163, 2204, 846}, 191: {163, 294}, 1253: {163, 965, 910}, 1729: {1016, 163}, 1136: {163, 1523}, 1457: {163, 1462, 569, 826, 828}, 1225: {163, 1117, 1333}, 2563: {163, 2220, 1583, 219, 2396}, 1689: {538, 163, 219}, 1498: {1534, 289, 163, 582}, 563: {163, 37, 38, 247, 219}, 2396: {1952, 503, 163, 1583}, 717: {163, 188}, 1890: {163, 1060, 2039}, 188: {129, 163, 42, 144, 189}, 982: {163, 910, 247}, 55: {1784, 163}, 346: {163, 132, 143}, 2673: {1224, 163, 638}, 1380: {1113, 163, 793}, 1207: {378, 163, 1333, 430}, 2451: {129, 163}, 961: {523, 163}, 42: {793, 163, 188, 189}, 624: {163, 659}, 1573: {163, 910}, 2232: {2058, 163, 411}, 145: {163, 1646}, 2659: {163, 2030}, 743: {744, 2659, 163, 2030}, 744: {163, 2030}, 1139: {793, 163}, 1572: {1139, 163}, 1303: {2202, 163, 343}, 1305: {1016, 1106, 163, 728}, 129: {1728, 163, 15}, 415: {523, 188, 163}, 2204: {163, 636, 2173, 846}, 1224: {145, 163, 1333}, 219: {163, 846}, 727: {1136, 1016, 163}, 448: {163, 191}, 2557: {163, 430, 686, 1947}, 2200: {1969, 163}, 22: {129, 163, 191}, 1218: {145, 163}, 2202: {163, 1980}, 1069: {793, 163, 1685}, 2175: {1696, 2361, 163, 1257}, 966: {163, 964, 965, 902, 910}, 1219: {1153, 163}, 757: {163, 2058, 846, 219, 188}, 1685: {163, 1069}, 1691: {793, 163, 422}, 1410: {1153, 163}, 380: {163, 188}, 1536: {1728, 163, 207}, 1650: {1466, 163}, 1558: {1136, 163}, 2564: {163, 1558, 1159}, 1362: {1184, 1178, 163, 2175}, 1784: {1184, 163, 380}, 1065: {1153, 163, 1063, 1106, 1305}, 290: {163, 2220, 1911, 1839}, 1277: {2152, 163, 1533, 2470}, 395: {163, 189, 191}, 1098: {1305, 163, 343, 1911}, 1404: {163, 219}, 1099: {1178, 163}, 2196: {938, 163, 380, 2267}, 2554: {1850, 163}, 1715: {793, 163}, 2248: {145, 163}, 2251: {145, 163}, 2280: {145, 163}, 2296: {2248, 145, 163, 2251}, 1113: {1153, 163}, 714: {2089, 1218, 163}, 2638: {2202, 163}, 602: {1114, 163, 2334}, 1530: {2089, 145, 163, 1218}, 1257: {1153, 163, 380}, 2259: {1257, 163, 2261, 2175}, 689: {163, 967}, 1060: {163, 965, 237, 1136, 1523}, 2039: {1890, 163, 1060, 2254}, 1594: {1136, 163}, 2116: {163, 1775}, 1775: {1728, 1793, 1795, 163}, 422: {163, 1069, 1685}, 2316: {2659, 163, 743, 744, 2030}, 266: {163, 262}, 658: {163, 427}, 1333: {578, 163, 1136, 793, 1467}, 1515: {961, 163, 988}, 523: {163, 188, 717}, 1467: {793, 163, 1333}, 1756: {168, 745, 2603, 2613, 1663}, 548: {1859, 1860, 552, 82, 565}, 374: {552, 455}, 451: {552, 2537, 2666}, 472: {552, 650, 566, 1006}, 2: {552, 410, 565, 471}, 305: {552, 82, 565, 410}, 239: {552, 1378, 565, 82}, 446: {552, 554}, 644: {552, 210, 565}, 220: {552, 490, 45, 295}, 311: {552, 406}, 653: {552, 2480}, 16: {552, 1697, 466, 566}, 173: {162, 294, 552, 565, 566}, 554: {552, 875, 565, 294}, 559: {552, 1378, 2341, 167}, 85: {552, 465, 565, 294}, 688: {552, 60, 207, 566, 412}, 621: {552, 738, 565}, 636: {1573, 294, 552, 764, 2687}, 738: {552, 565}, 749: {552, 1459, 2485, 483}, 57: {552, 1459, 565, 967}, 60: {552, 562, 565}, 210: {738, 295, 552, 1459, 63}, 671: {552, 562, 1459, 565, 566}, 167: {552, 554, 1459, 559}, 92: {552, 136, 1259, 565, 479}, 691: {552, 1649, 2658, 565}, 367: {552, 483, 565}, 466: {294, 552, 465, 565, 566}, 473: {552, 1459}, 483: {552, 294, 566}, 575: {738, 552, 843, 173, 565}, 298: {552, 410}, 601: {611, 3, 552, 1741, 565}, 370: {552, 562, 875}, 306: {967, 552, 554, 565, 57}, 277: {483, 294, 552, 523, 567}, 2078: {2406, 295, 2474, 1459, 1983}, 2221: {1459, 2404, 566, 1527}, 1687: {1649, 898, 523, 1459}, 775: {162, 875, 1459}, 1540: {1378, 1459, 565, 1118}, 633: {611, 294, 1459, 565, 566}, 45: {490, 1459, 492}, 1137: {1459, 558, 1687}, 566: {465, 1459, 565, 294}, 804: {1697, 1459}, 1859: {554, 565, 621, 471}, 34: {66, 35, 559, 565, 566}, 1816: {1934, 558, 559, 2485, 566}, 1791: {682, 1859, 1559}, 2430: {682, 294, 2647}, 383: {87, 638, 631}, 2245: {2243, 87, 2471}, 2097: {1994, 1995}, 1995: {1800, 1994}, 1800: {1994, 1995}, 1907: {1800, 1995, 2301}, 478: {523, 565, 479}, 870: {523, 1687}, 2442: {929, 2118, 71, 523, 565}, 1616: {882, 523, 565}, 882: {1378, 523, 565, 1529, 575}, 283: {523, 565, 747}, 1214: {523, 583}, 1931: {2624, 523}, 2408: {523, 1975}, 54: {615, 712, 523, 1687, 2334}, 2642: {2368, 523, 565}, 1676: {74, 523, 207, 565, 1146}, 508: {417, 523, 2326}, 1790: {523, 565, 22, 2334, 415}, 465: {523, 565, 294}, 565: {465, 523, 294}, 2480: {523, 565, 566}, 1921: {1408, 2402, 523, 1931}, 394: {608, 125, 133}, 2230: {608, 611, 157, 2095}, 680: {608, 74, 747, 90}, 975: {608, 74, 76, 1079, 668}, 179: {608, 611, 331}, 133: {608, 611}, 611: {608, 2378, 1935, 719, 1079}, 899: {1672, 611, 1564, 1565}, 2501: {611, 2260}, 1103: {611, 74, 1579, 427, 1079}, 2340: {611, 1935}, 2352: {611, 2055}, 718: {611, 271}, 864: {611, 1079}, 2505: {611, 2499, 359}, 1755: {2432, 611}, 1763: {611, 745, 2611, 407, 639}, 2050: {2378, 611}, 386: {745, 611, 747}, 716: {611, 157}, 1732: {611, 1644, 2423}, 511: {74, 611}, 2540: {160, 611, 157}, 2385: {611, 2676, 2499}, 1668: {1488, 2378, 611, 1910}, 28: {1606, 611, 1718}, 1352: {1672, 2674, 611}, 579: {696, 611, 747}, 1493: {611, 1732}, 1116: {1593, 611}, 1564: {1672, 745, 611, 687}, 368: {611, 747}, 1488: {1490, 611, 980, 427}, 938: {560, 611}, 1265: {611, 1725}, 1935: {611, 2499}, 1499: {611, 1306, 1050, 1310}, 1315: {668, 611, 2452, 1079}, 2325: {745, 2538, 611}, 1597: {1138, 1148, 1534, 486}, 1163: {1497, 1498, 1534}, 1556: {1537, 1534}, 1278: {1534, 289, 486, 583}, 1122: {578, 717, 1534}, 289: {1537, 612, 1534, 1535}, 1148: {1534, 285, 30, 486}, 1348: {1537, 1535}, 629: {1537, 1535}, 2568: {289, 612, 583, 717, 2544}, 1046: {1345, 2706, 612, 1121}, 403: {402, 2309, 2302}, 1353: {2302, 934, 2317, 294}, 2111: {2302, 2047}, 2104: {403, 2302}, 681: {712, 402, 2309}, 1325: {2170, 219, 2309, 1175}, 1956: {2649, 2527, 735}, 2100: {1961, 2527}, 1846: {2537, 1847, 1927}, 2198: {1560, 2537, 2695}, 1927: {2537, 1846, 1847}, 535: {1560, 761, 109, 672}, 791: {1560, 786, 1003, 1004}, 1161: {2657, 1702, 1560, 2423, 1592}, 456: {672, 549, 569, 1560, 25}, 2465: {2057, 228, 638}, 2146: {2182, 1897, 1003, 1004, 638}, 701: {1644, 638, 15}, 862: {1812, 2423, 986, 1019, 638}, 722: {635, 638}, 1808: {2210, 638}, 1189: {1643, 638, 1687}, 618: {638, 478}, 1460: {672, 638}, 1312: {418, 1428, 1269, 344, 638}, 1468: {848, 638, 847}, 1895: {638, 1893, 102}, 2171: {344, 2032, 638}, 227: {638, 142, 438, 378, 478}, 296: {227, 1236, 692, 438, 638}, 2541: {2673, 638, 686}, 1327: {1569, 638}, 264: {635, 638}, 779: {747, 1203, 638, 542}, 348: {344, 638}, 89: {638, 635, 430}, 1027: {322, 1322, 1336, 378, 638}, 521: {1259, 344, 1081, 1082, 638}, 2426: {344, 638, 1007}, 375: {635, 638}, 1919: {2426, 638, 1007}, 449: {638, 1539, 566, 479}, 83: {747, 294}, 600: {745, 747, 1052, 205}, 203: {747, 2499, 359}, 112: {624, 881, 747, 572}, 234: {747, 15}, 458: {745, 747}, 695: {747, 659, 1967}, 198: {747, 655}, 502: {294, 2536, 747, 565, 567}, 724: {747, 2612}, 328: {2619, 747}, 397: {2346, 747, 659}, 737: {2024, 747}, 665: {747, 427, 294}, 186: {294, 747, 588, 566}, 494: {747, 427}, 609: {747, 719}, 610: {747, 427}, 330: {576, 354, 359, 747, 731}, 229: {747, 477}, 700: {624, 625, 747, 879}, 526: {747, 427}, 527: {1656, 2662, 747, 1968}, 763: {747, 1879}, 146: {747, 427}, 104: {771, 747}, 771: {72, 745, 202, 747}, 572: {624, 881, 747}, 699: {294, 747, 588, 566}, 110: {745, 747, 687}, 202: {72, 747, 204, 427}, 204: {202, 747, 1645, 2410}, 72: {745, 771, 747}, 26: {747, 2612}, 685: {217, 747}, 18: {745, 747, 427}, 542: {779, 1203, 747}, 588: {294, 747, 699, 566}, 741: {747, 204}, 310: {74, 747, 668}, 124: {745, 122, 747, 127}, 127: {745, 18, 747, 122}, 153: {747, 294, 63}, 205: {745, 747, 427}, 696: {747, 427}, 561: {745, 747, 427}, 353: {747, 427}, 91: {745, 747}, 362: {747, 2191}, 2359: {2520, 66}, 1248: {688, 66, 835}, 2294: {2529, 66, 649, 2380, 1140}, 215: {66, 2380, 2061, 696, 478}, 2310: {66, 2583, 318, 2607}, 1531: {66, 887, 1154, 983}, 2441: {344, 2585, 66}, 2122: {66, 1864, 75, 1854, 1855}, 183: {66, 539, 541, 454}, 1698: {66, 2597, 430, 686, 1626}, 288: {1377, 66, 75, 2380, 2037}, 2157: {288, 66}, 1740: {1136, 1305}, 1063: {1136, 1153}, 1142: {1136, 1018}, 1018: {1136, 1139, 1142}, 1516: {1136, 833, 1218}, 117: {345, 155, 452, 513}, 2389: {345, 2421}, 2266: {345, 74, 2421}, 1924: {2040, 345, 2327}, 881: {1256, 74, 1203, 2423}, 900: {228, 1351, 1583, 1207, 1533}, 2425: {2341, 423, 2474, 788, 62}, 1212: {1497, 580, 1126}, 1125: {1496, 166}, 901: {1497, 1554, 1553}, 580: {1497, 581, 1126, 1215}, 222: {643, 581}, 49: {643, 581}, 50: {643, 581, 150, 583}, 818: {1125, 582, 817, 1212, 1215}, 259: {320, 228, 582}, 2398: {1498, 2372, 583}, 2158: {2449, 1498}, 1049: {1329, 1498, 1292}, 1071: {1329, 1498, 1292}, 2450: {2449, 1498}, 1483: {1498, 1123, 1125}, 1126: {2371, 2372, 583}, 64: {289, 284, 486, 583}, 284: {64, 486, 583}, 643: {1121, 150, 583}, 1235: {1512, 2508}, 1671: {1512, 344}, 949: {1512, 74, 1608}, 999: {70, 1512, 1463, 1240, 347}, 1463: {1512, 1053}, 1335: {1512, 1337, 1339, 1267}, 1267: {1512, 344}, 2577: {2180, 2342, 2508, 1020, 2367}, 2137: {2508, 430, 431}, 632: {2508, 430, 1582, 1909, 635}, 322: {160, 967, 2508, 1644, 15}, 2560: {1505, 865, 649, 251, 2555}, 6: {490, 251}, 932: {1505, 251, 865}, 1877: {578, 2215, 717, 251, 414}, 482: {578, 251, 13, 551}, 1505: {865, 251}, 1622: {1312, 1322, 1578, 1428, 344}, 2617: {228, 2091, 1428, 344, 1275}, 2609: {344, 2080, 2079}, 335: {344, 585, 426, 378}, 1881: {344, 2449, 2450}, 2339: {344, 675, 2126}, 2229: {344, 1428, 2663}, 963: {344, 1519}, 1442: {675, 1444, 1445, 344, 414}, 1445: {344, 675, 414}, 2589: {344, 2329}, 1627: {344, 1177, 479}, 1275: {585, 426, 1234, 344, 378}, 2290: {585, 2291, 344, 378, 1275}, 2291: {344, 585, 378, 1275}, 1260: {1312, 2090, 654, 1269, 344}, 1244: {344, 314, 348, 1061}, 675: {344, 2215}, 1181: {569, 25, 1431}, 550: {569, 438, 1422, 2423}, 1273: {686, 1422, 569, 826, 319}, 1092: {569, 1484}, 1712: {569, 1290, 453, 1025}, 255: {569, 421, 70, 767}, 1522: {569, 1484}, 1986: {2338, 70, 1329, 569, 2462}, 1430: {569, 1484, 1356}, 2099: {456, 569, 2098, 453}, 514: {569, 453}, 710: {569, 438, 847}, 1269: {1312, 569, 418, 453}, 1000: {569, 1185}, 1025: {453, 1290, 1006, 1238, 569}, 1521: {1185, 1484, 1237, 70}, 1057: {1484, 1359, 627, 631, 1307}, 1692: {1484, 228, 70}, 1220: {1608, 74, 1484, 1618}, 931: {70, 1000, 1484, 1463, 1053}, 822: {1484, 1644}, 1197: {1290, 1053, 1484, 2634}, 1500: {1484, 1267, 2180, 1238}, 1237: {1457, 826, 1484, 1422}, 212: {576, 169, 570}, 2026: {576, 570, 2552, 1922}, 1258: {576, 570, 1520}, 1381: {576, 570, 1524}, 1200: {576, 570, 1520}, 1243: {570, 1524, 1245}, 1246: {1520, 570}, 1524: {576, 570, 1520}, 1709: {1520, 570, 59, 839}, 1922: {576, 570}, 1814: {576, 834, 1801, 1067, 570}, 872: {1520, 570, 59}, 59: {576, 570, 1520, 839}, 874: {576, 839, 1520, 570, 59}, 1245: {576, 570, 1520, 1524}, 1067: {576, 570, 839}, 605: {576, 476, 676}, 863: {576, 808, 2499}, 293: {576, 387}, 1526: {576, 1520, 1524, 839}, 354: {576, 1520}, 1710: {576, 1656}, 570: {576, 1520, 59}, 1801: {576, 1067, 1821}, 834: {576, 1067, 1520, 1814, 1821}, 839: {576, 1072, 1520, 354}, 1014: {576, 676}, 1932: {576, 2024, 745, 18, 1912}, 355: {576, 745, 1562}, 2147: {2498, 2499, 2019}, 2567: {2498, 359}, 2500: {2498, 2499}, 2322: {2498, 2499, 2500, 359}, 2376: {2499, 2500, 2422, 359}, 1787: {2499, 2676, 359}, 1871: {2499, 359}, 2357: {1656, 2499}, 2422: {2499, 2500, 359}, 2498: {2322, 2499, 2500, 359}, 1418: {2499, 989, 726}, 1068: {2499, 2500, 359, 726, 989}, 480: {257, 2570, 667, 2270}, 1928: {2570, 667, 2124, 74}, 799: {2656, 1706, 1699, 1122}, 1194: {1122, 1621}, 594: {593, 1194, 1621, 1122}, 541: {1123, 539, 454}, 1188: {1354, 1123}, 1176: {1592, 1354, 1123, 1285}, 75: {1354, 1123}, 817: {818, 1212, 1125, 1367}, 857: {1483, 1125}, 2489: {2371, 643, 1126, 2372}, 923: {922, 1126}, 819: {1154, 817, 818, 1212, 1215}, 1552: {1553, 1554, 1212}, 1699: {2656, 1215}, 1209: {539, 1215}, 1707: {824, 825, 1215}, 824: {1621, 1215}, 713: {161, 347}, 998: {1240, 1241, 347, 84}, 452: {513, 347}, 84: {1240, 1241, 347}, 912: {1240, 1241}, 1201: {1240, 325}, 841: {1240, 2559}, 897: {1240, 157}, 987: {1240, 74, 1608}, 1086: {1240, 1527}, 742: {1240, 772, 645}, 1141: {1240, 1241, 1172}, 1241: {1240, 772, 2559}, 950: {1240, 1241, 84}, 1347: {1240, 1241, 84}, 1147: {161, 1347, 772, 1240, 1241}, 1240: {1241, 84, 772, 1527}, 1020: {1322, 430, 1328, 1329, 1337}, 2539: {1322, 1428}, 323: {2560, 2467, 228, 1322, 140}, 29: {426, 429, 382, 319}, 2369: {256, 426, 2607}, 1576: {426, 1595, 1266, 375}, 836: {426, 1595, 1266, 375}, 1187: {426, 1234}, 2126: {426, 2426}, 1486: {1008, 1569, 426}, 937: {426, 1234}, 303: {1174, 426, 2091, 478}, 1061: {585, 426, 378}, 318: {426, 1644}, 2431: {426, 2426, 1919}, 1423: {1328, 2583}, 1584: {2151, 1164, 1328, 2037, 1626}, 1101: {1328, 843, 1111}, 1031: {1328, 258}, 1513: {1328, 785, 1812, 634, 862}, 927: {1328, 1019}, 1412: {1328, 1431, 1111}, 1155: {1328, 1601, 1812, 862}, 1109: {1328, 1570}, 1111: {1328, 258, 1339, 1570}, 1568: {1328, 1053, 430}, 783: {1328, 1155, 1812, 1407}, 785: {1328, 1155, 1812, 862}, 1173: {1328, 318, 2583}, 1024: {2338, 74, 1053, 1335}, 1478: {1336, 2338, 1335}, 2677: {2017, 2338, 1582, 1909, 1339}, 2127: {1329, 2338}, 2443: {2338, 1339, 2343}, 1766: {1810, 427, 477}, 1914: {2176, 427}, 2255: {427, 2412}, 1811: {2410, 427, 204}, 2383: {427, 1820}, 2453: {2666, 427}, 1760: {427, 1879}, 2574: {2107, 427}, 2388: {427, 2575}, 1399: {427, 1463}, 1405: {427, 1463}, 587: {88, 427, 214, 2239}, 2258: {427, 1887}, 823: {1283, 427, 142, 227}, 1318: {1283, 427, 142, 227}, 1547: {1283, 427, 142, 227}, 1037: {427, 1028}, 2306: {2049, 427}, 312: {577, 427, 772, 645}, 1283: {427, 142}, 1284: {1283, 427, 430, 230}, 230: {1283, 427, 142}, 2658: {843, 427, 566, 2647, 477}, 399: {1250, 427, 15, 1251}, 406: {2307, 427, 230}, 1975: {427, 565, 2655}, 250: {577, 107, 76, 427}, 584: {250, 427, 639}, 304: {427, 108, 1461, 1912, 250}, 2448: {427, 2526}, 2683: {427, 668, 2044, 687}, 954: {969, 427}, 2437: {745, 427}, 916: {427, 157, 1038}, 1999: {427, 2003}, 946: {1600, 427}, 1577: {658, 427, 1580, 343}, 1340: {427, 157}, 391: {1283, 230, 427, 142, 438}, 2487: {136, 52, 430, 1823}, 1617: {1107, 236, 430}, 2593: {634, 430, 2342}, 1637: {1562, 157, 430}, 683: {1860, 430, 686, 632, 635}, 2033: {430, 2622}, 1165: {430, 1327}, 2197: {430, 2342}, 223: {430, 686}, 524: {520, 430}, 381: {635, 686, 430, 2342}, 651: {635, 430}, 2343: {208, 1337, 1618, 430}, 547: {74, 430}, 208: {88, 1569, 635, 430}, 686: {634, 635, 430, 318}, 172: {635, 245, 430}, 177: {965, 430, 911, 2192, 635}, 1270: {2342, 430, 208, 627, 1335}, 119: {228, 430, 686}, 52: {228, 430, 478}, 236: {430, 1270, 1271}, 497: {2342, 478}, 2151: {1109, 2037, 2342}, 944: {1337, 1335}, 1607: {848, 1335, 1053, 1223}, 1512: {1112, 1335}, 1223: {848, 1335, 1422, 1607}, 885: {177, 1335}, 1032: {967, 591, 848, 847, 1335}, 2300: {1337, 2636, 479}, 434: {2473, 1339, 597}, 1570: {1339, 1678}, 79: {438, 2207}, 96: {649, 438, 2061, 478}, 21: {1378, 438, 294, 567}, 607: {1306, 438}, 616: {438, 268, 613, 2046}, 692: {378, 438, 1191}, 1370: {136, 2692, 1102}, 2038: {985, 2692}, 2172: {136, 2692, 159}, 929: {1697, 740, 71, 1559}, 1559: {1697, 2658, 740, 71, 1703}, 1923: {1928, 2114, 2227, 71}, 661: {74, 76, 71}, 1641: {558, 1086, 71}, 906: {74, 1019, 1860}, 1017: {74, 157, 1574}, 1634: {74, 1252}, 604: {74, 668}, 2513: {2374, 74, 310, 1366, 2239}, 154: {392, 74}, 2654: {74, 2421}, 2284: {74, 2283, 1293, 2278}, 2335: {74, 2378}, 505: {74, 134}, 252: {294, 74, 688, 562, 60}, 345: {74, 76, 2421, 622}, 787: {1288, 74, 1287}, 2390: {74, 2227, 76, 36}, 263: {74, 668, 2421}, 377: {74, 622}, 19: {74, 571}, 867: {74, 667, 1493, 46}, 869: {76, 1374}, 1128: {1288, 1393, 76}, 694: {250, 76}, 1934: {423, 76, 558, 2463}, 181: {76, 157, 662, 766}, 2532: {2417, 76, 766, 518}, 2327: {2015, 1924, 76}, 518: {76, 181, 662, 766}, 1438: {565, 294, 807}, 103: {128, 294, 465, 565, 471}, 638: {2409, 294}, 1672: {696, 294}, 807: {1672, 294}, 2476: {2472, 294}, 477: {294, 745, 565, 566}, 564: {1841, 738, 565, 294}, 128: {565, 294}, 2475: {640, 2246, 591, 1363, 574}, 1944: {2091, 1259}, 2213: {1259, 2539, 2253, 2126, 2290}, 1081: {1082, 1259}, 893: {1290, 1259, 1174}, 2312: {521, 1082, 1259}, 136: {479, 986, 1539, 1259}, 2618: {2509, 1269, 453}, 876: {1025, 453, 1006, 1269}, 2155: {2467, 591}, 287: {640, 549, 591}, 1186: {492, 1309, 1268, 591}, 453: {1124, 549, 591}, 549: {640, 672, 591}, 1358: {453, 1661, 591, 1567}, 2182: {1897, 1003, 2011, 591}, 848: {591, 847, 967}, 199: {640, 591}, 488: {549, 109, 591}, 1105: {1026, 2356, 798, 591}, 2299: {75, 1854}, 1854: {1864, 75, 1855, 2299}, 1855: {1864, 75, 2299, 2075, 1854}, 2645: {2482, 1854}, 2482: {2123, 1854, 1855}, 155: {84, 12}, 1803: {99, 84}, 1550: {1419, 1427, 84, 1238, 157}, 1435: {1042, 1043, 1028}, 206: {1488, 1490, 1028}, 1042: {1043, 1028, 1037, 206}, 1610: {1028, 1037}, 1320: {808, 1300, 359}, 808: {731, 359}, 1342: {808, 1300, 2454, 359}, 1091: {1288, 476, 676, 1014}, 1056: {476, 676, 1014}, 300: {593, 1621}, 2695: {2696, 2057}, 790: {2057, 1234, 1510}, 1510: {2057, 1234, 790}, 1296: {456, 109, 1198}, 2455: {456, 453}, 2653: {456, 1363, 672}, 1748: {456, 2182}, 2413: {2373, 1366, 1718, 668, 2239}, 2414: {2373, 1366, 1718, 668, 2239}, 2562: {668, 1366}, 118: {465, 636, 565}, 2027: {636, 2409, 465, 565}, 2672: {636, 2409, 465, 565}, 2368: {465, 85}, 1440: {1378, 565, 82}, 82: {1378, 565, 39}, 970: {1378, 1533, 974, 567}, 1045: {1540, 471}, 1632: {471, 68, 1687}, 295: {82, 565, 471}, 463: {3, 483, 333, 564, 477}, 2409: {1644, 477, 1869, 2543}, 427: {2024, 201, 477, 350}, 333: {477, 565, 566}, 933: {875, 483}, 650: {656, 483, 1006}, 1401: {2605, 1554, 562, 565, 951}, 2018: {562, 1828, 2014}, 440: {562, 414, 367}, 1889: {2648, 730, 2586, 562}, 1117: {2089, 910, 562, 247, 1114}, 1183: {1193, 1202, 738, 562}, 1782: {134, 295, 565, 566, 2328}, 1585: {738, 804, 565}, 425: {565, 85}, 1012: {882, 565, 295}, 1376: {738, 1546, 306, 565, 1727}, 1316: {882, 565, 295}, 773: {656, 1643, 565, 1687}, 385: {565, 567}, 842: {1540, 565}, 1957: {2536, 565}, 1213: {1649, 738, 565}, 1741: {1841, 738, 565}, 1546: {565, 63}, 492: {565, 490, 2419, 1741}, 410: {82, 565}, 1727: {738, 565}, 244: {136, 565, 750}, 1378: {575, 565, 173, 2485}, 2485: {173, 2480, 565, 566, 575}, 2628: {738, 565}, 2629: {2536, 738, 2628, 565}, 1373: {565, 1718}, 1447: {1515, 1581, 566}, 858: {1084, 2556, 566, 2303}, 1643: {1649, 649, 566}, 274: {566, 204, 478}, 321: {2305, 1954, 2179, 1192, 566}, 1828: {2018, 2014, 567}, 903: {1672, 1491}, 2333: {318, 2583, 102, 2135}, 1809: {2585, 2071}, 1830: {2585, 2466}, 1263: {2585, 1234}, 116: {88, 2428, 686}, 225: {88, 2428, 686}, 760: {88, 2428, 686}, 44: {88, 2428, 686}, 430: {256, 1569, 686, 634, 635}, 1407: {1626, 1155, 1812, 862}, 1735: {1626, 1002, 983}, 1164: {1626, 1570}, 445: {690, 318}, 2457: {690, 1980}, 868: {1649, 1643}, 1228: {1688, 1649, 1693, 1687}, 148: {460, 656, 1649, 210, 1687}, 1557: {1649, 650, 691, 1687}, 1480: {924, 1532}, 1010: {1569, 1578, 1061}, 1651: {1010, 1061, 1578, 877}, 1074: {109, 1174}, 1135: {1442, 1444, 1174}, 1102: {136, 985, 1174}, 902: {1174, 964, 910}, 991: {1368, 858, 1084, 318}, 2427: {2394, 318}, 1323: {1062, 1195, 1145, 1149, 1150}, 1630: {1195, 1203}, 1119: {881, 1203}, 816: {881, 1203}, 1411: {881, 1203}, 211: {2076, 325, 645, 1527}, 810: {1718, 1287}, 945: {1414, 1287, 327, 788, 62}, 976: {1393, 1268, 1287}, 1414: {1287, 1288, 1393, 788, 1268}, 1437: {1287, 1393, 787, 1593, 891}, 1397: {1393, 905, 1287}, 499: {1288, 250, 1287}, 905: {976, 1393, 558, 1287}, 1502: {1288, 577, 772, 1393}, 107: {1288, 250}, 108: {1288, 250, 107}, 10: {192, 193, 420, 519, 280}, 2590: {420, 388, 2421, 519}, 519: {192, 193, 420, 2448, 2549}, 439: {192, 420}, 2608: {2549, 519}, 190: {356, 519, 2223}, 705: {192, 1910, 519}, 510: {192, 356, 519, 2223}, 2223: {2214, 519}, 2549: {193, 519}, 280: {192, 193, 2549, 519}, 74: {193, 2421}, 156: {521, 314, 236, 431}, 1686: {2426, 2282, 837}, 2493: {2426, 1428}, 1888: {2427, 109, 507}, 2548: {2427, 2391, 507}, 2591: {2427, 2253, 2126, 2431}, 2391: {2427, 507}, 507: {549, 109, 2391, 2427, 798}, 2424: {1428, 2589}, 1208: {1434, 1429}, 2225: {1608, 256, 1732, 2423}, 2162: {745, 2586, 2599}, 1647: {2586, 1908, 2615}, 1093: {355, 2085, 2228, 122, 1659}, 2400: {124, 122, 1916, 127}, 197: {633, 3, 749, 295}, 2183: {2305, 1954, 2179, 2150}, 1192: {2473, 875, 2150}, 1604: {1272, 1281, 1451}, 948: {1281, 1282, 1446}, 1471: {1281, 1282, 1633, 693, 447}, 1272: {1281, 1282, 1451, 1346}, 1451: {1281, 1346}, 1281: {1282, 1346}, 2308: {240, 2652}, 853: {1019, 1732, 1053, 2423}, 960: {378, 1292, 1301}, 2012: {378, 102, 2135}, 2195: {378, 1963, 286}, 2042: {378, 2428}, 1238: {378, 1290, 1550}, 218: {378, 1290, 478}, 178: {378, 286}, 585: {378, 2091}, 1644: {378, 1290, 2423}, 1290: {378, 1019}, 1963: {378, 286}, 396: {378, 478}, 2583: {378, 102, 2135}, 1450: {808, 731, 1710}, 576: {43, 1710, 731}, 1778: {2024, 731}, 249: {2649, 86}, 2106: {2649, 735}, 216: {196, 735}, 195: {196, 735}, 1606: {409, 826}, 1422: {826, 828}, 2580: {136, 1819, 52, 1823}, 224: {1601, 1819, 597}, 2399: {136, 2626, 2398}, 1479: {136, 2017, 1102, 2543}, 2543: {136, 159}, 1776: {136, 1595, 2543}, 1413: {136, 1102}, 159: {136, 985}, 2231: {136, 2635}, 1588: {1114, 1117, 2334}, 1261: {2089, 1114, 1117, 247}, 1603: {2089, 1114, 1117}, 2257: {1767, 1768, 2444, 22, 247}, 500: {247, 563, 2444, 1767}, 2446: {129, 2451, 247}, 1973: {129, 2451, 247}, 357: {803, 2089, 243, 537, 1563}, 1965: {712, 2089, 757}, 2332: {1117, 2334}, 2685: {2053, 2054}, 591: {640, 549, 2054, 672}, 2053: {2690, 2054, 488, 109, 2685}, 2159: {2081, 2661, 1910}, 2366: {2081, 1910}, 2545: {1794, 2082, 1795, 2178, 1773}, 2178: {1793, 2082, 1795, 1794, 2181}, 908: {2304, 809, 414, 919}, 924: {672, 1539, 1460, 1309}, 442: {635, 1908}, 474: {698, 635}, 634: {256, 1570, 635}, 635: {640, 660, 109}, 459: {640, 109}, 1363: {640, 2653, 672}, 1897: {640, 25, 1003, 732}, 20: {640, 267, 334, 25, 732}, 574: {640, 549, 672}, 130: {640, 459, 672}, 2054: {640, 109}, 2581: {649, 2403}, 457: {649, 166}, 2484: {2616, 986, 1644, 2614}, 1642: {1377, 986, 1475}, 2616: {986, 1644, 2614}, 400: {258, 555, 14, 2354, 2360}, 544: {0, 258, 14, 751}, 268: {616, 258, 613, 14}, 0: {258, 14}, 8: {0, 258, 14, 751}, 751: {258, 14}, 435: {0, 258, 8, 14, 751}, 520: {230, 478, 142, 719}, 568: {843, 478}, 418: {740, 1198, 479}, 1591: {954, 1050, 871}, 184: {432, 433, 428, 158}, 598: {428, 302, 432, 307, 158}, 433: {432, 428, 158}, 707: {195, 196, 158}, 1226: {1234, 1290}, 13: {1234, 578}, 861: {1236, 1678, 1431}, 2168: {1363, 2350}, 1364: {1363, 1004, 1343, 1003}, 1456: {1343, 308, 1469, 1095}, 556: {2360, 2354, 555, 615}, 1705: {459, 2356, 1469, 1364}, 2156: {2098, 453}, 9: {453, 761, 308, 481}, 2059: {2651, 453}, 2354: {2360, 555, 453, 2455}, 1566: {1124, 453, 1567}, 1661: {453, 109, 1358, 791}, 1449: {1025, 453}, 2034: {577, 453, 558, 2327, 2015}, 1095: {1360, 792, 798}, 2004: {2360, 2040, 672}, 1386: {2360, 1715, 672}, 1006: {2360, 574}, 2040: {2360, 574, 1006}, 267: {459, 732, 20, 334}, 1343: {672, 1363, 1004, 1003}, 786: {1363, 1004, 1003}, 1469: {1004, 1003, 1364, 1661}, 615: {555, 614}, 2072: {667, 36, 2227}, 1974: {667, 36, 2227}, 1293: {1592, 1600, 1606, 672}, 405: {1592, 1285}, 1298: {1592, 409, 1285, 407}, 2329: {672, 1290}, 2360: {672, 2040, 574}, 1003: {672, 1004}, 1004: {672, 1003}, 2381: {2667, 1606, 687}, 1673: {964, 1573, 965, 966, 910}, 1131: {910, 1253, 966}, 1432: {1573, 964, 965, 910}, 1140: {964, 965, 966, 910, 981}, 1311: {1253, 1140, 764, 910}, 964: {910, 902}, 764: {636, 1573, 910, 2687}, 811: {1140, 1253, 910}, 981: {910, 965, 902}, 1481: {964, 1573, 965, 966, 910}, 420: {192, 193, 2214}, 238: {193, 489, 46, 1815}, 373: {652, 86}, 1979: {1844, 1845, 68}, 1845: {1844, 68}, 979: {68, 879, 1845, 157, 766}, 2007: {1779, 68, 1780}, 1825: {68, 1966, 978, 979, 121}, 2393: {496, 663}, 501: {496, 503, 663}, 1896: {496, 2393}, 1813: {2393, 1818, 503}, 1952: {2393, 503}, 2029: {1952, 2393, 1818, 503}, 1851: {1896, 503}, 24: {269, 39}, 943: {1646, 793, 728, 1016, 1466}, 1670: {1016, 1218}, 1466: {1016, 728}, 1090: {1029, 1023}, 2456: {48, 41, 1029}, 1108: {1504, 1506, 1507, 1503}, 1276: {1504, 1506, 1525, 1503}, 1504: {1506, 1308, 1503}, 1507: {1504, 1506, 1308, 1503}, 1525: {1504, 1506, 1503}, 1308: {1504, 1506, 1503}, 1415: {1504, 1506, 1525}, 2074: {2607, 1636, 2615, 1647}, 1073: {1569, 1636, 1112, 314, 1182}, 928: {256, 1636, 983}, 983: {256, 1636, 887}, 1684: {1569, 1636, 1647, 314, 1182}, 821: {1389, 1421, 607}, 672: {2040, 109}, 884: {1658, 109}, 778: {25, 109}, 171: {114, 109}, 955: {560, 109}, 1665: {539, 883}, 1210: {539, 883, 1453}, 51: {772, 645, 2559}, 2076: {2559, 772, 1324, 1527}, 1747: {419, 1324, 1326, 1527, 2559}, 260: {2559, 419, 1324, 1527}, 2166: {772, 260, 1527}, 2167: {2088, 772, 2166}, 770: {730, 1285, 350}, 2594: {2306, 411}, 753: {409, 444, 405}, 512: {411, 407}, 2439: {1875, 407}, 1875: {2553, 1891, 407}, 2582: {2178, 462, 1718, 409, 411}, 1891: {200, 58, 411, 206}, 1470: {1464, 411, 1037}, 536: {200, 58, 411, 206}, 1733: {904, 41}, 889: {904, 41, 2153, 1713, 892}, 94: {41, 126}, 1982: {2153, 1713, 1716, 41}, 1713: {41, 2002, 1716}, 892: {904, 41, 138, 1713, 1716}, 352: {904, 41, 1372}, 23: {41, 759}, 115: {904, 41, 138, 2153, 1713}, 1783: {48, 2348}, 2682: {48, 2348}, 5: {490, 2164}, 734: {490, 492}, 213: {490, 492, 734}, 2323: {2419, 492, 788, 1268}, 2321: {2186, 2187}, 531: {2187, 693}, 135: {418, 654}, 2492: {577, 2461}, 1076: {1086, 2461, 1078}, 1078: {1076, 2461, 1086}, 401: {632, 577, 442, 2462}, 423: {558, 1290, 557, 62}, 2660: {577, 558}, 376: {704, 641, 558}, 557: {558, 423}, 1532: {558, 577, 782, 327}, 1555: {558, 1550}, 1739: {848, 641}, 484: {641, 642, 1574}, 657: {656, 1581}, 1711: {1515, 1581}, 2277: {2624, 1931, 2252, 2269, 1277}, 2402: {1921, 1931}, 1408: {1249, 1931}, 517: {1170, 164, 2509, 1198}, 1683: {1697, 740, 1198, 1559}, 1157: {1700, 741, 1704, 745, 687}, 687: {745, 741}, 2165: {745, 129, 538}, 1700: {745, 687}, 1704: {745, 687}, 36: {745, 1933, 257, 1591}, 1886: {745, 1884, 1885}, 2336: {745, 2579}, 78: {560, 157, 214, 766}, 1138: {1344, 1121, 2706}, 1121: {1344, 540}, 1047: {1344, 1345, 1354, 1348}, 1344: {1345, 1046, 1047}, 1640: {1345, 2706, 540, 1121}, 612: {1348, 629}, 313: {1377, 801, 1642, 1354, 534}, 1420: {1354, 1138, 1047}, 806: {1121, 1354, 1138}, 911: {585, 1354}, 1123: {1680, 1354, 1674}, 1680: {1354, 1674}, 1553: {82, 1554}, 1824: {1595, 102, 2675, 2006}, 1893: {1908, 102}, 1964: {102, 1862, 1644, 2675, 1595}, 2028: {1003, 308, 2653}, 2483: {1848, 1003, 1004}, 2217: {2282, 1007}, 2514: {485, 2380, 1838, 1908, 2558}, 2380: {2561, 485, 1838}, 1838: {719, 1908, 485, 2380}, 2561: {485, 2380, 719, 1908, 1527}, 2478: {2226, 2220}, 2139: {2226, 995, 2220}, 1958: {2226, 2220}, 1409: {129, 2220}, 1940: {2220, 2175, 860}, 1950: {2226, 2220}, 231: {285, 486, 30}, 285: {486, 30}, 2276: {486, 30}, 986: {2616, 1644, 2614}, 1862: {1595, 1964, 2675, 1644}, 1863: {1644, 1595, 1964, 1862}, 2364: {1644, 1812, 862}, 1256: {2675, 1644, 1019}, 1601: {1644, 2423}, 2355: {217, 763, 1879}, 2062: {2066, 2060, 2052, 2061}, 2035: {858, 1084, 2303}, 467: {2016, 495, 464, 342, 729}, 2000: {2115, 467}, 270: {495, 339, 467, 342, 573}, 276: {2115, 1993, 270, 467, 342}, 495: {342, 339, 467}, 729: {467, 342}, 1375: {560, 1452, 157}, 2287: {560, 2315}, 597: {675, 204}, 2410: {2049, 204}, 2650: {204, 2215}, 2080: {2329, 1290}, 1918: {2329, 1290}, 1191: {1290, 661, 2423}, 1389: {1050, 2707}, 774: {331, 2707, 1493}, 1901: {1800, 2301}, 1075: {1340, 157}, 393: {157, 14}, 2680: {2596, 101, 157}, 1289: {1452, 157}, 2021: {2428, 157}, 1718: {157, 1079}, 560: {1452, 740, 157, 246}, 2095: {2056, 157}, 371: {88, 157}, 2417: {2532, 157}, 2681: {160, 1857, 78}, 469: {160, 78, 214}, 1857: {160, 2540, 125, 2020}, 1055: {1059, 1284, 607}, 1169: {1284, 524}, 1009: {1079, 934, 2175}, 926: {1112, 1019}, 2415: {1112, 2530}, 101: {1112, 331}, 134: {1118, 2135}, 2633: {2011, 2182}, 1835: {2011, 2182}, 1433: {1026, 798, 1095}, 11: {624, 659}, 1562: {627, 631}, 1679: {1569, 627}, 2674: {2543, 159}, 1599: {1697, 671}, 1703: {1697, 740}, 1664: {1697, 740, 804, 1559}, 1667: {1697, 740}, 1860: {2658, 2428, 2430}, 2174: {636, 764, 1573, 2687}, 2233: {636, 703}, 995: {415, 2673, 1690, 1178}, 98: {2696, 1954, 615}, 978: {664, 1593, 1954}, 2150: {2305, 1954, 2179, 2222}, 2015: {2092, 2327}, 1567: {1124, 1566}, 139: {2361, 188}, 389: {577, 185, 324, 332}, 180: {350, 143}, 730: {350, 351}, 201: {350, 1990, 1718}, 1288: {1593, 1461}, 2319: {2131, 227}, 1657: {1595, 1307, 1462, 1422}, 232: {268, 613, 14}, 746: {299, 14}, 242: {1006, 574, 14}, 708: {250, 1116, 14}, 31: {250, 1116, 14}, 736: {250, 1116, 14}, 613: {2496, 2107, 14}, 1582: {2017, 1908, 1909, 1086}, 1545: {1582, 1909, 1086}, 2416: {2506, 1910, 2375}, 2506: {1910, 2661, 2366}, 2242: {2661, 1910}, 2375: {2661, 1910}, 2575: {2506, 1910, 2366}, 1251: {1250, 399}, 525: {522, 1419, 684, 1425, 1427}, 684: {161, 1425, 779, 1550}, 2571: {2496, 613, 268, 1878, 2107}, 254: {1176, 15}, 253: {15, 1815}, 538: {129, 2165, 15}, 80: {2044, 15}, 1306: {1050, 1310}, 1492: {1050, 1724}, 1962: {1827, 2509, 574}, 1788: {2178, 1795, 2181}, 1793: {2178, 2181}, 1795: {1793, 2178, 1794, 2181}, 2347: {2265, 2254, 1303}, 1190: {809, 1321, 1129}, 1229: {809, 1615}, 1639: {809, 1615}, 297: {809, 441, 1321}, 1222: {809, 1321, 1129}, 1044: {809, 812, 814, 1365, 1341}, 812: {809, 1365, 1341, 814}, 814: {1080, 809, 1321, 1129}, 919: {809, 1129}, 1080: {809, 812, 1321, 1129}, 1365: {809, 812, 814}, 2066: {985, 2674, 159}, 2220: {129, 1409}, 2193: {129, 538, 2165}, 1204: {2624, 129, 1153, 537, 538}, 1772: {2307, 2351}, 603: {1640, 540, 670, 1047}, 324: {62, 332, 326, 327}, 2702: {2624, 881}, 2203: {881, 1411, 2397, 2405}, 2405: {881, 2397, 1119}, 2454: {2129, 1058, 881}, 2309: {1969, 538}, 1593: {1116, 1461}, 170: {250, 4}, 498: {250, 107, 108}, 33: {250, 1701, 1591}, 1268: {2419, 788}, 623: {1172, 2341}, 788: {1268, 2341}, 1757: {1268, 2341, 2221}, 1842: {2341, 1268, 1757}, 789: {1409, 1206, 415}, 2664: {712, 189, 2102, 415}, 2103: {2334, 415}, 1737: {1178, 380, 415}, 1832: {1831, 1836, 55, 2334, 415}, 1836: {1832, 415, 2334, 1831}, 209: {2334, 415}, 2353: {2334, 415}, 712: {219, 415}, 93: {725, 759}, 95: {161, 759, 358, 1255}, 99: {2578, 759, 2679}, 1355: {1640, 1148, 1648, 1047}, 1726: {1569, 1679, 1684, 314, 1182}, 1011: {1008, 1569, 1010}, 877: {1569, 1010, 1578}, 1182: {1569, 1010, 1578}, 1701: {832, 33, 36, 1591}, 832: {1052, 1701}, 2420: {2169, 1527}, 1543: {1324, 1527}, 2524: {260, 1894, 2088, 2166, 1527}, 2525: {2088, 1527}, 827: {913, 909, 1894, 1527}, 909: {913, 1894, 1527}, 971: {1326, 2166, 1527}, 419: {260, 1326, 398, 1527, 248}, 1754: {1867, 1869}, 2458: {2506, 2242, 2366}, 2502: {2605, 717}, 951: {1554, 2605}, 1738: {1689, 538, 219}, 38: {1689, 219}, 2218: {2049, 2021, 2337}, 1837: {2132, 382}, 2102: {144, 189}, 1388: {848, 1025, 1586, 1027}, 1400: {848, 967}, 847: {848, 967}, 487: {538, 291}, 1036: {712, 538, 1662}, 2346: {1595, 1301}, 151: {1528, 1595, 360}, 1199: {1702, 1550}, 831: {2466, 2243, 885}, 53: {803, 1563, 243, 1439}, 890: {18, 866}, 898: {107, 108}, 1937: {389, 1064, 107, 623, 2293}, 1062: {930, 1254}, 2144: {1894, 1326}, 590: {364, 365, 366}, 2572: {2112, 365}, 450: {2558, 954, 646}, 854: {1178, 2211, 2598, 1231}, 956: {1178, 995, 2598, 1231}, 1476: {1178, 1277}, 1736: {1737, 1178}, 1690: {1736, 1178, 995}, 1230: {995, 2598, 2470, 1231, 1178}, 1231: {1178, 995, 1495}, 1232: {995, 2211, 2598, 1231, 1178}, 1745: {2641, 2175}, 1719: {1257, 2261, 2175, 1911}, 2250: {1696, 1940, 2175}, 2087: {1257, 2261, 2175, 1911}, 2206: {2379, 2261, 2133, 1911, 2175}, 120: {1184, 188, 2175}, 2199: {1184, 1784, 2133}, 844: {1184, 934, 974, 55, 1784}, 860: {1184, 934, 1911}, 965: {964, 902}, 1581: {1515, 764}, 1917: {1796, 2334}, 2324: {1161, 2326}, 1955: {2194, 2149, 2190}, 2188: {2189, 2194, 2149, 2190}, 652: {249, 86}, 2479: {2128, 2319}, 2030: {2128, 2130, 2131}, 921: {1265, 1725}, 815: {880, 1523}, 1959: {120, 177, 2259}, 1180: {1177, 1620}, 2281: {2429, 1134}, 2624: {2429, 2317}, 1143: {2429, 1404, 2165}, 1428: {1601, 2329}, 1429: {1601, 675}, 2473: {551, 2551}, 1549: {1602, 339}, 1602: {339, 1326, 971}, 1945: {729, 1993, 342}, 2546: {1993, 276, 342}, 1993: {729, 276, 342}, 776: {2088, 398, 1326, 342, 2166}, 56: {778, 1027, 967}, 2271: {2249, 2250, 2252, 2259, 2238}, 2083: {1827, 299, 2511}, 2320: {1853, 2511}, 2141: {2418, 2511}, 2418: {2141, 2511}, 2345: {2418, 2005, 2511}, 1487: {1528, 1532}, 1826: {689, 2630}, 1580: {1625, 689}, 1066: {36, 1052}, 1827: {299, 574}, 852: {1654, 1015}, 855: {1338, 1654}, 1338: {784, 917, 1654, 1015}, 1475: {801, 1642}, 731: {1656, 121}, 2279: {2579, 1806}, 837: {1489, 1007}, 2161: {1326, 2167}, 2434: {1747, 2069, 1326}, 248: {2088, 1326, 398}, 105: {248, 2434, 1326}, 1904: {643, 2556}, 464: {275, 573, 279}, 251: {1505, 865}, 2686: {916, 101, 2596}, 1938: {1770, 1764, 1765}, 679: {32, 30}, 1501: {447, 693, 1471}, 1596: {314, 1244, 348}, 2535: {2176, 1668}, 1898: {1268, 2108}, 2140: {1898, 623, 1268, 2292, 2108}, 1133: {1666, 719}, 1112: {1618, 719}, 87: {2021, 719}, 1215: {824, 825}, 1771: {1696, 1775}, 2082: {1793, 1795, 1775}, 388: {2590, 439}, 1158: {1555, 1550}, 1425: {684, 1550}, 959: {793, 1130}, 1694: {793, 1455}, 1330: {793, 1167}, 163: {793, 188, 189}, 1455: {793, 1153, 1069, 1685}, 1249: {1728, 793, 1474, 1473}, 262: {793, 1980, 188}, 2060: {857, 1483}, 1051: {2576, 1052}, 1514: {1333, 429}, 820: {1146, 1252}, 2091: {1539, 2438}, 342: {1800, 2016}, 2670: {489, 253, 1815}, 301: {489, 238}, 1815: {489, 253}, 46: {489, 301, 238}, 1631: {934, 2598, 2407, 2604, 2330}, 1149: {912, 1891, 2044}, 1769: {1696, 2330}, 2330: {1696, 2604, 2598, 1631}, 1933: {2660, 748}, 138: {416, 137, 748, 1713, 1716}, 1774: {1968, 1891, 2619, 990}, 1150: {912, 1149, 1630}, 278: {1898, 324, 326}, 1985: {2065, 995, 1231}, 326: {324, 327}, 185: {324, 389, 332}, 1899: {2292, 327}, 1064: {952, 1937, 332, 623}, 973: {993, 802}, 2606: {2337, 2662, 527, 2611, 2459}, 553: {123, 2459}, 626: {261, 687}, 2289: {1804, 1805}, 748: {1926, 137, 138, 1933, 1941}, 416: {137, 138, 2510, 1713, 1913}, 1416: {922, 923}, 1233: {922, 923}, 265: {100, 758}, 758: {100, 265, 1980}, 1822: {1656, 2662}, 1266: {1301, 1663}, 996: {161, 1015}, 2311: {2448, 161, 2285}, 1474: {1728, 1473, 380}, 1264: {1738, 1134}, 2703: {2200, 1409, 1221}, 1797: {2200, 1409, 1221}, 176: {379, 756}, 1508: {1724, 668}, 1858: {2528, 2481}, 1628: {1063, 1175}, 1852: {2592, 99}, 2578: {99, 2679}, 1613: {784, 99, 1619, 1614}, 1619: {1722, 99, 1723}, 1722: {99, 1723}, 1255: {894, 99, 358}, 2542: {2236, 2262}, 2435: {2236, 2262}, 337: {346, 363, 77, 143}, 529: {341, 340, 709}, 341: {340, 709}, 709: {340, 341}, 2093: {283, 1990}, 2445: {2275, 2268}, 2275: {1120, 2268}, 2110: {1827, 299}, 990: {58, 805, 1774}, 2495: {58, 571}, 1454: {1169, 524}, 372: {273, 38}, 915: {1172, 1294}, 637: {515, 623}, 2160: {2434, 1747}, 302: {432, 428}, 428: {432, 302}, 2046: {2107, 268}, 1383: {152, 1384}, 2138: {2285, 1718}, 1072: {1520, 354}, 829: {962, 1252, 820}, 1485: {1252, 1159}, 1115: {1252, 820}, 755: {2058, 188, 1404, 757}, 1458: {1714, 838}, 317: {417, 508, 2326}, 363: {346, 143}, 1730: {1034, 1533}, 920: {1520, 873}, 1211: {1520, 1058}, 233: {595, 1990}, 2190: {2194, 2189}, 2365: {2668, 2189, 2190}, 2189: {2194, 2190}, 2149: {2194, 2190}, 2504: {1536, 2667, 1663}, 65: {744, 743}, 1357: {1461, 1511}, 780: {1139, 1598}, 1598: {1139, 1572}, 914: {272, 1551}, 2114: {1928, 2124}, 2272: {1176, 1866}, 1313: {1005, 1382}, 2704: {209, 2407}, 1035: {1216, 1218, 714}, 1299: {1216, 1218, 714, 1392}, 1424: {1218, 1530}, 833: {1218, 714, 1796}, 1614: {1619, 1613}, 2086: {2065, 1231}, 2688: {2216, 1784, 1840, 1839}, 2705: {2216, 1784, 1840, 1839}, 2643: {2470, 2646}, 2646: {2643, 2470}, 1104: {1260, 1262}, 1925: {2392, 2133, 2206}, 1984: {2379, 2206}, 2700: {2697, 2698}, 2288: {2697, 1759}, 1759: {2697, 2698}, 2373: {916, 2374}, 2374: {916, 2373}, 896: {888, 468, 1372}, 41: {888, 1713, 1716}, 2377: {2025, 491, 468}, 1977: {2153, 2219, 2510, 2318}, 1992: {2153, 1713}, 1716: {2153, 1713}, 2002: {2153, 1713, 2510}, 2008: {2153, 2219, 2022}, 2022: {2008, 2153, 2219}, 2701: {2247, 2263}, 1870: {1713, 2362}, 1978: {1713, 2510}, 1991: {1713, 1716, 2510}, 2510: {1713, 2362}, 2208: {1761, 2234, 2235, 126}, 182: {491, 468}, 491: {468, 2318}, 2421: {1906, 115}, 2001: {833, 1796}, 282: {592, 589}, 1144: {592, 1509}, 589: {592, 1509}, 2515: {2003, 1999}, 2507: {371, 2373, 2374, 2239}, 1366: {1373, 2373, 2374}, 2595: {2562, 1315, 2452}, 2064: {1777, 1802}, 272: {914, 1551}, 2331: {571, 2694}, 1762: {571, 2495}, 2694: {1762, 571, 2495}, 571: {1762, 19, 2495}, 1094: {1659, 355, 2228, 2085}, 2068: {355, 2228}, 1252: {1477, 1485}, 745: {1884, 1885, 1886}, 1902: {1884, 1885, 1886}}, 'features': matrix([[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), 'labels': array([[0, 0, 0, ..., 0, 0, 0],
       [0, 0, 1, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 1],
       ...,
       [1, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 1, 0, 0],
       [0, 0, 0, ..., 0, 0, 0]], dtype=int32), 'n': 2708, 'feature': array([[[ 0.        ,  0.        ,  0.        , ..., -0.55357766,
         -0.49470094,  0.25302992]],

       [[ 0.        ,  0.        ,  0.        , ..., -0.14879404,
         -0.05142467,  0.2210307 ]],

       [[ 0.        ,  0.        ,  0.        , ...,  0.21510649,
          0.21510649,  0.        ]],

       ...,

       [[ 0.        ,  0.        ,  0.        , ..., -0.14535022,
         -0.14535022,  0.        ]],

       [[ 0.        ,  0.        ,  0.        , ..., -0.46566513,
         -0.44798303,  0.0920952 ]],

       [[ 0.        ,  0.        ,  0.        , ..., -0.65939647,
         -0.65939647,  0.08350319]]])})
The split is 1
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 1/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:2.0304 | train acc:76.43 | val acc:50.29 | test_acc_best_val: 51.19  | best_test_acc: 51.19 | test acc:51.19 | time:77.7ms
epoch:20 | loss:1.3229 | train acc:98.57 | val acc:69.47 | test_acc_best_val: 70.59  | best_test_acc: 70.59 | test acc:70.59 | time:9.1ms
epoch:40 | loss:1.2946 | train acc:99.29 | val acc:70.84 | test_acc_best_val: 71.41  | best_test_acc: 71.41 | test acc:71.17 | time:8.2ms
epoch:60 | loss:1.2960 | train acc:99.29 | val acc:70.25 | test_acc_best_val: 71.41  | best_test_acc: 71.41 | test acc:70.98 | time:8.8ms
epoch:80 | loss:1.2975 | train acc:99.29 | val acc:71.43 | test_acc_best_val: 71.37  | best_test_acc: 71.37 | test acc:71.37 | time:8.6ms
epoch:100 | loss:1.2959 | train acc:99.29 | val acc:71.43 | test_acc_best_val: 71.46  | best_test_acc: 71.46 | test acc:71.61 | time:12.7ms
epoch:120 | loss:1.3125 | train acc:99.29 | val acc:71.43 | test_acc_best_val: 71.46  | best_test_acc: 71.46 | test acc:71.56 | time:9.3ms
epoch:140 | loss:1.3059 | train acc:99.29 | val acc:72.02 | test_acc_best_val: 71.46  | best_test_acc: 71.46 | test acc:71.51 | time:8.4ms
epoch:160 | loss:1.3188 | train acc:99.29 | val acc:71.23 | test_acc_best_val: 71.46  | best_test_acc: 71.46 | test acc:71.22 | time:13.2ms
epoch:180 | loss:1.3114 | train acc:99.29 | val acc:70.65 | test_acc_best_val: 71.46  | best_test_acc: 71.46 | test acc:72.14 | time:10.1ms
Run 1/10, best test accuracy: 71.46, acc(last): 72.58, total time: 2.80s
The split is 2
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 2/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:2.0065 | train acc:77.86 | val acc:58.32 | test_acc_best_val: 58.05  | best_test_acc: 58.05 | test acc:58.05 | time:9.2ms
epoch:20 | loss:1.3469 | train acc:100.00 | val acc:67.51 | test_acc_best_val: 70.64  | best_test_acc: 70.64 | test acc:70.39 | time:9.4ms
epoch:40 | loss:1.3178 | train acc:100.00 | val acc:67.71 | test_acc_best_val: 70.64  | best_test_acc: 70.64 | test acc:70.54 | time:9.0ms
epoch:60 | loss:1.2993 | train acc:99.29 | val acc:67.91 | test_acc_best_val: 70.64  | best_test_acc: 70.64 | test acc:70.35 | time:11.7ms
epoch:80 | loss:1.3192 | train acc:99.29 | val acc:68.30 | test_acc_best_val: 70.64  | best_test_acc: 70.64 | test acc:70.73 | time:9.4ms
epoch:100 | loss:1.3356 | train acc:99.29 | val acc:69.08 | test_acc_best_val: 70.64  | best_test_acc: 70.64 | test acc:71.71 | time:8.2ms
epoch:120 | loss:1.3222 | train acc:99.29 | val acc:67.71 | test_acc_best_val: 70.64  | best_test_acc: 70.64 | test acc:71.56 | time:9.6ms
epoch:140 | loss:1.3173 | train acc:99.29 | val acc:67.51 | test_acc_best_val: 70.64  | best_test_acc: 70.64 | test acc:71.61 | time:8.5ms
epoch:160 | loss:1.3171 | train acc:99.29 | val acc:68.10 | test_acc_best_val: 70.64  | best_test_acc: 70.64 | test acc:71.90 | time:9.3ms
epoch:180 | loss:1.3306 | train acc:99.29 | val acc:68.10 | test_acc_best_val: 70.64  | best_test_acc: 70.64 | test acc:72.00 | time:9.3ms
Run 2/10, best test accuracy: 70.64, acc(last): 71.51, total time: 2.66s
The split is 3
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 3/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:2.0482 | train acc:64.29 | val acc:38.16 | test_acc_best_val: 36.32  | best_test_acc: 36.32 | test acc:36.32 | time:9.4ms
epoch:20 | loss:1.3426 | train acc:98.57 | val acc:72.80 | test_acc_best_val: 70.98  | best_test_acc: 70.98 | test acc:70.98 | time:13.3ms
epoch:40 | loss:1.3167 | train acc:99.29 | val acc:73.58 | test_acc_best_val: 71.12  | best_test_acc: 71.12 | test acc:70.98 | time:10.0ms
epoch:60 | loss:1.3280 | train acc:99.29 | val acc:73.58 | test_acc_best_val: 71.12  | best_test_acc: 71.12 | test acc:70.78 | time:9.4ms
epoch:80 | loss:1.2982 | train acc:99.29 | val acc:73.58 | test_acc_best_val: 70.83  | best_test_acc: 70.83 | test acc:70.88 | time:9.1ms
epoch:100 | loss:1.3066 | train acc:99.29 | val acc:73.19 | test_acc_best_val: 70.83  | best_test_acc: 70.83 | test acc:70.83 | time:8.3ms
epoch:120 | loss:1.3341 | train acc:99.29 | val acc:73.39 | test_acc_best_val: 70.83  | best_test_acc: 70.83 | test acc:71.90 | time:7.8ms
epoch:140 | loss:1.3184 | train acc:99.29 | val acc:73.39 | test_acc_best_val: 70.83  | best_test_acc: 70.83 | test acc:71.66 | time:8.3ms
epoch:160 | loss:1.3167 | train acc:99.29 | val acc:74.36 | test_acc_best_val: 71.66  | best_test_acc: 71.66 | test acc:71.80 | time:9.1ms
epoch:180 | loss:1.3181 | train acc:99.29 | val acc:73.97 | test_acc_best_val: 71.66  | best_test_acc: 71.66 | test acc:71.85 | time:8.0ms
Run 3/10, best test accuracy: 71.66, acc(last): 71.75, total time: 2.63s
The split is 4
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 4/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:2.0348 | train acc:81.43 | val acc:49.12 | test_acc_best_val: 48.76  | best_test_acc: 48.76 | test acc:48.76 | time:12.7ms
epoch:20 | loss:1.2984 | train acc:99.29 | val acc:67.71 | test_acc_best_val: 70.64  | best_test_acc: 70.64 | test acc:69.91 | time:9.3ms
epoch:40 | loss:1.2954 | train acc:100.00 | val acc:69.47 | test_acc_best_val: 70.93  | best_test_acc: 70.93 | test acc:71.03 | time:9.2ms
epoch:60 | loss:1.3035 | train acc:100.00 | val acc:70.06 | test_acc_best_val: 71.37  | best_test_acc: 71.37 | test acc:71.37 | time:12.0ms
epoch:80 | loss:1.3010 | train acc:100.00 | val acc:70.45 | test_acc_best_val: 71.71  | best_test_acc: 71.71 | test acc:71.71 | time:8.2ms
epoch:100 | loss:1.2817 | train acc:100.00 | val acc:71.04 | test_acc_best_val: 72.14  | best_test_acc: 72.14 | test acc:72.29 | time:8.5ms
epoch:120 | loss:1.2966 | train acc:100.00 | val acc:71.62 | test_acc_best_val: 72.63  | best_test_acc: 72.63 | test acc:72.63 | time:8.7ms
epoch:140 | loss:1.2834 | train acc:100.00 | val acc:71.43 | test_acc_best_val: 72.63  | best_test_acc: 72.63 | test acc:72.39 | time:11.4ms
epoch:160 | loss:1.2964 | train acc:100.00 | val acc:72.60 | test_acc_best_val: 72.24  | best_test_acc: 72.24 | test acc:72.34 | time:11.8ms
epoch:180 | loss:1.3037 | train acc:100.00 | val acc:72.21 | test_acc_best_val: 72.24  | best_test_acc: 72.24 | test acc:72.48 | time:10.2ms
Run 4/10, best test accuracy: 72.24, acc(last): 72.78, total time: 2.82s
The split is 5
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 5/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:1.9868 | train acc:75.00 | val acc:54.79 | test_acc_best_val: 54.40  | best_test_acc: 54.40 | test acc:54.40 | time:10.4ms
epoch:20 | loss:1.3064 | train acc:100.00 | val acc:72.99 | test_acc_best_val: 72.97  | best_test_acc: 72.97 | test acc:73.07 | time:8.7ms
epoch:40 | loss:1.3166 | train acc:100.00 | val acc:72.02 | test_acc_best_val: 72.97  | best_test_acc: 72.97 | test acc:72.97 | time:8.1ms
epoch:60 | loss:1.3144 | train acc:100.00 | val acc:71.04 | test_acc_best_val: 72.97  | best_test_acc: 72.97 | test acc:72.53 | time:10.5ms
epoch:80 | loss:1.3402 | train acc:100.00 | val acc:71.04 | test_acc_best_val: 72.97  | best_test_acc: 72.97 | test acc:72.92 | time:7.6ms
epoch:100 | loss:1.3178 | train acc:100.00 | val acc:71.23 | test_acc_best_val: 72.97  | best_test_acc: 72.97 | test acc:72.73 | time:8.2ms
epoch:120 | loss:1.3144 | train acc:100.00 | val acc:72.02 | test_acc_best_val: 72.97  | best_test_acc: 72.97 | test acc:72.87 | time:8.6ms
epoch:140 | loss:1.3245 | train acc:100.00 | val acc:72.21 | test_acc_best_val: 72.97  | best_test_acc: 72.97 | test acc:73.46 | time:9.5ms
epoch:160 | loss:1.3235 | train acc:100.00 | val acc:72.21 | test_acc_best_val: 72.97  | best_test_acc: 72.97 | test acc:73.07 | time:7.9ms
epoch:180 | loss:1.3204 | train acc:100.00 | val acc:72.41 | test_acc_best_val: 72.97  | best_test_acc: 72.97 | test acc:72.78 | time:8.9ms
Run 5/10, best test accuracy: 72.97, acc(last): 73.07, total time: 2.71s
The split is 6
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 6/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:2.0502 | train acc:74.29 | val acc:45.40 | test_acc_best_val: 44.09  | best_test_acc: 44.09 | test acc:44.09 | time:9.8ms
epoch:20 | loss:1.3585 | train acc:99.29 | val acc:67.51 | test_acc_best_val: 67.91  | best_test_acc: 67.91 | test acc:68.21 | time:9.8ms
epoch:40 | loss:1.3060 | train acc:99.29 | val acc:67.71 | test_acc_best_val: 67.91  | best_test_acc: 67.91 | test acc:68.11 | time:8.3ms
epoch:60 | loss:1.3209 | train acc:99.29 | val acc:68.10 | test_acc_best_val: 67.91  | best_test_acc: 67.91 | test acc:68.21 | time:9.1ms
epoch:80 | loss:1.3072 | train acc:99.29 | val acc:67.91 | test_acc_best_val: 67.91  | best_test_acc: 67.91 | test acc:68.74 | time:8.6ms
epoch:100 | loss:1.3232 | train acc:99.29 | val acc:67.71 | test_acc_best_val: 67.91  | best_test_acc: 67.91 | test acc:68.74 | time:8.2ms
epoch:120 | loss:1.3011 | train acc:100.00 | val acc:67.71 | test_acc_best_val: 67.91  | best_test_acc: 67.91 | test acc:68.98 | time:8.4ms
epoch:140 | loss:1.3175 | train acc:100.00 | val acc:67.91 | test_acc_best_val: 67.91  | best_test_acc: 67.91 | test acc:69.28 | time:8.2ms
epoch:160 | loss:1.3182 | train acc:100.00 | val acc:68.10 | test_acc_best_val: 67.91  | best_test_acc: 67.91 | test acc:69.47 | time:9.0ms
epoch:180 | loss:1.3220 | train acc:100.00 | val acc:68.49 | test_acc_best_val: 69.76  | best_test_acc: 69.76 | test acc:69.81 | time:9.6ms
Run 6/10, best test accuracy: 69.76, acc(last): 69.91, total time: 2.68s
The split is 7
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 7/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:1.9964 | train acc:63.57 | val acc:35.42 | test_acc_best_val: 35.49  | best_test_acc: 35.49 | test acc:35.49 | time:8.1ms
epoch:20 | loss:1.3263 | train acc:100.00 | val acc:67.51 | test_acc_best_val: 68.84  | best_test_acc: 68.84 | test acc:68.55 | time:9.5ms
epoch:40 | loss:1.3223 | train acc:100.00 | val acc:67.91 | test_acc_best_val: 68.84  | best_test_acc: 68.84 | test acc:68.45 | time:8.0ms
epoch:60 | loss:1.3003 | train acc:100.00 | val acc:68.49 | test_acc_best_val: 68.84  | best_test_acc: 68.84 | test acc:69.47 | time:12.9ms
epoch:80 | loss:1.3043 | train acc:100.00 | val acc:68.88 | test_acc_best_val: 69.42  | best_test_acc: 69.42 | test acc:69.47 | time:8.8ms
epoch:100 | loss:1.3091 | train acc:100.00 | val acc:68.49 | test_acc_best_val: 69.42  | best_test_acc: 69.42 | test acc:69.91 | time:8.4ms
epoch:120 | loss:1.3250 | train acc:100.00 | val acc:68.69 | test_acc_best_val: 69.42  | best_test_acc: 69.42 | test acc:70.73 | time:8.4ms
epoch:140 | loss:1.3331 | train acc:100.00 | val acc:68.88 | test_acc_best_val: 69.42  | best_test_acc: 69.42 | test acc:71.27 | time:8.7ms
epoch:160 | loss:1.3259 | train acc:100.00 | val acc:69.08 | test_acc_best_val: 69.42  | best_test_acc: 69.42 | test acc:70.35 | time:8.6ms
epoch:180 | loss:1.3197 | train acc:100.00 | val acc:68.49 | test_acc_best_val: 69.42  | best_test_acc: 69.42 | test acc:70.44 | time:12.4ms
Run 7/10, best test accuracy: 69.42, acc(last): 70.59, total time: 2.63s
The split is 8
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 8/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:1.9721 | train acc:77.86 | val acc:49.51 | test_acc_best_val: 48.32  | best_test_acc: 48.32 | test acc:48.32 | time:9.4ms
epoch:20 | loss:1.3284 | train acc:99.29 | val acc:72.99 | test_acc_best_val: 70.83  | best_test_acc: 70.83 | test acc:71.27 | time:8.1ms
epoch:40 | loss:1.3207 | train acc:99.29 | val acc:72.80 | test_acc_best_val: 70.83  | best_test_acc: 70.83 | test acc:70.73 | time:8.0ms
epoch:60 | loss:1.3100 | train acc:98.57 | val acc:73.19 | test_acc_best_val: 70.59  | best_test_acc: 70.59 | test acc:70.69 | time:7.4ms
epoch:80 | loss:1.3204 | train acc:100.00 | val acc:72.41 | test_acc_best_val: 70.59  | best_test_acc: 70.59 | test acc:71.27 | time:8.1ms
epoch:100 | loss:1.3204 | train acc:100.00 | val acc:72.21 | test_acc_best_val: 70.59  | best_test_acc: 70.59 | test acc:71.32 | time:9.4ms
epoch:120 | loss:1.3197 | train acc:100.00 | val acc:72.99 | test_acc_best_val: 70.59  | best_test_acc: 70.59 | test acc:72.05 | time:10.5ms
epoch:140 | loss:1.3139 | train acc:98.57 | val acc:73.58 | test_acc_best_val: 72.00  | best_test_acc: 72.00 | test acc:72.19 | time:11.0ms
epoch:160 | loss:1.3302 | train acc:98.57 | val acc:74.17 | test_acc_best_val: 72.53  | best_test_acc: 72.53 | test acc:72.48 | time:7.9ms
epoch:180 | loss:1.3452 | train acc:98.57 | val acc:74.17 | test_acc_best_val: 72.53  | best_test_acc: 72.53 | test acc:72.44 | time:8.4ms
Run 8/10, best test accuracy: 72.53, acc(last): 72.78, total time: 2.60s
The split is 9
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 9/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:2.0366 | train acc:78.57 | val acc:59.49 | test_acc_best_val: 58.48  | best_test_acc: 58.48 | test acc:58.48 | time:11.7ms
epoch:20 | loss:1.3486 | train acc:98.57 | val acc:70.25 | test_acc_best_val: 71.90  | best_test_acc: 71.90 | test acc:72.97 | time:9.6ms
epoch:40 | loss:1.3165 | train acc:99.29 | val acc:69.86 | test_acc_best_val: 71.90  | best_test_acc: 71.90 | test acc:72.44 | time:7.8ms
epoch:60 | loss:1.2933 | train acc:99.29 | val acc:70.45 | test_acc_best_val: 71.90  | best_test_acc: 71.90 | test acc:72.48 | time:8.5ms
epoch:80 | loss:1.3309 | train acc:99.29 | val acc:70.84 | test_acc_best_val: 71.90  | best_test_acc: 71.90 | test acc:72.34 | time:9.4ms
epoch:100 | loss:1.3040 | train acc:99.29 | val acc:71.23 | test_acc_best_val: 71.90  | best_test_acc: 71.90 | test acc:72.29 | time:9.0ms
epoch:120 | loss:1.3106 | train acc:99.29 | val acc:71.82 | test_acc_best_val: 71.90  | best_test_acc: 71.90 | test acc:72.63 | time:10.2ms
epoch:140 | loss:1.3238 | train acc:99.29 | val acc:71.43 | test_acc_best_val: 71.90  | best_test_acc: 71.90 | test acc:72.19 | time:8.6ms
epoch:160 | loss:1.3101 | train acc:99.29 | val acc:71.43 | test_acc_best_val: 72.19  | best_test_acc: 72.19 | test acc:72.34 | time:8.4ms
epoch:180 | loss:1.3072 | train acc:99.29 | val acc:71.04 | test_acc_best_val: 72.19  | best_test_acc: 72.19 | test acc:72.44 | time:8.4ms
Run 9/10, best test accuracy: 72.19, acc(last): 72.14, total time: 2.56s
The split is 10
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 10/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:2.0379 | train acc:70.71 | val acc:43.44 | test_acc_best_val: 41.13  | best_test_acc: 41.13 | test acc:41.13 | time:9.6ms
epoch:20 | loss:1.3280 | train acc:100.00 | val acc:73.39 | test_acc_best_val: 71.22  | best_test_acc: 71.22 | test acc:70.98 | time:9.6ms
epoch:40 | loss:1.2978 | train acc:100.00 | val acc:73.58 | test_acc_best_val: 71.22  | best_test_acc: 71.22 | test acc:71.03 | time:10.1ms
epoch:60 | loss:1.2806 | train acc:100.00 | val acc:73.19 | test_acc_best_val: 70.98  | best_test_acc: 70.98 | test acc:71.07 | time:7.9ms
epoch:80 | loss:1.2964 | train acc:100.00 | val acc:72.80 | test_acc_best_val: 70.98  | best_test_acc: 70.98 | test acc:71.12 | time:12.4ms
epoch:100 | loss:1.2945 | train acc:100.00 | val acc:72.02 | test_acc_best_val: 70.98  | best_test_acc: 70.98 | test acc:71.32 | time:8.8ms
epoch:120 | loss:1.3045 | train acc:100.00 | val acc:72.02 | test_acc_best_val: 70.98  | best_test_acc: 70.98 | test acc:71.37 | time:8.5ms
epoch:140 | loss:1.3033 | train acc:100.00 | val acc:72.02 | test_acc_best_val: 70.98  | best_test_acc: 70.98 | test acc:71.90 | time:7.9ms
epoch:160 | loss:1.3177 | train acc:100.00 | val acc:71.82 | test_acc_best_val: 70.98  | best_test_acc: 70.98 | test acc:72.00 | time:7.8ms
epoch:180 | loss:1.3407 | train acc:100.00 | val acc:72.41 | test_acc_best_val: 70.98  | best_test_acc: 70.98 | test acc:72.05 | time:8.1ms
Run 10/10, best test accuracy: 70.98, acc(last): 72.24, total time: 2.56s
The split is 1
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 1/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:1.9771 | train acc:76.43 | val acc:48.73 | test_acc_best_val: 49.78  | best_test_acc: 49.78 | test acc:49.78 | time:8.9ms
epoch:20 | loss:1.3230 | train acc:98.57 | val acc:74.36 | test_acc_best_val: 72.63  | best_test_acc: 72.63 | test acc:72.39 | time:8.0ms
epoch:40 | loss:1.3333 | train acc:99.29 | val acc:75.54 | test_acc_best_val: 71.95  | best_test_acc: 71.95 | test acc:71.90 | time:11.2ms
epoch:60 | loss:1.3074 | train acc:99.29 | val acc:75.15 | test_acc_best_val: 71.90  | best_test_acc: 71.90 | test acc:72.14 | time:9.1ms
epoch:80 | loss:1.3340 | train acc:99.29 | val acc:75.15 | test_acc_best_val: 71.90  | best_test_acc: 71.90 | test acc:72.00 | time:8.2ms
epoch:100 | loss:1.3227 | train acc:99.29 | val acc:74.95 | test_acc_best_val: 71.90  | best_test_acc: 71.90 | test acc:72.53 | time:8.3ms
epoch:120 | loss:1.3291 | train acc:99.29 | val acc:73.78 | test_acc_best_val: 71.90  | best_test_acc: 71.90 | test acc:72.73 | time:8.9ms
epoch:140 | loss:1.3262 | train acc:99.29 | val acc:74.76 | test_acc_best_val: 71.90  | best_test_acc: 71.90 | test acc:72.58 | time:7.8ms
epoch:160 | loss:1.3224 | train acc:99.29 | val acc:74.36 | test_acc_best_val: 71.90  | best_test_acc: 71.90 | test acc:72.39 | time:8.7ms
epoch:180 | loss:1.3393 | train acc:99.29 | val acc:75.15 | test_acc_best_val: 71.90  | best_test_acc: 71.90 | test acc:72.82 | time:9.6ms
Run 1/10, best test accuracy: 71.90, acc(last): 72.58, total time: 2.56s
The split is 2
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 2/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:1.9555 | train acc:75.71 | val acc:51.86 | test_acc_best_val: 56.34  | best_test_acc: 56.34 | test acc:56.34 | time:9.9ms
epoch:20 | loss:1.3203 | train acc:98.57 | val acc:68.30 | test_acc_best_val: 70.54  | best_test_acc: 70.54 | test acc:70.10 | time:9.0ms
epoch:40 | loss:1.3024 | train acc:100.00 | val acc:67.32 | test_acc_best_val: 70.54  | best_test_acc: 70.54 | test acc:69.81 | time:8.1ms
epoch:60 | loss:1.2954 | train acc:100.00 | val acc:66.14 | test_acc_best_val: 70.54  | best_test_acc: 70.54 | test acc:70.30 | time:8.1ms
epoch:80 | loss:1.3110 | train acc:99.29 | val acc:65.75 | test_acc_best_val: 70.54  | best_test_acc: 70.54 | test acc:70.35 | time:7.9ms
epoch:100 | loss:1.3259 | train acc:99.29 | val acc:66.34 | test_acc_best_val: 70.54  | best_test_acc: 70.54 | test acc:70.83 | time:8.5ms
epoch:120 | loss:1.2993 | train acc:99.29 | val acc:65.75 | test_acc_best_val: 70.54  | best_test_acc: 70.54 | test acc:70.73 | time:8.3ms
epoch:140 | loss:1.3027 | train acc:99.29 | val acc:65.95 | test_acc_best_val: 70.54  | best_test_acc: 70.54 | test acc:70.83 | time:8.3ms
epoch:160 | loss:1.3053 | train acc:99.29 | val acc:65.95 | test_acc_best_val: 70.54  | best_test_acc: 70.54 | test acc:70.35 | time:9.8ms
epoch:180 | loss:1.3143 | train acc:99.29 | val acc:66.14 | test_acc_best_val: 70.54  | best_test_acc: 70.54 | test acc:70.49 | time:11.8ms
Run 2/10, best test accuracy: 70.54, acc(last): 70.83, total time: 2.61s
The split is 3
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 3/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:2.0204 | train acc:58.57 | val acc:37.57 | test_acc_best_val: 35.88  | best_test_acc: 35.88 | test acc:35.88 | time:9.3ms
epoch:20 | loss:1.3485 | train acc:98.57 | val acc:72.41 | test_acc_best_val: 70.59  | best_test_acc: 70.59 | test acc:70.49 | time:9.3ms
epoch:40 | loss:1.3382 | train acc:99.29 | val acc:73.97 | test_acc_best_val: 70.83  | best_test_acc: 70.83 | test acc:70.83 | time:9.5ms
epoch:60 | loss:1.3223 | train acc:99.29 | val acc:74.56 | test_acc_best_val: 70.69  | best_test_acc: 70.69 | test acc:70.69 | time:8.6ms
epoch:80 | loss:1.3197 | train acc:99.29 | val acc:74.56 | test_acc_best_val: 71.22  | best_test_acc: 71.22 | test acc:70.93 | time:8.4ms
epoch:100 | loss:1.3168 | train acc:99.29 | val acc:74.17 | test_acc_best_val: 71.22  | best_test_acc: 71.22 | test acc:71.32 | time:8.1ms
epoch:120 | loss:1.3444 | train acc:99.29 | val acc:73.78 | test_acc_best_val: 71.22  | best_test_acc: 71.22 | test acc:71.22 | time:8.7ms
epoch:140 | loss:1.3154 | train acc:99.29 | val acc:74.56 | test_acc_best_val: 71.22  | best_test_acc: 71.22 | test acc:71.03 | time:8.0ms
epoch:160 | loss:1.3171 | train acc:99.29 | val acc:74.76 | test_acc_best_val: 71.22  | best_test_acc: 71.22 | test acc:71.12 | time:8.6ms
epoch:180 | loss:1.3058 | train acc:99.29 | val acc:74.17 | test_acc_best_val: 71.22  | best_test_acc: 71.22 | test acc:71.66 | time:7.7ms
Run 3/10, best test accuracy: 71.22, acc(last): 71.56, total time: 2.65s
The split is 4
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 4/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:1.9643 | train acc:74.29 | val acc:54.40 | test_acc_best_val: 55.86  | best_test_acc: 55.86 | test acc:55.86 | time:8.5ms
epoch:20 | loss:1.3169 | train acc:98.57 | val acc:68.88 | test_acc_best_val: 67.87  | best_test_acc: 67.87 | test acc:69.52 | time:8.3ms
epoch:40 | loss:1.2897 | train acc:100.00 | val acc:68.69 | test_acc_best_val: 67.87  | best_test_acc: 67.87 | test acc:69.62 | time:8.4ms
epoch:60 | loss:1.3156 | train acc:100.00 | val acc:67.91 | test_acc_best_val: 67.87  | best_test_acc: 67.87 | test acc:70.30 | time:8.5ms
epoch:80 | loss:1.2983 | train acc:100.00 | val acc:68.10 | test_acc_best_val: 67.87  | best_test_acc: 67.87 | test acc:71.07 | time:7.8ms
epoch:100 | loss:1.3135 | train acc:100.00 | val acc:67.51 | test_acc_best_val: 67.87  | best_test_acc: 67.87 | test acc:71.56 | time:7.9ms
epoch:120 | loss:1.3150 | train acc:100.00 | val acc:68.49 | test_acc_best_val: 67.87  | best_test_acc: 67.87 | test acc:70.98 | time:7.9ms
epoch:140 | loss:1.3072 | train acc:100.00 | val acc:68.69 | test_acc_best_val: 67.87  | best_test_acc: 67.87 | test acc:71.80 | time:9.6ms
epoch:160 | loss:1.3121 | train acc:100.00 | val acc:68.30 | test_acc_best_val: 67.87  | best_test_acc: 67.87 | test acc:72.05 | time:8.7ms
epoch:180 | loss:1.2992 | train acc:100.00 | val acc:68.49 | test_acc_best_val: 67.87  | best_test_acc: 67.87 | test acc:72.14 | time:8.3ms
Run 4/10, best test accuracy: 67.87, acc(last): 72.73, total time: 2.56s
The split is 5
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 5/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:1.9692 | train acc:74.29 | val acc:44.23 | test_acc_best_val: 44.19  | best_test_acc: 44.19 | test acc:44.19 | time:8.1ms
epoch:20 | loss:1.3141 | train acc:100.00 | val acc:72.99 | test_acc_best_val: 72.34  | best_test_acc: 72.34 | test acc:72.48 | time:8.7ms
epoch:40 | loss:1.3194 | train acc:100.00 | val acc:74.76 | test_acc_best_val: 72.87  | best_test_acc: 72.87 | test acc:73.07 | time:8.0ms
epoch:60 | loss:1.2967 | train acc:100.00 | val acc:74.17 | test_acc_best_val: 72.87  | best_test_acc: 72.87 | test acc:72.68 | time:9.4ms
epoch:80 | loss:1.2994 | train acc:100.00 | val acc:75.73 | test_acc_best_val: 72.82  | best_test_acc: 72.82 | test acc:72.78 | time:8.3ms
epoch:100 | loss:1.3113 | train acc:100.00 | val acc:75.34 | test_acc_best_val: 72.82  | best_test_acc: 72.82 | test acc:72.63 | time:7.8ms
epoch:120 | loss:1.3024 | train acc:100.00 | val acc:75.15 | test_acc_best_val: 72.82  | best_test_acc: 72.82 | test acc:72.63 | time:8.9ms
epoch:140 | loss:1.3147 | train acc:100.00 | val acc:75.15 | test_acc_best_val: 72.82  | best_test_acc: 72.82 | test acc:72.48 | time:10.7ms
epoch:160 | loss:1.3145 | train acc:100.00 | val acc:75.93 | test_acc_best_val: 72.82  | best_test_acc: 72.82 | test acc:72.39 | time:9.3ms
epoch:180 | loss:1.3142 | train acc:100.00 | val acc:75.54 | test_acc_best_val: 72.53  | best_test_acc: 72.53 | test acc:73.12 | time:8.6ms
Run 5/10, best test accuracy: 72.53, acc(last): 72.92, total time: 2.57s
The split is 6
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 6/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:2.0088 | train acc:65.00 | val acc:37.96 | test_acc_best_val: 41.71  | best_test_acc: 41.71 | test acc:41.71 | time:8.4ms
epoch:20 | loss:1.3323 | train acc:99.29 | val acc:64.77 | test_acc_best_val: 69.76  | best_test_acc: 69.76 | test acc:68.74 | time:9.2ms
epoch:40 | loss:1.3283 | train acc:100.00 | val acc:64.19 | test_acc_best_val: 69.76  | best_test_acc: 69.76 | test acc:68.79 | time:10.4ms
epoch:60 | loss:1.3222 | train acc:100.00 | val acc:64.58 | test_acc_best_val: 69.76  | best_test_acc: 69.76 | test acc:68.55 | time:8.0ms
epoch:80 | loss:1.3236 | train acc:100.00 | val acc:63.41 | test_acc_best_val: 69.76  | best_test_acc: 69.76 | test acc:68.45 | time:8.6ms
epoch:100 | loss:1.3277 | train acc:100.00 | val acc:63.41 | test_acc_best_val: 69.76  | best_test_acc: 69.76 | test acc:68.98 | time:8.5ms
epoch:120 | loss:1.3299 | train acc:100.00 | val acc:63.99 | test_acc_best_val: 69.76  | best_test_acc: 69.76 | test acc:68.74 | time:8.6ms
epoch:140 | loss:1.3636 | train acc:100.00 | val acc:64.19 | test_acc_best_val: 69.76  | best_test_acc: 69.76 | test acc:69.23 | time:8.4ms
epoch:160 | loss:1.3586 | train acc:100.00 | val acc:63.99 | test_acc_best_val: 69.76  | best_test_acc: 69.76 | test acc:68.98 | time:8.2ms
epoch:180 | loss:1.3501 | train acc:100.00 | val acc:63.99 | test_acc_best_val: 69.76  | best_test_acc: 69.76 | test acc:69.28 | time:8.1ms
Run 6/10, best test accuracy: 69.76, acc(last): 69.28, total time: 2.58s
The split is 7
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 7/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:1.9774 | train acc:71.43 | val acc:42.47 | test_acc_best_val: 40.16  | best_test_acc: 40.16 | test acc:40.16 | time:11.7ms
epoch:20 | loss:1.3450 | train acc:100.00 | val acc:70.06 | test_acc_best_val: 70.05  | best_test_acc: 70.05 | test acc:68.11 | time:8.0ms
epoch:40 | loss:1.3160 | train acc:100.00 | val acc:70.06 | test_acc_best_val: 70.05  | best_test_acc: 70.05 | test acc:68.60 | time:8.2ms
epoch:60 | loss:1.2956 | train acc:100.00 | val acc:68.49 | test_acc_best_val: 70.05  | best_test_acc: 70.05 | test acc:68.55 | time:8.3ms
epoch:80 | loss:1.3239 | train acc:100.00 | val acc:69.67 | test_acc_best_val: 70.05  | best_test_acc: 70.05 | test acc:68.74 | time:8.3ms
epoch:100 | loss:1.3172 | train acc:100.00 | val acc:70.84 | test_acc_best_val: 70.05  | best_test_acc: 70.05 | test acc:68.69 | time:7.7ms
epoch:120 | loss:1.3133 | train acc:100.00 | val acc:71.23 | test_acc_best_val: 70.05  | best_test_acc: 70.05 | test acc:68.98 | time:9.0ms
epoch:140 | loss:1.3164 | train acc:100.00 | val acc:71.62 | test_acc_best_val: 70.05  | best_test_acc: 70.05 | test acc:69.23 | time:9.4ms
epoch:160 | loss:1.3152 | train acc:100.00 | val acc:71.82 | test_acc_best_val: 70.05  | best_test_acc: 70.05 | test acc:69.08 | time:8.7ms
epoch:180 | loss:1.3244 | train acc:100.00 | val acc:72.21 | test_acc_best_val: 70.05  | best_test_acc: 70.05 | test acc:69.08 | time:9.5ms
Run 7/10, best test accuracy: 69.03, acc(last): 69.03, total time: 2.60s
The split is 8
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 8/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:2.0123 | train acc:82.14 | val acc:55.77 | test_acc_best_val: 54.89  | best_test_acc: 54.89 | test acc:54.89 | time:10.1ms
epoch:20 | loss:1.3165 | train acc:99.29 | val acc:74.17 | test_acc_best_val: 71.22  | best_test_acc: 71.22 | test acc:71.22 | time:8.1ms
epoch:40 | loss:1.2992 | train acc:100.00 | val acc:75.15 | test_acc_best_val: 70.83  | best_test_acc: 70.83 | test acc:70.83 | time:8.8ms
epoch:60 | loss:1.2961 | train acc:100.00 | val acc:74.95 | test_acc_best_val: 70.83  | best_test_acc: 70.83 | test acc:70.64 | time:9.1ms
epoch:80 | loss:1.2902 | train acc:100.00 | val acc:75.34 | test_acc_best_val: 70.69  | best_test_acc: 70.69 | test acc:70.39 | time:10.6ms
epoch:100 | loss:1.3210 | train acc:100.00 | val acc:75.34 | test_acc_best_val: 70.44  | best_test_acc: 70.44 | test acc:70.44 | time:7.7ms
epoch:120 | loss:1.2924 | train acc:100.00 | val acc:75.54 | test_acc_best_val: 70.44  | best_test_acc: 70.44 | test acc:70.83 | time:8.2ms
epoch:140 | loss:1.3186 | train acc:100.00 | val acc:75.54 | test_acc_best_val: 70.44  | best_test_acc: 70.44 | test acc:70.73 | time:11.5ms
epoch:160 | loss:1.3005 | train acc:100.00 | val acc:75.54 | test_acc_best_val: 70.44  | best_test_acc: 70.44 | test acc:70.88 | time:9.3ms
epoch:180 | loss:1.3078 | train acc:100.00 | val acc:75.54 | test_acc_best_val: 70.44  | best_test_acc: 70.44 | test acc:70.88 | time:8.2ms
Run 8/10, best test accuracy: 70.44, acc(last): 71.27, total time: 2.56s
The split is 9
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 9/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:1.9983 | train acc:73.57 | val acc:49.90 | test_acc_best_val: 46.18  | best_test_acc: 46.18 | test acc:46.18 | time:9.4ms
epoch:20 | loss:1.3093 | train acc:98.57 | val acc:72.99 | test_acc_best_val: 70.78  | best_test_acc: 70.78 | test acc:70.59 | time:8.3ms
epoch:40 | loss:1.3148 | train acc:99.29 | val acc:72.80 | test_acc_best_val: 70.78  | best_test_acc: 70.78 | test acc:71.22 | time:7.6ms
epoch:60 | loss:1.2971 | train acc:99.29 | val acc:74.56 | test_acc_best_val: 70.93  | best_test_acc: 70.93 | test acc:70.88 | time:9.5ms
epoch:80 | loss:1.3129 | train acc:99.29 | val acc:75.34 | test_acc_best_val: 70.83  | best_test_acc: 70.83 | test acc:70.88 | time:7.9ms
epoch:100 | loss:1.2985 | train acc:99.29 | val acc:74.95 | test_acc_best_val: 70.83  | best_test_acc: 70.83 | test acc:71.22 | time:9.4ms
epoch:120 | loss:1.3155 | train acc:99.29 | val acc:74.17 | test_acc_best_val: 70.83  | best_test_acc: 70.83 | test acc:71.32 | time:8.2ms
epoch:140 | loss:1.3311 | train acc:99.29 | val acc:75.15 | test_acc_best_val: 70.83  | best_test_acc: 70.83 | test acc:71.17 | time:8.5ms
epoch:160 | loss:1.3186 | train acc:99.29 | val acc:75.34 | test_acc_best_val: 71.46  | best_test_acc: 71.46 | test acc:71.46 | time:7.9ms
epoch:180 | loss:1.2988 | train acc:99.29 | val acc:75.54 | test_acc_best_val: 71.46  | best_test_acc: 71.46 | test acc:71.51 | time:8.0ms
Run 9/10, best test accuracy: 71.46, acc(last): 71.66, total time: 2.57s
The split is 10
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 10/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:2.0253 | train acc:65.71 | val acc:50.29 | test_acc_best_val: 52.07  | best_test_acc: 52.07 | test acc:52.07 | time:8.7ms
epoch:20 | loss:1.3267 | train acc:100.00 | val acc:70.45 | test_acc_best_val: 72.34  | best_test_acc: 72.34 | test acc:72.34 | time:7.9ms
epoch:40 | loss:1.2974 | train acc:100.00 | val acc:70.25 | test_acc_best_val: 72.24  | best_test_acc: 72.24 | test acc:72.24 | time:8.7ms
epoch:60 | loss:1.3299 | train acc:100.00 | val acc:70.84 | test_acc_best_val: 72.34  | best_test_acc: 72.34 | test acc:72.53 | time:8.2ms
epoch:80 | loss:1.3048 | train acc:100.00 | val acc:71.43 | test_acc_best_val: 73.02  | best_test_acc: 73.02 | test acc:73.07 | time:8.6ms
epoch:100 | loss:1.3122 | train acc:100.00 | val acc:71.04 | test_acc_best_val: 73.02  | best_test_acc: 73.02 | test acc:73.07 | time:8.5ms
epoch:120 | loss:1.2910 | train acc:100.00 | val acc:71.62 | test_acc_best_val: 73.36  | best_test_acc: 73.36 | test acc:73.31 | time:7.9ms
epoch:140 | loss:1.2847 | train acc:100.00 | val acc:71.82 | test_acc_best_val: 73.36  | best_test_acc: 73.36 | test acc:73.41 | time:8.0ms
epoch:160 | loss:1.3117 | train acc:100.00 | val acc:71.43 | test_acc_best_val: 73.36  | best_test_acc: 73.36 | test acc:73.41 | time:8.7ms
epoch:180 | loss:1.3219 | train acc:100.00 | val acc:71.23 | test_acc_best_val: 73.36  | best_test_acc: 73.36 | test acc:73.46 | time:9.3ms
Run 10/10, best test accuracy: 73.36, acc(last): 73.70, total time: 2.54s
The split is 1
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 1/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:2.0422 | train acc:72.86 | val acc:45.40 | test_acc_best_val: 44.92  | best_test_acc: 44.92 | test acc:44.92 | time:9.4ms
epoch:20 | loss:1.3201 | train acc:98.57 | val acc:66.34 | test_acc_best_val: 69.57  | best_test_acc: 69.57 | test acc:69.57 | time:8.0ms
epoch:40 | loss:1.3029 | train acc:99.29 | val acc:67.71 | test_acc_best_val: 70.54  | best_test_acc: 70.54 | test acc:70.54 | time:12.5ms
epoch:60 | loss:1.3046 | train acc:99.29 | val acc:67.32 | test_acc_best_val: 70.54  | best_test_acc: 70.54 | test acc:70.73 | time:9.1ms
epoch:80 | loss:1.3049 | train acc:99.29 | val acc:68.30 | test_acc_best_val: 70.83  | best_test_acc: 70.83 | test acc:71.12 | time:8.2ms
epoch:100 | loss:1.3129 | train acc:99.29 | val acc:68.49 | test_acc_best_val: 71.07  | best_test_acc: 71.07 | test acc:71.61 | time:8.0ms
epoch:120 | loss:1.2980 | train acc:99.29 | val acc:68.88 | test_acc_best_val: 71.95  | best_test_acc: 71.95 | test acc:71.90 | time:9.1ms
epoch:140 | loss:1.3254 | train acc:99.29 | val acc:69.67 | test_acc_best_val: 72.24  | best_test_acc: 72.24 | test acc:72.24 | time:8.7ms
epoch:160 | loss:1.2975 | train acc:99.29 | val acc:69.08 | test_acc_best_val: 72.48  | best_test_acc: 72.48 | test acc:72.34 | time:8.6ms
epoch:180 | loss:1.3117 | train acc:99.29 | val acc:69.67 | test_acc_best_val: 72.48  | best_test_acc: 72.48 | test acc:72.82 | time:8.7ms
Run 1/10, best test accuracy: 72.48, acc(last): 72.82, total time: 2.55s
The split is 2
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 2/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:1.9890 | train acc:65.00 | val acc:41.88 | test_acc_best_val: 44.63  | best_test_acc: 44.63 | test acc:44.63 | time:8.8ms
epoch:20 | loss:1.3325 | train acc:100.00 | val acc:68.69 | test_acc_best_val: 69.28  | best_test_acc: 69.28 | test acc:70.39 | time:8.0ms
epoch:40 | loss:1.3016 | train acc:100.00 | val acc:69.28 | test_acc_best_val: 71.12  | best_test_acc: 71.12 | test acc:70.93 | time:8.0ms
epoch:60 | loss:1.2942 | train acc:100.00 | val acc:70.06 | test_acc_best_val: 71.03  | best_test_acc: 71.03 | test acc:71.03 | time:9.7ms
epoch:80 | loss:1.2878 | train acc:100.00 | val acc:69.08 | test_acc_best_val: 71.03  | best_test_acc: 71.03 | test acc:71.03 | time:7.8ms
epoch:100 | loss:1.3156 | train acc:100.00 | val acc:68.49 | test_acc_best_val: 71.03  | best_test_acc: 71.03 | test acc:71.37 | time:8.2ms
epoch:120 | loss:1.3139 | train acc:100.00 | val acc:69.86 | test_acc_best_val: 71.03  | best_test_acc: 71.03 | test acc:71.75 | time:7.6ms
epoch:140 | loss:1.3110 | train acc:99.29 | val acc:70.65 | test_acc_best_val: 71.90  | best_test_acc: 71.90 | test acc:71.95 | time:9.4ms
epoch:160 | loss:1.3011 | train acc:99.29 | val acc:70.65 | test_acc_best_val: 71.95  | best_test_acc: 71.95 | test acc:72.24 | time:9.8ms
epoch:180 | loss:1.3232 | train acc:99.29 | val acc:70.84 | test_acc_best_val: 72.73  | best_test_acc: 72.73 | test acc:72.44 | time:12.0ms
Run 2/10, best test accuracy: 72.19, acc(last): 72.00, total time: 2.59s
The split is 3
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 3/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:1.9902 | train acc:62.86 | val acc:40.31 | test_acc_best_val: 42.73  | best_test_acc: 42.73 | test acc:42.73 | time:9.7ms
epoch:20 | loss:1.3498 | train acc:99.29 | val acc:68.30 | test_acc_best_val: 69.08  | best_test_acc: 69.08 | test acc:69.28 | time:8.9ms
epoch:40 | loss:1.3431 | train acc:99.29 | val acc:66.93 | test_acc_best_val: 69.08  | best_test_acc: 69.08 | test acc:69.37 | time:8.7ms
epoch:60 | loss:1.3342 | train acc:99.29 | val acc:69.28 | test_acc_best_val: 69.42  | best_test_acc: 69.42 | test acc:69.42 | time:8.6ms
epoch:80 | loss:1.3203 | train acc:99.29 | val acc:68.69 | test_acc_best_val: 69.42  | best_test_acc: 69.42 | test acc:69.42 | time:8.1ms
epoch:100 | loss:1.3148 | train acc:99.29 | val acc:68.10 | test_acc_best_val: 69.42  | best_test_acc: 69.42 | test acc:69.13 | time:7.9ms
epoch:120 | loss:1.3299 | train acc:99.29 | val acc:68.49 | test_acc_best_val: 69.42  | best_test_acc: 69.42 | test acc:69.18 | time:8.7ms
epoch:140 | loss:1.3348 | train acc:99.29 | val acc:69.08 | test_acc_best_val: 69.32  | best_test_acc: 69.32 | test acc:69.47 | time:8.6ms
epoch:160 | loss:1.3373 | train acc:99.29 | val acc:69.67 | test_acc_best_val: 69.32  | best_test_acc: 69.32 | test acc:69.66 | time:7.7ms
epoch:180 | loss:1.3411 | train acc:99.29 | val acc:69.28 | test_acc_best_val: 69.32  | best_test_acc: 69.32 | test acc:70.05 | time:8.5ms
Run 3/10, best test accuracy: 70.35, acc(last): 70.39, total time: 2.94s
The split is 4
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 4/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:1.9749 | train acc:77.14 | val acc:54.21 | test_acc_best_val: 51.48  | best_test_acc: 51.48 | test acc:51.48 | time:9.0ms
epoch:20 | loss:1.3239 | train acc:99.29 | val acc:71.62 | test_acc_best_val: 71.22  | best_test_acc: 71.22 | test acc:71.46 | time:8.1ms
epoch:40 | loss:1.3222 | train acc:100.00 | val acc:70.45 | test_acc_best_val: 71.22  | best_test_acc: 71.22 | test acc:71.22 | time:8.8ms
epoch:60 | loss:1.2887 | train acc:100.00 | val acc:70.84 | test_acc_best_val: 71.22  | best_test_acc: 71.22 | test acc:72.05 | time:8.3ms
epoch:80 | loss:1.3045 | train acc:100.00 | val acc:71.62 | test_acc_best_val: 71.22  | best_test_acc: 71.22 | test acc:72.39 | time:11.8ms
epoch:100 | loss:1.3001 | train acc:100.00 | val acc:71.82 | test_acc_best_val: 71.22  | best_test_acc: 71.22 | test acc:72.53 | time:8.7ms
epoch:120 | loss:1.3216 | train acc:100.00 | val acc:72.21 | test_acc_best_val: 71.22  | best_test_acc: 71.22 | test acc:72.19 | time:8.7ms
epoch:140 | loss:1.3153 | train acc:100.00 | val acc:72.41 | test_acc_best_val: 72.19  | best_test_acc: 72.19 | test acc:72.14 | time:8.4ms
epoch:160 | loss:1.3098 | train acc:100.00 | val acc:72.60 | test_acc_best_val: 72.19  | best_test_acc: 72.19 | test acc:72.39 | time:8.4ms
epoch:180 | loss:1.3301 | train acc:100.00 | val acc:73.19 | test_acc_best_val: 72.19  | best_test_acc: 72.19 | test acc:72.24 | time:8.3ms
Run 4/10, best test accuracy: 72.14, acc(last): 72.19, total time: 2.58s
The split is 5
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 5/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:1.9842 | train acc:76.43 | val acc:51.66 | test_acc_best_val: 52.36  | best_test_acc: 52.36 | test acc:52.36 | time:9.0ms
epoch:20 | loss:1.3248 | train acc:100.00 | val acc:69.86 | test_acc_best_val: 71.27  | best_test_acc: 71.27 | test acc:71.17 | time:8.7ms
epoch:40 | loss:1.3000 | train acc:100.00 | val acc:70.65 | test_acc_best_val: 71.90  | best_test_acc: 71.90 | test acc:72.14 | time:8.5ms
epoch:60 | loss:1.3156 | train acc:100.00 | val acc:71.04 | test_acc_best_val: 71.66  | best_test_acc: 71.66 | test acc:71.90 | time:8.8ms
epoch:80 | loss:1.3051 | train acc:100.00 | val acc:71.04 | test_acc_best_val: 72.34  | best_test_acc: 72.34 | test acc:72.34 | time:8.5ms
epoch:100 | loss:1.3139 | train acc:100.00 | val acc:70.84 | test_acc_best_val: 72.34  | best_test_acc: 72.34 | test acc:72.68 | time:9.4ms
epoch:120 | loss:1.3194 | train acc:100.00 | val acc:70.25 | test_acc_best_val: 72.34  | best_test_acc: 72.34 | test acc:72.48 | time:8.6ms
epoch:140 | loss:1.3179 | train acc:100.00 | val acc:70.45 | test_acc_best_val: 72.34  | best_test_acc: 72.34 | test acc:72.82 | time:8.0ms
epoch:160 | loss:1.3347 | train acc:100.00 | val acc:71.23 | test_acc_best_val: 72.68  | best_test_acc: 72.68 | test acc:72.68 | time:8.1ms
epoch:180 | loss:1.3172 | train acc:100.00 | val acc:71.23 | test_acc_best_val: 72.68  | best_test_acc: 72.68 | test acc:72.68 | time:8.7ms
Run 5/10, best test accuracy: 72.68, acc(last): 72.68, total time: 2.53s
The split is 6
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 6/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:2.0236 | train acc:72.14 | val acc:39.53 | test_acc_best_val: 41.23  | best_test_acc: 41.23 | test acc:41.23 | time:10.0ms
epoch:20 | loss:1.3318 | train acc:100.00 | val acc:64.77 | test_acc_best_val: 66.46  | best_test_acc: 66.46 | test acc:66.31 | time:8.7ms
epoch:40 | loss:1.3346 | train acc:100.00 | val acc:65.75 | test_acc_best_val: 66.70  | best_test_acc: 66.70 | test acc:66.36 | time:9.1ms
epoch:60 | loss:1.3136 | train acc:100.00 | val acc:65.36 | test_acc_best_val: 66.26  | best_test_acc: 66.26 | test acc:66.55 | time:8.8ms
epoch:80 | loss:1.3014 | train acc:100.00 | val acc:66.34 | test_acc_best_val: 66.26  | best_test_acc: 66.26 | test acc:67.33 | time:8.3ms
epoch:100 | loss:1.3190 | train acc:100.00 | val acc:66.14 | test_acc_best_val: 66.26  | best_test_acc: 66.26 | test acc:67.38 | time:8.6ms
epoch:120 | loss:1.3137 | train acc:100.00 | val acc:66.54 | test_acc_best_val: 66.26  | best_test_acc: 66.26 | test acc:67.62 | time:8.1ms
epoch:140 | loss:1.3196 | train acc:100.00 | val acc:66.34 | test_acc_best_val: 66.26  | best_test_acc: 66.26 | test acc:67.91 | time:8.5ms
epoch:160 | loss:1.3146 | train acc:100.00 | val acc:65.95 | test_acc_best_val: 66.26  | best_test_acc: 66.26 | test acc:68.16 | time:9.0ms
epoch:180 | loss:1.3233 | train acc:100.00 | val acc:66.34 | test_acc_best_val: 66.26  | best_test_acc: 66.26 | test acc:67.82 | time:10.5ms
Run 6/10, best test accuracy: 66.26, acc(last): 68.35, total time: 2.64s
The split is 7
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 7/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:1.9898 | train acc:73.57 | val acc:44.42 | test_acc_best_val: 45.07  | best_test_acc: 45.07 | test acc:45.07 | time:9.1ms
epoch:20 | loss:1.3258 | train acc:100.00 | val acc:68.10 | test_acc_best_val: 69.71  | best_test_acc: 69.71 | test acc:69.71 | time:8.1ms
epoch:40 | loss:1.3176 | train acc:100.00 | val acc:67.51 | test_acc_best_val: 69.86  | best_test_acc: 69.86 | test acc:68.84 | time:8.4ms
epoch:60 | loss:1.3353 | train acc:100.00 | val acc:68.10 | test_acc_best_val: 68.89  | best_test_acc: 68.89 | test acc:68.98 | time:9.5ms
epoch:80 | loss:1.3470 | train acc:100.00 | val acc:69.47 | test_acc_best_val: 69.47  | best_test_acc: 69.47 | test acc:69.32 | time:8.1ms
epoch:100 | loss:1.3280 | train acc:100.00 | val acc:70.06 | test_acc_best_val: 69.91  | best_test_acc: 69.91 | test acc:69.91 | time:8.8ms
epoch:120 | loss:1.3238 | train acc:100.00 | val acc:70.45 | test_acc_best_val: 69.81  | best_test_acc: 69.81 | test acc:70.10 | time:8.7ms
epoch:140 | loss:1.3301 | train acc:100.00 | val acc:69.67 | test_acc_best_val: 69.81  | best_test_acc: 69.81 | test acc:70.10 | time:8.8ms
epoch:160 | loss:1.3187 | train acc:100.00 | val acc:69.67 | test_acc_best_val: 69.81  | best_test_acc: 69.81 | test acc:70.20 | time:8.5ms
epoch:180 | loss:1.3239 | train acc:100.00 | val acc:69.47 | test_acc_best_val: 69.81  | best_test_acc: 69.81 | test acc:70.30 | time:9.0ms
Run 7/10, best test accuracy: 69.81, acc(last): 70.69, total time: 2.57s
The split is 8
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 8/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:1.9762 | train acc:74.29 | val acc:48.92 | test_acc_best_val: 51.82  | best_test_acc: 51.82 | test acc:51.82 | time:13.0ms
epoch:20 | loss:1.3443 | train acc:100.00 | val acc:71.62 | test_acc_best_val: 71.90  | best_test_acc: 71.90 | test acc:71.95 | time:8.3ms
epoch:40 | loss:1.3205 | train acc:100.00 | val acc:72.21 | test_acc_best_val: 72.10  | best_test_acc: 72.10 | test acc:71.95 | time:9.5ms
epoch:60 | loss:1.2905 | train acc:100.00 | val acc:70.84 | test_acc_best_val: 72.10  | best_test_acc: 72.10 | test acc:71.22 | time:8.0ms
epoch:80 | loss:1.3077 | train acc:100.00 | val acc:71.62 | test_acc_best_val: 72.10  | best_test_acc: 72.10 | test acc:72.00 | time:8.2ms
epoch:100 | loss:1.3213 | train acc:100.00 | val acc:72.60 | test_acc_best_val: 72.10  | best_test_acc: 72.10 | test acc:72.24 | time:8.0ms
epoch:120 | loss:1.3308 | train acc:100.00 | val acc:72.41 | test_acc_best_val: 72.34  | best_test_acc: 72.34 | test acc:72.48 | time:9.1ms
epoch:140 | loss:1.3158 | train acc:100.00 | val acc:72.21 | test_acc_best_val: 72.34  | best_test_acc: 72.34 | test acc:72.58 | time:7.9ms
epoch:160 | loss:1.3178 | train acc:100.00 | val acc:72.80 | test_acc_best_val: 72.34  | best_test_acc: 72.34 | test acc:72.63 | time:7.8ms
epoch:180 | loss:1.3210 | train acc:99.29 | val acc:73.19 | test_acc_best_val: 72.97  | best_test_acc: 72.97 | test acc:72.97 | time:8.6ms
Run 8/10, best test accuracy: 73.12, acc(last): 73.12, total time: 2.59s
The split is 9
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 9/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:1.9820 | train acc:75.71 | val acc:55.38 | test_acc_best_val: 55.18  | best_test_acc: 55.18 | test acc:55.18 | time:8.6ms
epoch:20 | loss:1.3222 | train acc:98.57 | val acc:69.47 | test_acc_best_val: 72.00  | best_test_acc: 72.00 | test acc:71.71 | time:8.2ms
epoch:40 | loss:1.3169 | train acc:98.57 | val acc:69.08 | test_acc_best_val: 72.00  | best_test_acc: 72.00 | test acc:71.61 | time:10.6ms
epoch:60 | loss:1.3187 | train acc:98.57 | val acc:68.30 | test_acc_best_val: 72.00  | best_test_acc: 72.00 | test acc:71.75 | time:9.4ms
epoch:80 | loss:1.3083 | train acc:99.29 | val acc:68.10 | test_acc_best_val: 72.00  | best_test_acc: 72.00 | test acc:72.10 | time:9.3ms
epoch:100 | loss:1.3067 | train acc:99.29 | val acc:68.88 | test_acc_best_val: 72.00  | best_test_acc: 72.00 | test acc:72.10 | time:9.2ms
epoch:120 | loss:1.3146 | train acc:99.29 | val acc:68.69 | test_acc_best_val: 72.00  | best_test_acc: 72.00 | test acc:71.75 | time:8.9ms
epoch:140 | loss:1.3164 | train acc:99.29 | val acc:68.10 | test_acc_best_val: 72.00  | best_test_acc: 72.00 | test acc:71.66 | time:9.0ms
epoch:160 | loss:1.3003 | train acc:99.29 | val acc:68.69 | test_acc_best_val: 72.00  | best_test_acc: 72.00 | test acc:72.24 | time:9.2ms
epoch:180 | loss:1.3273 | train acc:99.29 | val acc:68.69 | test_acc_best_val: 72.00  | best_test_acc: 72.00 | test acc:72.58 | time:7.9ms
Run 9/10, best test accuracy: 72.00, acc(last): 72.48, total time: 2.75s
The split is 10
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 10/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:1.9879 | train acc:74.29 | val acc:49.90 | test_acc_best_val: 43.95  | best_test_acc: 43.95 | test acc:43.95 | time:8.5ms
epoch:20 | loss:1.3038 | train acc:100.00 | val acc:72.60 | test_acc_best_val: 70.59  | best_test_acc: 70.59 | test acc:70.49 | time:11.0ms
epoch:40 | loss:1.3157 | train acc:100.00 | val acc:73.58 | test_acc_best_val: 69.91  | best_test_acc: 69.91 | test acc:69.91 | time:8.0ms
epoch:60 | loss:1.2961 | train acc:100.00 | val acc:72.02 | test_acc_best_val: 69.96  | best_test_acc: 69.96 | test acc:70.25 | time:8.6ms
epoch:80 | loss:1.3138 | train acc:100.00 | val acc:72.21 | test_acc_best_val: 69.96  | best_test_acc: 69.96 | test acc:70.83 | time:8.5ms
epoch:100 | loss:1.2938 | train acc:100.00 | val acc:72.41 | test_acc_best_val: 69.96  | best_test_acc: 69.96 | test acc:71.56 | time:8.3ms
epoch:120 | loss:1.3139 | train acc:100.00 | val acc:71.23 | test_acc_best_val: 69.96  | best_test_acc: 69.96 | test acc:71.66 | time:8.0ms
epoch:140 | loss:1.3341 | train acc:100.00 | val acc:71.82 | test_acc_best_val: 69.96  | best_test_acc: 69.96 | test acc:71.80 | time:8.4ms
epoch:160 | loss:1.3177 | train acc:100.00 | val acc:72.21 | test_acc_best_val: 69.96  | best_test_acc: 69.96 | test acc:72.24 | time:8.1ms
epoch:180 | loss:1.3062 | train acc:100.00 | val acc:72.41 | test_acc_best_val: 69.96  | best_test_acc: 69.96 | test acc:72.34 | time:9.2ms
Run 10/10, best test accuracy: 69.96, acc(last): 71.95, total time: 2.58s
The split is 1
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 1/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:2.0135 | train acc:80.00 | val acc:52.64 | test_acc_best_val: 54.89  | best_test_acc: 54.89 | test acc:54.89 | time:13.0ms
epoch:20 | loss:1.3323 | train acc:100.00 | val acc:71.23 | test_acc_best_val: 71.75  | best_test_acc: 71.75 | test acc:71.80 | time:7.8ms
epoch:40 | loss:1.2872 | train acc:100.00 | val acc:71.43 | test_acc_best_val: 71.75  | best_test_acc: 71.75 | test acc:71.12 | time:9.1ms
epoch:60 | loss:1.2928 | train acc:100.00 | val acc:71.04 | test_acc_best_val: 71.75  | best_test_acc: 71.75 | test acc:71.75 | time:9.0ms
epoch:80 | loss:1.2870 | train acc:99.29 | val acc:71.23 | test_acc_best_val: 71.75  | best_test_acc: 71.75 | test acc:71.90 | time:7.7ms
epoch:100 | loss:1.3010 | train acc:99.29 | val acc:70.45 | test_acc_best_val: 71.75  | best_test_acc: 71.75 | test acc:72.14 | time:8.9ms
epoch:120 | loss:1.3040 | train acc:99.29 | val acc:71.43 | test_acc_best_val: 71.75  | best_test_acc: 71.75 | test acc:72.82 | time:8.3ms
epoch:140 | loss:1.3108 | train acc:99.29 | val acc:71.43 | test_acc_best_val: 71.75  | best_test_acc: 71.75 | test acc:72.29 | time:9.3ms
epoch:160 | loss:1.3184 | train acc:99.29 | val acc:71.43 | test_acc_best_val: 71.75  | best_test_acc: 71.75 | test acc:72.34 | time:13.2ms
epoch:180 | loss:1.2937 | train acc:99.29 | val acc:71.43 | test_acc_best_val: 71.75  | best_test_acc: 71.75 | test acc:72.39 | time:8.1ms
Run 1/10, best test accuracy: 71.75, acc(last): 71.90, total time: 2.62s
The split is 2
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 2/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:2.0383 | train acc:70.71 | val acc:49.90 | test_acc_best_val: 49.49  | best_test_acc: 49.49 | test acc:49.49 | time:9.7ms
epoch:20 | loss:1.3511 | train acc:98.57 | val acc:69.86 | test_acc_best_val: 70.49  | best_test_acc: 70.49 | test acc:70.20 | time:8.3ms
epoch:40 | loss:1.3274 | train acc:99.29 | val acc:70.65 | test_acc_best_val: 70.49  | best_test_acc: 70.49 | test acc:70.49 | time:8.8ms
epoch:60 | loss:1.2958 | train acc:99.29 | val acc:69.47 | test_acc_best_val: 70.44  | best_test_acc: 70.44 | test acc:70.15 | time:8.7ms
epoch:80 | loss:1.2977 | train acc:99.29 | val acc:70.45 | test_acc_best_val: 70.44  | best_test_acc: 70.44 | test acc:71.03 | time:8.0ms
epoch:100 | loss:1.2978 | train acc:99.29 | val acc:70.45 | test_acc_best_val: 70.44  | best_test_acc: 70.44 | test acc:71.46 | time:8.6ms
epoch:120 | loss:1.3220 | train acc:99.29 | val acc:70.65 | test_acc_best_val: 70.44  | best_test_acc: 70.44 | test acc:71.37 | time:9.6ms
epoch:140 | loss:1.3276 | train acc:99.29 | val acc:71.04 | test_acc_best_val: 70.44  | best_test_acc: 70.44 | test acc:71.80 | time:9.2ms
epoch:160 | loss:1.3028 | train acc:99.29 | val acc:71.43 | test_acc_best_val: 71.85  | best_test_acc: 71.85 | test acc:71.66 | time:7.9ms
epoch:180 | loss:1.3376 | train acc:99.29 | val acc:71.82 | test_acc_best_val: 71.51  | best_test_acc: 71.51 | test acc:71.56 | time:10.3ms
Run 2/10, best test accuracy: 71.85, acc(last): 71.90, total time: 2.60s
The split is 3
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 3/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:1.9354 | train acc:61.43 | val acc:46.18 | test_acc_best_val: 47.93  | best_test_acc: 47.93 | test acc:47.93 | time:9.1ms
epoch:20 | loss:1.3227 | train acc:99.29 | val acc:67.71 | test_acc_best_val: 70.59  | best_test_acc: 70.59 | test acc:70.83 | time:10.3ms
epoch:40 | loss:1.2894 | train acc:99.29 | val acc:67.12 | test_acc_best_val: 70.59  | best_test_acc: 70.59 | test acc:70.64 | time:8.7ms
epoch:60 | loss:1.2999 | train acc:99.29 | val acc:66.73 | test_acc_best_val: 70.59  | best_test_acc: 70.59 | test acc:70.83 | time:8.4ms
epoch:80 | loss:1.2856 | train acc:99.29 | val acc:66.73 | test_acc_best_val: 70.59  | best_test_acc: 70.59 | test acc:71.12 | time:8.2ms
epoch:100 | loss:1.3192 | train acc:99.29 | val acc:67.12 | test_acc_best_val: 70.59  | best_test_acc: 70.59 | test acc:71.03 | time:8.6ms
epoch:120 | loss:1.3252 | train acc:99.29 | val acc:67.51 | test_acc_best_val: 70.59  | best_test_acc: 70.59 | test acc:71.37 | time:8.1ms
epoch:140 | loss:1.3171 | train acc:99.29 | val acc:67.71 | test_acc_best_val: 70.59  | best_test_acc: 70.59 | test acc:71.22 | time:8.1ms
epoch:160 | loss:1.3131 | train acc:99.29 | val acc:67.51 | test_acc_best_val: 70.59  | best_test_acc: 70.59 | test acc:70.93 | time:13.3ms
epoch:180 | loss:1.3172 | train acc:99.29 | val acc:67.32 | test_acc_best_val: 70.59  | best_test_acc: 70.59 | test acc:71.66 | time:8.1ms
Run 3/10, best test accuracy: 70.59, acc(last): 71.85, total time: 2.72s
The split is 4
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 4/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:2.0534 | train acc:74.29 | val acc:49.90 | test_acc_best_val: 49.98  | best_test_acc: 49.98 | test acc:49.98 | time:8.4ms
epoch:20 | loss:1.3208 | train acc:99.29 | val acc:70.65 | test_acc_best_val: 70.25  | best_test_acc: 70.25 | test acc:70.49 | time:8.3ms
epoch:40 | loss:1.3071 | train acc:100.00 | val acc:70.65 | test_acc_best_val: 71.41  | best_test_acc: 71.41 | test acc:71.56 | time:8.3ms
epoch:60 | loss:1.2949 | train acc:100.00 | val acc:70.65 | test_acc_best_val: 71.41  | best_test_acc: 71.41 | test acc:71.95 | time:8.1ms
epoch:80 | loss:1.3028 | train acc:100.00 | val acc:71.62 | test_acc_best_val: 71.41  | best_test_acc: 71.41 | test acc:72.78 | time:8.9ms
epoch:100 | loss:1.3143 | train acc:100.00 | val acc:71.62 | test_acc_best_val: 71.41  | best_test_acc: 71.41 | test acc:72.68 | time:8.0ms
epoch:120 | loss:1.3268 | train acc:100.00 | val acc:71.62 | test_acc_best_val: 71.41  | best_test_acc: 71.41 | test acc:72.92 | time:8.1ms
epoch:140 | loss:1.3313 | train acc:100.00 | val acc:71.62 | test_acc_best_val: 71.41  | best_test_acc: 71.41 | test acc:73.55 | time:9.4ms
epoch:160 | loss:1.3170 | train acc:100.00 | val acc:72.02 | test_acc_best_val: 73.36  | best_test_acc: 73.36 | test acc:73.31 | time:8.2ms
epoch:180 | loss:1.3165 | train acc:100.00 | val acc:72.60 | test_acc_best_val: 73.85  | best_test_acc: 73.85 | test acc:73.46 | time:8.6ms
Run 4/10, best test accuracy: 73.85, acc(last): 73.31, total time: 2.59s
The split is 5
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 5/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:1.9967 | train acc:75.71 | val acc:53.82 | test_acc_best_val: 53.28  | best_test_acc: 53.28 | test acc:53.28 | time:8.5ms
epoch:20 | loss:1.3204 | train acc:100.00 | val acc:69.67 | test_acc_best_val: 72.82  | best_test_acc: 72.82 | test acc:71.56 | time:8.5ms
epoch:40 | loss:1.3034 | train acc:100.00 | val acc:69.08 | test_acc_best_val: 72.82  | best_test_acc: 72.82 | test acc:71.80 | time:8.1ms
epoch:60 | loss:1.3155 | train acc:100.00 | val acc:69.86 | test_acc_best_val: 72.82  | best_test_acc: 72.82 | test acc:71.46 | time:9.8ms
epoch:80 | loss:1.3005 | train acc:100.00 | val acc:69.28 | test_acc_best_val: 72.82  | best_test_acc: 72.82 | test acc:70.98 | time:8.8ms
epoch:100 | loss:1.3139 | train acc:100.00 | val acc:69.28 | test_acc_best_val: 72.82  | best_test_acc: 72.82 | test acc:71.56 | time:8.7ms
epoch:120 | loss:1.3122 | train acc:100.00 | val acc:69.86 | test_acc_best_val: 72.82  | best_test_acc: 72.82 | test acc:71.61 | time:8.5ms
epoch:140 | loss:1.3238 | train acc:100.00 | val acc:69.86 | test_acc_best_val: 72.82  | best_test_acc: 72.82 | test acc:71.90 | time:8.4ms
epoch:160 | loss:1.3177 | train acc:100.00 | val acc:69.67 | test_acc_best_val: 72.82  | best_test_acc: 72.82 | test acc:72.10 | time:10.6ms
epoch:180 | loss:1.3146 | train acc:100.00 | val acc:70.65 | test_acc_best_val: 72.48  | best_test_acc: 72.48 | test acc:72.44 | time:9.0ms
Run 5/10, best test accuracy: 72.39, acc(last): 72.53, total time: 2.74s
The split is 6
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 6/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:1.9812 | train acc:68.57 | val acc:41.68 | test_acc_best_val: 43.27  | best_test_acc: 43.27 | test acc:43.27 | time:8.7ms
epoch:20 | loss:1.3261 | train acc:99.29 | val acc:70.65 | test_acc_best_val: 69.66  | best_test_acc: 69.66 | test acc:69.66 | time:9.2ms
epoch:40 | loss:1.3196 | train acc:100.00 | val acc:69.86 | test_acc_best_val: 69.66  | best_test_acc: 69.66 | test acc:69.47 | time:8.1ms
epoch:60 | loss:1.3319 | train acc:100.00 | val acc:69.47 | test_acc_best_val: 69.66  | best_test_acc: 69.66 | test acc:68.98 | time:9.0ms
epoch:80 | loss:1.3311 | train acc:100.00 | val acc:70.06 | test_acc_best_val: 69.66  | best_test_acc: 69.66 | test acc:68.79 | time:8.2ms
epoch:100 | loss:1.3154 | train acc:100.00 | val acc:69.47 | test_acc_best_val: 69.66  | best_test_acc: 69.66 | test acc:69.71 | time:7.9ms
epoch:120 | loss:1.3471 | train acc:100.00 | val acc:70.06 | test_acc_best_val: 69.66  | best_test_acc: 69.66 | test acc:69.71 | time:8.2ms
epoch:140 | loss:1.3301 | train acc:100.00 | val acc:69.08 | test_acc_best_val: 69.66  | best_test_acc: 69.66 | test acc:69.86 | time:9.4ms
epoch:160 | loss:1.3458 | train acc:100.00 | val acc:68.69 | test_acc_best_val: 69.66  | best_test_acc: 69.66 | test acc:69.66 | time:8.2ms
epoch:180 | loss:1.3186 | train acc:100.00 | val acc:69.67 | test_acc_best_val: 69.66  | best_test_acc: 69.66 | test acc:69.86 | time:9.1ms
Run 6/10, best test accuracy: 69.66, acc(last): 69.96, total time: 2.57s
The split is 7
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 7/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:1.9956 | train acc:62.14 | val acc:32.29 | test_acc_best_val: 33.01  | best_test_acc: 33.01 | test acc:33.01 | time:8.8ms
epoch:20 | loss:1.3290 | train acc:100.00 | val acc:67.32 | test_acc_best_val: 67.53  | best_test_acc: 67.53 | test acc:67.53 | time:8.6ms
epoch:40 | loss:1.3401 | train acc:100.00 | val acc:68.30 | test_acc_best_val: 67.48  | best_test_acc: 67.48 | test acc:68.89 | time:8.3ms
epoch:60 | loss:1.3098 | train acc:100.00 | val acc:68.88 | test_acc_best_val: 70.05  | best_test_acc: 70.05 | test acc:70.10 | time:7.7ms
epoch:80 | loss:1.3054 | train acc:100.00 | val acc:69.86 | test_acc_best_val: 70.59  | best_test_acc: 70.59 | test acc:71.22 | time:8.1ms
epoch:100 | loss:1.3145 | train acc:100.00 | val acc:70.45 | test_acc_best_val: 71.27  | best_test_acc: 71.27 | test acc:71.22 | time:12.5ms
epoch:120 | loss:1.3140 | train acc:100.00 | val acc:70.25 | test_acc_best_val: 71.27  | best_test_acc: 71.27 | test acc:71.56 | time:7.8ms
epoch:140 | loss:1.3161 | train acc:100.00 | val acc:70.65 | test_acc_best_val: 71.27  | best_test_acc: 71.27 | test acc:71.66 | time:8.3ms
epoch:160 | loss:1.3314 | train acc:100.00 | val acc:70.45 | test_acc_best_val: 71.27  | best_test_acc: 71.27 | test acc:71.71 | time:8.1ms
epoch:180 | loss:1.3372 | train acc:100.00 | val acc:70.25 | test_acc_best_val: 71.27  | best_test_acc: 71.27 | test acc:71.71 | time:8.8ms
Run 7/10, best test accuracy: 71.27, acc(last): 71.80, total time: 2.55s
The split is 8
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 8/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:1.9798 | train acc:81.43 | val acc:55.19 | test_acc_best_val: 54.21  | best_test_acc: 54.21 | test acc:54.21 | time:9.7ms
epoch:20 | loss:1.2950 | train acc:99.29 | val acc:70.25 | test_acc_best_val: 70.88  | best_test_acc: 70.88 | test acc:70.88 | time:8.5ms
epoch:40 | loss:1.3091 | train acc:100.00 | val acc:70.45 | test_acc_best_val: 70.83  | best_test_acc: 70.83 | test acc:70.98 | time:9.0ms
epoch:60 | loss:1.2936 | train acc:100.00 | val acc:70.25 | test_acc_best_val: 70.83  | best_test_acc: 70.83 | test acc:71.37 | time:8.0ms
epoch:80 | loss:1.2840 | train acc:100.00 | val acc:70.45 | test_acc_best_val: 70.83  | best_test_acc: 70.83 | test acc:71.12 | time:10.7ms
epoch:100 | loss:1.2908 | train acc:100.00 | val acc:69.47 | test_acc_best_val: 70.83  | best_test_acc: 70.83 | test acc:71.22 | time:8.1ms
epoch:120 | loss:1.3083 | train acc:100.00 | val acc:69.67 | test_acc_best_val: 70.83  | best_test_acc: 70.83 | test acc:71.32 | time:8.7ms
epoch:140 | loss:1.3125 | train acc:100.00 | val acc:69.86 | test_acc_best_val: 70.83  | best_test_acc: 70.83 | test acc:71.27 | time:8.1ms
epoch:160 | loss:1.3339 | train acc:99.29 | val acc:70.45 | test_acc_best_val: 70.83  | best_test_acc: 70.83 | test acc:71.27 | time:7.9ms
epoch:180 | loss:1.3113 | train acc:99.29 | val acc:70.45 | test_acc_best_val: 70.83  | best_test_acc: 70.83 | test acc:72.24 | time:9.7ms
Run 8/10, best test accuracy: 72.58, acc(last): 72.58, total time: 2.62s
The split is 9
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 9/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:2.0075 | train acc:80.00 | val acc:55.38 | test_acc_best_val: 57.12  | best_test_acc: 57.12 | test acc:57.12 | time:8.8ms
epoch:20 | loss:1.3249 | train acc:97.86 | val acc:70.06 | test_acc_best_val: 70.93  | best_test_acc: 70.93 | test acc:71.12 | time:7.8ms
epoch:40 | loss:1.3162 | train acc:99.29 | val acc:67.12 | test_acc_best_val: 70.93  | best_test_acc: 70.93 | test acc:70.54 | time:8.6ms
epoch:60 | loss:1.2926 | train acc:99.29 | val acc:69.47 | test_acc_best_val: 70.93  | best_test_acc: 70.93 | test acc:70.78 | time:7.9ms
epoch:80 | loss:1.2944 | train acc:99.29 | val acc:69.47 | test_acc_best_val: 70.93  | best_test_acc: 70.93 | test acc:71.61 | time:9.3ms
epoch:100 | loss:1.3017 | train acc:99.29 | val acc:69.67 | test_acc_best_val: 70.93  | best_test_acc: 70.93 | test acc:71.75 | time:8.8ms
epoch:120 | loss:1.2905 | train acc:99.29 | val acc:69.47 | test_acc_best_val: 70.93  | best_test_acc: 70.93 | test acc:71.61 | time:11.1ms
epoch:140 | loss:1.3050 | train acc:99.29 | val acc:69.67 | test_acc_best_val: 70.93  | best_test_acc: 70.93 | test acc:72.00 | time:8.7ms
epoch:160 | loss:1.3119 | train acc:99.29 | val acc:70.25 | test_acc_best_val: 70.93  | best_test_acc: 70.93 | test acc:71.85 | time:8.4ms
epoch:180 | loss:1.3044 | train acc:99.29 | val acc:70.84 | test_acc_best_val: 70.93  | best_test_acc: 70.93 | test acc:71.95 | time:9.5ms
Run 9/10, best test accuracy: 70.93, acc(last): 72.10, total time: 2.77s
The split is 10
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 10/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:1.9690 | train acc:75.71 | val acc:58.51 | test_acc_best_val: 56.98  | best_test_acc: 56.98 | test acc:56.98 | time:8.8ms
epoch:20 | loss:1.3244 | train acc:100.00 | val acc:72.21 | test_acc_best_val: 72.48  | best_test_acc: 72.48 | test acc:71.75 | time:8.1ms
epoch:40 | loss:1.3203 | train acc:100.00 | val acc:70.45 | test_acc_best_val: 72.48  | best_test_acc: 72.48 | test acc:71.61 | time:8.0ms
epoch:60 | loss:1.3208 | train acc:100.00 | val acc:71.43 | test_acc_best_val: 72.48  | best_test_acc: 72.48 | test acc:72.34 | time:8.4ms
epoch:80 | loss:1.3031 | train acc:100.00 | val acc:71.43 | test_acc_best_val: 72.48  | best_test_acc: 72.48 | test acc:72.39 | time:8.3ms
epoch:100 | loss:1.3153 | train acc:100.00 | val acc:72.80 | test_acc_best_val: 72.73  | best_test_acc: 72.73 | test acc:72.53 | time:9.1ms
epoch:120 | loss:1.3085 | train acc:100.00 | val acc:73.19 | test_acc_best_val: 72.73  | best_test_acc: 72.73 | test acc:72.53 | time:8.8ms
epoch:140 | loss:1.3146 | train acc:100.00 | val acc:73.58 | test_acc_best_val: 72.63  | best_test_acc: 72.63 | test acc:72.78 | time:9.4ms
epoch:160 | loss:1.3259 | train acc:100.00 | val acc:74.17 | test_acc_best_val: 73.31  | best_test_acc: 73.31 | test acc:73.31 | time:8.7ms
epoch:180 | loss:1.3136 | train acc:100.00 | val acc:73.97 | test_acc_best_val: 73.31  | best_test_acc: 73.31 | test acc:72.92 | time:9.6ms
Run 10/10, best test accuracy: 73.02, acc(last): 73.07, total time: 2.55s
The split is 1
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 1/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:1.9895 | train acc:85.00 | val acc:59.88 | test_acc_best_val: 61.21  | best_test_acc: 61.21 | test acc:61.21 | time:8.8ms
epoch:20 | loss:1.3151 | train acc:100.00 | val acc:67.51 | test_acc_best_val: 71.66  | best_test_acc: 71.66 | test acc:72.00 | time:8.0ms
epoch:40 | loss:1.2980 | train acc:99.29 | val acc:68.69 | test_acc_best_val: 71.61  | best_test_acc: 71.61 | test acc:71.61 | time:8.7ms
epoch:60 | loss:1.2889 | train acc:99.29 | val acc:69.08 | test_acc_best_val: 71.75  | best_test_acc: 71.75 | test acc:71.51 | time:8.2ms
epoch:80 | loss:1.2776 | train acc:99.29 | val acc:69.28 | test_acc_best_val: 71.90  | best_test_acc: 71.90 | test acc:72.19 | time:8.1ms
epoch:100 | loss:1.3207 | train acc:99.29 | val acc:70.06 | test_acc_best_val: 72.24  | best_test_acc: 72.24 | test acc:72.24 | time:8.1ms
epoch:120 | loss:1.3053 | train acc:99.29 | val acc:70.84 | test_acc_best_val: 71.95  | best_test_acc: 71.95 | test acc:71.95 | time:11.5ms
epoch:140 | loss:1.3219 | train acc:99.29 | val acc:70.65 | test_acc_best_val: 71.95  | best_test_acc: 71.95 | test acc:72.68 | time:7.5ms
epoch:160 | loss:1.2944 | train acc:99.29 | val acc:70.84 | test_acc_best_val: 71.95  | best_test_acc: 71.95 | test acc:72.82 | time:8.6ms
epoch:180 | loss:1.3014 | train acc:99.29 | val acc:70.06 | test_acc_best_val: 72.82  | best_test_acc: 72.82 | test acc:72.73 | time:8.0ms
Run 1/10, best test accuracy: 72.82, acc(last): 72.82, total time: 2.55s
The split is 2
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 2/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:1.9763 | train acc:73.57 | val acc:46.18 | test_acc_best_val: 50.46  | best_test_acc: 50.46 | test acc:50.46 | time:8.5ms
epoch:20 | loss:1.3203 | train acc:100.00 | val acc:71.04 | test_acc_best_val: 70.93  | best_test_acc: 70.93 | test acc:70.88 | time:8.9ms
epoch:40 | loss:1.3026 | train acc:100.00 | val acc:69.47 | test_acc_best_val: 70.93  | best_test_acc: 70.93 | test acc:70.54 | time:8.2ms
epoch:60 | loss:1.2904 | train acc:100.00 | val acc:68.88 | test_acc_best_val: 70.93  | best_test_acc: 70.93 | test acc:71.37 | time:13.0ms
epoch:80 | loss:1.2936 | train acc:100.00 | val acc:70.06 | test_acc_best_val: 70.93  | best_test_acc: 70.93 | test acc:71.03 | time:10.0ms
epoch:100 | loss:1.3175 | train acc:100.00 | val acc:69.67 | test_acc_best_val: 70.93  | best_test_acc: 70.93 | test acc:71.90 | time:8.8ms
epoch:120 | loss:1.2958 | train acc:100.00 | val acc:70.06 | test_acc_best_val: 70.93  | best_test_acc: 70.93 | test acc:71.75 | time:8.8ms
epoch:140 | loss:1.3216 | train acc:100.00 | val acc:70.65 | test_acc_best_val: 70.93  | best_test_acc: 70.93 | test acc:71.56 | time:8.4ms
epoch:160 | loss:1.2883 | train acc:100.00 | val acc:70.25 | test_acc_best_val: 70.93  | best_test_acc: 70.93 | test acc:71.56 | time:8.6ms
epoch:180 | loss:1.3082 | train acc:100.00 | val acc:70.06 | test_acc_best_val: 70.93  | best_test_acc: 70.93 | test acc:71.80 | time:8.4ms
Run 2/10, best test accuracy: 70.93, acc(last): 72.00, total time: 2.67s
The split is 3
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 3/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:2.0280 | train acc:70.71 | val acc:45.40 | test_acc_best_val: 48.66  | best_test_acc: 48.66 | test acc:48.66 | time:8.9ms
epoch:20 | loss:1.3325 | train acc:98.57 | val acc:69.67 | test_acc_best_val: 70.69  | best_test_acc: 70.69 | test acc:70.25 | time:8.8ms
epoch:40 | loss:1.3002 | train acc:99.29 | val acc:69.86 | test_acc_best_val: 70.69  | best_test_acc: 70.69 | test acc:71.37 | time:9.2ms
epoch:60 | loss:1.3405 | train acc:99.29 | val acc:69.28 | test_acc_best_val: 70.69  | best_test_acc: 70.69 | test acc:71.51 | time:10.2ms
epoch:80 | loss:1.3050 | train acc:99.29 | val acc:67.91 | test_acc_best_val: 70.69  | best_test_acc: 70.69 | test acc:71.17 | time:8.2ms
epoch:100 | loss:1.3001 | train acc:99.29 | val acc:68.49 | test_acc_best_val: 70.69  | best_test_acc: 70.69 | test acc:71.12 | time:8.8ms
epoch:120 | loss:1.3187 | train acc:99.29 | val acc:69.08 | test_acc_best_val: 70.69  | best_test_acc: 70.69 | test acc:71.66 | time:9.1ms
epoch:140 | loss:1.2976 | train acc:99.29 | val acc:68.88 | test_acc_best_val: 70.69  | best_test_acc: 70.69 | test acc:71.66 | time:7.8ms
epoch:160 | loss:1.3128 | train acc:99.29 | val acc:69.28 | test_acc_best_val: 70.69  | best_test_acc: 70.69 | test acc:72.05 | time:8.6ms
epoch:180 | loss:1.3131 | train acc:99.29 | val acc:68.49 | test_acc_best_val: 70.69  | best_test_acc: 70.69 | test acc:72.10 | time:9.5ms
Run 3/10, best test accuracy: 70.69, acc(last): 71.71, total time: 2.66s
The split is 4
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 4/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:1.9830 | train acc:81.43 | val acc:54.60 | test_acc_best_val: 53.38  | best_test_acc: 53.38 | test acc:53.38 | time:10.5ms
epoch:20 | loss:1.3169 | train acc:100.00 | val acc:70.25 | test_acc_best_val: 71.07  | best_test_acc: 71.07 | test acc:71.61 | time:8.2ms
epoch:40 | loss:1.2973 | train acc:100.00 | val acc:69.86 | test_acc_best_val: 71.07  | best_test_acc: 71.07 | test acc:71.27 | time:8.4ms
epoch:60 | loss:1.3007 | train acc:100.00 | val acc:69.28 | test_acc_best_val: 71.07  | best_test_acc: 71.07 | test acc:71.75 | time:8.7ms
epoch:80 | loss:1.2920 | train acc:100.00 | val acc:69.67 | test_acc_best_val: 71.07  | best_test_acc: 71.07 | test acc:72.34 | time:7.5ms
epoch:100 | loss:1.3087 | train acc:100.00 | val acc:71.23 | test_acc_best_val: 72.44  | best_test_acc: 72.44 | test acc:72.82 | time:8.0ms
epoch:120 | loss:1.2993 | train acc:100.00 | val acc:71.82 | test_acc_best_val: 72.68  | best_test_acc: 72.68 | test acc:72.78 | time:35.2ms
epoch:140 | loss:1.3170 | train acc:100.00 | val acc:72.21 | test_acc_best_val: 73.07  | best_test_acc: 73.07 | test acc:73.21 | time:8.8ms
epoch:160 | loss:1.3107 | train acc:100.00 | val acc:72.02 | test_acc_best_val: 73.36  | best_test_acc: 73.36 | test acc:73.36 | time:12.8ms
epoch:180 | loss:1.3172 | train acc:100.00 | val acc:71.43 | test_acc_best_val: 73.36  | best_test_acc: 73.36 | test acc:73.21 | time:8.0ms
Run 4/10, best test accuracy: 73.36, acc(last): 73.12, total time: 2.79s
The split is 5
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 5/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:2.0202 | train acc:81.43 | val acc:56.95 | test_acc_best_val: 56.39  | best_test_acc: 56.39 | test acc:56.39 | time:9.1ms
epoch:20 | loss:1.3458 | train acc:100.00 | val acc:72.02 | test_acc_best_val: 72.97  | best_test_acc: 72.97 | test acc:73.21 | time:8.1ms
epoch:40 | loss:1.3154 | train acc:100.00 | val acc:72.21 | test_acc_best_val: 73.51  | best_test_acc: 73.51 | test acc:73.41 | time:8.3ms
epoch:60 | loss:1.3358 | train acc:100.00 | val acc:71.43 | test_acc_best_val: 73.51  | best_test_acc: 73.51 | test acc:73.16 | time:8.1ms
epoch:80 | loss:1.3091 | train acc:100.00 | val acc:72.60 | test_acc_best_val: 73.51  | best_test_acc: 73.51 | test acc:73.26 | time:8.1ms
epoch:100 | loss:1.3453 | train acc:100.00 | val acc:72.99 | test_acc_best_val: 73.51  | best_test_acc: 73.51 | test acc:73.46 | time:9.1ms
epoch:120 | loss:1.3238 | train acc:100.00 | val acc:72.99 | test_acc_best_val: 73.51  | best_test_acc: 73.51 | test acc:73.31 | time:8.7ms
epoch:140 | loss:1.3265 | train acc:100.00 | val acc:72.99 | test_acc_best_val: 73.51  | best_test_acc: 73.51 | test acc:73.36 | time:8.4ms
epoch:160 | loss:1.3294 | train acc:100.00 | val acc:72.99 | test_acc_best_val: 73.36  | best_test_acc: 73.36 | test acc:73.21 | time:8.0ms
epoch:180 | loss:1.3335 | train acc:100.00 | val acc:72.41 | test_acc_best_val: 73.36  | best_test_acc: 73.36 | test acc:73.21 | time:8.3ms
Run 5/10, best test accuracy: 73.36, acc(last): 73.12, total time: 2.53s
The split is 6
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 6/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:2.0393 | train acc:70.00 | val acc:42.66 | test_acc_best_val: 43.90  | best_test_acc: 43.90 | test acc:43.90 | time:9.0ms
epoch:20 | loss:1.3247 | train acc:99.29 | val acc:66.73 | test_acc_best_val: 69.62  | best_test_acc: 69.62 | test acc:67.62 | time:8.3ms
epoch:40 | loss:1.3039 | train acc:100.00 | val acc:65.95 | test_acc_best_val: 69.62  | best_test_acc: 69.62 | test acc:65.87 | time:8.3ms
epoch:60 | loss:1.2942 | train acc:100.00 | val acc:67.51 | test_acc_best_val: 69.62  | best_test_acc: 69.62 | test acc:66.80 | time:8.3ms
epoch:80 | loss:1.2899 | train acc:100.00 | val acc:66.34 | test_acc_best_val: 69.62  | best_test_acc: 69.62 | test acc:66.65 | time:13.0ms
epoch:100 | loss:1.3064 | train acc:100.00 | val acc:67.51 | test_acc_best_val: 69.62  | best_test_acc: 69.62 | test acc:67.14 | time:10.1ms
epoch:120 | loss:1.3248 | train acc:100.00 | val acc:68.30 | test_acc_best_val: 69.62  | best_test_acc: 69.62 | test acc:67.57 | time:8.5ms
epoch:140 | loss:1.3103 | train acc:100.00 | val acc:67.91 | test_acc_best_val: 69.62  | best_test_acc: 69.62 | test acc:67.53 | time:8.2ms
epoch:160 | loss:1.3061 | train acc:100.00 | val acc:67.51 | test_acc_best_val: 69.62  | best_test_acc: 69.62 | test acc:67.57 | time:8.4ms
epoch:180 | loss:1.3240 | train acc:100.00 | val acc:67.71 | test_acc_best_val: 69.62  | best_test_acc: 69.62 | test acc:68.21 | time:8.7ms
Run 6/10, best test accuracy: 69.62, acc(last): 67.87, total time: 2.84s
The split is 7
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 7/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:2.0042 | train acc:65.00 | val acc:37.38 | test_acc_best_val: 39.67  | best_test_acc: 39.67 | test acc:39.67 | time:8.7ms
epoch:20 | loss:1.3541 | train acc:100.00 | val acc:66.54 | test_acc_best_val: 66.75  | best_test_acc: 66.75 | test acc:67.77 | time:8.0ms
epoch:40 | loss:1.3120 | train acc:100.00 | val acc:67.12 | test_acc_best_val: 68.11  | best_test_acc: 68.11 | test acc:68.11 | time:11.4ms
epoch:60 | loss:1.3163 | train acc:100.00 | val acc:68.30 | test_acc_best_val: 69.76  | best_test_acc: 69.76 | test acc:69.76 | time:8.6ms
epoch:80 | loss:1.3130 | train acc:100.00 | val acc:68.49 | test_acc_best_val: 69.86  | best_test_acc: 69.86 | test acc:69.96 | time:8.4ms
epoch:100 | loss:1.3251 | train acc:100.00 | val acc:69.28 | test_acc_best_val: 70.10  | best_test_acc: 70.10 | test acc:70.15 | time:8.9ms
epoch:120 | loss:1.3308 | train acc:100.00 | val acc:70.25 | test_acc_best_val: 70.20  | best_test_acc: 70.20 | test acc:70.20 | time:7.9ms
epoch:140 | loss:1.3266 | train acc:100.00 | val acc:70.25 | test_acc_best_val: 70.20  | best_test_acc: 70.20 | test acc:70.30 | time:9.0ms
epoch:160 | loss:1.3247 | train acc:100.00 | val acc:70.06 | test_acc_best_val: 70.20  | best_test_acc: 70.20 | test acc:69.76 | time:7.9ms
epoch:180 | loss:1.3246 | train acc:100.00 | val acc:70.65 | test_acc_best_val: 70.59  | best_test_acc: 70.59 | test acc:70.64 | time:8.2ms
Run 7/10, best test accuracy: 70.59, acc(last): 71.27, total time: 2.62s
The split is 8
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 8/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:1.9955 | train acc:56.43 | val acc:33.86 | test_acc_best_val: 38.07  | best_test_acc: 38.07 | test acc:38.07 | time:10.1ms
epoch:20 | loss:1.3419 | train acc:99.29 | val acc:69.67 | test_acc_best_val: 71.85  | best_test_acc: 71.85 | test acc:71.46 | time:9.3ms
epoch:40 | loss:1.3014 | train acc:99.29 | val acc:70.65 | test_acc_best_val: 71.95  | best_test_acc: 71.95 | test acc:72.58 | time:8.7ms
epoch:60 | loss:1.3240 | train acc:99.29 | val acc:70.65 | test_acc_best_val: 72.87  | best_test_acc: 72.87 | test acc:73.12 | time:8.2ms
epoch:80 | loss:1.3199 | train acc:99.29 | val acc:71.23 | test_acc_best_val: 73.41  | best_test_acc: 73.41 | test acc:73.36 | time:10.7ms
epoch:100 | loss:1.3265 | train acc:99.29 | val acc:71.62 | test_acc_best_val: 73.36  | best_test_acc: 73.36 | test acc:72.97 | time:9.0ms
epoch:120 | loss:1.3258 | train acc:99.29 | val acc:71.43 | test_acc_best_val: 72.97  | best_test_acc: 72.97 | test acc:73.02 | time:8.4ms
epoch:140 | loss:1.3178 | train acc:98.57 | val acc:71.43 | test_acc_best_val: 72.97  | best_test_acc: 72.97 | test acc:72.87 | time:9.4ms
epoch:160 | loss:1.3196 | train acc:98.57 | val acc:72.21 | test_acc_best_val: 73.07  | best_test_acc: 73.07 | test acc:73.12 | time:8.9ms
epoch:180 | loss:1.3354 | train acc:98.57 | val acc:72.60 | test_acc_best_val: 73.31  | best_test_acc: 73.31 | test acc:73.51 | time:8.2ms
Run 8/10, best test accuracy: 73.85, acc(last): 73.65, total time: 2.66s
The split is 9
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 9/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:1.9825 | train acc:51.43 | val acc:36.99 | test_acc_best_val: 32.96  | best_test_acc: 32.96 | test acc:32.96 | time:8.4ms
epoch:20 | loss:1.3086 | train acc:98.57 | val acc:71.23 | test_acc_best_val: 71.71  | best_test_acc: 71.71 | test acc:72.24 | time:8.2ms
epoch:40 | loss:1.2984 | train acc:99.29 | val acc:72.02 | test_acc_best_val: 71.71  | best_test_acc: 71.71 | test acc:71.12 | time:8.2ms
epoch:60 | loss:1.2983 | train acc:99.29 | val acc:71.04 | test_acc_best_val: 71.71  | best_test_acc: 71.71 | test acc:71.37 | time:7.8ms
epoch:80 | loss:1.3058 | train acc:99.29 | val acc:71.43 | test_acc_best_val: 71.71  | best_test_acc: 71.71 | test acc:71.37 | time:9.3ms
epoch:100 | loss:1.3173 | train acc:99.29 | val acc:71.04 | test_acc_best_val: 71.71  | best_test_acc: 71.71 | test acc:72.00 | time:9.4ms
epoch:120 | loss:1.2958 | train acc:99.29 | val acc:71.23 | test_acc_best_val: 71.71  | best_test_acc: 71.71 | test acc:72.19 | time:8.3ms
epoch:140 | loss:1.2985 | train acc:99.29 | val acc:71.43 | test_acc_best_val: 71.71  | best_test_acc: 71.71 | test acc:72.34 | time:13.1ms
epoch:160 | loss:1.3145 | train acc:99.29 | val acc:71.43 | test_acc_best_val: 71.71  | best_test_acc: 71.71 | test acc:72.87 | time:8.2ms
epoch:180 | loss:1.3185 | train acc:99.29 | val acc:72.21 | test_acc_best_val: 71.71  | best_test_acc: 71.71 | test acc:72.97 | time:9.2ms
Run 9/10, best test accuracy: 71.71, acc(last): 73.02, total time: 2.56s
The split is 10
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 10/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:1.9843 | train acc:74.29 | val acc:47.16 | test_acc_best_val: 45.94  | best_test_acc: 45.94 | test acc:45.94 | time:8.4ms
epoch:20 | loss:1.3539 | train acc:99.29 | val acc:70.84 | test_acc_best_val: 72.14  | best_test_acc: 72.14 | test acc:71.75 | time:8.5ms
epoch:40 | loss:1.3083 | train acc:100.00 | val acc:71.23 | test_acc_best_val: 72.14  | best_test_acc: 72.14 | test acc:71.85 | time:9.3ms
epoch:60 | loss:1.2911 | train acc:100.00 | val acc:72.60 | test_acc_best_val: 71.85  | best_test_acc: 71.85 | test acc:71.75 | time:9.7ms
epoch:80 | loss:1.3307 | train acc:100.00 | val acc:72.60 | test_acc_best_val: 71.85  | best_test_acc: 71.85 | test acc:71.51 | time:7.9ms
epoch:100 | loss:1.3107 | train acc:100.00 | val acc:73.39 | test_acc_best_val: 71.46  | best_test_acc: 71.46 | test acc:71.46 | time:11.9ms
epoch:120 | loss:1.3083 | train acc:100.00 | val acc:72.99 | test_acc_best_val: 71.46  | best_test_acc: 71.46 | test acc:71.27 | time:8.4ms
epoch:140 | loss:1.3045 | train acc:100.00 | val acc:73.39 | test_acc_best_val: 71.17  | best_test_acc: 71.17 | test acc:71.56 | time:9.5ms
epoch:160 | loss:1.3227 | train acc:100.00 | val acc:73.58 | test_acc_best_val: 71.17  | best_test_acc: 71.17 | test acc:71.71 | time:9.1ms
epoch:180 | loss:1.3330 | train acc:100.00 | val acc:74.17 | test_acc_best_val: 72.19  | best_test_acc: 72.19 | test acc:72.10 | time:8.1ms
Run 10/10, best test accuracy: 72.19, acc(last): 72.19, total time: 2.57s
The split is 1
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 1/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:1.9580 | train acc:78.57 | val acc:47.16 | test_acc_best_val: 49.68  | best_test_acc: 49.68 | test acc:49.68 | time:7.9ms
epoch:20 | loss:1.3275 | train acc:100.00 | val acc:72.21 | test_acc_best_val: 72.58  | best_test_acc: 72.58 | test acc:72.73 | time:8.3ms
epoch:40 | loss:1.3195 | train acc:100.00 | val acc:70.06 | test_acc_best_val: 72.58  | best_test_acc: 72.58 | test acc:72.34 | time:8.0ms
epoch:60 | loss:1.3032 | train acc:99.29 | val acc:70.06 | test_acc_best_val: 72.58  | best_test_acc: 72.58 | test acc:72.58 | time:8.5ms
epoch:80 | loss:1.2857 | train acc:99.29 | val acc:70.65 | test_acc_best_val: 72.58  | best_test_acc: 72.58 | test acc:72.58 | time:9.3ms
epoch:100 | loss:1.3286 | train acc:99.29 | val acc:71.04 | test_acc_best_val: 72.58  | best_test_acc: 72.58 | test acc:72.82 | time:9.1ms
epoch:120 | loss:1.3057 | train acc:99.29 | val acc:71.04 | test_acc_best_val: 72.58  | best_test_acc: 72.58 | test acc:73.16 | time:9.6ms
epoch:140 | loss:1.3204 | train acc:99.29 | val acc:71.04 | test_acc_best_val: 72.58  | best_test_acc: 72.58 | test acc:73.31 | time:8.6ms
epoch:160 | loss:1.3025 | train acc:99.29 | val acc:71.82 | test_acc_best_val: 72.58  | best_test_acc: 72.58 | test acc:73.41 | time:13.6ms
epoch:180 | loss:1.3018 | train acc:99.29 | val acc:71.82 | test_acc_best_val: 72.58  | best_test_acc: 72.58 | test acc:73.80 | time:9.0ms
Run 1/10, best test accuracy: 72.58, acc(last): 73.65, total time: 2.72s
The split is 2
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 2/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:1.9961 | train acc:68.57 | val acc:45.21 | test_acc_best_val: 45.94  | best_test_acc: 45.94 | test acc:45.94 | time:8.6ms
epoch:20 | loss:1.3289 | train acc:99.29 | val acc:69.67 | test_acc_best_val: 70.20  | best_test_acc: 70.20 | test acc:70.78 | time:12.6ms
epoch:40 | loss:1.2906 | train acc:100.00 | val acc:69.47 | test_acc_best_val: 70.88  | best_test_acc: 70.88 | test acc:71.46 | time:9.1ms
epoch:60 | loss:1.2912 | train acc:100.00 | val acc:68.69 | test_acc_best_val: 70.88  | best_test_acc: 70.88 | test acc:71.41 | time:8.6ms
epoch:80 | loss:1.2922 | train acc:100.00 | val acc:67.91 | test_acc_best_val: 70.88  | best_test_acc: 70.88 | test acc:71.32 | time:8.3ms
epoch:100 | loss:1.3062 | train acc:100.00 | val acc:68.69 | test_acc_best_val: 70.88  | best_test_acc: 70.88 | test acc:71.71 | time:8.3ms
epoch:120 | loss:1.3119 | train acc:100.00 | val acc:68.30 | test_acc_best_val: 70.88  | best_test_acc: 70.88 | test acc:71.95 | time:8.6ms
epoch:140 | loss:1.3111 | train acc:100.00 | val acc:68.69 | test_acc_best_val: 70.88  | best_test_acc: 70.88 | test acc:71.41 | time:8.1ms
epoch:160 | loss:1.3014 | train acc:99.29 | val acc:69.28 | test_acc_best_val: 70.88  | best_test_acc: 70.88 | test acc:71.27 | time:8.2ms
epoch:180 | loss:1.2928 | train acc:99.29 | val acc:68.88 | test_acc_best_val: 70.88  | best_test_acc: 70.88 | test acc:71.22 | time:14.6ms
Run 2/10, best test accuracy: 70.88, acc(last): 71.46, total time: 2.77s
The split is 3
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 3/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:1.9613 | train acc:60.71 | val acc:35.03 | test_acc_best_val: 40.01  | best_test_acc: 40.01 | test acc:40.01 | time:10.0ms
epoch:20 | loss:1.3600 | train acc:99.29 | val acc:67.51 | test_acc_best_val: 69.42  | best_test_acc: 69.42 | test acc:69.42 | time:9.1ms
epoch:40 | loss:1.3125 | train acc:99.29 | val acc:68.49 | test_acc_best_val: 69.37  | best_test_acc: 69.37 | test acc:70.00 | time:9.0ms
epoch:60 | loss:1.3067 | train acc:99.29 | val acc:67.91 | test_acc_best_val: 69.37  | best_test_acc: 69.37 | test acc:70.54 | time:7.9ms
epoch:80 | loss:1.3104 | train acc:99.29 | val acc:69.08 | test_acc_best_val: 70.83  | best_test_acc: 70.83 | test acc:71.03 | time:8.6ms
epoch:100 | loss:1.3235 | train acc:99.29 | val acc:69.86 | test_acc_best_val: 71.17  | best_test_acc: 71.17 | test acc:71.17 | time:8.9ms
epoch:120 | loss:1.3256 | train acc:99.29 | val acc:70.65 | test_acc_best_val: 71.07  | best_test_acc: 71.07 | test acc:71.12 | time:11.4ms
epoch:140 | loss:1.3273 | train acc:99.29 | val acc:70.65 | test_acc_best_val: 71.03  | best_test_acc: 71.03 | test acc:70.88 | time:13.5ms
epoch:160 | loss:1.3206 | train acc:99.29 | val acc:70.06 | test_acc_best_val: 71.03  | best_test_acc: 71.03 | test acc:70.73 | time:9.6ms
epoch:180 | loss:1.3469 | train acc:99.29 | val acc:70.65 | test_acc_best_val: 71.03  | best_test_acc: 71.03 | test acc:70.73 | time:9.6ms
Run 3/10, best test accuracy: 71.03, acc(last): 70.59, total time: 2.64s
The split is 4
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 4/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:1.9954 | train acc:71.43 | val acc:47.55 | test_acc_best_val: 48.86  | best_test_acc: 48.86 | test acc:48.86 | time:10.6ms
epoch:20 | loss:1.3453 | train acc:99.29 | val acc:69.86 | test_acc_best_val: 68.60  | best_test_acc: 68.60 | test acc:70.35 | time:10.0ms
epoch:40 | loss:1.3373 | train acc:100.00 | val acc:69.28 | test_acc_best_val: 70.39  | best_test_acc: 70.39 | test acc:69.81 | time:9.2ms
epoch:60 | loss:1.3168 | train acc:100.00 | val acc:68.69 | test_acc_best_val: 70.39  | best_test_acc: 70.39 | test acc:70.05 | time:7.9ms
epoch:80 | loss:1.3107 | train acc:100.00 | val acc:69.47 | test_acc_best_val: 70.39  | best_test_acc: 70.39 | test acc:69.91 | time:8.4ms
epoch:100 | loss:1.3181 | train acc:100.00 | val acc:71.62 | test_acc_best_val: 70.30  | best_test_acc: 70.30 | test acc:70.30 | time:13.7ms
epoch:120 | loss:1.3176 | train acc:100.00 | val acc:71.62 | test_acc_best_val: 70.30  | best_test_acc: 70.30 | test acc:70.59 | time:7.7ms
epoch:140 | loss:1.3248 | train acc:100.00 | val acc:71.43 | test_acc_best_val: 70.64  | best_test_acc: 70.64 | test acc:70.64 | time:7.8ms
epoch:160 | loss:1.3094 | train acc:100.00 | val acc:72.02 | test_acc_best_val: 70.88  | best_test_acc: 70.88 | test acc:71.03 | time:8.5ms
epoch:180 | loss:1.3191 | train acc:100.00 | val acc:71.82 | test_acc_best_val: 70.88  | best_test_acc: 70.88 | test acc:71.32 | time:8.0ms
Run 4/10, best test accuracy: 70.88, acc(last): 71.32, total time: 2.57s
The split is 5
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 5/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:2.0170 | train acc:71.43 | val acc:48.53 | test_acc_best_val: 48.32  | best_test_acc: 48.32 | test acc:48.32 | time:8.4ms
epoch:20 | loss:1.3205 | train acc:100.00 | val acc:72.41 | test_acc_best_val: 71.90  | best_test_acc: 71.90 | test acc:71.12 | time:8.4ms
epoch:40 | loss:1.3303 | train acc:100.00 | val acc:72.02 | test_acc_best_val: 71.90  | best_test_acc: 71.90 | test acc:71.22 | time:7.8ms
epoch:60 | loss:1.3189 | train acc:100.00 | val acc:71.43 | test_acc_best_val: 71.90  | best_test_acc: 71.90 | test acc:71.80 | time:9.1ms
epoch:80 | loss:1.3340 | train acc:99.29 | val acc:72.21 | test_acc_best_val: 71.90  | best_test_acc: 71.90 | test acc:71.56 | time:9.0ms
epoch:100 | loss:1.3281 | train acc:100.00 | val acc:72.21 | test_acc_best_val: 71.90  | best_test_acc: 71.90 | test acc:71.95 | time:10.4ms
epoch:120 | loss:1.3142 | train acc:100.00 | val acc:73.39 | test_acc_best_val: 71.80  | best_test_acc: 71.80 | test acc:71.75 | time:9.1ms
epoch:140 | loss:1.3210 | train acc:99.29 | val acc:72.80 | test_acc_best_val: 71.75  | best_test_acc: 71.75 | test acc:71.95 | time:8.9ms
epoch:160 | loss:1.3307 | train acc:99.29 | val acc:73.19 | test_acc_best_val: 71.75  | best_test_acc: 71.75 | test acc:72.14 | time:8.4ms
epoch:180 | loss:1.3396 | train acc:100.00 | val acc:72.99 | test_acc_best_val: 71.75  | best_test_acc: 71.75 | test acc:72.10 | time:8.2ms
Run 5/10, best test accuracy: 71.75, acc(last): 72.44, total time: 2.57s
The split is 6
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 6/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:1.9963 | train acc:62.14 | val acc:33.66 | test_acc_best_val: 31.75  | best_test_acc: 31.75 | test acc:31.75 | time:8.5ms
epoch:20 | loss:1.3548 | train acc:99.29 | val acc:63.99 | test_acc_best_val: 67.14  | best_test_acc: 67.14 | test acc:66.16 | time:8.8ms
epoch:40 | loss:1.3605 | train acc:99.29 | val acc:65.17 | test_acc_best_val: 67.14  | best_test_acc: 67.14 | test acc:66.99 | time:12.7ms
epoch:60 | loss:1.3272 | train acc:99.29 | val acc:66.93 | test_acc_best_val: 67.43  | best_test_acc: 67.43 | test acc:67.43 | time:8.7ms
epoch:80 | loss:1.3218 | train acc:99.29 | val acc:65.95 | test_acc_best_val: 67.82  | best_test_acc: 67.82 | test acc:67.77 | time:8.8ms
epoch:100 | loss:1.3323 | train acc:100.00 | val acc:65.95 | test_acc_best_val: 67.82  | best_test_acc: 67.82 | test acc:67.67 | time:9.4ms
epoch:120 | loss:1.3132 | train acc:100.00 | val acc:66.54 | test_acc_best_val: 67.82  | best_test_acc: 67.82 | test acc:67.38 | time:8.1ms
epoch:140 | loss:1.3293 | train acc:100.00 | val acc:67.12 | test_acc_best_val: 67.82  | best_test_acc: 67.82 | test acc:67.77 | time:8.0ms
epoch:160 | loss:1.3219 | train acc:100.00 | val acc:66.93 | test_acc_best_val: 67.91  | best_test_acc: 67.91 | test acc:68.30 | time:8.2ms
epoch:180 | loss:1.3215 | train acc:100.00 | val acc:67.12 | test_acc_best_val: 67.91  | best_test_acc: 67.91 | test acc:68.16 | time:8.0ms
Run 6/10, best test accuracy: 67.91, acc(last): 68.64, total time: 2.58s
The split is 7
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 7/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:1.9805 | train acc:57.86 | val acc:28.77 | test_acc_best_val: 28.20  | best_test_acc: 28.20 | test acc:28.20 | time:10.8ms
epoch:20 | loss:1.3126 | train acc:100.00 | val acc:70.65 | test_acc_best_val: 69.23  | best_test_acc: 69.23 | test acc:68.60 | time:8.2ms
epoch:40 | loss:1.3038 | train acc:100.00 | val acc:71.62 | test_acc_best_val: 69.96  | best_test_acc: 69.96 | test acc:69.96 | time:8.2ms
epoch:60 | loss:1.2998 | train acc:100.00 | val acc:72.21 | test_acc_best_val: 70.15  | best_test_acc: 70.15 | test acc:70.25 | time:7.6ms
epoch:80 | loss:1.2916 | train acc:100.00 | val acc:71.23 | test_acc_best_val: 70.20  | best_test_acc: 70.20 | test acc:69.91 | time:9.2ms
epoch:100 | loss:1.3103 | train acc:100.00 | val acc:72.21 | test_acc_best_val: 70.20  | best_test_acc: 70.20 | test acc:69.71 | time:8.7ms
epoch:120 | loss:1.3047 | train acc:100.00 | val acc:72.41 | test_acc_best_val: 70.20  | best_test_acc: 70.20 | test acc:69.96 | time:8.1ms
epoch:140 | loss:1.3114 | train acc:100.00 | val acc:72.21 | test_acc_best_val: 70.20  | best_test_acc: 70.20 | test acc:70.30 | time:8.0ms
epoch:160 | loss:1.3357 | train acc:100.00 | val acc:73.58 | test_acc_best_val: 70.35  | best_test_acc: 70.35 | test acc:70.39 | time:8.4ms
epoch:180 | loss:1.3194 | train acc:100.00 | val acc:73.39 | test_acc_best_val: 70.39  | best_test_acc: 70.39 | test acc:70.69 | time:8.8ms
Run 7/10, best test accuracy: 70.39, acc(last): 70.69, total time: 2.60s
The split is 8
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 8/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:2.0179 | train acc:75.71 | val acc:50.68 | test_acc_best_val: 48.23  | best_test_acc: 48.23 | test acc:48.23 | time:8.6ms
epoch:20 | loss:1.3159 | train acc:100.00 | val acc:71.23 | test_acc_best_val: 70.35  | best_test_acc: 70.35 | test acc:71.32 | time:8.0ms
epoch:40 | loss:1.3096 | train acc:100.00 | val acc:73.39 | test_acc_best_val: 71.85  | best_test_acc: 71.85 | test acc:71.51 | time:8.1ms
epoch:60 | loss:1.3051 | train acc:100.00 | val acc:73.39 | test_acc_best_val: 71.85  | best_test_acc: 71.85 | test acc:71.07 | time:8.1ms
epoch:80 | loss:1.3220 | train acc:100.00 | val acc:74.36 | test_acc_best_val: 71.22  | best_test_acc: 71.22 | test acc:71.12 | time:9.1ms
epoch:100 | loss:1.2773 | train acc:100.00 | val acc:73.97 | test_acc_best_val: 71.22  | best_test_acc: 71.22 | test acc:71.27 | time:8.8ms
epoch:120 | loss:1.2942 | train acc:99.29 | val acc:74.95 | test_acc_best_val: 71.51  | best_test_acc: 71.51 | test acc:71.51 | time:8.7ms
epoch:140 | loss:1.3295 | train acc:99.29 | val acc:74.76 | test_acc_best_val: 71.51  | best_test_acc: 71.51 | test acc:72.05 | time:11.5ms
epoch:160 | loss:1.3184 | train acc:99.29 | val acc:75.73 | test_acc_best_val: 72.14  | best_test_acc: 72.14 | test acc:71.95 | time:8.9ms
epoch:180 | loss:1.3211 | train acc:99.29 | val acc:75.15 | test_acc_best_val: 71.95  | best_test_acc: 71.95 | test acc:71.85 | time:7.8ms
Run 8/10, best test accuracy: 71.95, acc(last): 72.00, total time: 2.57s
The split is 9
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 9/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:1.9909 | train acc:66.43 | val acc:44.81 | test_acc_best_val: 49.15  | best_test_acc: 49.15 | test acc:49.15 | time:9.5ms
epoch:20 | loss:1.3149 | train acc:98.57 | val acc:70.06 | test_acc_best_val: 72.44  | best_test_acc: 72.44 | test acc:71.95 | time:8.1ms
epoch:40 | loss:1.2985 | train acc:99.29 | val acc:68.49 | test_acc_best_val: 72.44  | best_test_acc: 72.44 | test acc:70.83 | time:8.4ms
epoch:60 | loss:1.3017 | train acc:99.29 | val acc:70.45 | test_acc_best_val: 72.44  | best_test_acc: 72.44 | test acc:70.59 | time:7.9ms
epoch:80 | loss:1.2983 | train acc:99.29 | val acc:70.65 | test_acc_best_val: 72.44  | best_test_acc: 72.44 | test acc:71.17 | time:7.9ms
epoch:100 | loss:1.3223 | train acc:99.29 | val acc:70.25 | test_acc_best_val: 72.44  | best_test_acc: 72.44 | test acc:71.80 | time:8.0ms
epoch:120 | loss:1.3096 | train acc:99.29 | val acc:70.65 | test_acc_best_val: 72.44  | best_test_acc: 72.44 | test acc:71.71 | time:8.8ms
epoch:140 | loss:1.3320 | train acc:99.29 | val acc:71.04 | test_acc_best_val: 72.44  | best_test_acc: 72.44 | test acc:72.34 | time:8.5ms
epoch:160 | loss:1.2917 | train acc:99.29 | val acc:71.23 | test_acc_best_val: 72.44  | best_test_acc: 72.44 | test acc:72.39 | time:8.5ms
epoch:180 | loss:1.3336 | train acc:99.29 | val acc:71.23 | test_acc_best_val: 72.44  | best_test_acc: 72.44 | test acc:72.39 | time:8.5ms
Run 9/10, best test accuracy: 72.44, acc(last): 72.53, total time: 2.58s
The split is 10
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 10/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:1.9819 | train acc:77.14 | val acc:51.27 | test_acc_best_val: 49.73  | best_test_acc: 49.73 | test acc:49.73 | time:8.5ms
epoch:20 | loss:1.3145 | train acc:100.00 | val acc:72.02 | test_acc_best_val: 70.98  | best_test_acc: 70.98 | test acc:72.24 | time:9.3ms
epoch:40 | loss:1.2923 | train acc:100.00 | val acc:71.62 | test_acc_best_val: 70.98  | best_test_acc: 70.98 | test acc:71.41 | time:9.5ms
epoch:60 | loss:1.3019 | train acc:100.00 | val acc:70.45 | test_acc_best_val: 70.98  | best_test_acc: 70.98 | test acc:71.75 | time:8.4ms
epoch:80 | loss:1.2889 | train acc:100.00 | val acc:70.25 | test_acc_best_val: 70.98  | best_test_acc: 70.98 | test acc:72.00 | time:8.4ms
epoch:100 | loss:1.3090 | train acc:100.00 | val acc:70.65 | test_acc_best_val: 70.98  | best_test_acc: 70.98 | test acc:71.80 | time:9.2ms
epoch:120 | loss:1.3171 | train acc:100.00 | val acc:70.65 | test_acc_best_val: 70.98  | best_test_acc: 70.98 | test acc:71.90 | time:10.3ms
epoch:140 | loss:1.3094 | train acc:100.00 | val acc:71.04 | test_acc_best_val: 70.98  | best_test_acc: 70.98 | test acc:72.00 | time:8.9ms
epoch:160 | loss:1.2999 | train acc:100.00 | val acc:71.04 | test_acc_best_val: 70.98  | best_test_acc: 70.98 | test acc:72.19 | time:8.7ms
epoch:180 | loss:1.3352 | train acc:100.00 | val acc:71.62 | test_acc_best_val: 70.98  | best_test_acc: 70.98 | test acc:72.34 | time:8.5ms
Run 10/10, best test accuracy: 70.98, acc(last): 72.53, total time: 2.59s
The split is 1
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 1/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:2.0002 | train acc:79.29 | val acc:56.56 | test_acc_best_val: 56.39  | best_test_acc: 56.39 | test acc:56.39 | time:8.8ms
epoch:20 | loss:1.3197 | train acc:99.29 | val acc:73.39 | test_acc_best_val: 72.19  | best_test_acc: 72.19 | test acc:72.68 | time:8.4ms
epoch:40 | loss:1.3127 | train acc:99.29 | val acc:72.99 | test_acc_best_val: 72.19  | best_test_acc: 72.19 | test acc:72.78 | time:9.6ms
epoch:60 | loss:1.3228 | train acc:99.29 | val acc:72.02 | test_acc_best_val: 72.19  | best_test_acc: 72.19 | test acc:72.78 | time:12.0ms
epoch:80 | loss:1.2997 | train acc:99.29 | val acc:72.02 | test_acc_best_val: 72.19  | best_test_acc: 72.19 | test acc:73.12 | time:10.5ms
epoch:100 | loss:1.3006 | train acc:99.29 | val acc:73.19 | test_acc_best_val: 72.19  | best_test_acc: 72.19 | test acc:73.21 | time:11.0ms
epoch:120 | loss:1.3161 | train acc:99.29 | val acc:72.02 | test_acc_best_val: 72.19  | best_test_acc: 72.19 | test acc:73.31 | time:10.4ms
epoch:140 | loss:1.3258 | train acc:99.29 | val acc:71.04 | test_acc_best_val: 72.19  | best_test_acc: 72.19 | test acc:73.26 | time:10.3ms
epoch:160 | loss:1.3060 | train acc:99.29 | val acc:71.23 | test_acc_best_val: 72.19  | best_test_acc: 72.19 | test acc:72.92 | time:8.3ms
epoch:180 | loss:1.3269 | train acc:99.29 | val acc:72.02 | test_acc_best_val: 72.19  | best_test_acc: 72.19 | test acc:73.31 | time:12.7ms
Run 1/10, best test accuracy: 72.19, acc(last): 73.55, total time: 3.04s
The split is 2
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 2/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:2.0358 | train acc:79.29 | val acc:53.82 | test_acc_best_val: 51.53  | best_test_acc: 51.53 | test acc:51.53 | time:10.1ms
epoch:20 | loss:1.3139 | train acc:98.57 | val acc:68.30 | test_acc_best_val: 70.59  | best_test_acc: 70.59 | test acc:69.96 | time:12.9ms
epoch:40 | loss:1.2951 | train acc:100.00 | val acc:67.12 | test_acc_best_val: 70.59  | best_test_acc: 70.59 | test acc:70.30 | time:10.4ms
epoch:60 | loss:1.3030 | train acc:100.00 | val acc:67.91 | test_acc_best_val: 70.59  | best_test_acc: 70.59 | test acc:70.59 | time:10.2ms
epoch:80 | loss:1.3042 | train acc:100.00 | val acc:68.69 | test_acc_best_val: 70.59  | best_test_acc: 70.59 | test acc:71.17 | time:10.9ms
epoch:100 | loss:1.2910 | train acc:99.29 | val acc:70.25 | test_acc_best_val: 71.32  | best_test_acc: 71.32 | test acc:71.17 | time:10.1ms
epoch:120 | loss:1.2948 | train acc:99.29 | val acc:71.04 | test_acc_best_val: 71.37  | best_test_acc: 71.37 | test acc:71.37 | time:11.3ms
epoch:140 | loss:1.3195 | train acc:99.29 | val acc:71.04 | test_acc_best_val: 71.37  | best_test_acc: 71.37 | test acc:71.27 | time:11.0ms
epoch:160 | loss:1.3182 | train acc:99.29 | val acc:70.84 | test_acc_best_val: 71.32  | best_test_acc: 71.32 | test acc:71.71 | time:10.6ms
epoch:180 | loss:1.3127 | train acc:99.29 | val acc:70.45 | test_acc_best_val: 71.32  | best_test_acc: 71.32 | test acc:71.80 | time:12.0ms
Run 2/10, best test accuracy: 71.32, acc(last): 71.66, total time: 3.19s
The split is 3
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 3/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:2.0016 | train acc:67.14 | val acc:50.88 | test_acc_best_val: 49.34  | best_test_acc: 49.34 | test acc:49.34 | time:10.7ms
epoch:20 | loss:1.3445 | train acc:99.29 | val acc:72.60 | test_acc_best_val: 69.23  | best_test_acc: 69.23 | test acc:69.32 | time:11.2ms
epoch:40 | loss:1.3188 | train acc:99.29 | val acc:72.02 | test_acc_best_val: 69.23  | best_test_acc: 69.23 | test acc:69.86 | time:10.5ms
epoch:60 | loss:1.3137 | train acc:99.29 | val acc:72.80 | test_acc_best_val: 69.23  | best_test_acc: 69.23 | test acc:70.00 | time:9.0ms
epoch:80 | loss:1.3091 | train acc:99.29 | val acc:70.06 | test_acc_best_val: 69.23  | best_test_acc: 69.23 | test acc:69.18 | time:8.9ms
epoch:100 | loss:1.3238 | train acc:99.29 | val acc:69.86 | test_acc_best_val: 69.23  | best_test_acc: 69.23 | test acc:69.57 | time:9.7ms
epoch:120 | loss:1.3194 | train acc:99.29 | val acc:71.04 | test_acc_best_val: 69.23  | best_test_acc: 69.23 | test acc:70.15 | time:8.5ms
epoch:140 | loss:1.3201 | train acc:99.29 | val acc:72.02 | test_acc_best_val: 69.23  | best_test_acc: 69.23 | test acc:70.30 | time:10.6ms
epoch:160 | loss:1.3243 | train acc:99.29 | val acc:72.02 | test_acc_best_val: 69.23  | best_test_acc: 69.23 | test acc:70.59 | time:9.8ms
epoch:180 | loss:1.3254 | train acc:99.29 | val acc:71.62 | test_acc_best_val: 69.23  | best_test_acc: 69.23 | test acc:70.78 | time:9.3ms
Run 3/10, best test accuracy: 69.23, acc(last): 70.98, total time: 3.01s
The split is 4
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 4/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:2.0248 | train acc:73.57 | val acc:45.40 | test_acc_best_val: 48.23  | best_test_acc: 48.23 | test acc:48.23 | time:9.3ms
epoch:20 | loss:1.2957 | train acc:100.00 | val acc:70.25 | test_acc_best_val: 70.83  | best_test_acc: 70.83 | test acc:70.49 | time:9.2ms
epoch:40 | loss:1.2953 | train acc:100.00 | val acc:70.06 | test_acc_best_val: 70.49  | best_test_acc: 70.49 | test acc:70.49 | time:15.1ms
epoch:60 | loss:1.2960 | train acc:100.00 | val acc:69.47 | test_acc_best_val: 70.49  | best_test_acc: 70.49 | test acc:70.98 | time:10.2ms
epoch:80 | loss:1.2747 | train acc:100.00 | val acc:69.28 | test_acc_best_val: 70.49  | best_test_acc: 70.49 | test acc:71.66 | time:9.8ms
epoch:100 | loss:1.3010 | train acc:100.00 | val acc:70.25 | test_acc_best_val: 70.49  | best_test_acc: 70.49 | test acc:72.05 | time:9.6ms
epoch:120 | loss:1.3083 | train acc:100.00 | val acc:71.04 | test_acc_best_val: 70.49  | best_test_acc: 70.49 | test acc:72.34 | time:9.6ms
epoch:140 | loss:1.3277 | train acc:100.00 | val acc:70.84 | test_acc_best_val: 70.49  | best_test_acc: 70.49 | test acc:72.34 | time:9.7ms
epoch:160 | loss:1.3131 | train acc:100.00 | val acc:71.82 | test_acc_best_val: 72.58  | best_test_acc: 72.58 | test acc:72.58 | time:9.7ms
epoch:180 | loss:1.3035 | train acc:100.00 | val acc:71.82 | test_acc_best_val: 72.82  | best_test_acc: 72.82 | test acc:72.87 | time:10.0ms
Run 4/10, best test accuracy: 72.73, acc(last): 72.73, total time: 3.16s
The split is 5
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 5/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:2.0353 | train acc:77.86 | val acc:48.73 | test_acc_best_val: 49.68  | best_test_acc: 49.68 | test acc:49.68 | time:10.8ms
epoch:20 | loss:1.3250 | train acc:99.29 | val acc:71.04 | test_acc_best_val: 71.90  | best_test_acc: 71.90 | test acc:71.90 | time:8.8ms
epoch:40 | loss:1.3177 | train acc:100.00 | val acc:69.86 | test_acc_best_val: 71.90  | best_test_acc: 71.90 | test acc:72.39 | time:9.2ms
epoch:60 | loss:1.3031 | train acc:100.00 | val acc:70.06 | test_acc_best_val: 71.90  | best_test_acc: 71.90 | test acc:72.19 | time:8.7ms
epoch:80 | loss:1.3218 | train acc:100.00 | val acc:70.06 | test_acc_best_val: 71.90  | best_test_acc: 71.90 | test acc:72.53 | time:8.9ms
epoch:100 | loss:1.3208 | train acc:100.00 | val acc:70.84 | test_acc_best_val: 71.90  | best_test_acc: 71.90 | test acc:72.73 | time:9.6ms
epoch:120 | loss:1.3078 | train acc:100.00 | val acc:70.25 | test_acc_best_val: 71.90  | best_test_acc: 71.90 | test acc:72.63 | time:8.8ms
epoch:140 | loss:1.3321 | train acc:100.00 | val acc:70.45 | test_acc_best_val: 71.90  | best_test_acc: 71.90 | test acc:73.21 | time:8.3ms
epoch:160 | loss:1.3353 | train acc:100.00 | val acc:71.23 | test_acc_best_val: 71.90  | best_test_acc: 71.90 | test acc:72.87 | time:12.4ms
epoch:180 | loss:1.3323 | train acc:100.00 | val acc:70.45 | test_acc_best_val: 71.90  | best_test_acc: 71.90 | test acc:72.73 | time:9.2ms
Run 5/10, best test accuracy: 71.90, acc(last): 73.02, total time: 2.79s
The split is 6
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 6/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:2.0204 | train acc:63.57 | val acc:30.92 | test_acc_best_val: 34.13  | best_test_acc: 34.13 | test acc:34.13 | time:10.8ms
epoch:20 | loss:1.3484 | train acc:100.00 | val acc:63.41 | test_acc_best_val: 67.62  | best_test_acc: 67.62 | test acc:66.07 | time:8.6ms
epoch:40 | loss:1.3342 | train acc:100.00 | val acc:63.21 | test_acc_best_val: 67.62  | best_test_acc: 67.62 | test acc:66.36 | time:9.0ms
epoch:60 | loss:1.3130 | train acc:100.00 | val acc:64.58 | test_acc_best_val: 67.62  | best_test_acc: 67.62 | test acc:66.99 | time:8.4ms
epoch:80 | loss:1.3258 | train acc:100.00 | val acc:65.56 | test_acc_best_val: 68.11  | best_test_acc: 68.11 | test acc:68.11 | time:9.4ms
epoch:100 | loss:1.3264 | train acc:100.00 | val acc:64.97 | test_acc_best_val: 68.11  | best_test_acc: 68.11 | test acc:67.87 | time:9.1ms
epoch:120 | loss:1.3134 | train acc:100.00 | val acc:64.58 | test_acc_best_val: 68.11  | best_test_acc: 68.11 | test acc:68.21 | time:8.6ms
epoch:140 | loss:1.3428 | train acc:100.00 | val acc:64.38 | test_acc_best_val: 68.11  | best_test_acc: 68.11 | test acc:68.69 | time:9.5ms
epoch:160 | loss:1.3337 | train acc:100.00 | val acc:65.56 | test_acc_best_val: 68.11  | best_test_acc: 68.11 | test acc:69.13 | time:10.4ms
epoch:180 | loss:1.3435 | train acc:100.00 | val acc:65.17 | test_acc_best_val: 68.45  | best_test_acc: 68.45 | test acc:68.40 | time:14.5ms
Run 6/10, best test accuracy: 68.45, acc(last): 68.55, total time: 2.75s
The split is 7
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 7/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:2.0102 | train acc:72.86 | val acc:47.75 | test_acc_best_val: 45.02  | best_test_acc: 45.02 | test acc:45.02 | time:10.0ms
epoch:20 | loss:1.3425 | train acc:100.00 | val acc:71.82 | test_acc_best_val: 68.30  | best_test_acc: 68.30 | test acc:68.79 | time:9.1ms
epoch:40 | loss:1.3255 | train acc:100.00 | val acc:70.84 | test_acc_best_val: 68.30  | best_test_acc: 68.30 | test acc:69.18 | time:8.8ms
epoch:60 | loss:1.3101 | train acc:100.00 | val acc:70.06 | test_acc_best_val: 68.30  | best_test_acc: 68.30 | test acc:69.47 | time:9.9ms
epoch:80 | loss:1.3106 | train acc:100.00 | val acc:69.67 | test_acc_best_val: 68.30  | best_test_acc: 68.30 | test acc:69.47 | time:11.6ms
epoch:100 | loss:1.3051 | train acc:100.00 | val acc:70.06 | test_acc_best_val: 68.30  | best_test_acc: 68.30 | test acc:70.05 | time:9.8ms
epoch:120 | loss:1.3173 | train acc:100.00 | val acc:70.25 | test_acc_best_val: 68.30  | best_test_acc: 68.30 | test acc:69.71 | time:9.3ms
epoch:140 | loss:1.3374 | train acc:100.00 | val acc:71.23 | test_acc_best_val: 68.30  | best_test_acc: 68.30 | test acc:70.15 | time:10.6ms
epoch:160 | loss:1.3307 | train acc:100.00 | val acc:72.41 | test_acc_best_val: 70.15  | best_test_acc: 70.15 | test acc:70.39 | time:9.3ms
epoch:180 | loss:1.3220 | train acc:100.00 | val acc:71.43 | test_acc_best_val: 70.15  | best_test_acc: 70.15 | test acc:70.30 | time:8.8ms
Run 7/10, best test accuracy: 70.15, acc(last): 70.30, total time: 3.14s
The split is 8
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 8/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:1.9711 | train acc:80.00 | val acc:58.51 | test_acc_best_val: 56.20  | best_test_acc: 56.20 | test acc:56.20 | time:9.6ms
epoch:20 | loss:1.3099 | train acc:100.00 | val acc:75.54 | test_acc_best_val: 72.05  | best_test_acc: 72.05 | test acc:72.10 | time:8.3ms
epoch:40 | loss:1.3040 | train acc:100.00 | val acc:76.32 | test_acc_best_val: 72.39  | best_test_acc: 72.39 | test acc:72.78 | time:9.1ms
epoch:60 | loss:1.3094 | train acc:100.00 | val acc:75.54 | test_acc_best_val: 72.39  | best_test_acc: 72.39 | test acc:72.82 | time:14.9ms
epoch:80 | loss:1.3085 | train acc:100.00 | val acc:75.15 | test_acc_best_val: 72.39  | best_test_acc: 72.39 | test acc:73.02 | time:9.8ms
epoch:100 | loss:1.3137 | train acc:99.29 | val acc:75.15 | test_acc_best_val: 72.39  | best_test_acc: 72.39 | test acc:72.73 | time:9.7ms
epoch:120 | loss:1.3193 | train acc:99.29 | val acc:76.32 | test_acc_best_val: 72.39  | best_test_acc: 72.39 | test acc:73.16 | time:9.4ms
epoch:140 | loss:1.2962 | train acc:99.29 | val acc:76.13 | test_acc_best_val: 72.39  | best_test_acc: 72.39 | test acc:73.07 | time:8.6ms
epoch:160 | loss:1.3046 | train acc:100.00 | val acc:76.13 | test_acc_best_val: 72.39  | best_test_acc: 72.39 | test acc:73.21 | time:9.3ms
epoch:180 | loss:1.3082 | train acc:100.00 | val acc:75.93 | test_acc_best_val: 72.39  | best_test_acc: 72.39 | test acc:73.41 | time:12.3ms
Run 8/10, best test accuracy: 72.39, acc(last): 73.26, total time: 2.91s
The split is 9
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 9/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:1.9972 | train acc:75.00 | val acc:45.99 | test_acc_best_val: 44.77  | best_test_acc: 44.77 | test acc:44.77 | time:10.0ms
epoch:20 | loss:1.3314 | train acc:99.29 | val acc:70.45 | test_acc_best_val: 72.19  | best_test_acc: 72.19 | test acc:71.07 | time:9.6ms
epoch:40 | loss:1.3127 | train acc:99.29 | val acc:69.47 | test_acc_best_val: 72.19  | best_test_acc: 72.19 | test acc:70.15 | time:10.0ms
epoch:60 | loss:1.3124 | train acc:99.29 | val acc:68.69 | test_acc_best_val: 72.19  | best_test_acc: 72.19 | test acc:70.73 | time:8.8ms
epoch:80 | loss:1.3028 | train acc:99.29 | val acc:69.67 | test_acc_best_val: 72.19  | best_test_acc: 72.19 | test acc:71.03 | time:9.7ms
epoch:100 | loss:1.3191 | train acc:99.29 | val acc:70.06 | test_acc_best_val: 72.19  | best_test_acc: 72.19 | test acc:71.51 | time:9.8ms
epoch:120 | loss:1.3126 | train acc:99.29 | val acc:70.06 | test_acc_best_val: 72.19  | best_test_acc: 72.19 | test acc:71.51 | time:10.1ms
epoch:140 | loss:1.3250 | train acc:99.29 | val acc:70.45 | test_acc_best_val: 72.19  | best_test_acc: 72.19 | test acc:71.37 | time:11.0ms
epoch:160 | loss:1.3312 | train acc:99.29 | val acc:70.45 | test_acc_best_val: 72.19  | best_test_acc: 72.19 | test acc:71.71 | time:10.7ms
epoch:180 | loss:1.3470 | train acc:99.29 | val acc:71.23 | test_acc_best_val: 72.19  | best_test_acc: 72.19 | test acc:72.05 | time:10.4ms
Run 9/10, best test accuracy: 72.19, acc(last): 72.14, total time: 3.06s
The split is 10
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 10/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:1.9969 | train acc:69.29 | val acc:51.47 | test_acc_best_val: 48.91  | best_test_acc: 48.91 | test acc:48.91 | time:12.3ms
epoch:20 | loss:1.3331 | train acc:100.00 | val acc:72.21 | test_acc_best_val: 71.46  | best_test_acc: 71.46 | test acc:71.27 | time:11.0ms
epoch:40 | loss:1.2989 | train acc:100.00 | val acc:72.21 | test_acc_best_val: 71.46  | best_test_acc: 71.46 | test acc:70.64 | time:10.2ms
epoch:60 | loss:1.3075 | train acc:100.00 | val acc:72.21 | test_acc_best_val: 71.46  | best_test_acc: 71.46 | test acc:70.73 | time:10.6ms
epoch:80 | loss:1.3185 | train acc:100.00 | val acc:72.60 | test_acc_best_val: 71.46  | best_test_acc: 71.46 | test acc:71.46 | time:10.3ms
epoch:100 | loss:1.3310 | train acc:100.00 | val acc:73.58 | test_acc_best_val: 71.17  | best_test_acc: 71.17 | test acc:71.41 | time:11.2ms
epoch:120 | loss:1.3172 | train acc:100.00 | val acc:73.39 | test_acc_best_val: 71.17  | best_test_acc: 71.17 | test acc:71.37 | time:10.3ms
epoch:140 | loss:1.3142 | train acc:100.00 | val acc:73.58 | test_acc_best_val: 71.17  | best_test_acc: 71.17 | test acc:72.19 | time:11.3ms
epoch:160 | loss:1.3395 | train acc:100.00 | val acc:74.56 | test_acc_best_val: 72.00  | best_test_acc: 72.00 | test acc:71.95 | time:10.8ms
epoch:180 | loss:1.3116 | train acc:100.00 | val acc:74.56 | test_acc_best_val: 72.00  | best_test_acc: 72.00 | test acc:71.95 | time:11.5ms
Run 10/10, best test accuracy: 71.80, acc(last): 72.19, total time: 3.24s
The split is 1
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 1/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:1.9922 | train acc:70.71 | val acc:45.40 | test_acc_best_val: 44.43  | best_test_acc: 44.43 | test acc:44.43 | time:9.9ms
epoch:20 | loss:1.2976 | train acc:99.29 | val acc:71.04 | test_acc_best_val: 70.49  | best_test_acc: 70.49 | test acc:69.62 | time:12.1ms
epoch:40 | loss:1.2769 | train acc:99.29 | val acc:70.84 | test_acc_best_val: 70.49  | best_test_acc: 70.49 | test acc:70.05 | time:9.9ms
epoch:60 | loss:1.2828 | train acc:99.29 | val acc:71.23 | test_acc_best_val: 70.49  | best_test_acc: 70.49 | test acc:70.54 | time:9.5ms
epoch:80 | loss:1.2691 | train acc:99.29 | val acc:72.02 | test_acc_best_val: 70.49  | best_test_acc: 70.49 | test acc:71.12 | time:8.5ms
epoch:100 | loss:1.2770 | train acc:99.29 | val acc:72.02 | test_acc_best_val: 70.49  | best_test_acc: 70.49 | test acc:71.17 | time:8.5ms
epoch:120 | loss:1.2858 | train acc:99.29 | val acc:72.02 | test_acc_best_val: 70.49  | best_test_acc: 70.49 | test acc:71.22 | time:10.4ms
epoch:140 | loss:1.2850 | train acc:99.29 | val acc:72.21 | test_acc_best_val: 70.49  | best_test_acc: 70.49 | test acc:71.41 | time:11.8ms
epoch:160 | loss:1.2868 | train acc:99.29 | val acc:72.41 | test_acc_best_val: 70.49  | best_test_acc: 70.49 | test acc:71.95 | time:12.5ms
epoch:180 | loss:1.2996 | train acc:99.29 | val acc:72.21 | test_acc_best_val: 70.49  | best_test_acc: 70.49 | test acc:72.14 | time:11.4ms
Run 1/10, best test accuracy: 70.49, acc(last): 72.24, total time: 3.05s
The split is 2
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 2/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:2.0379 | train acc:70.71 | val acc:52.25 | test_acc_best_val: 52.41  | best_test_acc: 52.41 | test acc:52.41 | time:14.7ms
epoch:20 | loss:1.3330 | train acc:97.86 | val acc:71.43 | test_acc_best_val: 69.52  | best_test_acc: 69.52 | test acc:70.35 | time:11.4ms
epoch:40 | loss:1.3119 | train acc:100.00 | val acc:70.25 | test_acc_best_val: 69.52  | best_test_acc: 69.52 | test acc:70.93 | time:8.4ms
epoch:60 | loss:1.3203 | train acc:100.00 | val acc:71.62 | test_acc_best_val: 69.52  | best_test_acc: 69.52 | test acc:71.51 | time:10.4ms
epoch:80 | loss:1.3169 | train acc:100.00 | val acc:72.99 | test_acc_best_val: 71.56  | best_test_acc: 71.56 | test acc:71.61 | time:11.2ms
epoch:100 | loss:1.3243 | train acc:100.00 | val acc:72.60 | test_acc_best_val: 71.56  | best_test_acc: 71.56 | test acc:71.32 | time:17.3ms
epoch:120 | loss:1.3131 | train acc:100.00 | val acc:73.58 | test_acc_best_val: 71.75  | best_test_acc: 71.75 | test acc:71.66 | time:10.9ms
epoch:140 | loss:1.3292 | train acc:99.29 | val acc:74.17 | test_acc_best_val: 72.10  | best_test_acc: 72.10 | test acc:72.00 | time:11.0ms
epoch:160 | loss:1.3280 | train acc:99.29 | val acc:74.56 | test_acc_best_val: 72.00  | best_test_acc: 72.00 | test acc:72.19 | time:9.1ms
epoch:180 | loss:1.3292 | train acc:99.29 | val acc:74.56 | test_acc_best_val: 72.10  | best_test_acc: 72.10 | test acc:72.19 | time:11.7ms
Run 2/10, best test accuracy: 72.10, acc(last): 71.71, total time: 3.30s
The split is 3
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 3/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:1.9801 | train acc:62.14 | val acc:49.71 | test_acc_best_val: 47.40  | best_test_acc: 47.40 | test acc:47.40 | time:12.3ms
epoch:20 | loss:1.3018 | train acc:98.57 | val acc:71.43 | test_acc_best_val: 70.35  | best_test_acc: 70.35 | test acc:70.93 | time:8.8ms
epoch:40 | loss:1.2949 | train acc:99.29 | val acc:69.47 | test_acc_best_val: 70.64  | best_test_acc: 70.64 | test acc:71.27 | time:12.5ms
epoch:60 | loss:1.2882 | train acc:99.29 | val acc:70.84 | test_acc_best_val: 70.64  | best_test_acc: 70.64 | test acc:71.61 | time:8.6ms
epoch:80 | loss:1.3054 | train acc:99.29 | val acc:70.65 | test_acc_best_val: 70.64  | best_test_acc: 70.64 | test acc:71.80 | time:7.6ms
epoch:100 | loss:1.2949 | train acc:99.29 | val acc:70.65 | test_acc_best_val: 70.64  | best_test_acc: 70.64 | test acc:72.00 | time:12.0ms
epoch:120 | loss:1.3243 | train acc:99.29 | val acc:71.43 | test_acc_best_val: 70.64  | best_test_acc: 70.64 | test acc:72.14 | time:9.8ms
epoch:140 | loss:1.3197 | train acc:99.29 | val acc:71.04 | test_acc_best_val: 70.64  | best_test_acc: 70.64 | test acc:72.24 | time:10.0ms
epoch:160 | loss:1.3229 | train acc:99.29 | val acc:71.43 | test_acc_best_val: 72.48  | best_test_acc: 72.48 | test acc:72.24 | time:8.2ms
epoch:180 | loss:1.3371 | train acc:99.29 | val acc:70.25 | test_acc_best_val: 72.48  | best_test_acc: 72.48 | test acc:72.92 | time:10.2ms
Run 3/10, best test accuracy: 72.48, acc(last): 72.34, total time: 2.96s
The split is 4
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 4/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:2.0305 | train acc:75.71 | val acc:54.40 | test_acc_best_val: 54.93  | best_test_acc: 54.93 | test acc:54.93 | time:8.8ms
epoch:20 | loss:1.3238 | train acc:99.29 | val acc:70.65 | test_acc_best_val: 70.30  | best_test_acc: 70.30 | test acc:70.83 | time:12.2ms
epoch:40 | loss:1.2919 | train acc:100.00 | val acc:71.23 | test_acc_best_val: 71.03  | best_test_acc: 71.03 | test acc:71.51 | time:12.9ms
epoch:60 | loss:1.3058 | train acc:100.00 | val acc:71.43 | test_acc_best_val: 71.03  | best_test_acc: 71.03 | test acc:73.02 | time:11.4ms
epoch:80 | loss:1.3103 | train acc:100.00 | val acc:71.43 | test_acc_best_val: 73.31  | best_test_acc: 73.31 | test acc:73.46 | time:10.5ms
epoch:100 | loss:1.3148 | train acc:100.00 | val acc:72.21 | test_acc_best_val: 73.51  | best_test_acc: 73.51 | test acc:73.70 | time:11.4ms
epoch:120 | loss:1.3068 | train acc:100.00 | val acc:71.82 | test_acc_best_val: 73.70  | best_test_acc: 73.70 | test acc:73.21 | time:17.4ms
epoch:140 | loss:1.3046 | train acc:100.00 | val acc:72.21 | test_acc_best_val: 73.70  | best_test_acc: 73.70 | test acc:73.36 | time:11.0ms
epoch:160 | loss:1.3032 | train acc:100.00 | val acc:72.41 | test_acc_best_val: 73.70  | best_test_acc: 73.70 | test acc:73.94 | time:11.4ms
epoch:180 | loss:1.3085 | train acc:99.29 | val acc:72.21 | test_acc_best_val: 73.51  | best_test_acc: 73.51 | test acc:73.41 | time:10.1ms
Run 4/10, best test accuracy: 73.51, acc(last): 73.21, total time: 3.13s
The split is 5
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 5/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:1.9685 | train acc:79.29 | val acc:55.38 | test_acc_best_val: 53.67  | best_test_acc: 53.67 | test acc:53.67 | time:10.9ms
epoch:20 | loss:1.3234 | train acc:99.29 | val acc:73.19 | test_acc_best_val: 70.98  | best_test_acc: 70.98 | test acc:70.73 | time:9.0ms
epoch:40 | loss:1.3284 | train acc:100.00 | val acc:73.39 | test_acc_best_val: 70.98  | best_test_acc: 70.98 | test acc:69.91 | time:12.4ms
epoch:60 | loss:1.3100 | train acc:100.00 | val acc:73.39 | test_acc_best_val: 70.98  | best_test_acc: 70.98 | test acc:69.62 | time:8.7ms
epoch:80 | loss:1.3147 | train acc:100.00 | val acc:72.60 | test_acc_best_val: 70.98  | best_test_acc: 70.98 | test acc:70.25 | time:8.9ms
epoch:100 | loss:1.3282 | train acc:100.00 | val acc:72.99 | test_acc_best_val: 70.98  | best_test_acc: 70.98 | test acc:70.25 | time:8.1ms
epoch:120 | loss:1.3164 | train acc:100.00 | val acc:73.19 | test_acc_best_val: 70.98  | best_test_acc: 70.98 | test acc:71.12 | time:13.4ms
epoch:140 | loss:1.3259 | train acc:100.00 | val acc:73.19 | test_acc_best_val: 70.98  | best_test_acc: 70.98 | test acc:71.75 | time:10.0ms
epoch:160 | loss:1.3234 | train acc:100.00 | val acc:74.17 | test_acc_best_val: 72.14  | best_test_acc: 72.14 | test acc:72.14 | time:9.2ms
epoch:180 | loss:1.3292 | train acc:100.00 | val acc:73.97 | test_acc_best_val: 72.10  | best_test_acc: 72.10 | test acc:72.10 | time:8.9ms
Run 5/10, best test accuracy: 72.10, acc(last): 72.14, total time: 2.74s
The split is 6
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 6/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:2.0012 | train acc:72.14 | val acc:43.25 | test_acc_best_val: 43.75  | best_test_acc: 43.75 | test acc:43.75 | time:9.1ms
epoch:20 | loss:1.3602 | train acc:100.00 | val acc:67.12 | test_acc_best_val: 68.69  | best_test_acc: 68.69 | test acc:68.74 | time:9.2ms
epoch:40 | loss:1.3161 | train acc:100.00 | val acc:66.34 | test_acc_best_val: 68.69  | best_test_acc: 68.69 | test acc:68.74 | time:9.0ms
epoch:60 | loss:1.3309 | train acc:100.00 | val acc:66.34 | test_acc_best_val: 68.69  | best_test_acc: 68.69 | test acc:69.47 | time:11.3ms
epoch:80 | loss:1.3158 | train acc:100.00 | val acc:65.95 | test_acc_best_val: 68.69  | best_test_acc: 68.69 | test acc:69.28 | time:11.5ms
epoch:100 | loss:1.3399 | train acc:100.00 | val acc:66.34 | test_acc_best_val: 68.69  | best_test_acc: 68.69 | test acc:69.37 | time:9.9ms
epoch:120 | loss:1.3298 | train acc:100.00 | val acc:67.12 | test_acc_best_val: 68.69  | best_test_acc: 68.69 | test acc:69.86 | time:8.6ms
epoch:140 | loss:1.3181 | train acc:100.00 | val acc:67.91 | test_acc_best_val: 69.91  | best_test_acc: 69.91 | test acc:69.86 | time:9.7ms
epoch:160 | loss:1.3457 | train acc:100.00 | val acc:67.51 | test_acc_best_val: 69.91  | best_test_acc: 69.91 | test acc:69.71 | time:9.0ms
epoch:180 | loss:1.3455 | train acc:100.00 | val acc:67.91 | test_acc_best_val: 69.57  | best_test_acc: 69.57 | test acc:69.52 | time:8.0ms
Run 6/10, best test accuracy: 69.57, acc(last): 69.37, total time: 2.82s
The split is 7
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 7/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:1.9741 | train acc:70.00 | val acc:41.10 | test_acc_best_val: 42.83  | best_test_acc: 42.83 | test acc:42.83 | time:9.0ms
epoch:20 | loss:1.3296 | train acc:100.00 | val acc:69.67 | test_acc_best_val: 69.32  | best_test_acc: 69.32 | test acc:69.47 | time:9.3ms
epoch:40 | loss:1.3029 | train acc:100.00 | val acc:69.28 | test_acc_best_val: 69.32  | best_test_acc: 69.32 | test acc:70.69 | time:13.5ms
epoch:60 | loss:1.2983 | train acc:100.00 | val acc:70.65 | test_acc_best_val: 71.07  | best_test_acc: 71.07 | test acc:71.51 | time:10.4ms
epoch:80 | loss:1.3148 | train acc:100.00 | val acc:71.43 | test_acc_best_val: 71.32  | best_test_acc: 71.32 | test acc:71.56 | time:8.2ms
epoch:100 | loss:1.3130 | train acc:100.00 | val acc:70.84 | test_acc_best_val: 71.32  | best_test_acc: 71.32 | test acc:71.51 | time:9.9ms
epoch:120 | loss:1.3162 | train acc:100.00 | val acc:71.62 | test_acc_best_val: 71.32  | best_test_acc: 71.32 | test acc:71.61 | time:10.0ms
epoch:140 | loss:1.3010 | train acc:100.00 | val acc:71.43 | test_acc_best_val: 71.80  | best_test_acc: 71.80 | test acc:71.80 | time:8.8ms
epoch:160 | loss:1.3432 | train acc:100.00 | val acc:71.04 | test_acc_best_val: 71.80  | best_test_acc: 71.80 | test acc:71.71 | time:10.2ms
epoch:180 | loss:1.3124 | train acc:100.00 | val acc:71.23 | test_acc_best_val: 71.80  | best_test_acc: 71.80 | test acc:72.05 | time:9.3ms
Run 7/10, best test accuracy: 71.80, acc(last): 72.00, total time: 2.81s
The split is 8
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 8/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:2.0236 | train acc:78.57 | val acc:51.27 | test_acc_best_val: 55.57  | best_test_acc: 55.57 | test acc:55.57 | time:10.7ms
epoch:20 | loss:1.2891 | train acc:99.29 | val acc:74.95 | test_acc_best_val: 72.92  | best_test_acc: 72.92 | test acc:73.21 | time:12.2ms
epoch:40 | loss:1.2942 | train acc:99.29 | val acc:72.99 | test_acc_best_val: 72.92  | best_test_acc: 72.92 | test acc:73.21 | time:12.7ms
epoch:60 | loss:1.3113 | train acc:99.29 | val acc:73.97 | test_acc_best_val: 72.92  | best_test_acc: 72.92 | test acc:73.46 | time:9.0ms
epoch:80 | loss:1.2866 | train acc:99.29 | val acc:73.58 | test_acc_best_val: 72.92  | best_test_acc: 72.92 | test acc:73.60 | time:10.0ms
epoch:100 | loss:1.3064 | train acc:98.57 | val acc:72.60 | test_acc_best_val: 72.92  | best_test_acc: 72.92 | test acc:73.65 | time:9.9ms
epoch:120 | loss:1.3049 | train acc:98.57 | val acc:72.41 | test_acc_best_val: 72.92  | best_test_acc: 72.92 | test acc:73.65 | time:7.6ms
epoch:140 | loss:1.3070 | train acc:98.57 | val acc:72.60 | test_acc_best_val: 72.92  | best_test_acc: 72.92 | test acc:73.65 | time:8.1ms
epoch:160 | loss:1.2995 | train acc:98.57 | val acc:71.82 | test_acc_best_val: 72.92  | best_test_acc: 72.92 | test acc:73.36 | time:9.0ms
epoch:180 | loss:1.3074 | train acc:98.57 | val acc:72.60 | test_acc_best_val: 72.92  | best_test_acc: 72.92 | test acc:73.55 | time:11.2ms
Run 8/10, best test accuracy: 72.92, acc(last): 73.85, total time: 2.85s
The split is 9
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 9/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:2.0296 | train acc:69.29 | val acc:45.40 | test_acc_best_val: 44.29  | best_test_acc: 44.29 | test acc:44.29 | time:9.9ms
epoch:20 | loss:1.3219 | train acc:98.57 | val acc:71.82 | test_acc_best_val: 70.88  | best_test_acc: 70.88 | test acc:71.03 | time:11.3ms
epoch:40 | loss:1.3006 | train acc:98.57 | val acc:70.25 | test_acc_best_val: 70.88  | best_test_acc: 70.88 | test acc:69.76 | time:11.9ms
epoch:60 | loss:1.3101 | train acc:99.29 | val acc:71.04 | test_acc_best_val: 70.88  | best_test_acc: 70.88 | test acc:70.49 | time:9.2ms
epoch:80 | loss:1.3080 | train acc:99.29 | val acc:72.02 | test_acc_best_val: 70.88  | best_test_acc: 70.88 | test acc:70.93 | time:10.3ms
epoch:100 | loss:1.2969 | train acc:99.29 | val acc:72.60 | test_acc_best_val: 70.88  | best_test_acc: 70.88 | test acc:70.98 | time:10.6ms
epoch:120 | loss:1.3154 | train acc:99.29 | val acc:72.41 | test_acc_best_val: 70.88  | best_test_acc: 70.88 | test acc:71.12 | time:13.6ms
epoch:140 | loss:1.3271 | train acc:99.29 | val acc:72.60 | test_acc_best_val: 70.88  | best_test_acc: 70.88 | test acc:71.03 | time:8.6ms
epoch:160 | loss:1.3125 | train acc:99.29 | val acc:73.19 | test_acc_best_val: 71.66  | best_test_acc: 71.66 | test acc:71.75 | time:11.2ms
epoch:180 | loss:1.3204 | train acc:99.29 | val acc:72.99 | test_acc_best_val: 71.66  | best_test_acc: 71.66 | test acc:71.27 | time:10.9ms
Run 9/10, best test accuracy: 71.66, acc(last): 71.46, total time: 3.08s
The split is 10
number of hyperedges is 1579
The hypergraph cora/cocitation has 1579 hyperedges where authors are hyperedges
The average hyperedge contains 3.0310322989233693 nodes
Run 10/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGCNConv(64, 7, heads=1)
  (convs): ModuleList(
    (0): UniGCNConv(1433, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:92160
epoch:0 | loss:2.0089 | train acc:75.00 | val acc:50.29 | test_acc_best_val: 48.13  | best_test_acc: 48.13 | test acc:48.13 | time:9.4ms
epoch:20 | loss:1.3062 | train acc:100.00 | val acc:73.58 | test_acc_best_val: 72.24  | best_test_acc: 72.24 | test acc:72.00 | time:8.2ms
epoch:40 | loss:1.2991 | train acc:100.00 | val acc:71.23 | test_acc_best_val: 72.05  | best_test_acc: 72.05 | test acc:71.95 | time:9.5ms
epoch:60 | loss:1.2869 | train acc:100.00 | val acc:70.45 | test_acc_best_val: 72.05  | best_test_acc: 72.05 | test acc:72.14 | time:8.6ms
epoch:80 | loss:1.3154 | train acc:100.00 | val acc:70.84 | test_acc_best_val: 72.05  | best_test_acc: 72.05 | test acc:72.19 | time:8.8ms
epoch:100 | loss:1.3146 | train acc:100.00 | val acc:71.04 | test_acc_best_val: 72.05  | best_test_acc: 72.05 | test acc:73.02 | time:8.5ms
epoch:120 | loss:1.3076 | train acc:100.00 | val acc:71.43 | test_acc_best_val: 72.05  | best_test_acc: 72.05 | test acc:72.63 | time:13.0ms
epoch:140 | loss:1.3125 | train acc:100.00 | val acc:71.62 | test_acc_best_val: 72.05  | best_test_acc: 72.05 | test acc:72.29 | time:7.9ms
epoch:160 | loss:1.3137 | train acc:100.00 | val acc:71.43 | test_acc_best_val: 72.05  | best_test_acc: 72.05 | test acc:72.53 | time:11.6ms
epoch:180 | loss:1.3054 | train acc:100.00 | val acc:71.62 | test_acc_best_val: 72.05  | best_test_acc: 72.05 | test acc:72.82 | time:10.0ms
Run 10/10, best test accuracy: 72.05, acc(last): 73.12, total time: 2.64s
We had 80 runs
Average test accuracy for best val: 71.39705918729305  1.4405654150083274
Average final test accuracy: 71.86254277825356  1.3139116581805297
Average best test accuracy: 71.39705918729305  1.4405654150083274
