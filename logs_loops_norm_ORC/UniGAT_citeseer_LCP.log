[ Info: Welcome to Julia
[ Info: Importing Hypergraph
[ Info: Aggregation, the aggregation function, is (Orchid.AggregateMean, Orchid.AggregateMax)
[ Info: Edges is [[1, 541], [1, 541], [1, 193, 157, 91, 192], [1, 193, 157, 91, 192], [1, 193, 157, 91, 192], [1, 193, 157, 91, 192], [1, 193, 157, 91, 192], [1, 60], [1, 60], [1, 392, 116], [1, 392, 116], [1, 392, 116], [1, 133, 298], [1, 133, 298], [1, 133, 298], [2, 134, 224, 199, 287, 405, 428, 120, 397, 23], [2, 134, 224, 199, 287, 405, 428, 120, 397, 23], [2, 134, 224, 199, 287, 405, 428, 120, 397, 23], [2, 134, 224, 199, 287, 405, 428, 120, 397, 23], [2, 134, 224, 199, 287, 405, 428, 120, 397, 23], [2, 134, 224, 199, 287, 405, 428, 120, 397, 23], [2, 134, 224, 199, 287, 405, 428, 120, 397, 23], [2, 134, 224, 199, 287, 405, 428, 120, 397, 23], [2, 134, 224, 199, 287, 405, 428, 120, 397, 23], [2, 134, 224, 199, 287, 405, 428, 120, 397, 23], [2, 362, 516, 112, 448, 293, 294, 127, 26, 132, 138, 229, 230, 56, 254, 104, 493, 501, 277], [2, 362, 516, 112, 448, 293, 294, 127, 26, 132, 138, 229, 230, 56, 254, 104, 493, 501, 277], [2, 362, 516, 112, 448, 293, 294, 127, 26, 132, 138, 229, 230, 56, 254, 104, 493, 501, 277], [2, 362, 516, 112, 448, 293, 294, 127, 26, 132, 138, 229, 230, 56, 254, 104, 493, 501, 277], [2, 362, 516, 112, 448, 293, 294, 127, 26, 132, 138, 229, 230, 56, 254, 104, 493, 501, 277], [2, 362, 516, 112, 448, 293, 294, 127, 26, 132, 138, 229, 230, 56, 254, 104, 493, 501, 277], [2, 362, 516, 112, 448, 293, 294, 127, 26, 132, 138, 229, 230, 56, 254, 104, 493, 501, 277], [2, 362, 516, 112, 448, 293, 294, 127, 26, 132, 138, 229, 230, 56, 254, 104, 493, 501, 277], [2, 362, 516, 112, 448, 293, 294, 127, 26, 132, 138, 229, 230, 56, 254, 104, 493, 501, 277], [2, 362, 516, 112, 448, 293, 294, 127, 26, 132, 138, 229, 230, 56, 254, 104, 493, 501, 277], [2, 362, 516, 112, 448, 293, 294, 127, 26, 132, 138, 229, 230, 56, 254, 104, 493, 501, 277], [2, 362, 516, 112, 448, 293, 294, 127, 26, 132, 138, 229, 230, 56, 254, 104, 493, 501, 277], [2, 362, 516, 112, 448, 293, 294, 127, 26, 132, 138, 229, 230, 56, 254, 104, 493, 501, 277], [2, 362, 516, 112, 448, 293, 294, 127, 26, 132, 138, 229, 230, 56, 254, 104, 493, 501, 277], [2, 362, 516, 112, 448, 293, 294, 127, 26, 132, 138, 229, 230, 56, 254, 104, 493, 501, 277], [2, 362, 516, 112, 448, 293, 294, 127, 26, 132, 138, 229, 230, 56, 254, 104, 493, 501, 277], [2, 362, 516, 112, 448, 293, 294, 127, 26, 132, 138, 229, 230, 56, 254, 104, 493, 501, 277], [2, 362, 516, 112, 448, 293, 294, 127, 26, 132, 138, 229, 230, 56, 254, 104, 493, 501, 277], [2, 362, 516, 112, 448, 293, 294, 127, 26, 132, 138, 229, 230, 56, 254, 104, 493, 501, 277], [2, 964, 493, 649], [2, 964, 493, 649], [2, 964, 493, 649], [2, 964, 493, 649], [577, 1060], [577, 1060], [1076, 577], [1076, 577], [1076, 577], [1076, 577], [3, 1404], [3, 1404], [364, 1315, 578], [364, 1315, 578], [364, 1315, 578], [578, 712], [578, 712], [785, 579, 896], [785, 579, 896], [785, 579, 896], [812, 579, 896], [812, 579, 896], [812, 579, 896], [579, 896], [579, 896], [608, 758, 1108, 941, 690, 579, 601, 785, 896], [608, 758, 1108, 941, 690, 579, 601, 785, 896], [608, 758, 1108, 941, 690, 579, 601, 785, 896], [608, 758, 1108, 941, 690, 579, 601, 785, 896], [608, 758, 1108, 941, 690, 579, 601, 785, 896], [608, 758, 1108, 941, 690, 579, 601, 785, 896], [608, 758, 1108, 941, 690, 579, 601, 785, 896], [608, 758, 1108, 941, 690, 579, 601, 785, 896], [608, 758, 1108, 941, 690, 579, 601, 785, 896], [907, 726, 901, 580], [907, 726, 901, 580], [907, 726, 901, 580], [907, 726, 901, 580], [609, 580], [609, 580], [609, 726, 871, 907, 580], [609, 726, 871, 907, 580], [609, 726, 871, 907, 580], [609, 726, 871, 907, 580], [609, 726, 871, 907, 580], [629, 608, 991, 683, 972, 615, 617, 581, 736, 874, 239, 692, 785, 661], [629, 608, 991, 683, 972, 615, 617, 581, 736, 874, 239, 692, 785, 661], [629, 608, 991, 683, 972, 615, 617, 581, 736, 874, 239, 692, 785, 661], [629, 608, 991, 683, 972, 615, 617, 581, 736, 874, 239, 692, 785, 661], [629, 608, 991, 683, 972, 615, 617, 581, 736, 874, 239, 692, 785, 661], [629, 608, 991, 683, 972, 615, 617, 581, 736, 874, 239, 692, 785, 661], [629, 608, 991, 683, 972, 615, 617, 581, 736, 874, 239, 692, 785, 661], [629, 608, 991, 683, 972, 615, 617, 581, 736, 874, 239, 692, 785, 661], [629, 608, 991, 683, 972, 615, 617, 581, 736, 874, 239, 692, 785, 661], [629, 608, 991, 683, 972, 615, 617, 581, 736, 874, 239, 692, 785, 661], [629, 608, 991, 683, 972, 615, 617, 581, 736, 874, 239, 692, 785, 661], [629, 608, 991, 683, 972, 615, 617, 581, 736, 874, 239, 692, 785, 661], [629, 608, 991, 683, 972, 615, 617, 581, 736, 874, 239, 692, 785, 661], [629, 608, 991, 683, 972, 615, 617, 581, 736, 874, 239, 692, 785, 661], [991, 757, 776, 581, 627], [991, 757, 776, 581, 627], [991, 757, 776, 581, 627], [991, 757, 776, 581, 627], [991, 757, 776, 581, 627], [757, 776, 581, 627, 756], [757, 776, 581, 627, 756], [757, 776, 581, 627, 756], [757, 776, 581, 627, 756], [757, 776, 581, 627, 756], [627, 737, 581], [627, 737, 581], [627, 737, 581], [1130, 57], [1130, 57], [27, 389, 1130], [27, 389, 1130], [27, 389, 1130], [250, 1130], [250, 1130], [582, 840], [582, 840], [4, 138, 429, 145, 470, 275, 75], [4, 138, 429, 145, 470, 275, 75], [4, 138, 429, 145, 470, 275, 75], [4, 138, 429, 145, 470, 275, 75], [4, 138, 429, 145, 470, 275, 75], [4, 138, 429, 145, 470, 275, 75], [4, 138, 429, 145, 470, 275, 75], [1131, 1162, 1153], [1131, 1162, 1153], [1131, 1162, 1153], [1131, 1162, 1153], [1131, 1162, 1153], [1131, 1162, 1153], [1132, 1366, 1301], [1132, 1366, 1301], [1132, 1366, 1301], [842, 583, 554, 813], [842, 583, 554, 813], [842, 583, 554, 813], [842, 583, 554, 813], [583, 159], [583, 159], [422, 5, 137], [422, 5, 137], [422, 5, 137], [362, 1098, 448, 584, 25], [362, 1098, 448, 584, 25], [362, 1098, 448, 584, 25], [362, 1098, 448, 584, 25], [362, 1098, 448, 584, 25], [64, 6, 161, 119, 520, 373], [64, 6, 161, 119, 520, 373], [64, 6, 161, 119, 520, 373], [64, 6, 161, 119, 520, 373], [64, 6, 161, 119, 520, 373], [64, 6, 161, 119, 520, 373], [64, 6, 161, 119, 520, 373], [64, 6, 161, 119, 520, 373], [64, 6, 161, 119, 520, 373], [64, 6, 161, 119, 520, 373], [64, 6, 161, 119, 520, 373], [64, 6, 161, 119, 520, 373], [161, 6, 373], [161, 6, 373], [161, 6, 373], [6, 373], [6, 373], [64, 6, 48], [64, 6, 48], [64, 6, 48], [996, 585], [996, 585], [360, 7], [360, 7], [360, 7], [360, 7], [586, 622], [586, 622], [282, 496, 383, 8, 35, 13, 115, 231, 147, 168, 524, 259], [282, 496, 383, 8, 35, 13, 115, 231, 147, 168, 524, 259], [282, 496, 383, 8, 35, 13, 115, 231, 147, 168, 524, 259], [282, 496, 383, 8, 35, 13, 115, 231, 147, 168, 524, 259], [282, 496, 383, 8, 35, 13, 115, 231, 147, 168, 524, 259], [282, 496, 383, 8, 35, 13, 115, 231, 147, 168, 524, 259], [282, 496, 383, 8, 35, 13, 115, 231, 147, 168, 524, 259], [282, 496, 383, 8, 35, 13, 115, 231, 147, 168, 524, 259], [282, 496, 383, 8, 35, 13, 115, 231, 147, 168, 524, 259], [282, 496, 383, 8, 35, 13, 115, 231, 147, 168, 524, 259], [282, 496, 383, 8, 35, 13, 115, 231, 147, 168, 524, 259], [282, 496, 383, 8, 35, 13, 115, 231, 147, 168, 524, 259], [8, 147], [8, 147], [1102, 850, 587, 880], [1102, 850, 587, 880], [1102, 850, 587, 880], [1102, 850, 587, 880], [738, 587], [738, 587], [806, 587, 656, 1006], [806, 587, 656, 1006], [806, 587, 656, 1006], [806, 587, 656, 1006], [1111, 1025, 588], [1111, 1025, 588], [1111, 1025, 588], [936, 1056, 589, 1020], [936, 1056, 589, 1020], [936, 1056, 589, 1020], [936, 1056, 589, 1020], [545, 856, 589], [545, 856, 589], [545, 856, 589], [780, 589], [780, 589], [314, 9, 323], [314, 9, 323], [314, 9, 323], [194, 195, 196, 529, 530, 324, 9, 366, 483], [194, 195, 196, 529, 530, 324, 9, 366, 483], [194, 195, 196, 529, 530, 324, 9, 366, 483], [194, 195, 196, 529, 530, 324, 9, 366, 483], [194, 195, 196, 529, 530, 324, 9, 366, 483], [194, 195, 196, 529, 530, 324, 9, 366, 483], [194, 195, 196, 529, 530, 324, 9, 366, 483], [194, 195, 196, 529, 530, 324, 9, 366, 483], [194, 195, 196, 529, 530, 324, 9, 366, 483], [9, 133, 28], [9, 133, 28], [9, 133, 28], [9, 432], [9, 432], [9, 205], [9, 205], [473, 9, 530, 86], [473, 9, 530, 86], [473, 9, 530, 86], [473, 9, 530, 86], [234, 10], [234, 10], [10, 100], [10, 100], [590, 1027, 719, 1065], [590, 1027, 719, 1065], [590, 1027, 719, 1065], [590, 1027, 719, 1065], [590, 1066], [590, 1066], [590, 1065], [590, 1065], [591, 605], [591, 605], [592, 837], [592, 837], [595, 592], [595, 592], [742, 775, 592, 595, 597, 602], [742, 775, 592, 595, 597, 602], [742, 775, 592, 595, 597, 602], [742, 775, 592, 595, 597, 602], [742, 775, 592, 595, 597, 602], [742, 775, 592, 595, 597, 602], [1133, 550], [1133, 550], [1133, 550, 1323, 41], [1133, 550, 1323, 41], [1133, 550, 1323, 41], [1133, 550, 1323, 41], [926, 921, 630, 593], [926, 921, 630, 593], [926, 921, 630, 593], [926, 921, 630, 593], [926, 921, 630, 593], [926, 921, 630, 593], [926, 921, 630, 593], [926, 921, 630, 593], [630, 1014, 593, 926, 641, 669], [630, 1014, 593, 926, 641, 669], [630, 1014, 593, 926, 641, 669], [630, 1014, 593, 926, 641, 669], [630, 1014, 593, 926, 641, 669], [630, 1014, 593, 926, 641, 669], [43, 593, 1109], [43, 593, 1109], [43, 593, 1109], [43, 593, 602], [43, 593, 602], [43, 593, 602], [43, 851, 593], [43, 851, 593], [43, 851, 593], [43, 593], [43, 593], [1023, 964, 493, 594], [1023, 964, 493, 594], [1023, 964, 493, 594], [1023, 964, 493, 594], [595, 832], [595, 832], [595, 722, 621, 832], [595, 722, 621, 832], [595, 722, 621, 832], [595, 722, 621, 832], [596, 704, 775, 645], [596, 704, 775, 645], [596, 704, 775, 645], [596, 704, 775, 645], [866, 811, 1134, 57, 1405, 1358, 1409, 474], [866, 811, 1134, 57, 1405, 1358, 1409, 474], [866, 811, 1134, 57, 1405, 1358, 1409, 474], [866, 811, 1134, 57, 1405, 1358, 1409, 474], [866, 811, 1134, 57, 1405, 1358, 1409, 474], [866, 811, 1134, 57, 1405, 1358, 1409, 474], [866, 811, 1134, 57, 1405, 1358, 1409, 474], [866, 811, 1134, 57, 1405, 1358, 1409, 474], [865, 700, 995, 575, 503, 93, 1358, 505, 62, 1134, 26], [865, 700, 995, 575, 503, 93, 1358, 505, 62, 1134, 26], [865, 700, 995, 575, 503, 93, 1358, 505, 62, 1134, 26], [865, 700, 995, 575, 503, 93, 1358, 505, 62, 1134, 26], [865, 700, 995, 575, 503, 93, 1358, 505, 62, 1134, 26], [865, 700, 995, 575, 503, 93, 1358, 505, 62, 1134, 26], [865, 700, 995, 575, 503, 93, 1358, 505, 62, 1134, 26], [865, 700, 995, 575, 503, 93, 1358, 505, 62, 1134, 26], [865, 700, 995, 575, 503, 93, 1358, 505, 62, 1134, 26], [865, 700, 995, 575, 503, 93, 1358, 505, 62, 1134, 26], [865, 700, 995, 575, 503, 93, 1358, 505, 62, 1134, 26], [339, 29, 866, 31, 811, 252, 57, 1358, 122, 1134], [339, 29, 866, 31, 811, 252, 57, 1358, 122, 1134], [339, 29, 866, 31, 811, 252, 57, 1358, 122, 1134], [339, 29, 866, 31, 811, 252, 57, 1358, 122, 1134], [339, 29, 866, 31, 811, 252, 57, 1358, 122, 1134], [339, 29, 866, 31, 811, 252, 57, 1358, 122, 1134], [339, 29, 866, 31, 811, 252, 57, 1358, 122, 1134], [339, 29, 866, 31, 811, 252, 57, 1358, 122, 1134], [339, 29, 866, 31, 811, 252, 57, 1358, 122, 1134], [339, 29, 866, 31, 811, 252, 57, 1358, 122, 1134], [339, 1134, 1358, 57], [339, 1134, 1358, 57], [339, 1134, 1358, 57], [339, 1134, 1358, 57], [597, 837], [597, 837], [850, 598], [850, 598], [850, 598, 1116, 880], [850, 598, 1116, 880], [850, 598, 1116, 880], [850, 598, 1116, 880], [850, 994, 713, 598, 644], [850, 994, 713, 598, 644], [850, 994, 713, 598, 644], [850, 994, 713, 598, 644], [850, 994, 713, 598, 644], [778, 850, 598, 644], [778, 850, 598, 644], [778, 850, 598, 644], [778, 850, 598, 644], [850, 598], [850, 598], [992, 599], [992, 599], [600, 772], [600, 772], [1091, 885, 600, 772], [1091, 885, 600, 772], [1091, 885, 600, 772], [1091, 885, 600, 772], [1091, 1010, 600, 772], [1091, 1010, 600, 772], [1091, 1010, 600, 772], [1091, 1010, 600, 772], [987, 772, 600, 978], [987, 772, 600, 978], [987, 772, 600, 978], [987, 772, 600, 978], [1091, 1055, 600, 772], [1091, 1055, 600, 772], [1091, 1055, 600, 772], [1091, 1055, 600, 772], [1055, 600, 772], [1055, 600, 772], [1055, 600, 772], [1080, 810, 749, 600, 657, 772], [1080, 810, 749, 600, 657, 772], [1080, 810, 749, 600, 657, 772], [1080, 810, 749, 600, 657, 772], [1080, 810, 749, 600, 657, 772], [1080, 810, 749, 600, 657, 772], [1056, 600, 772], [1056, 600, 772], [1056, 600, 772], [1259, 647, 1000, 601, 803], [1259, 647, 1000, 601, 803], [1259, 647, 1000, 601, 803], [1259, 647, 1000, 601, 803], [1259, 647, 1000, 601, 803], [1259, 647, 1000, 601, 803], [1259, 647, 1000, 601, 803], [1259, 647, 1000, 601, 803], [1259, 647, 1000, 601, 803], [1259, 647, 1000, 601, 803], [1260, 803, 601], [1260, 803, 601], [1260, 803, 601], [876, 647, 601], [876, 647, 601], [876, 647, 601], [43, 602], [43, 602], [602, 832], [602, 832], [1075, 602, 925], [1075, 602, 925], [1075, 602, 925], [1384, 1135], [1384, 1135], [603, 90, 798], [603, 90, 798], [603, 90, 798], [607, 603, 930, 798], [607, 603, 930, 798], [607, 603, 930, 798], [607, 603, 930, 798], [11, 274, 367], [11, 274, 367], [11, 274, 367], [905, 11], [905, 11], [182, 11, 459], [182, 11, 459], [182, 11, 459], [990, 851, 693, 43, 604], [990, 851, 693, 43, 604], [990, 851, 693, 43, 604], [990, 851, 693, 43, 604], [990, 851, 693, 43, 604], [1136, 887, 1380], [1136, 887, 1380], [1136, 887, 1380], [549, 1136, 611], [549, 1136, 611], [549, 1136, 611], [614, 606], [614, 606], [614, 606], [614, 606], [614, 606], [614, 606], [392, 12], [392, 12], [14, 271, 472, 255, 300], [14, 271, 472, 255, 300], [14, 271, 472, 255, 300], [14, 271, 472, 255, 300], [14, 271, 472, 255, 300], [850, 1137], [850, 1137], [1436, 1138], [1436, 1138], [871, 609, 726, 907], [871, 609, 726, 907], [871, 609, 726, 907], [871, 609, 726, 907], [15, 335, 483], [15, 335, 483], [15, 335, 483], [16, 375], [16, 375], [975, 610], [975, 610], [66, 531, 385, 199, 304, 574, 17, 237], [66, 531, 385, 199, 304, 574, 17, 237], [66, 531, 385, 199, 304, 574, 17, 237], [66, 531, 385, 199, 304, 574, 17, 237], [66, 531, 385, 199, 304, 574, 17, 237], [66, 531, 385, 199, 304, 574, 17, 237], [66, 531, 385, 199, 304, 574, 17, 237], [66, 531, 385, 199, 304, 574, 17, 237], [396, 199, 18], [396, 199, 18], [396, 199, 18], [576, 199, 18], [576, 199, 18], [576, 199, 18], [224, 287, 307, 120, 18], [224, 287, 307, 120, 18], [224, 287, 307, 120, 18], [224, 287, 307, 120, 18], [224, 287, 307, 120, 18], [332, 18], [332, 18], [199, 345, 113, 397, 18, 237, 396, 77, 399, 441], [199, 345, 113, 397, 18, 237, 396, 77, 399, 441], [199, 345, 113, 397, 18, 237, 396, 77, 399, 441], [199, 345, 113, 397, 18, 237, 396, 77, 399, 441], [199, 345, 113, 397, 18, 237, 396, 77, 399, 441], [199, 345, 113, 397, 18, 237, 396, 77, 399, 441], [199, 345, 113, 397, 18, 237, 396, 77, 399, 441], [199, 345, 113, 397, 18, 237, 396, 77, 399, 441], [199, 345, 113, 397, 18, 237, 396, 77, 399, 441], [199, 345, 113, 397, 18, 237, 396, 77, 399, 441], [199, 18], [199, 18], [1200, 1139], [1200, 1139], [1146, 421, 1377, 239, 1140, 1287, 26], [1146, 421, 1377, 239, 1140, 1287, 26], [1146, 421, 1377, 239, 1140, 1287, 26], [1146, 421, 1377, 239, 1140, 1287, 26], [1146, 421, 1377, 239, 1140, 1287, 26], [1146, 421, 1377, 239, 1140, 1287, 26], [1146, 421, 1377, 239, 1140, 1287, 26], [1287, 1140], [1287, 1140], [40, 19], [40, 19], [19, 489], [19, 489], [20, 140, 254], [20, 140, 254], [20, 140, 254], [444, 515, 109, 516, 112, 250, 20], [444, 515, 109, 516, 112, 250, 20], [444, 515, 109, 516, 112, 250, 20], [444, 515, 109, 516, 112, 250, 20], [444, 515, 109, 516, 112, 250, 20], [444, 515, 109, 516, 112, 250, 20], [444, 515, 109, 516, 112, 250, 20], [20, 190], [20, 190], [65, 87, 284, 225, 305, 326, 20], [65, 87, 284, 225, 305, 326, 20], [65, 87, 284, 225, 305, 326, 20], [65, 87, 284, 225, 305, 326, 20], [65, 87, 284, 225, 305, 326, 20], [65, 87, 284, 225, 305, 326, 20], [65, 87, 284, 225, 305, 326, 20], [887, 611], [887, 611], [1141, 1225], [1141, 1225], [917, 612, 998, 689, 457, 1011, 897], [917, 612, 998, 689, 457, 1011, 897], [917, 612, 998, 689, 457, 1011, 897], [917, 612, 998, 689, 457, 1011, 897], [917, 612, 998, 689, 457, 1011, 897], [917, 612, 998, 689, 457, 1011, 897], [917, 612, 998, 689, 457, 1011, 897], [139, 111, 251, 166, 121, 186, 277, 21], [139, 111, 251, 166, 121, 186, 277, 21], [139, 111, 251, 166, 121, 186, 277, 21], [139, 111, 251, 166, 121, 186, 277, 21], [139, 111, 251, 166, 121, 186, 277, 21], [139, 111, 251, 166, 121, 186, 277, 21], [139, 111, 251, 166, 121, 186, 277, 21], [139, 111, 251, 166, 121, 186, 277, 21], [506, 21], [506, 21], [139, 111, 310, 166, 121, 186, 295, 21], [139, 111, 310, 166, 121, 186, 295, 21], [139, 111, 310, 166, 121, 186, 295, 21], [139, 111, 310, 166, 121, 186, 295, 21], [139, 111, 310, 166, 121, 186, 295, 21], [139, 111, 310, 166, 121, 186, 295, 21], [139, 111, 310, 166, 121, 186, 295, 21], [139, 111, 310, 166, 121, 186, 295, 21], [21, 134], [21, 134], [111, 251, 166, 121, 186, 21], [111, 251, 166, 121, 186, 21], [111, 251, 166, 121, 186, 21], [111, 251, 166, 121, 186, 21], [111, 251, 166, 121, 186, 21], [111, 251, 166, 121, 186, 21], [1142, 1222, 1357], [1142, 1222, 1357], [1142, 1222, 1357], [22, 534], [22, 534], [613, 1072], [613, 1072], [818, 422, 687, 616, 329], [818, 422, 687, 616, 329], [818, 422, 687, 616, 329], [818, 422, 687, 616, 329], [818, 422, 687, 616, 329], [27, 818, 422, 344, 201, 687, 616, 389], [27, 818, 422, 344, 201, 687, 616, 389], [27, 818, 422, 344, 201, 687, 616, 389], [27, 818, 422, 344, 201, 687, 616, 389], [27, 818, 422, 344, 201, 687, 616, 389], [27, 818, 422, 344, 201, 687, 616, 389], [27, 818, 422, 344, 201, 687, 616, 389], [27, 818, 422, 344, 201, 687, 616, 389], [1143, 1361, 1426], [1143, 1361, 1426], [1143, 1361, 1426], [1143, 1371, 1426, 1215], [1143, 1371, 1426, 1215], [1143, 1371, 1426, 1215], [1143, 1371, 1426, 1215], [1143, 1166, 1426], [1143, 1166, 1426], [1143, 1166, 1426], [1144, 1235], [1144, 1235], [1144, 167], [1144, 167], [901, 618], [901, 618], [23, 428, 224], [23, 428, 224], [23, 428, 224], [51, 224, 569, 199, 568, 287, 405, 428, 23, 120, 95, 397, 543], [51, 224, 569, 199, 568, 287, 405, 428, 23, 120, 95, 397, 543], [51, 224, 569, 199, 568, 287, 405, 428, 23, 120, 95, 397, 543], [51, 224, 569, 199, 568, 287, 405, 428, 23, 120, 95, 397, 543], [51, 224, 569, 199, 568, 287, 405, 428, 23, 120, 95, 397, 543], [51, 224, 569, 199, 568, 287, 405, 428, 23, 120, 95, 397, 543], [51, 224, 569, 199, 568, 287, 405, 428, 23, 120, 95, 397, 543], [51, 224, 569, 199, 568, 287, 405, 428, 23, 120, 95, 397, 543], [51, 224, 569, 199, 568, 287, 405, 428, 23, 120, 95, 397, 543], [51, 224, 569, 199, 568, 287, 405, 428, 23, 120, 95, 397, 543], [51, 224, 569, 199, 568, 287, 405, 428, 23, 120, 95, 397, 543], [51, 224, 569, 199, 568, 287, 405, 428, 23, 120, 95, 397, 543], [51, 224, 569, 199, 568, 287, 405, 428, 23, 120, 95, 397, 543], [51, 224, 569, 568, 287, 405, 428, 120, 397, 23], [51, 224, 569, 568, 287, 405, 428, 120, 397, 23], [51, 224, 569, 568, 287, 405, 428, 120, 397, 23], [51, 224, 569, 568, 287, 405, 428, 120, 397, 23], [51, 224, 569, 568, 287, 405, 428, 120, 397, 23], [51, 224, 569, 568, 287, 405, 428, 120, 397, 23], [51, 224, 569, 568, 287, 405, 428, 120, 397, 23], [51, 224, 569, 568, 287, 405, 428, 120, 397, 23], [51, 224, 569, 568, 287, 405, 428, 120, 397, 23], [51, 224, 569, 568, 287, 405, 428, 120, 397, 23], [134, 224, 569, 199, 568, 287, 307, 405, 428, 38, 120, 95, 240, 397, 23], [134, 224, 569, 199, 568, 287, 307, 405, 428, 38, 120, 95, 240, 397, 23], [134, 224, 569, 199, 568, 287, 307, 405, 428, 38, 120, 95, 240, 397, 23], [134, 224, 569, 199, 568, 287, 307, 405, 428, 38, 120, 95, 240, 397, 23], [134, 224, 569, 199, 568, 287, 307, 405, 428, 38, 120, 95, 240, 397, 23], [134, 224, 569, 199, 568, 287, 307, 405, 428, 38, 120, 95, 240, 397, 23], [134, 224, 569, 199, 568, 287, 307, 405, 428, 38, 120, 95, 240, 397, 23], [134, 224, 569, 199, 568, 287, 307, 405, 428, 38, 120, 95, 240, 397, 23], [134, 224, 569, 199, 568, 287, 307, 405, 428, 38, 120, 95, 240, 397, 23], [134, 224, 569, 199, 568, 287, 307, 405, 428, 38, 120, 95, 240, 397, 23], [134, 224, 569, 199, 568, 287, 307, 405, 428, 38, 120, 95, 240, 397, 23], [134, 224, 569, 199, 568, 287, 307, 405, 428, 38, 120, 95, 240, 397, 23], [134, 224, 569, 199, 568, 287, 307, 405, 428, 38, 120, 95, 240, 397, 23], [134, 224, 569, 199, 568, 287, 307, 405, 428, 38, 120, 95, 240, 397, 23], [134, 224, 569, 199, 568, 287, 307, 405, 428, 38, 120, 95, 240, 397, 23], [253, 543, 23, 224], [253, 543, 23, 224], [253, 543, 23, 224], [253, 543, 23, 224], [224, 569, 405, 287, 428, 120, 397, 23], [224, 569, 405, 287, 428, 120, 397, 23], [224, 569, 405, 287, 428, 120, 397, 23], [224, 569, 405, 287, 428, 120, 397, 23], [224, 569, 405, 287, 428, 120, 397, 23], [224, 569, 405, 287, 428, 120, 397, 23], [224, 569, 405, 287, 428, 120, 397, 23], [224, 569, 405, 287, 428, 120, 397, 23], [1145, 752], [1145, 752], [1119, 448, 25], [1119, 448, 25], [1119, 448, 25], [931, 448, 25], [931, 448, 25], [931, 448, 25], [918, 362, 905, 448, 93, 25, 26], [918, 362, 905, 448, 93, 25, 26], [918, 362, 905, 448, 93, 25, 26], [918, 362, 905, 448, 93, 25, 26], [918, 362, 905, 448, 93, 25, 26], [918, 362, 905, 448, 93, 25, 26], [918, 362, 905, 448, 93, 25, 26], [362, 729, 448, 859, 93, 313, 25], [362, 729, 448, 859, 93, 313, 25], [362, 729, 448, 859, 93, 313, 25], [362, 729, 448, 859, 93, 313, 25], [362, 729, 448, 859, 93, 313, 25], [362, 729, 448, 859, 93, 313, 25], [362, 729, 448, 859, 93, 313, 25], [1087, 448, 667, 25], [1087, 448, 667, 25], [1087, 448, 667, 25], [1087, 448, 667, 25], [865, 995, 448, 93, 25], [865, 995, 448, 93, 25], [865, 995, 448, 93, 25], [865, 995, 448, 93, 25], [865, 995, 448, 93, 25], [724, 1046, 362, 448, 1041, 25], [724, 1046, 362, 448, 1041, 25], [724, 1046, 362, 448, 1041, 25], [724, 1046, 362, 448, 1041, 25], [724, 1046, 362, 448, 1041, 25], [724, 1046, 362, 448, 1041, 25], [619, 93, 684, 254], [619, 93, 684, 254], [619, 93, 684, 254], [619, 93, 684, 254], [455, 24], [455, 24], [454, 455, 24], [454, 455, 24], [454, 455, 24], [455, 888, 24], [455, 888, 24], [455, 888, 24], [454, 455, 24], [454, 455, 24], [454, 455, 24], [26, 254], [26, 254], [112, 93, 26], [112, 93, 26], [112, 93, 26], [104, 501, 26, 974], [104, 501, 26, 974], [104, 501, 26, 974], [104, 501, 26, 974], [717, 26], [717, 26], [26, 254], [26, 254], [93, 26], [93, 26], [866, 26], [866, 26], [26, 138], [26, 138], [1093, 1094, 26], [1093, 1094, 26], [1093, 1094, 26], [620, 732, 684, 254], [620, 732, 684, 254], [620, 732, 684, 254], [620, 732, 684, 254], [836, 623], [836, 623], [623, 832], [623, 832], [27, 443, 389, 1092], [27, 443, 389, 1092], [27, 443, 389, 1092], [27, 443, 389, 1092], [27, 444, 1203, 1159, 516, 1263, 870, 389, 980, 505, 771, 1267], [27, 444, 1203, 1159, 516, 1263, 870, 389, 980, 505, 771, 1267], [27, 444, 1203, 1159, 516, 1263, 870, 389, 980, 505, 771, 1267], [27, 444, 1203, 1159, 516, 1263, 870, 389, 980, 505, 771, 1267], [27, 444, 1203, 1159, 516, 1263, 870, 389, 980, 505, 771, 1267], [27, 444, 1203, 1159, 516, 1263, 870, 389, 980, 505, 771, 1267], [27, 444, 1203, 1159, 516, 1263, 870, 389, 980, 505, 771, 1267], [27, 444, 1203, 1159, 516, 1263, 870, 389, 980, 505, 771, 1267], [27, 444, 1203, 1159, 516, 1263, 870, 389, 980, 505, 771, 1267], [27, 444, 1203, 1159, 516, 1263, 870, 389, 980, 505, 771, 1267], [27, 444, 1203, 1159, 516, 1263, 870, 389, 980, 505, 771, 1267], [27, 444, 1203, 1159, 516, 1263, 870, 389, 980, 505, 771, 1267], [27, 444, 516, 389, 980, 505, 1267], [27, 444, 516, 389, 980, 505, 1267], [27, 444, 516, 389, 980, 505, 1267], [27, 444, 516, 389, 980, 505, 1267], [27, 444, 516, 389, 980, 505, 1267], [27, 444, 516, 389, 980, 505, 1267], [27, 444, 516, 389, 980, 505, 1267], [199, 516, 287, 120, 128, 27, 29, 303, 389, 237, 95, 396, 397, 398, 44, 315, 316, 318, 249, 59, 253, 254, 261, 182, 428, 505], [199, 516, 287, 120, 128, 27, 29, 303, 389, 237, 95, 396, 397, 398, 44, 315, 316, 318, 249, 59, 253, 254, 261, 182, 428, 505], [199, 516, 287, 120, 128, 27, 29, 303, 389, 237, 95, 396, 397, 398, 44, 315, 316, 318, 249, 59, 253, 254, 261, 182, 428, 505], [199, 516, 287, 120, 128, 27, 29, 303, 389, 237, 95, 396, 397, 398, 44, 315, 316, 318, 249, 59, 253, 254, 261, 182, 428, 505], [199, 516, 287, 120, 128, 27, 29, 303, 389, 237, 95, 396, 397, 398, 44, 315, 316, 318, 249, 59, 253, 254, 261, 182, 428, 505], [199, 516, 287, 120, 128, 27, 29, 303, 389, 237, 95, 396, 397, 398, 44, 315, 316, 318, 249, 59, 253, 254, 261, 182, 428, 505], [199, 516, 287, 120, 128, 27, 29, 303, 389, 237, 95, 396, 397, 398, 44, 315, 316, 318, 249, 59, 253, 254, 261, 182, 428, 505], [199, 516, 287, 120, 128, 27, 29, 303, 389, 237, 95, 396, 397, 398, 44, 315, 316, 318, 249, 59, 253, 254, 261, 182, 428, 505], [199, 516, 287, 120, 128, 27, 29, 303, 389, 237, 95, 396, 397, 398, 44, 315, 316, 318, 249, 59, 253, 254, 261, 182, 428, 505], [199, 516, 287, 120, 128, 27, 29, 303, 389, 237, 95, 396, 397, 398, 44, 315, 316, 318, 249, 59, 253, 254, 261, 182, 428, 505], [199, 516, 287, 120, 128, 27, 29, 303, 389, 237, 95, 396, 397, 398, 44, 315, 316, 318, 249, 59, 253, 254, 261, 182, 428, 505], [199, 516, 287, 120, 128, 27, 29, 303, 389, 237, 95, 396, 397, 398, 44, 315, 316, 318, 249, 59, 253, 254, 261, 182, 428, 505], [199, 516, 287, 120, 128, 27, 29, 303, 389, 237, 95, 396, 397, 398, 44, 315, 316, 318, 249, 59, 253, 254, 261, 182, 428, 505], [199, 516, 287, 120, 128, 27, 29, 303, 389, 237, 95, 396, 397, 398, 44, 315, 316, 318, 249, 59, 253, 254, 261, 182, 428, 505], [199, 516, 287, 120, 128, 27, 29, 303, 389, 237, 95, 396, 397, 398, 44, 315, 316, 318, 249, 59, 253, 254, 261, 182, 428, 505], [199, 516, 287, 120, 128, 27, 29, 303, 389, 237, 95, 396, 397, 398, 44, 315, 316, 318, 249, 59, 253, 254, 261, 182, 428, 505], [199, 516, 287, 120, 128, 27, 29, 303, 389, 237, 95, 396, 397, 398, 44, 315, 316, 318, 249, 59, 253, 254, 261, 182, 428, 505], [199, 516, 287, 120, 128, 27, 29, 303, 389, 237, 95, 396, 397, 398, 44, 315, 316, 318, 249, 59, 253, 254, 261, 182, 428, 505], [199, 516, 287, 120, 128, 27, 29, 303, 389, 237, 95, 396, 397, 398, 44, 315, 316, 318, 249, 59, 253, 254, 261, 182, 428, 505], [199, 516, 287, 120, 128, 27, 29, 303, 389, 237, 95, 396, 397, 398, 44, 315, 316, 318, 249, 59, 253, 254, 261, 182, 428, 505], [199, 516, 287, 120, 128, 27, 29, 303, 389, 237, 95, 396, 397, 398, 44, 315, 316, 318, 249, 59, 253, 254, 261, 182, 428, 505], [199, 516, 287, 120, 128, 27, 29, 303, 389, 237, 95, 396, 397, 398, 44, 315, 316, 318, 249, 59, 253, 254, 261, 182, 428, 505], [199, 516, 287, 120, 128, 27, 29, 303, 389, 237, 95, 396, 397, 398, 44, 315, 316, 318, 249, 59, 253, 254, 261, 182, 428, 505], [199, 516, 287, 120, 128, 27, 29, 303, 389, 237, 95, 396, 397, 398, 44, 315, 316, 318, 249, 59, 253, 254, 261, 182, 428, 505], [199, 516, 287, 120, 128, 27, 29, 303, 389, 237, 95, 396, 397, 398, 44, 315, 316, 318, 249, 59, 253, 254, 261, 182, 428, 505], [199, 516, 287, 120, 128, 27, 29, 303, 389, 237, 95, 396, 397, 398, 44, 315, 316, 318, 249, 59, 253, 254, 261, 182, 428, 505], [1089, 624], [1089, 624], [624, 1445, 952], [624, 1445, 952], [624, 1445, 952], [1445, 624], [1445, 624], [625, 862, 823], [625, 862, 823], [625, 862, 823], [707, 899, 903, 718, 625, 862], [707, 899, 903, 718, 625, 862], [707, 899, 903, 718, 625, 862], [707, 899, 903, 718, 625, 862], [707, 899, 903, 718, 625, 862], [707, 899, 903, 718, 625, 862], [626, 976], [626, 976], [626, 832], [626, 832], [1147, 1167, 1418], [1147, 1167, 1418], [1147, 1167, 1418], [1147, 1167, 1205, 1418], [1147, 1167, 1205, 1418], [1147, 1167, 1205, 1418], [1147, 1167, 1205, 1418], [1007, 627, 723], [1007, 627, 723], [1007, 627, 723], [627, 673], [627, 673], [29, 44, 199, 69, 481, 408, 144, 390, 396, 561, 316], [29, 44, 199, 69, 481, 408, 144, 390, 396, 561, 316], [29, 44, 199, 69, 481, 408, 144, 390, 396, 561, 316], [29, 44, 199, 69, 481, 408, 144, 390, 396, 561, 316], [29, 44, 199, 69, 481, 408, 144, 390, 396, 561, 316], [29, 44, 199, 69, 481, 408, 144, 390, 396, 561, 316], [29, 44, 199, 69, 481, 408, 144, 390, 396, 561, 316], [29, 44, 199, 69, 481, 408, 144, 390, 396, 561, 316], [29, 44, 199, 69, 481, 408, 144, 390, 396, 561, 316], [29, 44, 199, 69, 481, 408, 144, 390, 396, 561, 316], [29, 44, 199, 69, 481, 408, 144, 390, 396, 561, 316], [29, 397, 398, 59], [29, 397, 398, 59], [29, 397, 398, 59], [29, 397, 398, 59], [29, 826, 678], [29, 826, 678], [29, 826, 678], [29, 303, 199, 249, 391, 93, 85, 316], [29, 303, 199, 249, 391, 93, 85, 316], [29, 303, 199, 249, 391, 93, 85, 316], [29, 303, 199, 249, 391, 93, 85, 316], [29, 303, 199, 249, 391, 93, 85, 316], [29, 303, 199, 249, 391, 93, 85, 316], [29, 303, 199, 249, 391, 93, 85, 316], [29, 303, 199, 249, 391, 93, 85, 316], [29, 308, 317], [29, 308, 317], [29, 308, 317], [1148, 1394], [1148, 1394], [1148, 1394], [1148, 1394], [1148, 1394], [1148, 1394], [628, 777], [628, 777], [30, 508], [30, 508], [735, 43, 630], [735, 43, 630], [735, 43, 630], [1362, 1149], [1362, 1149], [844, 631, 924], [844, 631, 924], [844, 631, 924], [895, 31, 122], [895, 31, 122], [895, 31, 122], [79, 31, 199, 288, 122], [79, 31, 199, 288, 122], [79, 31, 199, 288, 122], [79, 31, 199, 288, 122], [79, 31, 199, 288, 122], [1313, 31, 122], [1313, 31, 122], [1313, 31, 122], [31, 283, 117, 122, 77, 397, 318], [31, 283, 117, 122, 77, 397, 318], [31, 283, 117, 122, 77, 397, 318], [31, 283, 117, 122, 77, 397, 318], [31, 283, 117, 122, 77, 397, 318], [31, 283, 117, 122, 77, 397, 318], [31, 283, 117, 122, 77, 397, 318], [31, 199, 187, 188, 237, 122, 243], [31, 199, 187, 188, 237, 122, 243], [31, 199, 187, 188, 237, 122, 243], [31, 199, 187, 188, 237, 122, 243], [31, 199, 187, 188, 237, 122, 243], [31, 199, 187, 188, 237, 122, 243], [31, 199, 187, 188, 237, 122, 243], [31, 501, 869, 57, 122, 104], [31, 501, 869, 57, 122, 104], [31, 501, 869, 57, 122, 104], [31, 501, 869, 57, 122, 104], [31, 501, 869, 57, 122, 104], [31, 501, 869, 57, 122, 104], [1039, 632, 852], [1039, 632, 852], [1039, 632, 852], [1150, 1425, 1283], [1150, 1425, 1283], [1150, 1425, 1283], [642, 633], [642, 633], [642, 633], [642, 633], [361, 32, 265, 141, 504, 522], [361, 32, 265, 141, 504, 522], [361, 32, 265, 141, 504, 522], [361, 32, 265, 141, 504, 522], [361, 32, 265, 141, 504, 522], [361, 32, 265, 141, 504, 522], [218, 220, 105, 33, 81, 472], [218, 220, 105, 33, 81, 472], [218, 220, 105, 33, 81, 472], [218, 220, 105, 33, 81, 472], [218, 220, 105, 33, 81, 472], [218, 220, 105, 33, 81, 472], [33, 105], [33, 105], [33, 105], [33, 105], [33, 215, 105, 355], [33, 215, 105, 355], [33, 215, 105, 355], [33, 215, 105, 355], [33, 105], [33, 105], [105, 33, 348, 350, 351], [105, 33, 348, 350, 351], [105, 33, 348, 350, 351], [105, 33, 348, 350, 351], [105, 33, 348, 350, 351], [33, 351, 105], [33, 351, 105], [33, 351, 105], [634, 1081, 889], [634, 1081, 889], [634, 1081, 889], [635, 157], [635, 157], [218, 34, 142, 472, 125], [218, 34, 142, 472, 125], [218, 34, 142, 472, 125], [218, 34, 142, 472, 125], [218, 34, 142, 472, 125], [1102, 636], [1102, 636], [801, 239, 636, 1102], [801, 239, 636, 1102], [801, 239, 636, 1102], [801, 239, 636, 1102], [1037, 637], [1037, 637], [638, 1031], [638, 1031], [867, 177, 638, 1031], [867, 177, 638, 1031], [867, 177, 638, 1031], [867, 177, 638, 1031], [497, 35], [497, 35], [639, 657], [639, 657], [36, 417, 489], [36, 417, 489], [36, 417, 489], [400, 567, 36, 417], [400, 567, 36, 417], [400, 567, 36, 417], [400, 567, 36, 417], [1284, 1151, 1234], [1284, 1151, 1234], [1284, 1151, 1234], [640, 972], [640, 972], [816, 698, 640], [816, 698, 640], [816, 698, 640], [1307, 1152, 1280, 1376], [1307, 1152, 1280, 1376], [1307, 1152, 1280, 1376], [1307, 1152, 1280, 1376], [641, 669], [641, 669], [641, 669], [641, 669], [1335, 946, 1337, 1154], [1335, 946, 1337, 1154], [1335, 946, 1337, 1154], [1335, 946, 1337, 1154], [1337, 1154], [1337, 1154], [1155, 1415, 1202], [1155, 1415, 1202], [1155, 1415, 1202], [1156, 1291], [1156, 1291], [136, 498, 203, 204, 37, 244], [136, 498, 203, 204, 37, 244], [136, 498, 203, 204, 37, 244], [136, 498, 203, 204, 37, 244], [136, 498, 203, 204, 37, 244], [136, 498, 203, 204, 37, 244], [136, 498, 203, 204, 37, 244], [136, 498, 203, 204, 37, 244], [136, 498, 203, 204, 37, 244], [136, 498, 203, 204, 37, 244], [136, 498, 203, 204, 37, 244], [136, 498, 203, 204, 37, 244], [104, 501, 643, 1095], [104, 501, 643, 1095], [104, 501, 643, 1095], [104, 501, 643, 1095], [262, 446, 52, 199, 427, 182, 573, 552, 553, 39, 237], [262, 446, 52, 199, 427, 182, 573, 552, 553, 39, 237], [262, 446, 52, 199, 427, 182, 573, 552, 553, 39, 237], [262, 446, 52, 199, 427, 182, 573, 552, 553, 39, 237], [262, 446, 52, 199, 427, 182, 573, 552, 553, 39, 237], [262, 446, 52, 199, 427, 182, 573, 552, 553, 39, 237], [262, 446, 52, 199, 427, 182, 573, 552, 553, 39, 237], [262, 446, 52, 199, 427, 182, 573, 552, 553, 39, 237], [262, 446, 52, 199, 427, 182, 573, 552, 553, 39, 237], [262, 446, 52, 199, 427, 182, 573, 552, 553, 39, 237], [262, 446, 52, 199, 427, 182, 573, 552, 553, 39, 237], [134, 178, 447, 52, 199, 426, 427, 39, 128], [134, 178, 447, 52, 199, 426, 427, 39, 128], [134, 178, 447, 52, 199, 426, 427, 39, 128], [134, 178, 447, 52, 199, 426, 427, 39, 128], [134, 178, 447, 52, 199, 426, 427, 39, 128], [134, 178, 447, 52, 199, 426, 427, 39, 128], [134, 178, 447, 52, 199, 426, 427, 39, 128], [134, 178, 447, 52, 199, 426, 427, 39, 128], [134, 178, 447, 52, 199, 426, 427, 39, 128], [426, 143, 39, 93, 235, 237], [426, 143, 39, 93, 235, 237], [426, 143, 39, 93, 235, 237], [426, 143, 39, 93, 235, 237], [426, 143, 39, 93, 235, 237], [426, 143, 39, 93, 235, 237], [39, 134, 182, 199], [39, 134, 182, 199], [39, 134, 182, 199], [39, 134, 182, 199], [261, 262, 199, 345, 427, 182, 573, 574, 39, 237, 396, 154], [261, 262, 199, 345, 427, 182, 573, 574, 39, 237, 396, 154], [261, 262, 199, 345, 427, 182, 573, 574, 39, 237, 396, 154], [261, 262, 199, 345, 427, 182, 573, 574, 39, 237, 396, 154], [261, 262, 199, 345, 427, 182, 573, 574, 39, 237, 396, 154], [261, 262, 199, 345, 427, 182, 573, 574, 39, 237, 396, 154], [261, 262, 199, 345, 427, 182, 573, 574, 39, 237, 396, 154], [261, 262, 199, 345, 427, 182, 573, 574, 39, 237, 396, 154], [261, 262, 199, 345, 427, 182, 573, 574, 39, 237, 396, 154], [261, 262, 199, 345, 427, 182, 573, 574, 39, 237, 396, 154], [261, 262, 199, 345, 427, 182, 573, 574, 39, 237, 396, 154], [261, 262, 199, 345, 427, 182, 573, 574, 39, 237, 396, 154], [134, 446, 52, 199, 262, 463, 426, 427, 182, 573, 117, 552, 553, 39, 235, 237, 213, 397], [134, 446, 52, 199, 262, 463, 426, 427, 182, 573, 117, 552, 553, 39, 235, 237, 213, 397], [134, 446, 52, 199, 262, 463, 426, 427, 182, 573, 117, 552, 553, 39, 235, 237, 213, 397], [134, 446, 52, 199, 262, 463, 426, 427, 182, 573, 117, 552, 553, 39, 235, 237, 213, 397], [134, 446, 52, 199, 262, 463, 426, 427, 182, 573, 117, 552, 553, 39, 235, 237, 213, 397], [134, 446, 52, 199, 262, 463, 426, 427, 182, 573, 117, 552, 553, 39, 235, 237, 213, 397], [134, 446, 52, 199, 262, 463, 426, 427, 182, 573, 117, 552, 553, 39, 235, 237, 213, 397], [134, 446, 52, 199, 262, 463, 426, 427, 182, 573, 117, 552, 553, 39, 235, 237, 213, 397], [134, 446, 52, 199, 262, 463, 426, 427, 182, 573, 117, 552, 553, 39, 235, 237, 213, 397], [134, 446, 52, 199, 262, 463, 426, 427, 182, 573, 117, 552, 553, 39, 235, 237, 213, 397], [134, 446, 52, 199, 262, 463, 426, 427, 182, 573, 117, 552, 553, 39, 235, 237, 213, 397], [134, 446, 52, 199, 262, 463, 426, 427, 182, 573, 117, 552, 553, 39, 235, 237, 213, 397], [134, 446, 52, 199, 262, 463, 426, 427, 182, 573, 117, 552, 553, 39, 235, 237, 213, 397], [134, 446, 52, 199, 262, 463, 426, 427, 182, 573, 117, 552, 553, 39, 235, 237, 213, 397], [134, 446, 52, 199, 262, 463, 426, 427, 182, 573, 117, 552, 553, 39, 235, 237, 213, 397], [134, 446, 52, 199, 262, 463, 426, 427, 182, 573, 117, 552, 553, 39, 235, 237, 213, 397], [134, 446, 52, 199, 262, 463, 426, 427, 182, 573, 117, 552, 553, 39, 235, 237, 213, 397], [134, 446, 52, 199, 262, 463, 426, 427, 182, 573, 117, 552, 553, 39, 235, 237, 213, 397], [338, 262, 199, 163, 39, 61, 237, 154], [338, 262, 199, 163, 39, 61, 237, 154], [338, 262, 199, 163, 39, 61, 237, 154], [338, 262, 199, 163, 39, 61, 237, 154], [338, 262, 199, 163, 39, 61, 237, 154], [338, 262, 199, 163, 39, 61, 237, 154], [338, 262, 199, 163, 39, 61, 237, 154], [338, 262, 199, 163, 39, 61, 237, 154], [40, 148], [40, 148], [1261, 1157, 685, 940], [1261, 1157, 685, 940], [1261, 1157, 685, 940], [1261, 1157, 685, 940], [646, 949, 681], [646, 949, 681], [646, 949, 681], [646, 949, 681], [646, 949, 681], [646, 949, 681], [646, 681, 835], [646, 681, 835], [646, 681, 835], [646, 681], [646, 681], [646, 816, 696, 681], [646, 816, 696, 681], [646, 816, 696, 681], [646, 816, 696, 681], [180, 550, 289, 41], [180, 550, 289, 41], [180, 550, 289, 41], [180, 550, 289, 41], [42, 792], [42, 792], [43, 133, 359], [43, 133, 359], [43, 133, 359], [43, 1075], [43, 1075], [44, 182, 237], [44, 182, 237], [44, 182, 237], [1310, 1158, 60], [1310, 1158, 60], [1310, 1158, 60], [224, 253, 95, 241, 45], [224, 253, 95, 241, 45], [224, 253, 95, 241, 45], [224, 253, 95, 241, 45], [224, 253, 95, 241, 45], [1159, 1203, 870], [1159, 1203, 870], [1159, 1203, 870], [1159, 1203, 870], [1159, 1203, 870], [1159, 1203, 870], [516, 1203, 1159], [516, 1203, 1159], [516, 1203, 1159], [46, 537], [46, 537], [382, 461, 199, 516, 140, 573, 433, 312, 254, 379, 47], [382, 461, 199, 516, 140, 573, 433, 312, 254, 379, 47], [382, 461, 199, 516, 140, 573, 433, 312, 254, 379, 47], [382, 461, 199, 516, 140, 573, 433, 312, 254, 379, 47], [382, 461, 199, 516, 140, 573, 433, 312, 254, 379, 47], [382, 461, 199, 516, 140, 573, 433, 312, 254, 379, 47], [382, 461, 199, 516, 140, 573, 433, 312, 254, 379, 47], [382, 461, 199, 516, 140, 573, 433, 312, 254, 379, 47], [382, 461, 199, 516, 140, 573, 433, 312, 254, 379, 47], [382, 461, 199, 516, 140, 573, 433, 312, 254, 379, 47], [382, 461, 199, 516, 140, 573, 433, 312, 254, 379, 47], [648, 763], [648, 763], [649, 516, 493], [649, 516, 493], [649, 516, 493], [650, 985, 986, 857], [650, 985, 986, 857], [650, 985, 986, 857], [650, 985, 986, 857], [49, 387, 457], [49, 387, 457], [49, 387, 457], [49, 387, 123], [49, 387, 123], [49, 387, 123], [49, 400, 360, 387, 99, 272], [49, 400, 360, 387, 99, 272], [49, 400, 360, 387, 99, 272], [49, 400, 360, 387, 99, 272], [49, 400, 360, 387, 99, 272], [49, 400, 360, 387, 99, 272], [49, 495, 342, 265, 387, 73, 333], [49, 495, 342, 265, 387, 73, 333], [49, 495, 342, 265, 387, 73, 333], [49, 495, 342, 265, 387, 73, 333], [49, 495, 342, 265, 387, 73, 333], [49, 495, 342, 265, 387, 73, 333], [49, 495, 342, 265, 387, 73, 333], [50, 134, 112, 312, 62], [50, 134, 112, 312, 62], [50, 134, 112, 312, 62], [50, 134, 112, 312, 62], [50, 134, 112, 312, 62], [51, 199, 568, 69, 561, 475], [51, 199, 568, 69, 561, 475], [51, 199, 568, 69, 561, 475], [51, 199, 568, 69, 561, 475], [51, 199, 568, 69, 561, 475], [51, 199, 568, 69, 561, 475], [810, 651, 1059, 861, 1010], [810, 651, 1059, 861, 1010], [810, 651, 1059, 861, 1010], [810, 651, 1059, 861, 1010], [810, 651, 1059, 861, 1010], [1043, 1045, 496, 383, 384, 652, 893, 1115, 846], [1043, 1045, 496, 383, 384, 652, 893, 1115, 846], [1043, 1045, 496, 383, 384, 652, 893, 1115, 846], [1043, 1045, 496, 383, 384, 652, 893, 1115, 846], [1043, 1045, 496, 383, 384, 652, 893, 1115, 846], [1043, 1045, 496, 383, 384, 652, 893, 1115, 846], [1043, 1045, 496, 383, 384, 652, 893, 1115, 846], [1043, 1045, 496, 383, 384, 652, 893, 1115, 846], [1043, 1045, 496, 383, 384, 652, 893, 1115, 846], [653, 97, 70, 820], [653, 97, 70, 820], [653, 97, 70, 820], [653, 97, 70, 820], [1079, 686, 1110, 654, 680, 1037], [1079, 686, 1110, 654, 680, 1037], [1079, 686, 1110, 654, 680, 1037], [1079, 686, 1110, 654, 680, 1037], [1079, 686, 1110, 654, 680, 1037], [1079, 686, 1110, 654, 680, 1037], [1160, 1245], [1160, 1245], [848, 849, 655, 1127, 239, 965], [848, 849, 655, 1127, 239, 965], [848, 849, 655, 1127, 239, 965], [848, 849, 655, 1127, 239, 965], [848, 849, 655, 1127, 239, 965], [848, 849, 655, 1127, 239, 965], [848, 655, 1127, 239, 965], [848, 655, 1127, 239, 965], [848, 655, 1127, 239, 965], [848, 655, 1127, 239, 965], [848, 655, 1127, 239, 965], [53, 371, 555, 355, 560], [53, 371, 555, 355, 560], [53, 371, 555, 355, 560], [53, 371, 555, 355, 560], [53, 371, 555, 355, 560], [528, 54, 533, 534, 278], [528, 54, 533, 534, 278], [528, 54, 533, 534, 278], [528, 54, 533, 534, 278], [528, 54, 533, 534, 278], [167, 520, 55, 418], [167, 520, 55, 418], [167, 520, 55, 418], [167, 520, 55, 418], [658, 960, 891], [658, 960, 891], [658, 960, 891], [277, 293, 56, 493], [277, 293, 56, 493], [277, 293, 56, 493], [277, 293, 56, 493], [1321, 1161], [1321, 1161], [177, 1161], [177, 1161], [659, 906], [659, 906], [706, 716, 57, 906, 1024], [706, 716, 57, 906, 1024], [706, 716, 57, 906, 1024], [706, 716, 57, 906, 1024], [706, 716, 57, 906, 1024], [117, 182, 199, 57], [117, 182, 199, 57], [117, 182, 199, 57], [117, 182, 199, 57], [57, 670], [57, 670], [297, 60, 57], [297, 60, 57], [297, 60, 57], [660, 804], [660, 804], [246, 660, 804], [246, 660, 804], [246, 660, 804], [661, 790, 857, 223], [661, 790, 857, 223], [661, 790, 857, 223], [661, 790, 857, 223], [788, 661], [788, 661], [176, 661, 857, 874], [176, 661, 857, 874], [176, 661, 857, 874], [176, 661, 857, 874], [661, 874], [661, 874], [692, 661], [692, 661], [662, 394], [662, 394], [662, 394], [662, 394], [1163, 1400, 1403], [1163, 1400, 1403], [1163, 1400, 1403], [58, 269], [58, 269], [199, 287, 573, 120, 59, 396, 397, 398, 173], [199, 287, 573, 120, 59, 396, 397, 398, 173], [199, 287, 573, 120, 59, 396, 397, 398, 173], [199, 287, 573, 120, 59, 396, 397, 398, 173], [199, 287, 573, 120, 59, 396, 397, 398, 173], [199, 287, 573, 120, 59, 396, 397, 398, 173], [199, 287, 573, 120, 59, 396, 397, 398, 173], [199, 287, 573, 120, 59, 396, 397, 398, 173], [199, 287, 573, 120, 59, 396, 397, 398, 173], [663, 794, 755], [663, 794, 755], [663, 794, 755], [663, 794, 755], [663, 794, 755], [663, 794, 755], [663, 825], [663, 825], [850, 60, 109], [850, 60, 109], [850, 60, 109], [254, 448, 117, 187, 60, 505], [254, 448, 117, 187, 60, 505], [254, 448, 117, 187, 60, 505], [254, 448, 117, 187, 60, 505], [254, 448, 117, 187, 60, 505], [254, 448, 117, 187, 60, 505], [448, 60, 254], [448, 60, 254], [448, 60, 254], [60, 109], [60, 109], [140, 108, 60, 254], [140, 108, 60, 254], [140, 108, 60, 254], [140, 108, 60, 254], [250, 448, 60], [250, 448, 60], [250, 448, 60], [664, 1049, 888, 1057, 1059], [664, 1049, 888, 1057, 1059], [664, 1049, 888, 1057, 1059], [664, 1049, 888, 1057, 1059], [664, 1049, 888, 1057, 1059], [353, 1057, 664], [353, 1057, 664], [353, 1057, 664], [664, 1057, 1059], [664, 1057, 1059], [664, 1057, 1059], [134, 262, 199, 250, 62], [134, 262, 199, 250, 62], [134, 262, 199, 250, 62], [134, 262, 199, 250, 62], [134, 262, 199, 250, 62], [870, 1052, 1053, 505, 62, 964], [870, 1052, 1053, 505, 62, 964], [870, 1052, 1053, 505, 62, 964], [870, 1052, 1053, 505, 62, 964], [870, 1052, 1053, 505, 62, 964], [870, 1052, 1053, 505, 62, 964], [62, 964, 505], [62, 964, 505], [62, 964, 505], [870, 505, 1038, 964, 62], [870, 505, 1038, 964, 62], [870, 505, 1038, 964, 62], [870, 505, 1038, 964, 62], [870, 505, 1038, 964, 62], [62, 870], [62, 870], [984, 966, 665], [984, 966, 665], [984, 966, 665], [760, 819, 665], [760, 819, 665], [760, 819, 665], [666, 1094], [666, 1094], [63, 452], [63, 452], [63, 302, 560], [63, 302, 560], [63, 302, 560], [1319, 1320, 1164], [1319, 1320, 1164], [1319, 1320, 1164], [1165, 104, 501], [1165, 104, 501], [1165, 104, 501], [1325, 501, 1206, 1165, 104], [1325, 501, 1206, 1165, 104], [1325, 501, 1206, 1165, 104], [1325, 501, 1206, 1165, 104], [1325, 501, 1206, 1165, 104], [1166, 1243], [1166, 1243], [1238, 1168], [1238, 1168], [668, 887], [668, 887], [668, 909], [668, 909], [1169, 866], [1169, 866], [946, 1335, 1333, 1170], [946, 1335, 1333, 1170], [946, 1335, 1333, 1170], [946, 1335, 1333, 1170], [1002, 670], [1002, 670], [730, 1129, 670], [730, 1129, 670], [730, 1129, 670], [1129, 670], [1129, 670], [671, 1338], [671, 1338], [1306, 1171], [1306, 1171], [64, 520], [64, 520], [1026, 672, 978], [1026, 672, 978], [1026, 672, 978], [65, 110, 203, 467, 483, 468, 439], [65, 110, 203, 467, 483, 468, 439], [65, 110, 203, 467, 483, 468, 439], [65, 110, 203, 467, 483, 468, 439], [65, 110, 203, 467, 483, 468, 439], [65, 110, 203, 467, 483, 468, 439], [65, 110, 203, 467, 483, 468, 439], [957, 956, 912, 674], [957, 956, 912, 674], [957, 956, 912, 674], [957, 956, 912, 674], [955, 912, 957, 674], [955, 912, 957, 674], [955, 912, 957, 674], [955, 912, 957, 674], [190, 1172], [190, 1172], [1172, 539], [1172, 539], [93, 448, 67], [93, 448, 67], [93, 448, 67], [501, 412, 1173], [501, 412, 1173], [501, 412, 1173], [563, 104, 501, 1173], [563, 104, 501, 1173], [563, 104, 501, 1173], [563, 104, 501, 1173], [68, 327], [68, 327], [68, 327], [68, 327], [68, 392, 327], [68, 392, 327], [68, 392, 327], [68, 327], [68, 327], [1026, 675, 978], [1026, 675, 978], [1026, 675, 978], [676, 1080, 767], [676, 1080, 767], [676, 1080, 767], [1174, 1219, 1250, 1433], [1174, 1219, 1250, 1433], [1174, 1219, 1250, 1433], [1174, 1219, 1250, 1433], [1174, 1433, 1456], [1174, 1433, 1456], [1174, 1433, 1456], [1175, 1448], [1175, 1448], [97, 282, 70, 480], [97, 282, 70, 480], [97, 282, 70, 480], [97, 282, 70, 480], [97, 70, 783], [97, 70, 783], [97, 70, 783], [1176, 574], [1176, 574], [677, 1032], [677, 1032], [1080, 677, 1032, 1059, 1042, 1105, 1106], [1080, 677, 1032, 1059, 1042, 1105, 1106], [1080, 677, 1032, 1059, 1042, 1105, 1106], [1080, 677, 1032, 1059, 1042, 1105, 1106], [1080, 677, 1032, 1059, 1042, 1105, 1106], [1080, 677, 1032, 1059, 1042, 1105, 1106], [1080, 677, 1032, 1059, 1042, 1105, 1106], [125, 198, 71], [125, 198, 71], [125, 198, 71], [1177, 1239], [1177, 1239], [199, 200, 182, 72, 237], [199, 200, 182, 72, 237], [199, 200, 182, 72, 237], [199, 200, 182, 72, 237], [199, 200, 182, 72, 237], [1452, 471, 970, 1178], [1452, 471, 970, 1178], [1452, 471, 970, 1178], [1452, 471, 970, 1178], [959, 679], [959, 679], [333, 73], [333, 73], [680, 1121], [680, 1121], [680, 1011], [680, 1011], [1300, 1179], [1300, 1179], [682, 793], [682, 793], [245, 219, 512, 292, 472, 74], [245, 219, 512, 292, 472, 74], [245, 219, 512, 292, 472, 74], [245, 219, 512, 292, 472, 74], [245, 219, 512, 292, 472, 74], [245, 219, 512, 292, 472, 74], [1180, 1309], [1180, 1309], [1181, 784], [1181, 784], [1181, 1221, 797], [1181, 1221, 797], [1181, 1221, 797], [395, 1182], [395, 1182], [1289, 1352, 1183], [1289, 1352, 1183], [1289, 1352, 1183], [468, 76, 360, 467], [468, 76, 360, 467], [468, 76, 360, 467], [468, 76, 360, 467], [199, 310, 237, 77, 397], [199, 310, 237, 77, 397], [199, 310, 237, 77, 397], [199, 310, 237, 77, 397], [199, 310, 237, 77, 397], [684, 254], [684, 254], [809, 685, 940], [809, 685, 940], [809, 685, 940], [1311, 685, 940], [1311, 685, 940], [1311, 685, 940], [382, 1184, 571], [382, 1184, 571], [382, 1184, 571], [381, 1021, 1185], [381, 1021, 1185], [381, 1021, 1185], [1186, 1375], [1186, 1375], [1431, 1186], [1431, 1186], [1431, 1186], [1431, 1186], [1187, 1227], [1187, 1227], [160, 106, 520, 373, 78], [160, 106, 520, 373, 78], [160, 106, 520, 373, 78], [160, 106, 520, 373, 78], [160, 106, 520, 373, 78], [688, 1249], [688, 1249], [79, 1116], [79, 1116], [1188, 1266, 1265], [1188, 1266, 1265], [1188, 1266, 1265], [1188, 1266, 1265], [1188, 1266, 1265], [1188, 1266, 1265], [689, 897], [689, 897], [1046, 689, 691, 1041, 897], [1046, 689, 691, 1041, 897], [1046, 689, 691, 1041, 897], [1046, 689, 691, 1041, 897], [1046, 689, 691, 1041, 897], [689, 897], [689, 897], [849, 1108, 502, 690, 239], [849, 1108, 502, 690, 239], [849, 1108, 502, 690, 239], [849, 1108, 502, 690, 239], [849, 1108, 502, 690, 239], [81, 469, 220, 80], [81, 469, 220, 80], [81, 469, 220, 80], [81, 469, 220, 80], [81, 220], [81, 220], [81, 219, 220, 218], [81, 219, 220, 218], [81, 219, 220, 218], [81, 219, 220, 218], [1316, 1189], [1316, 1189], [82, 416, 268], [82, 416, 268], [82, 416, 268], [158, 790, 882, 83, 84, 521, 356], [158, 790, 882, 83, 84, 521, 356], [158, 790, 882, 83, 84, 521, 356], [158, 790, 882, 83, 84, 521, 356], [158, 790, 882, 83, 84, 521, 356], [158, 790, 882, 83, 84, 521, 356], [158, 790, 882, 83, 84, 521, 356], [158, 790, 882, 83, 84, 521, 356], [158, 790, 882, 83, 84, 521, 356], [158, 790, 882, 83, 84, 521, 356], [158, 790, 882, 83, 84, 521, 356], [158, 790, 882, 83, 84, 521, 356], [158, 790, 882, 83, 84, 521, 356], [158, 790, 882, 83, 84, 521, 356], [801, 1116, 695], [801, 1116, 695], [801, 1116, 695], [564, 229, 694, 1012], [564, 229, 694, 1012], [564, 229, 694, 1012], [564, 229, 694, 1012], [461, 694, 450], [461, 694, 450], [461, 694, 450], [694, 821, 699, 720], [694, 821, 699, 720], [694, 821, 699, 720], [694, 821, 699, 720], [697, 855, 745], [697, 855, 745], [697, 855, 745], [338, 88, 358, 459], [338, 88, 358, 459], [338, 88, 358, 459], [338, 88, 358, 459], [89, 344], [89, 344], [89, 572, 442], [89, 572, 442], [89, 572, 442], [816, 698, 972], [816, 698, 972], [816, 698, 972], [345, 698, 134, 1267], [345, 698, 134, 1267], [345, 698, 134, 1267], [345, 698, 134, 1267], [1190, 1279, 1445], [1190, 1279, 1445], [1190, 1279, 1445], [277, 90, 781], [277, 90, 781], [277, 90, 781], [90, 798], [90, 798], [91, 480, 192], [91, 480, 192], [91, 480, 192], [91, 480, 192], [91, 480, 192], [91, 480, 192], [381, 1191, 1410, 1454], [381, 1191, 1410, 1454], [381, 1191, 1410, 1454], [381, 1191, 1410, 1454], [1398, 1192], [1398, 1192], [717, 700], [717, 700], [1193, 487, 1390], [1193, 487, 1390], [1193, 487, 1390], [146, 92, 138], [146, 92, 138], [146, 92, 138], [146, 92, 138], [146, 92, 138], [146, 92, 138], [146, 92, 138], [146, 92, 138], [146, 92, 138], [865, 774, 1120, 1048, 306, 746, 448, 1086, 1125, 93], [865, 774, 1120, 1048, 306, 746, 448, 1086, 1125, 93], [865, 774, 1120, 1048, 306, 746, 448, 1086, 1125, 93], [865, 774, 1120, 1048, 306, 746, 448, 1086, 1125, 93], [865, 774, 1120, 1048, 306, 746, 448, 1086, 1125, 93], [865, 774, 1120, 1048, 306, 746, 448, 1086, 1125, 93], [865, 774, 1120, 1048, 306, 746, 448, 1086, 1125, 93], [865, 774, 1120, 1048, 306, 746, 448, 1086, 1125, 93], [865, 774, 1120, 1048, 306, 746, 448, 1086, 1125, 93], [865, 774, 1120, 1048, 306, 746, 448, 1086, 1125, 93], [1367, 1446, 1312, 1252, 94, 336], [1367, 1446, 1312, 1252, 94, 336], [1367, 1446, 1312, 1252, 94, 336], [1367, 1446, 1312, 1252, 94, 336], [1367, 1446, 1312, 1252, 94, 336], [1367, 1446, 1312, 1252, 94, 336], [564, 247, 191, 94], [564, 247, 191, 94], [564, 247, 191, 94], [564, 247, 191, 94], [1252, 1367, 94], [1252, 1367, 94], [1252, 1367, 94], [95, 287, 120, 745], [95, 287, 120, 745], [95, 287, 120, 745], [95, 287, 120, 745], [95, 307], [95, 307], [428, 766, 800, 769, 95], [428, 766, 800, 769, 95], [428, 766, 800, 769, 95], [428, 766, 800, 769, 95], [428, 766, 800, 769, 95], [134, 199, 345, 95, 396], [134, 199, 345, 95, 396], [134, 199, 345, 95, 396], [134, 199, 345, 95, 396], [134, 199, 345, 95, 396], [224, 569, 345, 428, 95, 397], [224, 569, 345, 428, 95, 397], [224, 569, 345, 428, 95, 397], [224, 569, 345, 428, 95, 397], [224, 569, 345, 428, 95, 397], [224, 569, 345, 428, 95, 397], [95, 1439], [95, 1439], [345, 405, 287, 428, 95, 397], [345, 405, 287, 428, 95, 397], [345, 405, 287, 428, 95, 397], [345, 405, 287, 428, 95, 397], [345, 405, 287, 428, 95, 397], [345, 405, 287, 428, 95, 397], [199, 569, 345, 568, 287, 307, 405, 428, 570, 120, 95, 397], [199, 569, 345, 568, 287, 307, 405, 428, 570, 120, 95, 397], [199, 569, 345, 568, 287, 307, 405, 428, 570, 120, 95, 397], [199, 569, 345, 568, 287, 307, 405, 428, 570, 120, 95, 397], [199, 569, 345, 568, 287, 307, 405, 428, 570, 120, 95, 397], [199, 569, 345, 568, 287, 307, 405, 428, 570, 120, 95, 397], [199, 569, 345, 568, 287, 307, 405, 428, 570, 120, 95, 397], [199, 569, 345, 568, 287, 307, 405, 428, 570, 120, 95, 397], [199, 569, 345, 568, 287, 307, 405, 428, 570, 120, 95, 397], [199, 569, 345, 568, 287, 307, 405, 428, 570, 120, 95, 397], [199, 569, 345, 568, 287, 307, 405, 428, 570, 120, 95, 397], [199, 569, 345, 568, 287, 307, 405, 428, 570, 120, 95, 397], [1450, 1194, 464], [1450, 1194, 464], [1450, 1194, 464], [1302, 1195, 1276], [1302, 1195, 1276], [1302, 1195, 1276], [96, 485], [96, 485], [989, 701, 728, 1016], [989, 701, 728, 1016], [989, 701, 728, 1016], [989, 701, 728, 1016], [915, 702], [915, 702], [915, 702], [915, 702], [98, 366], [98, 366], [703, 1033], [703, 1033], [703, 710, 785], [703, 710, 785], [703, 710, 785], [703, 1033], [703, 1033], [196, 100], [196, 100], [1196, 1224], [1196, 1224], [218, 392, 101], [218, 392, 101], [218, 392, 101], [102, 410], [102, 410], [125, 492, 1197], [125, 492, 1197], [125, 492, 1197], [704, 1433], [704, 1433], [103, 328], [103, 328], [563, 104, 112, 501], [563, 104, 112, 501], [563, 104, 112, 501], [563, 104, 112, 501], [104, 501], [104, 501], [1339, 104, 501], [1339, 104, 501], [1339, 104, 501], [779, 705], [779, 705], [1198, 794], [1198, 794], [107, 451, 371, 164], [107, 451, 371, 164], [107, 451, 371, 164], [107, 451, 371, 164], [156, 423, 392, 371, 107, 355, 414], [156, 423, 392, 371, 107, 355, 414], [156, 423, 392, 371, 107, 355, 414], [156, 423, 392, 371, 107, 355, 414], [156, 423, 392, 371, 107, 355, 414], [156, 423, 392, 371, 107, 355, 414], [156, 423, 392, 371, 107, 355, 414], [1107, 708, 1080, 1058], [1107, 708, 1080, 1058], [1107, 708, 1080, 1058], [1107, 708, 1080, 1058], [709, 971, 393, 1420], [709, 971, 393, 1420], [709, 971, 393, 1420], [709, 971, 393, 1420], [140, 108, 254], [140, 108, 254], [140, 108, 254], [134, 283, 199, 108, 306, 516, 228, 182, 296], [134, 283, 199, 108, 306, 516, 228, 182, 296], [134, 283, 199, 108, 306, 516, 228, 182, 296], [134, 283, 199, 108, 306, 516, 228, 182, 296], [134, 283, 199, 108, 306, 516, 228, 182, 296], [134, 283, 199, 108, 306, 516, 228, 182, 296], [134, 283, 199, 108, 306, 516, 228, 182, 296], [134, 283, 199, 108, 306, 516, 228, 182, 296], [134, 283, 199, 108, 306, 516, 228, 182, 296], [175, 1199], [175, 1199], [109, 140, 573, 254, 1287], [109, 140, 573, 254, 1287], [109, 140, 573, 254, 1287], [109, 140, 573, 254, 1287], [109, 140, 573, 254, 1287], [1102, 126, 109], [1102, 126, 109], [1102, 126, 109], [226, 227, 109], [226, 227, 109], [226, 227, 109], [109, 281, 254], [109, 281, 254], [109, 281, 254], [516, 109], [516, 109], [444, 109], [444, 109], [360, 109], [360, 109], [111, 166, 186, 121, 274], [111, 166, 186, 121, 274], [111, 166, 186, 121, 274], [111, 166, 186, 121, 274], [111, 166, 186, 121, 274], [224, 111, 1391, 186, 243], [224, 111, 1391, 186, 243], [224, 111, 1391, 186, 243], [224, 111, 1391, 186, 243], [224, 111, 1391, 186, 243], [134, 111, 573, 186, 274], [134, 111, 573, 186, 274], [134, 111, 573, 186, 274], [134, 111, 573, 186, 274], [134, 111, 573, 186, 274], [376, 111, 186, 573], [376, 111, 186, 573], [376, 111, 186, 573], [376, 111, 186, 573], [516, 112, 448], [516, 112, 448], [516, 112, 448], [516, 112, 1263], [516, 112, 1263], [516, 112, 1263], [516, 112], [516, 112], [715, 711, 1001], [715, 711, 1001], [715, 711, 1001], [1270, 1201], [1270, 1201], [812, 114], [812, 114], [714, 807], [714, 807], [714, 1040], [714, 1040], [117, 254], [117, 254], [117, 166, 121], [117, 166, 121], [117, 166, 121], [118, 747, 401, 489], [118, 747, 401, 489], [118, 747, 401, 489], [118, 747, 401, 489], [232, 118], [232, 118], [119, 360], [119, 360], [119, 373], [119, 373], [119, 373], [119, 373], [199, 224, 569, 287, 405, 428, 120], [199, 224, 569, 287, 405, 428, 120], [199, 224, 569, 287, 405, 428, 120], [199, 224, 569, 287, 405, 428, 120], [199, 224, 569, 287, 405, 428, 120], [199, 224, 569, 287, 405, 428, 120], [199, 224, 569, 287, 405, 428, 120], [569, 570, 405, 287, 428, 120], [569, 570, 405, 287, 428, 120], [569, 570, 405, 287, 428, 120], [569, 570, 405, 287, 428, 120], [569, 570, 405, 287, 428, 120], [569, 570, 405, 287, 428, 120], [166, 121], [166, 121], [166, 121, 513, 514], [166, 121, 513, 514], [166, 121, 513, 514], [166, 121, 513, 514], [166, 121, 321], [166, 121, 321], [166, 121, 321], [166, 121], [166, 121], [166, 121, 573], [166, 121, 573], [166, 121, 573], [166, 121, 513], [166, 121, 513], [166, 121, 513], [1338, 123], [1338, 123], [123, 179], [123, 179], [1218, 1349, 123], [1218, 1349, 123], [1218, 1349, 123], [790, 159, 882, 124, 356], [790, 159, 882, 124, 356], [790, 159, 882, 124, 356], [790, 159, 882, 124, 356], [790, 159, 882, 124, 356], [159, 344, 354, 124, 559], [159, 344, 354, 124, 559], [159, 344, 354, 124, 559], [159, 344, 354, 124, 559], [159, 344, 354, 124, 559], [131, 159, 554, 124, 559], [131, 159, 554, 124, 559], [131, 159, 554, 124, 559], [131, 159, 554, 124, 559], [131, 159, 554, 124, 559], [218, 125, 472], [218, 125, 472], [218, 125, 472], [719, 914], [719, 914], [948, 938, 720], [948, 938, 720], [948, 938, 720], [126, 250], [126, 250], [127, 890], [127, 890], [199, 187, 188, 237, 242, 128, 243], [199, 187, 188, 237, 242, 128, 243], [199, 187, 188, 237, 242, 128, 243], [199, 187, 188, 237, 242, 128, 243], [199, 187, 188, 237, 242, 128, 243], [199, 187, 188, 237, 242, 128, 243], [199, 187, 188, 237, 242, 128, 243], [276, 134, 303, 128], [276, 134, 303, 128], [276, 134, 303, 128], [276, 134, 303, 128], [128, 1332], [128, 1332], [134, 199, 187, 188, 237, 128], [134, 199, 187, 188, 237, 128], [134, 199, 187, 188, 237, 128], [134, 199, 187, 188, 237, 128], [134, 199, 187, 188, 237, 128], [134, 199, 187, 188, 237, 128], [338, 246, 446, 199, 481, 523, 128], [338, 246, 446, 199, 481, 523, 128], [338, 246, 446, 199, 481, 523, 128], [338, 246, 446, 199, 481, 523, 128], [338, 246, 446, 199, 481, 523, 128], [338, 246, 446, 199, 481, 523, 128], [338, 246, 446, 199, 481, 523, 128], [721, 744], [721, 744], [721, 744], [721, 744], [721, 744, 933], [721, 744, 933], [721, 744, 933], [1217, 1204], [1217, 1204], [766, 724], [766, 724], [173, 724], [173, 724], [766, 724], [766, 724], [301, 129], [301, 129], [725, 382, 571, 1047], [725, 382, 571, 1047], [725, 382, 571, 1047], [725, 382, 571, 1047], [285, 172, 482, 130], [285, 172, 482, 130], [285, 172, 482, 130], [285, 172, 482, 130], [165, 130], [165, 130], [1256, 130], [1256, 130], [727, 795], [727, 795], [727, 795], [727, 795], [1069, 727, 795], [1069, 727, 795], [1069, 727, 795], [134, 568, 569, 345, 570, 405, 428], [134, 568, 569, 345, 570, 405, 428], [134, 568, 569, 345, 570, 405, 428], [134, 568, 569, 345, 570, 405, 428], [134, 568, 569, 345, 570, 405, 428], [134, 568, 569, 345, 570, 405, 428], [134, 568, 569, 345, 570, 405, 428], [338, 134, 199, 182, 252, 237, 459], [338, 134, 199, 182, 252, 237, 459], [338, 134, 199, 182, 252, 237, 459], [338, 134, 199, 182, 252, 237, 459], [338, 134, 199, 182, 252, 237, 459], [338, 134, 199, 182, 252, 237, 459], [338, 134, 199, 182, 252, 237, 459], [134, 388], [134, 388], [134, 199], [134, 199], [199, 134, 332], [199, 134, 332], [199, 134, 332], [339, 134, 199, 182, 308], [339, 134, 199, 182, 308], [339, 134, 199, 182, 308], [339, 134, 199, 182, 308], [339, 134, 199, 182, 308], [134, 182, 199, 237], [134, 182, 199, 237], [134, 182, 199, 237], [134, 182, 199, 237], [134, 303, 290], [134, 303, 290], [134, 303, 290], [134, 357, 459], [134, 357, 459], [134, 357, 459], [134, 402, 403, 199, 262, 459], [134, 402, 403, 199, 262, 459], [134, 402, 403, 199, 262, 459], [134, 402, 403, 199, 262, 459], [134, 402, 403, 199, 262, 459], [134, 402, 403, 199, 262, 459], [134, 446, 463, 199, 462, 426, 427, 182, 143, 397], [134, 446, 463, 199, 462, 426, 427, 182, 143, 397], [134, 446, 463, 199, 462, 426, 427, 182, 143, 397], [134, 446, 463, 199, 462, 426, 427, 182, 143, 397], [134, 446, 463, 199, 462, 426, 427, 182, 143, 397], [134, 446, 463, 199, 462, 426, 427, 182, 143, 397], [134, 446, 463, 199, 462, 426, 427, 182, 143, 397], [134, 446, 463, 199, 462, 426, 427, 182, 143, 397], [134, 446, 463, 199, 462, 426, 427, 182, 143, 397], [134, 446, 463, 199, 462, 426, 427, 182, 143, 397], [134, 1395], [134, 1395], [134, 525], [134, 525], [199, 222, 135], [199, 222, 135], [199, 222, 135], [1324, 728], [1324, 728], [203, 202, 365, 136], [203, 202, 365, 136], [203, 202, 365, 136], [203, 202, 365, 136], [527, 136, 184, 392, 560], [527, 136, 184, 392, 560], [527, 136, 184, 392, 560], [527, 136, 184, 392, 560], [527, 136, 184, 392, 560], [859, 729], [859, 729], [309, 257, 424, 138], [309, 257, 424, 138], [309, 257, 424, 138], [309, 257, 424, 138], [138, 244], [138, 244], [169, 445, 312, 138], [169, 445, 312, 138], [169, 445, 312, 138], [169, 445, 312, 138], [139, 251], [139, 251], [139, 367, 186, 273, 274], [139, 367, 186, 273, 274], [139, 367, 186, 273, 274], [139, 367, 186, 273, 274], [139, 367, 186, 273, 274], [816, 731, 1441], [816, 731, 1441], [816, 731, 1441], [733, 754], [733, 754], [763, 140, 764, 254], [763, 140, 764, 254], [763, 140, 764, 254], [763, 140, 764, 254], [199, 140, 427, 254, 281], [199, 140, 427, 254, 281], [199, 140, 427, 254, 281], [199, 140, 427, 254, 281], [199, 140, 427, 254, 281], [1278, 1413, 1353, 140, 187, 254], [1278, 1413, 1353, 140, 187, 254], [1278, 1413, 1353, 140, 187, 254], [1278, 1413, 1353, 140, 187, 254], [1278, 1413, 1353, 140, 187, 254], [1278, 1413, 1353, 140, 187, 254], [141, 522], [141, 522], [967, 734], [967, 734], [1207, 1340], [1207, 1340], [1208, 1396], [1208, 1396], [1029, 283, 426, 182, 143, 187, 236, 397, 379, 380], [1029, 283, 426, 182, 143, 187, 236, 397, 379, 380], [1029, 283, 426, 182, 143, 187, 236, 397, 379, 380], [1029, 283, 426, 182, 143, 187, 236, 397, 379, 380], [1029, 283, 426, 182, 143, 187, 236, 397, 379, 380], [1029, 283, 426, 182, 143, 187, 236, 397, 379, 380], [1029, 283, 426, 182, 143, 187, 236, 397, 379, 380], [1029, 283, 426, 182, 143, 187, 236, 397, 379, 380], [1029, 283, 426, 182, 143, 187, 236, 397, 379, 380], [1029, 283, 426, 182, 143, 187, 236, 397, 379, 380], [446, 463, 199, 426, 427, 182, 143, 237], [446, 463, 199, 426, 427, 182, 143, 237], [446, 463, 199, 426, 427, 182, 143, 237], [446, 463, 199, 426, 427, 182, 143, 237], [446, 463, 199, 426, 427, 182, 143, 237], [446, 463, 199, 426, 427, 182, 143, 237], [446, 463, 199, 426, 427, 182, 143, 237], [446, 463, 199, 426, 427, 182, 143, 237], [446, 462, 426, 427, 182, 143], [446, 462, 426, 427, 182, 143], [446, 462, 426, 427, 182, 143], [446, 462, 426, 427, 182, 143], [446, 462, 426, 427, 182, 143], [446, 462, 426, 427, 182, 143], [390, 316, 144], [390, 316, 144], [390, 316, 144], [1209, 1281], [1209, 1281], [862, 899, 739], [862, 899, 739], [862, 899, 739], [899, 902, 903, 904, 824, 862, 739], [899, 902, 903, 904, 824, 862, 739], [899, 902, 903, 904, 824, 862, 739], [899, 902, 903, 904, 824, 862, 739], [899, 902, 903, 904, 824, 862, 739], [899, 902, 903, 904, 824, 862, 739], [899, 902, 903, 904, 824, 862, 739], [899, 902, 903, 904, 862, 739], [899, 902, 903, 904, 862, 739], [899, 902, 903, 904, 862, 739], [899, 902, 903, 904, 862, 739], [899, 902, 903, 904, 862, 739], [899, 902, 903, 904, 862, 739], [899, 739], [899, 739], [862, 899, 1044, 739], [862, 899, 1044, 739], [862, 899, 1044, 739], [862, 899, 1044, 739], [899, 739], [899, 739], [741, 739], [741, 739], [899, 900, 739], [899, 900, 739], [899, 900, 739], [899, 902, 904, 862, 739], [899, 902, 904, 862, 739], [899, 902, 904, 862, 739], [899, 902, 904, 862, 739], [899, 902, 904, 862, 739], [862, 899, 739, 902], [862, 899, 739, 902], [862, 899, 739, 902], [862, 899, 739, 902], [1050, 1051, 740], [1050, 1051, 740], [1050, 1051, 740], [494, 743, 868, 748, 796, 963], [494, 743, 868, 748, 796, 963], [494, 743, 868, 748, 796, 963], [494, 743, 868, 748, 796, 963], [494, 743, 868, 748, 796, 963], [494, 743, 868, 748, 796, 963], [494, 868, 743], [494, 868, 743], [494, 868, 743], [1210, 183], [1210, 183], [1211, 163], [1211, 163], [1212, 149], [1212, 149], [745, 778, 1102], [745, 778, 1102], [745, 778, 1102], [745, 444, 516, 1122], [745, 444, 516, 1122], [745, 444, 516, 1122], [745, 444, 516, 1122], [745, 444, 516], [745, 444, 516], [745, 444, 516], [745, 444], [745, 444], [150, 151, 152], [150, 151, 152], [150, 151, 152], [1213, 1355], [1213, 1355], [215, 153, 558], [215, 153, 558], [215, 153, 558], [1394, 1214], [1394, 1214], [260, 261, 262, 199, 345, 427, 182, 574, 237, 154], [260, 261, 262, 199, 345, 427, 182, 574, 237, 154], [260, 261, 262, 199, 345, 427, 182, 574, 237, 154], [260, 261, 262, 199, 345, 427, 182, 574, 237, 154], [260, 261, 262, 199, 345, 427, 182, 574, 237, 154], [260, 261, 262, 199, 345, 427, 182, 574, 237, 154], [260, 261, 262, 199, 345, 427, 182, 574, 237, 154], [260, 261, 262, 199, 345, 427, 182, 574, 237, 154], [260, 261, 262, 199, 345, 427, 182, 574, 237, 154], [260, 261, 262, 199, 345, 427, 182, 574, 237, 154], [340, 406, 407, 185, 155], [340, 406, 407, 185, 155], [340, 406, 407, 185, 155], [340, 406, 407, 185, 155], [340, 406, 407, 185, 155], [747, 1393], [747, 1393], [1389, 747, 489], [1389, 747, 489], [1389, 747, 489], [923, 747], [923, 747], [747, 489], [747, 489], [494, 868, 748], [494, 868, 748], [494, 868, 748], [1080, 884, 749, 1042, 787], [1080, 884, 749, 1042, 787], [1080, 884, 749, 1042, 787], [1080, 884, 749, 1042, 787], [1080, 884, 749, 1042, 787], [1080, 749], [1080, 749], [1054, 1079, 749], [1054, 1079, 749], [1054, 1079, 749], [750, 911, 1028, 910], [750, 911, 1028, 910], [750, 911, 1028, 910], [750, 911, 1028, 910], [759, 751], [759, 751], [484, 157], [484, 157], [1216, 970, 971, 1387], [1216, 970, 971, 1387], [1216, 970, 971, 1387], [1216, 970, 971, 1387], [161, 360, 520], [161, 360, 520], [161, 360, 520], [161, 1235], [161, 1235], [161, 373], [161, 373], [218, 162], [218, 162], [802, 753], [802, 753], [165, 482], [165, 482], [1294, 1220, 1295, 1359], [1294, 1220, 1295, 1359], [1294, 1220, 1295, 1359], [1294, 1220, 1295, 1359], [1220, 1295], [1220, 1295], [969, 167], [969, 167], [1222, 1357], [1222, 1357], [466, 1223], [466, 1223], [529, 170], [529, 170], [171, 1226], [171, 1226], [444, 1082, 761], [444, 1082, 761], [444, 1082, 761], [953, 1081, 1083, 889, 762, 982], [953, 1081, 1083, 889, 762, 982], [953, 1081, 1083, 889, 762, 982], [953, 1081, 1083, 889, 762, 982], [953, 1081, 1083, 889, 762, 982], [953, 1081, 1083, 889, 762, 982], [1228, 344], [1228, 344], [1071, 814, 763, 827, 908], [1071, 814, 763, 827, 908], [1071, 814, 763, 827, 908], [1071, 814, 763, 827, 908], [1071, 814, 763, 827, 908], [765, 493], [765, 493], [287, 766], [287, 766], [287, 766], [287, 766], [858, 886, 353, 767], [858, 886, 353, 767], [858, 886, 353, 767], [858, 886, 353, 767], [432, 767], [432, 767], [1059, 767, 1105], [1059, 767, 1105], [1059, 767, 1105], [172, 354, 189], [172, 354, 189], [172, 354, 189], [172, 557], [172, 557], [177, 1229], [177, 1229], [218, 177, 1229], [218, 177, 1229], [218, 177, 1229], [177, 1230], [177, 1230], [177, 1230], [177, 1230], [1081, 828, 768], [1081, 828, 768], [1081, 828, 768], [345, 396, 173], [345, 396, 173], [345, 396, 173], [345, 396, 397, 173], [345, 396, 397, 173], [345, 396, 397, 173], [345, 396, 397, 173], [484, 354, 189, 559, 174], [484, 354, 189, 559, 174], [484, 354, 189, 559, 174], [484, 354, 189, 559, 174], [484, 354, 189, 559, 174], [770, 922, 1011], [770, 922, 1011], [770, 922, 1011], [175, 177], [175, 177], [177, 1233, 449], [177, 1233, 449], [177, 1233, 449], [1231, 177, 1233], [1231, 177, 1233], [1231, 177, 1233], [1232, 1233], [1232, 1233], [181, 440, 199], [181, 440, 199], [181, 440, 199], [315, 182, 199], [315, 182, 199], [315, 182, 199], [303, 262, 199, 182, 237, 525], [303, 262, 199, 182, 237, 525], [303, 262, 199, 182, 237, 525], [303, 262, 199, 182, 237, 525], [303, 262, 199, 182, 237, 525], [303, 262, 199, 182, 237, 525], [303, 262, 199, 182, 237, 525], [303, 262, 199, 182, 237, 525], [303, 262, 199, 182, 237, 525], [303, 262, 199, 182, 237, 525], [303, 262, 199, 182, 237, 525], [303, 262, 199, 182, 237, 525], [303, 262, 199, 182, 237, 525], [303, 262, 199, 182, 237, 525], [303, 262, 199, 182, 237, 525], [303, 262, 199, 182, 237, 525], [303, 262, 199, 182, 237, 525], [303, 262, 199, 182, 237, 525], [182, 237, 573, 459], [182, 237, 573, 459], [182, 237, 573, 459], [182, 237, 573, 459], [199, 182, 525, 237], [199, 182, 525, 237], [199, 182, 525, 237], [199, 182, 525, 237], [182, 199, 237], [182, 199, 237], [182, 199, 237], [182, 199], [182, 199], [262, 283, 199, 182, 237], [262, 283, 199, 182, 237], [262, 283, 199, 182, 237], [262, 283, 199, 182, 237], [262, 283, 199, 182, 237], [182, 199], [182, 199], [199, 286, 182, 396, 397, 459], [199, 286, 182, 396, 397, 459], [199, 286, 182, 396, 397, 459], [199, 286, 182, 396, 397, 459], [199, 286, 182, 396, 397, 459], [199, 286, 182, 396, 397, 459], [338, 262, 446, 447, 199, 182, 538], [338, 262, 446, 447, 199, 182, 538], [338, 262, 446, 447, 199, 182, 538], [338, 262, 446, 447, 199, 182, 538], [338, 262, 446, 447, 199, 182, 538], [338, 262, 446, 447, 199, 182, 538], [338, 262, 446, 447, 199, 182, 538], [426, 427, 182, 199], [426, 427, 182, 199], [426, 427, 182, 199], [426, 427, 182, 199], [262, 182, 525, 237], [262, 182, 525, 237], [262, 182, 525, 237], [262, 182, 525, 237], [1284, 1234], [1284, 1234], [1236, 488], [1236, 488], [1237, 1238], [1237, 1238], [185, 978], [185, 978], [833, 773], [833, 773], [1086, 774], [1086, 774], [187, 1263], [187, 1263], [438, 187], [438, 187], [1413, 187], [1413, 187], [510, 1240], [510, 1240], [190, 965], [190, 965], [1241, 1348, 1381], [1241, 1348, 1381], [1241, 1348, 1381], [863, 799, 777], [863, 799, 777], [863, 799, 777], [1063, 1064, 777, 983], [1063, 1064, 777, 983], [1063, 1064, 777, 983], [1063, 1064, 777, 983], [1242, 1257], [1242, 1257], [1274, 1242], [1274, 1242], [541, 549, 191], [541, 549, 191], [541, 549, 191], [191, 458], [191, 458], [960, 961, 782], [960, 961, 782], [960, 961, 782], [1246, 1247, 1244], [1246, 1247, 1244], [1246, 1247, 1244], [1246, 1248], [1246, 1248], [324, 194, 529], [324, 194, 529], [324, 194, 529], [195, 529, 324, 346, 366, 456], [195, 529, 324, 346, 366, 456], [195, 529, 324, 346, 366, 456], [195, 529, 324, 346, 366, 456], [195, 529, 324, 346, 366, 456], [195, 529, 324, 346, 366, 456], [432, 452, 198, 197], [432, 452, 198, 197], [432, 452, 198, 197], [432, 452, 198, 197], [551, 198], [551, 198], [434, 198], [434, 198], [198, 442], [198, 442], [1355, 198], [1355, 198], [345, 571, 573, 199], [345, 571, 573, 199], [345, 571, 573, 199], [345, 571, 573, 199], [238, 199], [238, 199], [397, 199], [397, 199], [307, 199, 217, 237], [307, 199, 217, 237], [307, 199, 217, 237], [307, 199, 217, 237], [370, 199], [370, 199], [396, 397, 199], [396, 397, 199], [396, 397, 199], [397, 199], [397, 199], [248, 199, 526], [248, 199, 526], [248, 199, 526], [199, 332], [199, 332], [397, 199], [397, 199], [397, 199], [397, 199], [199, 322], [199, 322], [546, 199, 237], [546, 199, 237], [546, 199, 237], [386, 199], [386, 199], [386, 199], [386, 199], [386, 199], [386, 199], [199, 459], [199, 459], [199, 237], [199, 237], [396, 199, 526], [396, 199, 526], [396, 199, 526], [299, 199], [299, 199], [510, 199], [510, 199], [499, 199], [499, 199], [396, 397, 199], [396, 397, 199], [396, 397, 199], [262, 199, 237], [262, 199, 237], [262, 199, 237], [544, 382, 235, 199], [544, 382, 235, 199], [544, 382, 235, 199], [544, 382, 235, 199], [443, 377, 199, 548], [443, 377, 199, 548], [443, 377, 199, 548], [443, 377, 199, 548], [199, 244], [199, 244], [338, 447, 199], [338, 447, 199], [338, 447, 199], [199, 212], [199, 212], [239, 199], [239, 199], [397, 199], [397, 199], [262, 200], [262, 200], [201, 1256], [201, 1256], [206, 207, 1421], [206, 207, 1421], [206, 207, 1421], [206, 207, 208, 209, 393], [206, 207, 208, 209, 393], [206, 207, 208, 209, 393], [206, 207, 208, 209, 393], [206, 207, 208, 209, 393], [207, 520, 393], [207, 520, 393], [207, 520, 393], [209, 393, 1420], [209, 393, 1420], [209, 393, 1420], [209, 355], [209, 355], [210, 791], [210, 791], [421, 211], [421, 211], [214, 373, 483], [214, 373, 483], [214, 373, 483], [786, 981], [786, 981], [215, 280], [215, 280], [215, 371, 355], [215, 371, 355], [215, 371, 355], [215, 371, 267], [215, 371, 267], [215, 371, 267], [215, 311], [215, 311], [1317, 216], [1317, 216], [218, 270, 255], [218, 270, 255], [218, 270, 255], [218, 291, 472], [218, 291, 472], [218, 291, 472], [218, 371, 355], [218, 371, 355], [218, 371, 355], [221, 472], [221, 472], [221, 472], [221, 472], [997, 224], [997, 224], [1007, 789], [1007, 789], [790, 356, 521, 882], [790, 356, 521, 882], [790, 356, 521, 882], [790, 356, 521, 882], [790, 356, 521, 882], [790, 356, 521, 882], [790, 356, 521, 882], [790, 356, 521, 882], [790, 356, 521, 882], [790, 356, 521, 882], [790, 356, 521, 882], [790, 356, 521, 882], [867, 790], [867, 790], [229, 890], [229, 890], [792, 229, 890], [792, 229, 890], [792, 229, 890], [1030, 792, 229, 890, 872], [1030, 792, 229, 890, 872], [1030, 792, 229, 890, 872], [1030, 792, 229, 890, 872], [1030, 792, 229, 890, 872], [229, 890], [229, 890], [1251, 450], [1251, 450], [1253, 1254], [1253, 1254], [232, 233], [232, 233], [1255, 489], [1255, 489], [234, 533, 483], [234, 533, 483], [234, 533, 483], [237, 459], [237, 459], [397, 274, 237], [397, 274, 237], [397, 274, 237], [283, 237], [283, 237], [1259, 1260, 239], [1259, 1260, 239], [1259, 1260, 239], [801, 239, 876], [801, 239, 876], [801, 239, 876], [239, 1260, 941], [239, 1260, 941], [239, 1260, 941], [801, 239, 1346], [801, 239, 1346], [801, 239, 1346], [502, 239, 849], [502, 239, 849], [502, 239, 849], [801, 876], [801, 876], [801, 1430], [801, 1430], [1356, 1128, 1458, 1258], [1356, 1128, 1458, 1258], [1356, 1128, 1458, 1258], [1356, 1128, 1458, 1258], [1356, 1128, 1458, 1258], [1356, 1128, 1458, 1258], [1356, 1128, 1458, 1258], [1356, 1128, 1458, 1258], [460, 438, 244], [460, 438, 244], [460, 438, 244], [438, 244], [438, 244], [246, 447], [246, 447], [1115, 805, 383], [1115, 805, 383], [1115, 805, 383], [516, 1262], [516, 1262], [516, 1262], [516, 1262], [808, 973], [808, 973], [1292, 811], [1292, 811], [866, 937, 811, 474, 864], [866, 937, 811, 474, 864], [866, 937, 811, 474, 864], [866, 937, 811, 474, 864], [866, 937, 811, 474, 864], [250, 1407], [250, 1407], [813, 1001], [813, 1001], [813, 328], [813, 328], [457, 1264], [457, 1264], [277, 254], [277, 254], [1322, 254], [1322, 254], [1268, 1269], [1268, 1269], [815, 954], [815, 954], [256, 493, 847], [256, 493, 847], [256, 493, 847], [258, 488], [258, 488], [817, 857], [817, 857], [1389, 1271, 489], [1389, 1271, 489], [1389, 1271, 489], [263, 483], [263, 483], [265, 361, 264], [265, 361, 264], [265, 361, 264], [360, 342, 361, 264, 265], [360, 342, 361, 264, 265], [360, 342, 361, 264, 265], [360, 342, 361, 264, 265], [360, 342, 361, 264, 265], [265, 341, 264], [265, 341, 264], [265, 341, 264], [265, 334, 477, 375], [265, 334, 477, 375], [265, 334, 477, 375], [265, 334, 477, 375], [265, 334, 476], [265, 334, 476], [265, 334, 476], [265, 342, 343], [265, 342, 343], [265, 342, 343], [265, 478, 342], [265, 478, 342], [265, 478, 342], [1372, 1272], [1372, 1272], [1380, 1273], [1380, 1273], [426, 266, 427, 396, 438], [426, 266, 427, 396, 438], [426, 266, 427, 396, 438], [426, 266, 427, 396, 438], [426, 266, 427, 396, 438], [1275, 1314, 1454, 1421], [1275, 1314, 1454, 1421], [1275, 1314, 1454, 1421], [1275, 1314, 1454, 1421], [268, 355], [268, 355], [416, 269], [416, 269], [371, 269], [371, 269], [378, 269], [378, 269], [566, 404, 269, 369, 392, 452, 555, 560], [566, 404, 269, 369, 392, 452, 555, 560], [566, 404, 269, 369, 392, 452, 555, 560], [566, 404, 269, 369, 392, 452, 555, 560], [566, 404, 269, 369, 392, 452, 555, 560], [566, 404, 269, 369, 392, 452, 555, 560], [566, 404, 269, 369, 392, 452, 555, 560], [566, 404, 269, 369, 392, 452, 555, 560], [425, 269, 371, 355, 416], [425, 269, 371, 355, 416], [425, 269, 371, 355, 416], [425, 269, 371, 355, 416], [425, 269, 371, 355, 416], [1438, 1302, 1276], [1438, 1302, 1276], [1438, 1302, 1276], [1355, 1276], [1355, 1276], [1355, 1276], [1355, 1276], [1438, 1276], [1438, 1276], [469, 278], [469, 278], [533, 278, 534, 483], [533, 278, 534, 483], [533, 278, 534, 483], [533, 278, 534, 483], [411, 278], [411, 278], [278, 483], [278, 483], [279, 845], [279, 845], [1440, 1277], [1440, 1277], [1305, 821], [1305, 821], [1019, 822], [1019, 822], [286, 287], [286, 287], [954, 974, 573, 574, 826], [954, 974, 573, 574, 826], [954, 974, 573, 574, 826], [954, 974, 573, 574, 826], [954, 974, 573, 574, 826], [828, 829], [828, 829], [830, 831, 878], [830, 831, 878], [830, 831, 878], [834, 1363], [834, 1363], [1282, 1393], [1282, 1393], [292, 467, 453], [292, 467, 453], [292, 467, 453], [1425, 1283], [1425, 1283], [1285, 1342], [1285, 1342], [876, 1286], [876, 1286], [1055, 838], [1055, 838], [839, 1011], [839, 1011], [1287, 422], [1287, 422], [1014, 841], [1014, 841], [843, 945], [843, 945], [1062, 886, 844], [1062, 886, 844], [1062, 886, 844], [1288, 1303], [1288, 1303], [301, 846], [301, 846], [345, 396, 303, 318], [345, 396, 303, 318], [345, 396, 303, 318], [345, 396, 303, 318], [493, 847], [493, 847], [850, 1428], [850, 1428], [1075, 851], [1075, 851], [852, 853], [852, 853], [852, 853], [852, 853], [1094, 853], [1094, 853], [1094, 853], [1094, 853], [1290, 1304], [1290, 1304], [854, 879], [854, 879], [1417, 306, 516, 505, 507], [1417, 306, 516, 505, 507], [1417, 306, 516, 505, 507], [1417, 306, 516, 505, 507], [1417, 306, 516, 505, 507], [1429, 306], [1429, 306], [306, 964, 505], [306, 964, 505], [306, 964, 505], [306, 516, 1092], [306, 516, 1092], [306, 516, 1092], [308, 1322], [308, 1322], [308, 317], [308, 317], [491, 311, 483], [491, 311, 483], [491, 311, 483], [1364, 1365, 312], [1364, 1365, 312], [1364, 1365, 312], [968, 860], [968, 860], [903, 862, 899], [903, 862, 899], [903, 862, 899], [315, 396, 1326, 318], [315, 396, 1326, 318], [315, 396, 1326, 318], [315, 396, 1326, 318], [362, 345, 396, 316, 318, 319], [362, 345, 396, 316, 318, 319], [362, 345, 396, 316, 318, 319], [362, 345, 396, 316, 318, 319], [362, 345, 396, 316, 318, 319], [362, 345, 396, 316, 318, 319], [320, 342], [320, 342], [1293, 1350], [1293, 1350], [1294, 1359], [1294, 1359], [1294, 1295], [1294, 1295], [871, 899, 1114], [871, 899, 1114], [871, 899, 1114], [465, 325, 360, 467], [465, 325, 360, 467], [465, 325, 360, 467], [465, 325, 360, 467], [386, 1296], [386, 1296], [873, 958], [873, 958], [873, 982], [873, 982], [1297, 1046], [1297, 1046], [328, 344], [328, 344], [875, 1004], [875, 1004], [330, 483], [330, 483], [330, 483], [330, 483], [877, 947], [877, 947], [432, 556, 331], [432, 556, 331], [432, 556, 331], [1298, 1299], [1298, 1299], [337, 1312], [337, 1312], [881, 942], [881, 942], [1079, 883], [1079, 883], [885, 951], [885, 951], [886, 1059], [886, 1059], [886, 406], [886, 406], [1306, 1408], [1306, 1408], [342, 483], [342, 483], [361, 343], [361, 343], [1435, 344], [1435, 344], [345, 396], [345, 396], [345, 397], [345, 397], [568, 569, 345, 570, 428], [568, 569, 345, 570, 428], [568, 569, 345, 570, 428], [568, 569, 345, 570, 428], [568, 569, 345, 570, 428], [347, 573, 574], [347, 573, 574], [347, 573, 574], [1389, 1308], [1389, 1308], [348, 349, 350, 351, 352], [348, 349, 350, 351, 352], [348, 349, 350, 351, 352], [348, 349, 350, 351, 352], [348, 349, 350, 351, 352], [371, 355], [371, 355], [560, 355], [560, 355], [560, 371, 355], [560, 371, 355], [560, 371, 355], [483, 484, 371, 555, 355, 560], [483, 484, 371, 555, 355, 560], [483, 484, 371, 555, 355, 560], [483, 484, 371, 555, 355, 560], [483, 484, 371, 555, 355, 560], [483, 484, 371, 555, 355, 560], [560, 355], [560, 355], [898, 891, 892], [898, 891, 892], [898, 891, 892], [988, 892], [988, 892], [894, 1088], [894, 1088], [1044, 899], [1044, 899], [899, 900], [899, 900], [899, 1114], [899, 1114], [899, 900], [899, 900], [360, 467], [360, 467], [468, 495, 360, 467], [468, 495, 360, 467], [468, 495, 360, 467], [468, 495, 360, 467], [465, 360, 467], [465, 360, 467], [465, 360, 467], [486, 360, 373], [486, 360, 373], [486, 360, 373], [360, 373], [360, 373], [484, 360], [484, 360], [465, 360, 467], [465, 360, 467], [465, 360, 467], [484, 360, 483], [484, 360, 483], [484, 360, 483], [360, 567], [360, 567], [448, 362], [448, 362], [1119, 362], [1119, 362], [511, 363], [511, 363], [392, 363], [392, 363], [1315, 1073], [1315, 1073], [1318, 1432], [1318, 1432], [1318, 1432], [1318, 1432], [910, 911], [910, 911], [492, 366], [492, 366], [1070, 1017, 1018, 929, 913], [1070, 1017, 1018, 929, 913], [1070, 1017, 1018, 929, 913], [1070, 1017, 1018, 929, 913], [1070, 1017, 1018, 929, 913], [1018, 913, 1017], [1018, 913, 1017], [1018, 913, 1017], [1319, 1320], [1319, 1320], [1320, 1368], [1320, 1368], [368, 416], [368, 416], [916, 1014, 1005], [916, 1014, 1005], [916, 1014, 1005], [532, 409, 371, 373, 374, 490], [532, 409, 371, 373, 374, 490], [532, 409, 371, 373, 374, 490], [532, 409, 371, 373, 374, 490], [532, 409, 371, 373, 374, 490], [532, 409, 371, 373, 374, 490], [371, 372], [371, 372], [485, 372, 483], [485, 372, 483], [485, 372, 483], [560, 372, 452], [560, 372, 452], [560, 372, 452], [540, 373], [540, 373], [479, 373], [479, 373], [517, 373], [517, 373], [449, 375], [449, 375], [443, 377, 413], [443, 377, 413], [443, 377, 413], [920, 919], [920, 919], [922, 923], [922, 923], [383, 384], [383, 384], [383, 384], [383, 384], [542, 384], [542, 384], [930, 927], [930, 927], [392, 467, 483], [392, 467, 483], [392, 467, 483], [928, 1022], [928, 1022], [932, 1327], [932, 1327], [934, 457], [934, 457], [1078, 935], [1078, 935], [1328, 1354], [1328, 1354], [1329, 1367], [1329, 1367], [1329, 1367], [1329, 1367], [400, 565, 567, 431], [400, 565, 567, 431], [400, 565, 567, 431], [400, 565, 567, 431], [1330, 1449], [1330, 1449], [1103, 939], [1103, 939], [403, 459], [403, 459], [1331, 1414], [1331, 1414], [1118, 943], [1118, 943], [944, 1039], [944, 1039], [1334, 1336], [1334, 1336], [1335, 1416], [1335, 1416], [1422, 407], [1422, 407], [1422, 407], [1422, 407], [1079, 1003, 950], [1079, 1003, 950], [1079, 1003, 950], [1339, 1457], [1339, 1457], [414, 420, 536, 483], [414, 420, 536, 483], [414, 420, 536, 483], [414, 420, 536, 483], [419, 420, 415], [419, 420, 415], [419, 420, 415], [1397, 1341], [1397, 1341], [1344, 1345, 1343], [1344, 1345, 1343], [1344, 1345, 1343], [1345, 1343], [1345, 1343], [1345, 1343], [1345, 1343], [520, 418], [520, 418], [420, 483], [420, 483], [1070, 1017, 1018, 962, 1009], [1070, 1017, 1018, 962, 1009], [1070, 1017, 1018, 962, 1009], [1070, 1017, 1018, 962, 1009], [1070, 1017, 1018, 962, 1009], [963, 1393], [963, 1393], [1351, 965], [1351, 965], [1067, 966], [1067, 966], [1434, 1347], [1434, 1347], [430, 483], [430, 483], [432, 1406], [432, 1406], [437, 435, 436], [437, 435, 436], [437, 435, 436], [977, 1012], [977, 1012], [1013, 979], [1013, 979], [1360, 1442], [1360, 1442], [472, 442], [472, 442], [443, 1092], [443, 1092], [444, 516], [444, 516], [544, 1365], [544, 1365], [543, 993], [543, 993], [1369, 1370], [1369, 1370], [1369, 1370], [1369, 1370], [1091, 999], [1091, 999], [1079, 1080, 1001, 1074, 1042], [1079, 1080, 1001, 1074, 1042], [1079, 1080, 1001, 1074, 1042], [1079, 1080, 1001, 1074, 1042], [1079, 1080, 1001, 1074, 1042], [1079, 1080, 1042, 1001], [1079, 1080, 1042, 1001], [1079, 1080, 1042, 1001], [1079, 1080, 1042, 1001], [1373, 1374], [1373, 1374], [1003, 1392], [1003, 1392], [572, 530, 464], [572, 530, 464], [572, 530, 464], [1378, 1379], [1378, 1379], [1008, 1117], [1008, 1117], [484, 467, 483], [484, 467, 483], [484, 467, 483], [1080, 1010], [1080, 1010], [1099, 1011], [1099, 1011], [1079, 1011], [1079, 1011], [1079, 1090, 1011, 1123], [1079, 1090, 1011, 1123], [1079, 1090, 1011, 1123], [1079, 1090, 1011, 1123], [1383, 1382], [1383, 1382], [1383, 1382], [1383, 1382], [1079, 1015], [1079, 1015], [555, 472, 483], [555, 472, 483], [555, 472, 483], [1385, 1386], [1385, 1386], [1018, 1070], [1018, 1070], [1388, 1419], [1388, 1419], [1019, 1392], [1019, 1392], [486, 482], [486, 482], [547, 483], [547, 483], [484, 483], [484, 483], [520, 483], [520, 483], [519, 483], [519, 483], [484, 483], [484, 483], [484, 483], [484, 483], [487, 1390], [487, 1390], [1079, 488], [1079, 488], [1034, 493], [1034, 493], [1124, 1037, 493], [1124, 1037, 493], [1124, 1037, 493], [1035, 1036, 1068], [1035, 1036, 1068], [1035, 1036, 1068], [1035, 1036, 1068], [1035, 1036, 1068], [1035, 1036, 1068], [1399, 1400, 1402], [1399, 1400, 1402], [1399, 1400, 1402], [1400, 1401, 1402, 1403, 1404], [1400, 1401, 1402, 1403, 1404], [1400, 1401, 1402, 1403, 1404], [1400, 1401, 1402, 1403, 1404], [1400, 1401, 1402, 1403, 1404], [500, 1406], [500, 1406], [1104, 1059], [1104, 1059], [1061, 1085], [1061, 1085], [1455, 1411], [1455, 1411], [1412, 1455, 1411], [1412, 1455, 1411], [1412, 1455, 1411], [1455, 1411], [1455, 1411], [508, 509], [508, 509], [538, 1073], [538, 1073], [1423, 1424], [1423, 1424], [1451, 518], [1451, 518], [1128, 1077], [1128, 1077], [1078, 1079], [1078, 1079], [1426, 1427], [1426, 1427], [1118, 1084], [1118, 1084], [1097, 1096], [1097, 1096], [1100, 1101], [1100, 1101], [1437, 1453], [1437, 1453], [534, 535], [534, 535], [1443, 1444], [1443, 1444], [1446, 1447], [1446, 1447], [1112, 1113, 546], [1112, 1113, 546], [1112, 1113, 546], [565, 556, 562], [565, 556, 562], [565, 556, 562], [1128, 1126], [1128, 1126]]
[ Info: Preparing Input
[ Info: Preparing Neighborhoods
[ Info: Preparing Cost Matrix
[ Info: Preparing Dispersion
[ Info: Computing Dispersions
[ Info: Computing Directional Curvature
[ Info: Computing Node Curvature Neighborhood
[ Info: Computing Edge Curvature
[ Info: Computing Node Curvature Edges
[ Info: Computing Edge Curvature
[ Info: Computing Node Curvature Edges
[ Info: Converting Curvatures to JSON
[ Info: Writing JSON to /Users/pellegrinraphael/Desktop/Repos_GNN/Hypergraph_Encodings/src/encodings_hnns/results.alpha-0.dispersion-UnweightedClique.orc.json
['/Users/pellegrinraphael/Desktop/Repos_GNN/Hypergraph_Encodings/scripts', '/Users/pellegrinraphael/anaconda3/envs/our_repo_2/lib/python311.zip', '/Users/pellegrinraphael/anaconda3/envs/our_repo_2/lib/python3.11', '/Users/pellegrinraphael/anaconda3/envs/our_repo_2/lib/python3.11/lib-dynload', '/Users/pellegrinraphael/anaconda3/envs/our_repo_2/lib/python3.11/site-packages', '/Users/pellegrinraphael/anaconda3/envs/our_repo_2/lib/python3.11/site-packages/torch_scatter-2.1.2-py3.11-macosx-10.9-x86_64.egg', '/Users/pellegrinraphael/Desktop/Repos_GNN/Hypergraph_Encodings/src']
We are adding the LCP encodings
The split is 1
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
We are adding encodings!
Adding the LCP encodings
Current working directory: /Users/pellegrinraphael/Desktop/Repos_GNN/Hypergraph_Encodings
The node curvatures are 
 [-0.6307221174240112, -0.10012395011967626, 0.0, 0.6079807033141454, 0.13651812076568604, 0.27726465711990994, 0.0, 0.7525061240250414, -0.3126959130167961, -0.08620458841323853, -0.5410125494003296, 0.0, 0.7525061240250414, 0.401481956243515, 0.0331120491027832, 0.0, 0.33567008801868986, 0.054516323407491046, -0.38888370990753174, -0.4890304406483968, 0.16993868350982666, 0.0, 0.28778797884782154, 0.22122427821159363, -0.18761561810970306, -0.33426078870182946, -0.1530007981207399, -0.15585783123970032, -0.07775409297740206, 0.0, -0.2119403544526834, 0.4964961647987366, -0.2749859794974327, 0.11502885818481445, 0.6397657642761866, -0.056274719536304474, 0.3879681587219238, 0.32789343542286326, 0.1806900511185328, -2.2709369659423828e-05, 0.4792454898357391, 0.0, -0.314751257499059, 0.2610241540016667, 0.11494702100753784, 0.0, 0.17779080793261529, 0.1666553020477295, -0.1431712651004394, -0.29386788606643677, 0.17454110185305277, 0.35872446969151495, 0.07168811559677124, 0.510180875658989, 0.22142778833707175, 0.3858471309973134, -0.44460394520025986, 0.0, 0.36636320998271305, -0.6099433342615763, 0.14128504480634416, -0.2617425322532654, 0.01889405647913615, 0.27726465711990994, -0.18288244307041168, 0.33567008801868986, -0.0021345913410186768, 0.0567132830619812, 0.18974358760393584, 0.09901122252146403, -0.14584308862686157, 0.014797098934650421, 0.31601256132125854, 0.4448864579200745, 0.6079807033141454, 0.029564499855041504, 0.002957717577616374, 0.2983550727367401, -0.41751465797424314, 0.27820058663686115, -0.010082105174660683, 0.2025585174560547, 0.6638476798931757, 0.6638476798931757, 0.055226970996175496, 0.19343343377113342, 0.5503316422303518, 0.14493361115455627, -0.39885679880777997, -0.3376622796058655, 0.1873209983110428, 0.0444754958152771, -0.3426346353122166, 0.024160020053386688, 0.06750636166996427, 0.0, 0.09901122252146403, 0.0, 0.21668819189071656, -0.33619385957717896, -0.27209967374801636, 0.0, 0.0, -0.17578021292724916, -0.27500440925359726, 0.2983550727367401, 0.14585357159376144, -0.021300418810410934, -0.5418009128835466, 0.16917048891385397, 0.026475953869521618, -0.08900030851364135, 0.22186242706245846, 0.0, 0.7525061240250414, -0.29103147983551025, -0.02393230398495992, -0.034767523407936096, 0.2119144101937612, 0.17861306927231854, 0.041348761816819506, -0.21189357856145272, -0.4718484083811442, -0.17929266393184662, -0.3496808409690857, -0.20647716522216797, 0.31030089408159256, 0.07274155145467714, 0.0, 0.08119263052940369, 0.30530183017253876, 0.38584170904424453, -0.5581617454687754, -0.2616366263892915, 0.009295523166656494, -0.18002340468493375, 0.13651812076568604, -0.3435446258747216, 0.27104032039642334, -0.2664789013240648, 0.4964961647987366, 0.11502885818481445, 0.1539477360875983, 0.26459643095731733, 0.6079807033141454, 0.0444754958152771, 0.7525061240250414, 0.0, 0.0, 0.498043417930603, 0.498043417930603, 0.498043417930603, 0.16568386554718018, 0.1352701763312022, 0.6365780830383301, 0.27997786303361255, -0.08468858400980632, 0.6638476798931757, -0.24483617395162582, 0.2983550727367401, 0.010176258427756173, 0.0, -0.004114963114261627, 0.1884558896223704, 0.19998186826705933, 0.04136033703883489, -0.09279069900512696, 0.7525061240250414, -0.0018949608008066814, 0.0, 0.0, -0.32055312395095825, -0.037845319509506224, 0.24255748093128204, -0.4090868830680847, 0.0151787797609965, -0.4903801679611206, 0.11534666270017624, 0.0, 0.4878770609696706, 0.009295523166656494, -0.04704579185037052, 0.0, 0.04993319511413574, 0.21702665090560913, -0.022861236499415502, -0.17107042156416793, -0.0013515088293287489, 0.0010189771652221679, -0.8595446745554606, -0.2770635684331258, 0.1882912904024124, 0.39258162677288055, 0.4095465615391731, 0.3977833978831768, 0.23434527052773368, -0.13420410950978598, -0.541018565495809, -0.36703800420238547, 0.022404557466506957, 0.08242858201265335, 0.2229414482911428, -0.2061567765015822, 0.38743880987167356, 0.0, 0.24784187078475953, -0.012960225343704224, 0.4921673685312271, -0.01848221818606059, 0.0, 0.0, 0.0, 0.40106147264733033, -0.2818204164505005, -0.4110226233800252, 0.0, -0.08815733591715495, -0.497010190235941, 0.08173898607492447, -0.009879747405648232, 0.0, 0.009295523166656494, -0.13986249764760336, -0.017531674217294763, 0.5503316422303518, 0.08235567808151245, 0.08235567808151245, 0.07432550191879272, -0.14774275898933412, 0.3858529445197847, 0.7525061240250414, -0.24999922513961792, 0.0, -0.39133238792419434, 0.2235032320022583, 0.2731403625673718, -0.12434712092081705, 0.0, -0.51893640226788, 0.3281708950442927, 0.11467093229293823, 0.026272887984911602, -0.18293498704830805, -0.46732670068740845, 0.4449532747268677, -0.20198645525508457, 0.09771070877710979, -0.06965771317481995, 0.3331549923334803, -0.3629993120829264, 0.2898607168878828, -0.0636874258518219, 0.2511932909488678, -0.2957390367984772, 0.13659208019574484, 0.052592337131500244, 0.2707388997077942, 0.0, 0.7525061240250414, 0.12535609470473397, 0.2772111269560727, 0.0359148528124835, 0.0, -0.0005178332328796386, -0.23703882098197937, -0.04042269289493561, -0.19753068685531616, -0.024851481119791668, -0.007889055288754977, -0.06130307912826538, 0.4018595442175865, 0.21668819189071656, 0.26150064170360565, -0.12834951281547546, 0.6079807033141454, -0.05290478467941284, -0.17852770840680157, -0.11460901158196586, 0.0, 0.0, -0.1844051718711853, 0.20301332644053868, 0.008213704046995743, 0.5503316422303518, 0.32246895631154376, 0.011695583661397299, 0.13677736471096674, -0.03139752149581909, 0.4878770609696706, -0.1112290620803833, 0.04828009009361267, 0.02773606777191162, 0.38583777182632023, 0.38583777182632023, 0.3898980106626238, 0.07432247698307037, -0.2886841893196106, -0.09841209650039673, 0.0, 0.4018595442175865, -0.3888610005378723, 0.028168708086013794, 0.23542072229525624, 0.3358111636979239, 0.5503316422303518, -0.24388838332632315, 0.26603510505274724, -0.1744540674345834, 0.2707388997077942, -0.05283147096633911, -0.28611191113789874, -0.26517038282595184, 0.20142738024393717, 0.09277766942977905, 0.36765843687149197, 0.1623376978410257, -0.11474889516830444, 0.213046092320891, -0.10691626071929931, 0.0, 0.06666314601898193, 0.0, 0.09277766942977905, 0.39776762053370474, 0.11095757285753886, 0.5503316422303518, 0.0567132830619812, -0.5333677530288696, 0.3446987122297287, 0.0, 0.06198793649673462, -0.11411770184834798, 0.3159796992937724, 0.13783883303403854, 0.0331120491027832, 0.6240749478340148, 0.0, -0.0503401905298233, -0.002298967196391179, 0.6365780830383301, -1.6480684280395508e-05, -0.08590418558854324, 0.15142865975697836, -0.5599172433217366, -0.07583264300697728, 0.44839116334915163, -0.06728744506835938, 0.34317997594674426, 0.5615590363740921, 0.3431684921185176, 0.3431684921185176, 0.5615934878587723, 0.044831585884094236, -0.21870341897010803, -0.41829008162021636, 0.34073350951075554, -0.12312179803848267, 0.14424402515093485, -0.12451690435409546, -0.6670646238327026, 0.06588317950566609, -0.19673639039198557, -0.4615384340286255, 0.3333030939102173, 0.2228465179602305, 0.1221059833963712, 0.0541670560836792, 0.0, 0.3873513213225773, 0.0, -0.389831440448761, -0.28929193019866944, -0.3499392092227936, 0.33779633045196533, -0.24912118911743164, -0.0839874545733134, 0.04074307531118393, 0.0, 0.06481394642277767, 0.27309052646160126, 0.1339552879333496, -0.13386817649006844, 0.16766812887630964, 0.5071448087692261, 0.3358111636979239, -0.4931507110595703, -0.14325666862229505, 0.0, -0.1527745019910591, 0.26452475041151047, 0.055264249444007874, -0.4509065105364873, -0.03127618134021759, 0.0, 0.0, -0.019468768879219337, -0.040361762046813965, 0.36637066773794313, 0.2218039615286721, -0.1765049636363983, 0.27616767088572186, 0.07950083017349244, 0.07950083017349244, 0.38716969745499746, 0.309378834931474, 0.21702665090560913, 0.4798838019371033, 0.2645242974162102, 0.33779633045196533, 0.0, 0.0, -0.046865254640579224, 0.19637170433998108, -0.05287271406915453, 0.299458384513855, -0.12011822632380895, -0.056292660534381866, 0.22151250640551248, 0.299458384513855, -0.18101584911346436, 0.13268675122942244, -0.14380151575261896, 0.2800513803958893, 0.2707388997077942, 0.060134708881378174, 0.15101960767060518, 0.1273172398408254, 0.14669197407506762, 0.6079807033141454, 0.0, 0.24416005611419678, -0.8029441982507706, 0.17792488262057304, 0.0, 0.498043417930603, 0.498043417930603, 0.498043417930603, -0.31611577953611103, 0.1691680649916331, 0.009295523166656494, 0.2218039615286721, -0.5016163289546967, -0.14777507100786483, -0.16362823843955993, -0.0018876492977142334, 0.21351799254234022, 0.11419405255998884, -0.3014631794423473, -0.4638618230819702, -0.38876716295878094, 0.18861292799313864, -0.15801681578159332, -0.1856616735458374, 0.3332880139350891, 0.22122427821159363, 0.4492295742034912, -0.23849589824676515, 0.0, -0.1624342168078703, -0.11905288696289062, -0.12437706689039867, 0.14020799928241306, 0.3806260777147193, -0.16666468977928162, 0.11111461122830708, 0.0, -0.3865587592124939, -0.07390915022956, -0.1432829201221466, 0.6079807033141454, 0.4982505242029826, -0.43431970748034393, 0.19343343377113342, 0.23856093486150107, -0.059340476989746094, 0.0009483397006988525, 0.15595329801241556, 0.0634126365184784, 0.0, -0.45884687900543214, 0.05761645634969075, 0.013716500997543336, -0.7594953965057026, -0.7172952095667521, -0.30960941314697266, -0.7364330689112345, 0.498043417930603, -0.41111111640930176, -0.34243229362699723, 0.33779633045196533, -0.04925811290740967, -0.37385884920756024, -0.16050950757094792, 0.7700032234191895, 0.010609421465131972, 0.20017477828595373, 0.0, 0.38743880987167356, 0.0, 0.0, -0.19822640833444893, 0.07465581595897675, 0.25624772533774376, 0.4961027860641479, -0.187359810979278, 0.0, 0.03402775526046753, 0.0, 0.0, -0.4931507110595703, 0.0, 0.445265007019043, 0.1767756442228953, 0.1767756442228953, -0.0485513707002004, -0.2745376722017924, 0.0, 0.0, 0.0, -0.4804776708285014, 0.664033442735672, 0.4961027860641479, 0.03806028266747793, 0.7532977278936993, 0.06536280115445454, -0.09078147013982137, 0.049929261207580566, 0.5107511281967163, 0.29917569864879956, -0.0772247165441513, 0.3358111636979239, 0.33779633045196533, 0.11902208129564922, 0.03502194370542254, 0.0, -0.012137393156687418, 0.0, -0.19372259719031198, 0.0, 0.0, -0.4860130548477173, 0.0, 0.16582657183919633, -0.34495726227760315, 0.24940791726112366, -0.35927361249923706, 0.0, 0.05585591991742452, -0.2077094465494156, 0.47916755080223083, 0.0, 0.40128349118372975, 0.40128349118372975, 0.08828021798815046, -0.03444025608209463, -0.17383864521980286, 0.0, 0.16568386554718018, -0.06778893205854628, -0.15567533506287468, 0.19007225220020002, 0.17603164911270142, 0.018364623188972473, -0.40512699882189435, -0.13650765419006347, 0.38702043890953064, -0.08524542053540547, 0.2557977877912067, 0.3617057278752327, 0.23343494286139807, -0.26731434890202116, -0.27154025435447693, -0.23653759479522704, -0.15484617785973984, 0.2560964606702328, -0.16187602281570435, 0.0, 0.2221818765004476, 0.33358485500017804, 0.5656534492969513, 0.17933406763606602, 0.0, 0.1650029942393303, 0.12190312147140503, 0.0, 0.0, -0.25921154022216797, 0.498043417930603, 0.02276952068010966, 0.3749319165945053, 0.0, 0.37948411454757053, -0.07660501599311828, 0.055216739575068154, 0.13880745321512222, 0.37397781014442444, 0.37941014518340427, 0.22938872235161917, 0.0, -0.23077482047180334, 0.04845532774925232, -0.2903628349304199, 0.37310361862182617, 0.4977700114250183, 0.0, 0.0, 0.5000345607598623, 0.17617649659514428, 0.5542786866426468, 0.0, 0.22217905521392822, 0.46122875809669495, 0.0, 0.0, 0.6253561962109345, 0.2698763981461525, 0.6253561962109345, 0.0, -0.09718463818232219, 0.16562995314598083, 0.359783540169398, 0.0, -0.3318369388580322, 0.11167263984680176, 0.3558320701122284, -0.3318369388580322, -0.1053707136048211, 0.0, 0.6253561962109345, 0.18616395443677902, 0.37351489067077637, 0.3333030939102173, 0.0, 0.14581725001335144, 0.0, -0.15609323978424072, 0.0, 0.31726013620694477, 0.0, -0.07615643739700317, 0.5114587604999542, 0.0, 0.08368618289629619, 0.4081496596336365, 0.37397781014442444, 0.22918803095817566, 0.2814937114715576, 0.0, -0.08732672035694122, 0.4054151972134908, 0.2787383645772934, 0.6267164442688227, 0.4431312382221222, 0.432669597864151, 0.4023846983909607, 0.45832480986913043, 0.002742121616999308, 0.2489863634109497, 0.0, 0.16568386554718018, 0.10052070683903164, 0.0, 0.2221818765004476, 0.2916279584169388, 0.188821941614151, 0.0, 0.09307836492856343, -0.24999845027923584, 0.5114587604999542, -0.2301742434501648, 0.0, 0.2025585174560547, 0.0, 0.6232742071151733, 0.2025585174560547, -0.07371896505355835, 0.4077822069327037, -0.21769943833351135, 0.0, 0.04274726339748928, 0.22918803095817566, 0.0, 0.6253564529694043, -0.02106761932373047, 0.3992150008678436, 0.43289864659309385, 0.2698763981461525, 0.0, 0.10842625382873747, 0.27570007199590857, 0.18822431564331055, 0.6253564529694043, 0.4977700114250183, -0.5156503319740295, -0.06211903691291809, 0.25281858444213867, 0.14901793003082275, -0.7042341331640879, 0.26405592759450275, 0.1933875788341869, 0.6091012358665466, 0.0, -0.16162761052449545, -0.08726108074188232, 0.0, 0.4030972123146057, 0.4619684100151062, 0.2959909240404765, 0.2515786488850911, -0.01516646146774292, 0.21331006288528442, 0.0, 0.4488832652568817, 0.0, 0.21331006288528442, 0.4030972123146057, -0.06170174479484558, 0.4619684100151062, 0.3749319165945053, 0.05738376379013062, 0.498043417930603, 0.359783540169398, 0.0833137035369873, -0.37284026827131, 0.2467574973901113, 0.5655859768390655, 0.498043417930603, 0.3749489486217499, 0.20119462410608926, 0.37461668252944946, 0.18651646375656128, 0.16569490234057108, 0.0, 0.0, -0.08333650231361389, 0.6253564529694043, 0.00017720460891723633, 0.0, 0.3100782566600376, 0.498043417930603, 0.0, 0.45169233083724974, 0.7700032234191895, 0.498043417930603, -0.5637100219726563, 0.4404981955885887, -0.28817959626515705, 0.7700032234191895, -0.14305450916290283, 0.6636858880519867, 0.0, 0.0, 0.0, 0.0, 0.3333030939102173, 0.30058324337005615, 0.2724410593509674, 0.4590054154396057, 0.0, 0.37461668252944946, 0.07402104139328003, 0.6655653953552246, -0.2503325939178467, -0.16916783650716147, 0.0, -0.14876852432886759, -0.39651478826999664, 0.10414630174636841, 0.14244642853736877, 0.023900330066680908, 0.34652677652510727, -0.23129204474389553, 0.0, 0.4404981955885887, 0.03446446359157562, 0.2724410593509674, 0.08392566442489624, -0.10397062301635743, 0.0, 0.0, -0.06885039806365967, 0.37351489067077637, 0.1666553020477295, 0.0, 0.027433386580510574, 0.0, 0.25310471653938293, 0.0, 0.0, -0.30937055746714276, 0.0, 0.23185055255889891, 0.0, 0.2221818765004476, 0.498043417930603, 0.7700032234191895, 0.41567325592041016, 0.3730108216404915, 0.24933266639709473, 0.14244642853736877, -0.3344260975718498, 0.0, 0.40856861472129824, 0.16568386554718018, 0.06990796327590942, 0.45832480986913043, 0.0, 0.0, 0.1999848484992981, 0.08209589123725891, 0.05998520652453105, -0.22001977761586508, -0.28311346769332885, 0.591219574213028, 0.0, -0.3678736984729767, 0.0, 0.2698763981461525, 0.37461668252944946, 0.4431312382221222, 0.11298562586307526, 0.0, 0.1286923885345459, 0.5575198928515116, 0.0, -0.15826180577278137, 0.591219574213028, -0.05544304847717285, 0.0, 0.498043417930603, 0.498043417930603, -0.2497465411822001, 0.0, 0.0, 0.1999848484992981, 0.0, 0.1666553020477295, 0.0, 0.0, 0.0, 0.0, 0.3428035279115041, 0.0, 0.018188118934631348, 0.0, 0.4207255045572917, 0.05259221792221069, 0.4023846983909607, 0.06605266779661179, -0.2707124214905959, 0.16276372969150543, 0.11184310913085938, -0.41550660133361816, 0.0, 0.15029865503311157, 0.24933266639709473, -0.30781106154123944, 0.20481900374094644, 0.20119462410608926, 0.0, 0.2788531929254532, 0.36661161346869037, 0.24933266639709473, 0.299907922744751, 0.005736755697350753, -3.3991411328315735e-05, -0.19700679183006287, 0.7700032234191895, -0.2164201021194458, 0.06413640826940536, 0.03761819998423258, 0.37910357117652893, -0.3318369388580322, 0.3956616401672363, 0.0, -0.4388031244277954, 0.0, 0.498043417930603, 0.0, 0.01458137035369873, 0.0, 0.3407846987247467, 0.0, 0.25310471653938293, -0.04222381114959717, -0.44954826150621685, 0.06274883449077606, -0.04532504081726074, 0.5617221196492513, 0.20502455830574035, 0.06245735287666321, 0.11167367299397786, 0.6267164442688227, 0.0, 0.0384596586227417, 0.33287467890315586, 0.10842918107906978, 0.2922465205192566, 0.09802867816044734, 0.09406986832618713, 0.22659258544445038, 0.5574837923049927, 0.4545949101448059, 0.5576513260602951, -0.11123885427202497, 0.22146890163421631, 0.5656354129314423, 0.591219574213028, 0.0, 0.6636858880519867, 0.6636858880519867, 0.6232742071151733, 0.561541810631752, 0.0, 0.0, 0.18651646375656128, 0.46125856041908264, 0.11607509851455688, 0.0, 0.0, 0.26118234793345135, -0.2373348077138265, -0.49834108352661133, 0.37461668252944946, 0.07499536871910095, 0.47317275404930115, 0.0, 0.0, 0.561541810631752, 0.3727079927921295, -0.010321855545043945, 0.0, 0.498043417930603, 0.0, 0.0, 0.3883476157983144, 0.29988203942775726, 0.29947778582572937, 0.0, 0.3992150008678436, 0.32735825777053834, 0.0, 0.0, 0.0, 0.0, 0.4533712327480316, 0.0, 0.29947778582572937, 0.1999848484992981, -0.008346378803253174, 0.0, 0.2911447286605835, 0.6656194686889648, -0.13297363519668579, 0.49988675117492676, 0.49988675117492676, 0.6232742071151733, 0.0, 0.0, 0.12427446246147156, 0.37461668252944946, 0.561541810631752, 0.3887633482615153, -0.21746142208576202, -0.03444133486066546, 0.11167367299397786, 0.0, 0.0, 0.0, 0.08318166931470235, -0.06214183568954468, 0.19705757312476635, 0.0, -0.37364877121789114, 0.0, 0.0, 0.0, -0.40191725322178434, 0.0, 0.3465268957343968, 0.0, 0.3823424776395162, 0.4982694983482361, 0.2922465205192566, 0.4066370924313863, 0.4066370924313863, 0.015862782796223957, 0.0, 0.6091012358665466, 0.498143769800663, 0.3097848407924175, 0.0, 0.0, 0.4491780251264572, 0.1246806134780248, 0.0, 0.0, 0.46125856041908264, 0.0, 0.3983888253569603, -0.47410608189446585, 0.0, -0.2659058968226115, 0.0, 0.18651646375656128, 0.45832480986913043, -0.07407212257385254, 0.0, 0.561541810631752, -0.011325296014547348, -0.36804820810045513, -0.2523111402988434, 0.0, 0.056620121002197266, 0.0, 0.6091012358665466, 0.6097759554783503, 0.6097759554783503, -2.2709369659423828e-05, 0.3883476157983144, 0.29947778582572937, 0.0, 0.0554750661055247, 0.4030972123146057, 0.498043417930603, 0.23806389172871908, 0.5545165836811066, 0.6636858880519867, 0.27362284892135197, 0.3793168440461159, 0.31726013620694477, 0.4077822069327037, 0.0, 0.0, 0.498043417930603, 0.498043417930603, -0.15345436334609985, -0.06095464527606964, 0.2221818765004476, 0.0, -0.24920203536748886, -0.06761572261651357, 0.6267164442688227, 0.18593033154805502, 0.6267164442688227, -0.3016863862673442, 0.2467574973901113, 0.44057223780287635, 0.3199775591492653, 0.498043417930603, 0.498043417930603, 0.09028682708740235, 0.09028682708740235, -0.12248247861862183, -0.04222381114959717, -0.28636248111724855, 0.29242763221263884, 0.2959909240404765, -0.29493503360187306, 0.0, 0.0, 0.08987993001937866, 0.4982694983482361, 0.4982694983482361, 0.5545165836811066, 0.0, 0.0, 0.498043417930603, 0.498043417930603, 0.6097759554783503, 0.591219574213028, 0.0, -0.5219849348068237, -0.0488814115524292, -0.06804327666759491, 0.0, 0.0, -0.44999998807907104, -0.5856365263462067, -0.3351470069451766, 0.19336897879838943, 0.07402104139328003, 0.6655479669570923, 0.0, 0.0, 0.44057223780287635, 0.09307836492856343, 0.0, 0.0, 0.020485868056615193, 0.0001865824063618978, -0.21456079483032225, -0.0897931456565857, -0.3514437973499298, 0.08368618289629619, 0.0, 0.0, 0.12190312147140503, 0.0, 0.0, 0.0, -0.5074431419372558, 0.0, 0.0, 0.2647622653416225, 0.4075825015703837, 0.29599084456761676, 0.2747709575024518, 0.05080854892730713, 0.433010596036911, 0.498043417930603, 0.37461668252944946, 0.37461668252944946, -0.1342335343360901, 0.5383204089270698, -0.3659212787946065, 0.0, 0.0, 0.06526390711466472, 0.44057223780287635, 0.0, -0.14196622371673584, 0.020485868056615193, -0.2589349150657654, 0.44057223780287635, 0.0, 0.40246334075927737, 0.20050463676452637, 0.37461668252944946, -0.4331907033920288, 0.498043417930603, 0.498043417930603, 0.48794201016426086, -0.02810647249931381, 0.0, 0.2087060511112213, 0.0, 0.0, 0.0, 0.25720343987147015, 0.0, 0.498043417930603, 0.32046873569488527, -0.20227932929992676, 0.0, 0.25720343987147015, 0.6636858880519867, 0.0, 0.0, 0.498043417930603, 0.498043417930603, 0.6636858880519867, 0.498043417930603, 0.4652181367079417, 0.498043417930603, 0.0, 0.48794201016426086, 0.09903532266616821, 0.3465268957343968, 0.0, -0.40909016132354736, 0.498043417930603, 0.18331819772720337, 0.41567325592041016, 0.2185797095298767, -0.07699179649353027, 0.6636858880519867, 0.0, 0.0, 0.4652181367079417, 0.0, -0.16550815105438232, 0.02960073947906494, 0.4626118987798691, 0.0, 0.0, 0.0, 0.4982694983482361, 0.0, 0.0, 0.22220206260681152, 0.0, 0.498043417930603, 0.04016381502151489, 0.29947778582572937, 0.0, 0.0, 0.498043417930603, 0.0, 0.37351489067077637, 0.38841335972150165, 0.0, 0.498043417930603, 0.37461668252944946, 0.29163265228271484, 0.0, 0.10414612293243408, 0.0, 0.0, 0.0, 0.0, 0.498043417930603, 0.3465268957343968, 0.0, 0.6636858880519867, 0.2185797095298767, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4652181367079417, 0.3328531086444855, 0.0, 0.24933266639709473, 0.5212817986806234, 0.6636858880519867, 0.41567325592041016, 0.498043417930603, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.2842406630516052, 0.0, 0.05679944157600403, 0.0, -0.029344573616981506, 0.498043417930603, -0.2885676622390747, 0.0, 0.0, 0.0, 0.0, 0.0, 0.498043417930603, 0.0, 0.0, 0.41567325592041016, 0.0, 0.22220206260681152, 0.41567325592041016, 0.0, 0.0, 0.5212817986806234, 0.0, 0.6240749478340148, 0.0, 0.0, 0.0, -0.671244740486145, 0.0, 0.5538487235705057, 0.20350823799769083, 0.0028186440467834473, 0.48794201016426086, 0.0, 0.19011516066697928, 0.0, 0.498043417930603, 0.498043417930603, 0.040304139256477356, 0.0, 0.0, 0.0, 0.09744289517402649, 0.0, 0.0, 0.0, 0.3883286416530609, 0.04173874855041504, 0.0, 0.1821089506149292, 0.37351489067077637, 0.6636858880519867, 0.0, 0.0, 0.498043417930603, 0.498043417930603, 0.0, 0.0, -0.43573762611909345, 0.0, 0.498043417930603, 0.0, 0.0, 0.0, 0.0, 0.6636858880519867, 0.6636858880519867, 0.0, 0.0, 0.0, 0.0, 0.0, 0.498043417930603, 0.3888435363769531, 0.0, 0.0, 0.0, 0.0, 0.6636858880519867, 0.0, 0.0, 0.09903532266616821, 0.1999848484992981, 0.4859662155310313, 0.0384596586227417, 0.3883286416530609, 0.11222521464029948, 0.0, 0.0, 0.0, 0.41567325592041016, 0.22220206260681152, 0.0, -0.22152501344680786, 0.48794201016426086, 0.0, 0.2185797095298767, 0.020542601744333904, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4652181367079417, 0.0, 0.33261984090010327, 0.0, 0.4657345910867055, -0.332832932472229, -0.380937139193217, 0.0, 0.0, 0.0, 0.498043417930603, 0.498043417930603, 0.498043417930603, -0.10406264662742615, 0.0, 0.498043417930603, 0.24933266639709473, 0.0, 0.0, 0.498043417930603, 0.1821089506149292, 0.0, -0.6480217774709066, 0.5538487235705057, 0.498043417930603, -0.02810647249931381, 0.6636858880519867, 0.0, 0.18331819772720337, 0.0, 0.0, -0.0044013261795043945, -0.05108324686686198, 0.498043417930603, 0.4859662155310313, 0.0, 0.0, 0.0, 0.4652181367079417, 0.0, 0.0, 0.0, 0.0, 0.6636858880519867, 0.2569860816001892, 0.0, 0.0, 0.0571978489557902, 0.498043417930603, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3328531086444855, 0.0, -0.04920192062854767, 0.498043417930603, -0.16839522123336792, -0.16553086042404175, -0.6666644414265951, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18331819772720337, 0.4282546391089757, 0.5750918164849281, 0.5304683864116668, 0.5304683864116668, 0.4382529616355896, 0.33995348853724344, -0.37497806549072266, 0.0, 0.0, 0.33995348853724344, 0.38841335972150165, 0.498043417930603, 0.498043417930603, 0.1821089506149292, 0.0, 0.498043417930603, 0.0, 0.03402170538902283, 0.6636858880519867, 0.0, 0.1461194008588791, -0.16624922752380372, 0.0, 0.0, 0.0, 0.498043417930603, 0.22173939148585, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09993727207183838, 0.0, 0.0, 0.0, 0.0, 0.29163265228271484, 0.0, 0.0, 0.18651628494262695, 0.0, 0.0, 0.0, 0.18716847896575928, 0.4859662155310313, 0.0, 0.0, 0.0, 0.37461668252944946, 0.0, 0.4982694983482361, 0.0, 0.04478764533996582, 0.498043417930603, 0.22497957944869995, 0.0, 0.5538487235705057]
The edge curvatures are 
 [-1.1333332061767578, -1.1333332061767578, 0.23828909844160084, 0.23828909844160084, 0.23828909844160084, 0.23828909844160084, 0.23828909844160084, -1.6666667461395264, -1.6666667461395264, -0.6629748344421387, -0.6629748344421387, -0.6629748344421387, -0.3989694515864055, -0.3989694515864055, -0.3989694515864055, -0.008875124487611963, -0.008875124487611963, -0.008875124487611963, -0.008875124487611963, -0.008875124487611963, -0.008875124487611963, -0.008875124487611963, -0.008875124487611963, -0.008875124487611963, -0.008875124487611963, 0.07571536502991505, 0.07571536502991505, 0.07571536502991505, 0.07571536502991505, 0.07571536502991505, 0.07571536502991505, 0.07571536502991505, 0.07571536502991505, 0.07571536502991505, 0.07571536502991505, 0.07571536502991505, 0.07571536502991505, 0.07571536502991505, 0.07571536502991505, 0.07571536502991505, 0.07571536502991505, 0.07571536502991505, 0.07571536502991505, 0.07571536502991505, -0.1629398067792256, -0.1629398067792256, -0.1629398067792256, -0.1629398067792256, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3332829078038534, 0.3332829078038534, 0.3332829078038534, 0.0, 0.0, 0.18749700486660004, 0.18749700486660004, 0.18749700486660004, 0.07460831105709076, 0.07460831105709076, 0.07460831105709076, 0.8838842660188675, 0.8838842660188675, 0.3139993891947799, 0.3139993891947799, 0.3139993891947799, 0.3139993891947799, 0.3139993891947799, 0.3139993891947799, 0.3139993891947799, 0.3139993891947799, 0.3139993891947799, 0.549017312626044, 0.549017312626044, 0.549017312626044, 0.549017312626044, 0.6001611948013306, 0.6001611948013306, 0.5607576206326484, 0.5607576206326484, 0.5607576206326484, 0.5607576206326484, 0.5607576206326484, 0.43558431965309186, 0.43558431965309186, 0.43558431965309186, 0.43558431965309186, 0.43558431965309186, 0.43558431965309186, 0.43558431965309186, 0.43558431965309186, 0.43558431965309186, 0.43558431965309186, 0.43558431965309186, 0.43558431965309186, 0.43558431965309186, 0.43558431965309186, 0.03685106039047237, 0.03685106039047237, 0.03685106039047237, 0.03685106039047237, 0.03685106039047237, 0.18830518722534184, 0.18830518722534184, 0.18830518722534184, 0.18830518722534184, 0.18830518722534184, -0.1651134888331096, -0.1651134888331096, -0.1651134888331096, -0.6532715559005737, -0.6532715559005737, 0.10136390912036097, 0.10136390912036097, 0.10136390912036097, -0.41377222537994385, -0.41377222537994385, 0.0, 0.0, 0.4496937607015882, 0.4496937607015882, 0.4496937607015882, 0.4496937607015882, 0.4496937607015882, 0.4496937607015882, 0.4496937607015882, 0.498043417930603, 0.498043417930603, 0.498043417930603, 0.498043417930603, 0.498043417930603, 0.498043417930603, 0.498043417930603, 0.498043417930603, 0.498043417930603, 0.25866617262363434, 0.25866617262363434, 0.25866617262363434, 0.25866617262363434, -0.4494889974594116, -0.4494889974594116, 0.01600968837738037, 0.01600968837738037, 0.01600968837738037, 0.04903112649917607, 0.04903112649917607, 0.04903112649917607, 0.04903112649917607, 0.04903112649917607, 0.16337156196435287, 0.16337156196435287, 0.16337156196435287, 0.16337156196435287, 0.16337156196435287, 0.16337156196435287, 0.16337156196435287, 0.16337156196435287, 0.16337156196435287, 0.16337156196435287, 0.16337156196435287, 0.16337156196435287, -0.012844473123550415, -0.012844473123550415, -0.012844473123550415, -0.3330414295196533, -0.3330414295196533, 0.3876510212818781, 0.3876510212818781, 0.3876510212818781, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6510499321827383, 0.6510499321827383, 0.6510499321827383, 0.6510499321827383, 0.6510499321827383, 0.6510499321827383, 0.6510499321827383, 0.6510499321827383, 0.6510499321827383, 0.6510499321827383, 0.6510499321827383, 0.6510499321827383, 0.9034562930464745, 0.9034562930464745, -0.34452716509501147, -0.34452716509501147, -0.34452716509501147, -0.34452716509501147, 0.0, 0.0, 0.35564427077770233, 0.35564427077770233, 0.35564427077770233, 0.35564427077770233, 0.498043417930603, 0.498043417930603, 0.498043417930603, 0.2444581935803095, 0.2444581935803095, 0.2444581935803095, 0.2444581935803095, 0.16647924979527795, 0.16647924979527795, 0.16647924979527795, 0.0, 0.0, -0.042310913403828865, -0.042310913403828865, -0.042310913403828865, 0.14540301325420535, 0.14540301325420535, 0.14540301325420535, 0.14540301325420535, 0.14540301325420535, 0.14540301325420535, 0.14540301325420535, 0.14540301325420535, 0.14540301325420535, -0.49969921509424853, -0.49969921509424853, -0.49969921509424853, -1.5606911182403564, -1.5606911182403564, 0.0, 0.0, 0.1113555679718653, 0.1113555679718653, 0.1113555679718653, 0.1113555679718653, -0.16669702529907227, -0.16669702529907227, -0.005712151527404785, -0.005712151527404785, 0.5272129029035568, 0.5272129029035568, 0.5272129029035568, 0.5272129029035568, 0.0, 0.0, 0.4999319314956665, 0.4999319314956665, 0.0, 0.0, 0.1666553020477295, 0.1666553020477295, 0.2923867106437683, 0.2923867106437683, 0.27800241212050125, 0.27800241212050125, 0.27800241212050125, 0.27800241212050125, 0.27800241212050125, 0.27800241212050125, 0.40007007122039795, 0.40007007122039795, 0.5099855562051137, 0.5099855562051137, 0.5099855562051137, 0.5099855562051137, 0.3592146535714468, 0.3592146535714468, 0.3592146535714468, 0.3592146535714468, 0.3592146535714468, 0.3592146535714468, 0.3592146535714468, 0.3592146535714468, 0.38290265401204426, 0.38290265401204426, 0.38290265401204426, 0.38290265401204426, 0.38290265401204426, 0.38290265401204426, -0.11482826868693041, -0.11482826868693041, -0.11482826868693041, -0.8475130796432495, -0.8475130796432495, -0.8475130796432495, -0.2971960504849751, -0.2971960504849751, -0.2971960504849751, -0.4461019039154053, -0.4461019039154053, -0.1524625072876613, -0.1524625072876613, -0.1524625072876613, -0.1524625072876613, -0.12486577033996582, -0.12486577033996582, 0.2283582637707392, 0.2283582637707392, 0.2283582637707392, 0.2283582637707392, 0.22187862296899163, 0.22187862296899163, 0.22187862296899163, 0.22187862296899163, 0.22036535758525133, 0.22036535758525133, 0.22036535758525133, 0.22036535758525133, 0.22036535758525133, 0.22036535758525133, 0.22036535758525133, 0.22036535758525133, 0.0025116327811370853, 0.0025116327811370853, 0.0025116327811370853, 0.0025116327811370853, 0.0025116327811370853, 0.0025116327811370853, 0.0025116327811370853, 0.0025116327811370853, 0.0025116327811370853, 0.0025116327811370853, 0.0025116327811370853, -0.03248266023066315, -0.03248266023066315, -0.03248266023066315, -0.03248266023066315, -0.03248266023066315, -0.03248266023066315, -0.03248266023066315, -0.03248266023066315, -0.03248266023066315, -0.03248266023066315, 0.19852120118836558, 0.19852120118836558, 0.19852120118836558, 0.19852120118836558, 0.1666553020477295, 0.1666553020477295, 0.27745622396469116, 0.27745622396469116, -0.011032750209172493, -0.011032750209172493, -0.011032750209172493, -0.011032750209172493, 0.37665314674377437, 0.37665314674377437, 0.37665314674377437, 0.37665314674377437, 0.37665314674377437, 0.2184887031714121, 0.2184887031714121, 0.2184887031714121, 0.2184887031714121, 0.27745622396469116, 0.27745622396469116, 0.0, 0.0, 0.911143533885479, 0.911143533885479, 0.12383125101526582, 0.12383125101526582, 0.12383125101526582, 0.12383125101526582, 0.04098500683903694, 0.04098500683903694, 0.04098500683903694, 0.04098500683903694, -0.12987991298238444, -0.12987991298238444, -0.12987991298238444, -0.12987991298238444, 0.12383125101526582, 0.12383125101526582, 0.12383125101526582, 0.12383125101526582, 0.1918013865749041, 0.1918013865749041, 0.1918013865749041, -0.1271113917231559, -0.1271113917231559, -0.1271113917231559, -0.1271113917231559, -0.1271113917231559, -0.1271113917231559, -0.3296195690830548, -0.3296195690830548, -0.3296195690830548, 0.3372563540935516, 0.3372563540935516, 0.3372563540935516, 0.3372563540935516, 0.3372563540935516, 0.3372563540935516, 0.3372563540935516, 0.3372563540935516, 0.3372563540935516, 0.3372563540935516, 0.11978715658187866, 0.11978715658187866, 0.11978715658187866, -0.22306525707244873, -0.22306525707244873, -0.22306525707244873, -0.9993129968643188, -0.9993129968643188, -0.7026834487915039, -0.7026834487915039, -0.015892883141835457, -0.015892883141835457, -0.015892883141835457, 0.0, 0.0, 0.08223393559455872, 0.08223393559455872, 0.08223393559455872, 0.5396146476268768, 0.5396146476268768, 0.5396146476268768, 0.5396146476268768, -0.1438846786816914, -0.1438846786816914, -0.1438846786816914, -0.8246229887008667, -0.8246229887008667, -0.5045947233835857, -0.5045947233835857, -0.5045947233835857, 0.39841862618923185, 0.39841862618923185, 0.39841862618923185, 0.39841862618923185, 0.39841862618923185, 0.2235668500264486, 0.2235668500264486, 0.2235668500264486, 0.0834792057673136, 0.0834792057673136, 0.0834792057673136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.22778262495994572, 0.22778262495994572, 0.22778262495994572, 0.22778262495994572, 0.22778262495994572, 0.0, 0.0, 0.0, 0.0, 0.5135263875126839, 0.5135263875126839, 0.5135263875126839, 0.5135263875126839, -0.12186507383982348, -0.12186507383982348, -0.12186507383982348, 0.0, 0.0, 0.0, 0.0, 0.10370221840483806, 0.10370221840483806, 0.10370221840483806, 0.10370221840483806, 0.10370221840483806, 0.10370221840483806, 0.10370221840483806, 0.10370221840483806, -0.11421324809392286, -0.11421324809392286, -0.11421324809392286, -0.18913507461547852, -0.18913507461547852, -0.18913507461547852, 0.1530881993472576, 0.1530881993472576, 0.1530881993472576, 0.1530881993472576, 0.1530881993472576, 0.0666348934173584, 0.0666348934173584, 0.10076948354641602, 0.10076948354641602, 0.10076948354641602, 0.10076948354641602, 0.10076948354641602, 0.10076948354641602, 0.10076948354641602, 0.10076948354641602, 0.10076948354641602, 0.10076948354641602, -0.24365317821502686, -0.24365317821502686, 0.0, 0.0, -0.08905310432116198, -0.08905310432116198, -0.08905310432116198, -0.08905310432116198, -0.08905310432116198, -0.08905310432116198, -0.08905310432116198, 0.24540787935256958, 0.24540787935256958, -4.5418739318847656e-05, -4.5418739318847656e-05, -0.7777220010757446, -0.7777220010757446, -0.6368415355682373, -0.6368415355682373, -0.6368415355682373, -0.3268168228013175, -0.3268168228013175, -0.3268168228013175, -0.3268168228013175, -0.3268168228013175, -0.3268168228013175, -0.3268168228013175, -1.2000000476837158, -1.2000000476837158, 0.36496250970023014, 0.36496250970023014, 0.36496250970023014, 0.36496250970023014, 0.36496250970023014, 0.36496250970023014, 0.36496250970023014, 0.16608846187591553, 0.16608846187591553, 0.0, 0.0, 0.2610739316968691, 0.2610739316968691, 0.2610739316968691, 0.2610739316968691, 0.2610739316968691, 0.2610739316968691, 0.2610739316968691, 0.16685880800443031, 0.16685880800443031, 0.16685880800443031, 0.16685880800443031, 0.16685880800443031, 0.16685880800443031, 0.16685880800443031, 0.16685880800443031, 0.0, 0.0, 0.3784898016601801, 0.3784898016601801, 0.3784898016601801, 0.3784898016601801, 0.3784898016601801, 0.3784898016601801, 0.3784898016601801, 0.3784898016601801, -1.0357842445373535, -1.0357842445373535, 0.4073447520534198, 0.4073447520534198, 0.4073447520534198, 0.4073447520534198, 0.4073447520534198, 0.4073447520534198, 0.498043417930603, 0.498043417930603, 0.498043417930603, 0.0, 0.0, 0.0, 0.0, 0.5455297559499741, 0.5455297559499741, 0.5455297559499741, 0.5455297559499741, 0.5455297559499741, -0.041275407387209695, -0.041275407387209695, -0.041275407387209695, -0.041275407387209695, -0.041275407387209695, -0.041275407387209695, -0.041275407387209695, -0.041275407387209695, 0.34440912803014123, 0.34440912803014123, 0.34440912803014123, 0.46570232013861335, 0.46570232013861335, 0.46570232013861335, 0.46570232013861335, 0.14520519971847534, 0.14520519971847534, 0.14520519971847534, -0.005712151527404785, -0.005712151527404785, -0.39884650707244873, -0.39884650707244873, 0.0, 0.0, 0.26621402303377784, 0.26621402303377784, 0.26621402303377784, 0.22044549624507248, 0.22044549624507248, 0.22044549624507248, 0.22044549624507248, 0.22044549624507248, 0.22044549624507248, 0.22044549624507248, 0.22044549624507248, 0.22044549624507248, 0.22044549624507248, 0.22044549624507248, 0.22044549624507248, 0.22044549624507248, 0.3156913520561324, 0.3156913520561324, 0.3156913520561324, 0.3156913520561324, 0.3156913520561324, 0.3156913520561324, 0.3156913520561324, 0.3156913520561324, 0.3156913520561324, 0.3156913520561324, 0.21539697136197766, 0.21539697136197766, 0.21539697136197766, 0.21539697136197766, 0.21539697136197766, 0.21539697136197766, 0.21539697136197766, 0.21539697136197766, 0.21539697136197766, 0.21539697136197766, 0.21539697136197766, 0.21539697136197766, 0.21539697136197766, 0.21539697136197766, 0.21539697136197766, 0.1919879366954168, 0.1919879366954168, 0.1919879366954168, 0.1919879366954168, 0.32248920228864464, 0.32248920228864464, 0.32248920228864464, 0.32248920228864464, 0.32248920228864464, 0.32248920228864464, 0.32248920228864464, 0.32248920228864464, 0.0, 0.0, 0.042999545733133915, 0.042999545733133915, 0.042999545733133915, -0.01069255669911695, -0.01069255669911695, -0.01069255669911695, -0.07376716818128304, -0.07376716818128304, -0.07376716818128304, -0.07376716818128304, -0.07376716818128304, -0.07376716818128304, -0.07376716818128304, 0.027459869782129886, 0.027459869782129886, 0.027459869782129886, 0.027459869782129886, 0.027459869782129886, 0.027459869782129886, 0.027459869782129886, -0.01944160958131147, -0.01944160958131147, -0.01944160958131147, -0.01944160958131147, -0.17317150235176082, -0.17317150235176082, -0.17317150235176082, -0.17317150235176082, -0.17317150235176082, -0.3225319147109986, -0.3225319147109986, -0.3225319147109986, -0.3225319147109986, -0.3225319147109986, -0.3225319147109986, -0.3395279745260875, -0.3395279745260875, -0.3395279745260875, -0.3395279745260875, 0.6637159287929535, 0.6637159287929535, 0.44343065222104394, 0.44343065222104394, 0.44343065222104394, -0.0009820957978565747, -0.0009820957978565747, -0.0009820957978565747, 0.44343065222104394, 0.44343065222104394, 0.44343065222104394, -0.3909364938735962, -0.3909364938735962, -0.27547287940979004, -0.27547287940979004, -0.27547287940979004, -0.23723134025931358, -0.23723134025931358, -0.23723134025931358, -0.23723134025931358, -0.21430754661560059, -0.21430754661560059, -0.3909364938735962, -0.3909364938735962, -0.2696906328201294, -0.2696906328201294, -1.0002117156982422, -1.0002117156982422, -0.43492627143859863, -0.43492627143859863, -0.4447836478551228, -0.4447836478551228, -0.4447836478551228, -0.0303659588098526, -0.0303659588098526, -0.0303659588098526, -0.0303659588098526, 0.0, 0.0, -0.6636738777160645, -0.6636738777160645, -0.08102638926357031, -0.08102638926357031, -0.08102638926357031, -0.08102638926357031, 0.14707264606135362, 0.14707264606135362, 0.14707264606135362, 0.14707264606135362, 0.14707264606135362, 0.14707264606135362, 0.14707264606135362, 0.14707264606135362, 0.14707264606135362, 0.14707264606135362, 0.14707264606135362, 0.14707264606135362, -0.012460573532041996, -0.012460573532041996, -0.012460573532041996, -0.012460573532041996, -0.012460573532041996, -0.012460573532041996, -0.012460573532041996, 0.18158499682751983, 0.18158499682751983, 0.18158499682751983, 0.18158499682751983, 0.18158499682751983, 0.18158499682751983, 0.18158499682751983, 0.18158499682751983, 0.18158499682751983, 0.18158499682751983, 0.18158499682751983, 0.18158499682751983, 0.18158499682751983, 0.18158499682751983, 0.18158499682751983, 0.18158499682751983, 0.18158499682751983, 0.18158499682751983, 0.18158499682751983, 0.18158499682751983, 0.18158499682751983, 0.18158499682751983, 0.18158499682751983, 0.18158499682751983, 0.18158499682751983, 0.18158499682751983, 0.0, 0.0, 0.1946680943171183, 0.1946680943171183, 0.1946680943171183, 0.0017148256301879883, 0.0017148256301879883, 0.19185594717661536, 0.19185594717661536, 0.19185594717661536, 0.41503509481747947, 0.41503509481747947, 0.41503509481747947, 0.41503509481747947, 0.41503509481747947, 0.41503509481747947, 0.0, 0.0, -0.6636738777160645, -0.6636738777160645, 0.6636858880519867, 0.6636858880519867, 0.6636858880519867, 0.6636858880519867, 0.6636858880519867, 0.6636858880519867, 0.6636858880519867, -0.1296306848526001, -0.1296306848526001, -0.1296306848526001, 0.0, 0.0, 0.13491014851765204, 0.13491014851765204, 0.13491014851765204, 0.13491014851765204, 0.13491014851765204, 0.13491014851765204, 0.13491014851765204, 0.13491014851765204, 0.13491014851765204, 0.13491014851765204, 0.13491014851765204, 0.32299052054683364, 0.32299052054683364, 0.32299052054683364, 0.32299052054683364, -0.3656140764554341, -0.3656140764554341, -0.3656140764554341, -0.0025513156184129304, -0.0025513156184129304, -0.0025513156184129304, -0.0025513156184129304, -0.0025513156184129304, -0.0025513156184129304, -0.0025513156184129304, -0.0025513156184129304, -0.12215669949849439, -0.12215669949849439, -0.12215669949849439, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.30618522564570116, -0.30618522564570116, -0.30618522564570116, 0.0, 0.0, 0.3327399094899496, 0.3327399094899496, 0.3327399094899496, 0.34423025945822394, 0.34423025945822394, 0.34423025945822394, -0.1771967276930808, -0.1771967276930808, -0.1771967276930808, -0.1771967276930808, -0.1771967276930808, 0.34423025945822394, 0.34423025945822394, 0.34423025945822394, -0.09883768714609609, -0.09883768714609609, -0.09883768714609609, -0.09883768714609609, -0.09883768714609609, -0.09883768714609609, -0.09883768714609609, -0.1534710391646339, -0.1534710391646339, -0.1534710391646339, -0.1534710391646339, -0.1534710391646339, -0.1534710391646339, -0.1534710391646339, -0.29827439934015265, -0.29827439934015265, -0.29827439934015265, -0.29827439934015265, -0.29827439934015265, -0.29827439934015265, 0.3332829078038534, 0.3332829078038534, 0.3332829078038534, 0.498043417930603, 0.498043417930603, 0.498043417930603, 0.0, 0.0, 0.0, 0.0, 0.3525061051050822, 0.3525061051050822, 0.3525061051050822, 0.3525061051050822, 0.3525061051050822, 0.3525061051050822, -0.16162950595219927, -0.16162950595219927, -0.16162950595219927, -0.16162950595219927, -0.16162950595219927, -0.16162950595219927, 0.8948440700769424, 0.8948440700769424, 0.8948440700769424, 0.8948440700769424, -0.4382191672921181, -0.4382191672921181, -0.4382191672921181, -0.4382191672921181, 0.8948440700769424, 0.8948440700769424, 0.15836866199970245, 0.15836866199970245, 0.15836866199970245, 0.15836866199970245, 0.15836866199970245, 0.09827193121115363, 0.09827193121115363, 0.09827193121115363, 0.27775202194849646, 0.27775202194849646, 0.27775202194849646, 0.0, 0.0, -0.14795572757720943, -0.14795572757720943, -0.14795572757720943, -0.14795572757720943, -0.14795572757720943, -0.16506421566009521, -0.16506421566009521, -0.5303727785746257, -0.5303727785746257, -0.5303727785746257, -0.5303727785746257, 0.0, 0.0, 0.6636858880519867, 0.6636858880519867, 0.13088619212309516, 0.13088619212309516, 0.13088619212309516, 0.13088619212309516, 0.0, 0.0, 0.0, 0.0, -0.2881140410900116, -0.2881140410900116, -0.2881140410900116, 0.12506664295991266, 0.12506664295991266, 0.12506664295991266, 0.12506664295991266, 0.498043417930603, 0.498043417930603, 0.498043417930603, -0.354164719581604, -0.354164719581604, -0.15290536483128858, -0.15290536483128858, -0.15290536483128858, 0.6636858880519867, 0.6636858880519867, 0.6636858880519867, 0.6636858880519867, 0.7958751916885376, 0.7958751916885376, 0.7958751916885376, 0.7958751916885376, 0.46595802903175354, 0.46595802903175354, 0.46595802903175354, 0.46595802903175354, 0.6636858880519867, 0.6636858880519867, 0.498043417930603, 0.498043417930603, 0.498043417930603, 0.0, 0.0, 0.17545550664265952, 0.17545550664265952, 0.17545550664265952, 0.17545550664265952, 0.17545550664265952, 0.17545550664265952, 0.17545550664265952, 0.17545550664265952, 0.17545550664265952, 0.17545550664265952, 0.17545550664265952, 0.17545550664265952, 0.12922277922431624, 0.12922277922431624, 0.12922277922431624, 0.12922277922431624, 0.22664109414274047, 0.22664109414274047, 0.22664109414274047, 0.22664109414274047, 0.22664109414274047, 0.22664109414274047, 0.22664109414274047, 0.22664109414274047, 0.22664109414274047, 0.22664109414274047, 0.22664109414274047, 0.10662712901830673, 0.10662712901830673, 0.10662712901830673, 0.10662712901830673, 0.10662712901830673, 0.10662712901830673, 0.10662712901830673, 0.10662712901830673, 0.10662712901830673, -0.09295591115951529, -0.09295591115951529, -0.09295591115951529, -0.09295591115951529, -0.09295591115951529, -0.09295591115951529, 0.0774736603101095, 0.0774736603101095, 0.0774736603101095, 0.0774736603101095, 0.03851919101946277, 0.03851919101946277, 0.03851919101946277, 0.03851919101946277, 0.03851919101946277, 0.03851919101946277, 0.03851919101946277, 0.03851919101946277, 0.03851919101946277, 0.03851919101946277, 0.03851919101946277, 0.03851919101946277, 0.23672435515456725, 0.23672435515456725, 0.23672435515456725, 0.23672435515456725, 0.23672435515456725, 0.23672435515456725, 0.23672435515456725, 0.23672435515456725, 0.23672435515456725, 0.23672435515456725, 0.23672435515456725, 0.23672435515456725, 0.23672435515456725, 0.23672435515456725, 0.23672435515456725, 0.23672435515456725, 0.23672435515456725, 0.23672435515456725, 0.040572496397154656, 0.040572496397154656, 0.040572496397154656, 0.040572496397154656, 0.040572496397154656, 0.040572496397154656, 0.040572496397154656, 0.040572496397154656, 0.0, 0.0, 0.5099885563055675, 0.5099885563055675, 0.5099885563055675, 0.5099885563055675, 0.39864495396614075, 0.39864495396614075, 0.39864495396614075, 0.39864495396614075, 0.39864495396614075, 0.39864495396614075, 0.39864495396614075, 0.39864495396614075, 0.39864495396614075, 0.795965164899826, 0.795965164899826, 0.10904856026172638, 0.10904856026172638, 0.10904856026172638, 0.10904856026172638, 0.5099206070105236, 0.5099206070105236, 0.5099206070105236, 0.5099206070105236, 0.0, 0.0, -0.4708514610926311, -0.4708514610926311, -0.4708514610926311, -0.2489323616027832, -0.2489323616027832, 0.1770964662233988, 0.1770964662233988, 0.1770964662233988, -0.03396737575531006, -0.03396737575531006, -0.03396737575531006, 0.06993178427219393, 0.06993178427219393, 0.06993178427219393, 0.06993178427219393, 0.06993178427219393, 0.5888954276839893, 0.5888954276839893, 0.5888954276839893, 0.5888954276839893, 0.5888954276839893, 0.5888954276839893, 0.10586685687303543, 0.10586685687303543, 0.10586685687303543, 0.0, 0.0, -0.1070854589343071, -0.1070854589343071, -0.1070854589343071, -0.1070854589343071, -0.1070854589343071, -0.1070854589343071, -0.1070854589343071, -0.1070854589343071, -0.1070854589343071, -0.1070854589343071, -0.1070854589343071, 0.0, 0.0, -0.31231405337651563, -0.31231405337651563, -0.31231405337651563, 0.27750174701213837, 0.27750174701213837, 0.27750174701213837, 0.27750174701213837, -0.5963033810257912, -0.5963033810257912, -0.5963033810257912, -0.41852577279011416, -0.41852577279011416, -0.41852577279011416, -0.0015592570106188752, -0.0015592570106188752, -0.0015592570106188752, -0.0015592570106188752, -0.0015592570106188752, -0.0015592570106188752, 0.16306850349619273, 0.16306850349619273, 0.16306850349619273, 0.16306850349619273, 0.16306850349619273, 0.16306850349619273, 0.16306850349619273, -0.4777550458908082, -0.4777550458908082, -0.4777550458908082, -0.4777550458908082, -0.4777550458908082, -0.03194506764411931, -0.03194506764411931, -0.03194506764411931, -0.03194506764411931, -0.03194506764411931, -0.03194506764411931, 0.14223094284534454, 0.14223094284534454, 0.14223094284534454, 0.14223094284534454, 0.14223094284534454, 0.4980417645225922, 0.4980417645225922, 0.4980417645225922, 0.4980417645225922, 0.4980417645225922, 0.4980417645225922, 0.4980417645225922, 0.4980417645225922, 0.4980417645225922, 0.4708006729682287, 0.4708006729682287, 0.4708006729682287, 0.4708006729682287, 0.2137589991092682, 0.2137589991092682, 0.2137589991092682, 0.2137589991092682, 0.2137589991092682, 0.2137589991092682, 0.0, 0.0, 0.17496976057688396, 0.17496976057688396, 0.17496976057688396, 0.17496976057688396, 0.17496976057688396, 0.17496976057688396, 0.16641474962234493, 0.16641474962234493, 0.16641474962234493, 0.16641474962234493, 0.16641474962234493, 0.0030807018280029075, 0.0030807018280029075, 0.0030807018280029075, 0.0030807018280029075, 0.0030807018280029075, 0.4489760577678681, 0.4489760577678681, 0.4489760577678681, 0.4489760577678681, 0.4489760577678681, -0.011344437797864204, -0.011344437797864204, -0.011344437797864204, -0.011344437797864204, 0.08196930090586341, 0.08196930090586341, 0.08196930090586341, 0.3865304092566172, 0.3865304092566172, 0.3865304092566172, 0.3865304092566172, 0.0, 0.0, -0.8181803226470947, -0.8181803226470947, 0.0, 0.0, 0.19049204587936397, 0.19049204587936397, 0.19049204587936397, 0.19049204587936397, 0.19049204587936397, -0.39226960142453504, -0.39226960142453504, -0.39226960142453504, -0.39226960142453504, -1.423076868057251, -1.423076868057251, -0.41385507583618164, -0.41385507583618164, -0.41385507583618164, 0.498043417930603, 0.498043417930603, 0.05489734808603919, 0.05489734808603919, 0.05489734808603919, -0.5827154119809468, -0.5827154119809468, -0.5827154119809468, -0.5827154119809468, 0.0, 0.0, -0.14510884881019592, -0.14510884881019592, -0.14510884881019592, -0.14510884881019592, 0.6888715028762817, 0.6888715028762817, 0.4869115948677063, 0.4869115948677063, 0.0, 0.0, 0.0, 0.0, 0.3444699148337046, 0.3444699148337046, 0.3444699148337046, 0.0, 0.0, 0.14422957971692085, 0.14422957971692085, 0.14422957971692085, 0.14422957971692085, 0.14422957971692085, 0.14422957971692085, 0.14422957971692085, 0.14422957971692085, 0.14422957971692085, 0.3332829078038534, 0.3332829078038534, 0.3332829078038534, 0.3332829078038534, 0.3332829078038534, 0.3332829078038534, 0.0, 0.0, -1.0050462086995444, -1.0050462086995444, -1.0050462086995444, -0.4689839522043864, -0.4689839522043864, -0.4689839522043864, -0.4689839522043864, -0.4689839522043864, -0.4689839522043864, -0.6476667722066243, -0.6476667722066243, -0.6476667722066243, -0.39445924758911133, -0.39445924758911133, -0.2999201019605, -0.2999201019605, -0.2999201019605, -0.2999201019605, -0.629088004430135, -0.629088004430135, -0.629088004430135, 0.1511185184121132, 0.1511185184121132, 0.1511185184121132, 0.1511185184121132, 0.1511185184121132, 0.26407432059446967, 0.26407432059446967, 0.26407432059446967, 0.08472672601540887, 0.08472672601540887, 0.08472672601540887, -0.49714070558547974, -0.49714070558547974, -0.49714070558547974, -0.49714070558547974, -0.49714070558547974, -0.0827820618947348, -0.0827820618947348, -0.0827820618947348, -0.0827820618947348, -0.0827820618947348, -0.0827820618947348, -0.2593158483505249, -0.2593158483505249, -0.2593158483505249, -0.15925425887107858, -0.15925425887107858, -0.15925425887107858, -0.15925425887107858, -0.15925425887107858, -0.17214882373809814, -0.17214882373809814, 0.19540365537007653, 0.19540365537007653, 0.19540365537007653, 0.33347443739573157, 0.33347443739573157, 0.33347443739573157, 0.0, 0.0, -0.16568541526794434, -0.16568541526794434, -0.01819433768590284, -0.01819433768590284, -0.01819433768590284, 0.38821653525034583, 0.38821653525034583, 0.38821653525034583, 0.10598201304674149, 0.10598201304674149, 0.10598201304674149, 0.1319944582879543, 0.1319944582879543, 0.1319944582879543, 0.1319944582879543, 0.1319944582879543, 0.0, 0.0, 0.0, 0.0, -0.4999969005584717, -0.4999969005584717, 0.0, 0.0, 0.0, 0.0, 0.4656998018423716, 0.4656998018423716, 0.4656998018423716, 0.4656998018423716, 0.0, 0.0, 0.33347443739573157, 0.33347443739573157, 0.33347443739573157, 0.2511899471282959, 0.2511899471282959, 0.0, 0.0, 0.0, 0.0, -0.16674339771270752, -0.16674339771270752, 0.15091089407602942, 0.15091089407602942, 0.15091089407602942, -0.18305539942923055, -0.18305539942923055, -0.18305539942923055, -0.18305539942923055, -0.18305539942923055, -0.18305539942923055, -0.18305539942923055, 0.6232742071151733, 0.6232742071151733, 0.6232742071151733, 0.6232742071151733, 0.6232742071151733, 0.6232742071151733, 0.6232742071151733, 0.6232742071151733, -0.33101630210876465, -0.33101630210876465, 0.0, 0.0, -0.021245698134104485, -0.021245698134104485, -0.021245698134104485, -0.12496338287989306, -0.12496338287989306, -0.12496338287989306, 0.10907578840851784, 0.10907578840851784, 0.10907578840851784, 0.10907578840851784, 0.498043417930603, 0.498043417930603, 0.498043417930603, 0.498043417930603, -0.09039676189422607, -0.09039676189422607, -0.09039676189422607, 0.498043417930603, 0.498043417930603, 0.15091089407602942, 0.15091089407602942, 0.15091089407602942, -0.29585071404774976, -0.29585071404774976, -0.29585071404774976, 0.5106553783019383, 0.5106553783019383, 0.5106553783019383, 0.5106553783019383, 0.34996217489242554, 0.34996217489242554, 0.34996217489242554, 0.0, 0.0, -0.34626968701680494, -0.34626968701680494, -0.34626968701680494, -0.34626968701680494, 0.38767103354136145, 0.38767103354136145, 0.38767103354136145, 0.0, 0.0, 0.8295824229717255, 0.8295824229717255, 0.2092852266061873, 0.2092852266061873, 0.2092852266061873, 0.2092852266061873, 0.2092852266061873, 0.2092852266061873, 0.2092852266061873, -0.3800077438354492, -0.3800077438354492, -0.3800077438354492, 0.0, 0.0, 0.059215179085731484, 0.059215179085731484, 0.059215179085731484, 0.059215179085731484, 0.059215179085731484, 0.41555181642373407, 0.41555181642373407, 0.41555181642373407, 0.41555181642373407, 0.0, 0.0, 0.8295824229717255, 0.8295824229717255, 0.0, 0.0, -1.1428608894348145, -1.1428608894348145, 0.0, 0.0, 0.0, 0.0, 0.27348151604334514, 0.27348151604334514, 0.27348151604334514, 0.27348151604334514, 0.27348151604334514, 0.27348151604334514, 0.0, 0.0, 0.0, 0.0, 0.38821653525034583, 0.38821653525034583, 0.38821653525034583, 0.0, 0.0, 0.498043417930603, 0.498043417930603, 0.498043417930603, -0.1219818194707234, -0.1219818194707234, -0.1219818194707234, -0.1219818194707234, -0.1841246366500855, -0.1841246366500855, -0.1841246366500855, -0.1841246366500855, -0.1841246366500855, -0.5124844312667847, -0.5124844312667847, 0.39864495396614075, 0.39864495396614075, 0.39864495396614075, 0.39864495396614075, 0.39864495396614075, 0.39864495396614075, 0.02379322052001953, 0.02379322052001953, 0.02379322052001953, 0.23328924179077148, 0.23328924179077148, 0.23328924179077148, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11084549427032475, 0.11084549427032475, 0.11084549427032475, 0.11084549427032475, 0.11084549427032475, 0.0, 0.0, -1.0998635292053223, -1.0998635292053223, 0.498043417930603, 0.498043417930603, 0.498043417930603, 0.498043417930603, 0.498043417930603, 0.498043417930603, 0.8839641883969307, 0.8839641883969307, 0.0366704948246479, 0.0366704948246479, 0.0366704948246479, 0.0366704948246479, 0.0366704948246479, 0.8839641883969307, 0.8839641883969307, -0.05337665230035782, -0.05337665230035782, -0.05337665230035782, -0.05337665230035782, -0.05337665230035782, 0.28387630730867386, 0.28387630730867386, 0.28387630730867386, 0.28387630730867386, 0.8704280108213425, 0.8704280108213425, -0.10778933018445969, -0.10778933018445969, -0.10778933018445969, -0.10778933018445969, 0.0, 0.0, 0.21557901302973426, 0.21557901302973426, 0.21557901302973426, 0.5993889811493102, 0.5993889811493102, 0.5993889811493102, 0.5993889811493102, 0.5993889811493102, 0.5993889811493102, 0.5993889811493102, 0.5993889811493102, 0.5993889811493102, 0.5993889811493102, 0.5993889811493102, 0.5993889811493102, 0.5993889811493102, 0.5993889811493102, -0.20794488986333204, -0.20794488986333204, -0.20794488986333204, -0.6236745913823445, -0.6236745913823445, -0.6236745913823445, -0.6236745913823445, -0.720775802930196, -0.720775802930196, -0.720775802930196, 0.04902544617652893, 0.04902544617652893, 0.04902544617652893, 0.04902544617652893, 0.033529917399088505, 0.033529917399088505, 0.033529917399088505, 0.0557589282592138, 0.0557589282592138, 0.0557589282592138, 0.0557589282592138, -1.2000000476837158, -1.2000000476837158, 0.0003002484639486047, 0.0003002484639486047, 0.0003002484639486047, -0.8685761292775471, -0.8685761292775471, -0.8685761292775471, -0.762980043888092, -0.762980043888092, -0.762980043888092, -0.762980043888092, 0.3320053815841675, 0.3320053815841675, 0.3320053815841675, -0.41318583488464355, -0.41318583488464355, -0.41318583488464355, -0.24999117851257324, -0.24999117851257324, -0.13388244807720184, -0.13388244807720184, -0.13388244807720184, -0.13388244807720184, -0.13388244807720184, -0.13388244807720184, 0.2443996022144953, 0.2443996022144953, 0.2443996022144953, 0.2443996022144953, 0.0, 0.0, 0.09090405702590942, 0.09090405702590942, 0.498043417930603, 0.498043417930603, 0.498043417930603, -0.10671381155649828, -0.10671381155649828, -0.10671381155649828, -0.10671381155649828, -0.10671381155649828, -0.10671381155649828, -0.10671381155649828, -0.10671381155649828, -0.10671381155649828, 0.21671589364608124, 0.21671589364608124, 0.21671589364608124, 0.21671589364608124, 0.21671589364608124, 0.21671589364608124, 0.21671589364608124, 0.21671589364608124, 0.21671589364608124, 0.21671589364608124, 0.5462569952011108, 0.5462569952011108, 0.5462569952011108, 0.5462569952011108, 0.5462569952011108, 0.5462569952011108, -0.2015847464402516, -0.2015847464402516, -0.2015847464402516, -0.2015847464402516, 0.41378533840179443, 0.41378533840179443, 0.41378533840179443, 0.017557157824436787, 0.017557157824436787, 0.017557157824436787, 0.017557157824436787, 0.15153199434280396, 0.15153199434280396, 0.05490880310535429, 0.05490880310535429, 0.05490880310535429, 0.05490880310535429, 0.05490880310535429, -0.035348355770111084, -0.035348355770111084, -0.035348355770111084, -0.035348355770111084, -0.035348355770111084, 0.14073745210965471, 0.14073745210965471, 0.14073745210965471, 0.14073745210965471, 0.14073745210965471, 0.14073745210965471, 0.0, 0.0, 0.27064045766989386, 0.27064045766989386, 0.27064045766989386, 0.27064045766989386, 0.27064045766989386, 0.27064045766989386, 0.22493805228309194, 0.22493805228309194, 0.22493805228309194, 0.22493805228309194, 0.22493805228309194, 0.22493805228309194, 0.22493805228309194, 0.22493805228309194, 0.22493805228309194, 0.22493805228309194, 0.22493805228309194, 0.22493805228309194, 0.33347443739573157, 0.33347443739573157, 0.33347443739573157, 0.36107329527537024, 0.36107329527537024, 0.36107329527537024, 0.0, 0.0, 0.5818089097738266, 0.5818089097738266, 0.5818089097738266, 0.5818089097738266, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.28283961613972974, -0.28283961613972974, -0.28283961613972974, 0.0, 0.0, -0.6666755676269531, -0.6666755676269531, 0.0, 0.0, -0.5443293650945027, -0.5443293650945027, -0.5443293650945027, 0.0, 0.0, -0.05443274974822998, -0.05443274974822998, -0.05443274974822998, -1.099998950958252, -1.099998950958252, 0.0, 0.0, 0.08710297321279847, 0.08710297321279847, 0.08710297321279847, 0.08710297321279847, 0.9369054660201073, 0.9369054660201073, -0.06863531718651461, -0.06863531718651461, -0.06863531718651461, 0.0, 0.0, 0.0, 0.0, 0.06712479889392853, 0.06712479889392853, 0.06712479889392853, 0.06712479889392853, 0.05258041904086164, 0.05258041904086164, 0.05258041904086164, 0.05258041904086164, 0.05258041904086164, 0.05258041904086164, 0.05258041904086164, 0.11214340229829156, 0.11214340229829156, 0.11214340229829156, 0.11214340229829156, 0.07798381646474206, 0.07798381646474206, 0.07798381646474206, 0.07798381646474206, -0.15080853303273511, -0.15080853303273511, -0.15080853303273511, -0.133326358265347, -0.133326358265347, -0.133326358265347, -0.133326358265347, -0.133326358265347, -0.133326358265347, -0.133326358265347, -0.133326358265347, -0.133326358265347, 0.0, 0.0, -0.54581618309021, -0.54581618309021, -0.54581618309021, -0.54581618309021, -0.54581618309021, -0.3844738006591797, -0.3844738006591797, -0.3844738006591797, -0.056206901868184334, -0.056206901868184334, -0.056206901868184334, -0.33513307571411133, -0.33513307571411133, -0.33513307571411133, -0.6754007339477539, -0.6754007339477539, -0.523209810256958, -0.523209810256958, -1.7529244422912598, -1.7529244422912598, 0.32383293136954305, 0.32383293136954305, 0.32383293136954305, 0.32383293136954305, 0.32383293136954305, -0.24220991283655158, -0.24220991283655158, -0.24220991283655158, -0.24220991283655158, -0.24220991283655158, -0.287446142733097, -0.287446142733097, -0.287446142733097, -0.287446142733097, -0.287446142733097, -0.1466114645202954, -0.1466114645202954, -0.1466114645202954, -0.1466114645202954, -0.2414625883102417, -0.2414625883102417, -0.2414625883102417, -0.23850576082865405, -0.23850576082865405, -0.23850576082865405, -0.03976774215698242, -0.03976774215698242, 0.11839894453684485, 0.11839894453684485, 0.11839894453684485, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.26365792751312256, -0.26365792751312256, -0.2667145604888599, -0.2667145604888599, -0.2667145604888599, 0.07789586981137597, 0.07789586981137597, 0.07789586981137597, 0.07789586981137597, -0.49999845027923584, -0.49999845027923584, -0.7666428089141846, -0.7666428089141846, -0.06596994400024414, -0.06596994400024414, -0.06596994400024414, -0.06596994400024414, 0.20590575287739432, 0.20590575287739432, 0.20590575287739432, 0.20590575287739432, 0.20590575287739432, 0.20590575287739432, 0.20590575287739432, 0.43250752935806913, 0.43250752935806913, 0.43250752935806913, 0.43250752935806913, 0.43250752935806913, 0.43250752935806913, 0.9287522211670876, 0.9287522211670876, 0.2209533664087454, 0.2209533664087454, 0.2209533664087454, 0.2209533664087454, 0.35402617106835044, 0.35402617106835044, 0.35402617106835044, 0.9287522211670876, 0.9287522211670876, -0.29479812333981203, -0.29479812333981203, -0.29479812333981203, 0.26513108859459555, 0.26513108859459555, 0.26513108859459555, -0.665665864944458, -0.665665864944458, 0.0, 0.0, 0.16642908255259192, 0.16642908255259192, 0.16642908255259192, -0.11263505667448048, -0.11263505667448048, -0.11263505667448048, -0.11263505667448048, -0.11263505667448048, -0.25288225263357167, -0.25288225263357167, -0.25288225263357167, -0.25288225263357167, -0.25288225263357167, 0.19256286770105357, 0.19256286770105357, 0.19256286770105357, 0.19256286770105357, 0.19256286770105357, -0.5510496695836384, -0.5510496695836384, -0.5510496695836384, 0.0, 0.0, 0.23328924179077148, 0.23328924179077148, 0.23328924179077148, -0.2663078308105469, -0.2663078308105469, -0.494739294052124, -0.494739294052124, -0.06395301932380315, -0.06395301932380315, -0.06395301932380315, -0.06395301932380315, -0.06395301932380315, -0.06395301932380315, -0.06395301932380315, 0.0279100239276886, 0.0279100239276886, 0.0279100239276886, 0.0279100239276886, 0.0, 0.0, -0.07931273778279624, -0.07931273778279624, -0.07931273778279624, -0.07931273778279624, -0.07931273778279624, -0.07931273778279624, -0.13620619404883616, -0.13620619404883616, -0.13620619404883616, -0.13620619404883616, -0.13620619404883616, -0.13620619404883616, -0.13620619404883616, 0.498043417930603, 0.498043417930603, 0.498043417930603, 0.498043417930603, 0.498043417930603, 0.498043417930603, 0.498043417930603, 0.0, 0.0, -0.8058673143386841, -0.8058673143386841, -0.6851334571838379, -0.6851334571838379, -0.8058673143386841, -0.8058673143386841, 0.0, 0.0, 0.13465185463428497, 0.13465185463428497, 0.13465185463428497, 0.13465185463428497, 0.19466730455557502, 0.19466730455557502, 0.19466730455557502, 0.19466730455557502, 0.19998186826705933, 0.19998186826705933, -0.5925333499908447, -0.5925333499908447, 0.498043417930603, 0.498043417930603, 0.498043417930603, 0.498043417930603, 0.498043417930603, 0.498043417930603, 0.498043417930603, 0.1841652890046438, 0.1841652890046438, 0.1841652890046438, 0.1841652890046438, 0.1841652890046438, 0.1841652890046438, 0.1841652890046438, -0.14092019626072472, -0.14092019626072472, -0.14092019626072472, -0.14092019626072472, -0.14092019626072472, -0.14092019626072472, -0.14092019626072472, 0.0, 0.0, 0.06052583456039429, 0.06052583456039429, -0.11615405480066943, -0.11615405480066943, -0.11615405480066943, -0.23248760700225835, -0.23248760700225835, -0.23248760700225835, -0.23248760700225835, -0.23248760700225835, 0.2114598353703817, 0.2114598353703817, 0.2114598353703817, 0.2114598353703817, -0.104038159052531, -0.104038159052531, -0.104038159052531, -0.17747513453165698, -0.17747513453165698, -0.17747513453165698, -0.05004841486612954, -0.05004841486612954, -0.05004841486612954, -0.05004841486612954, -0.05004841486612954, -0.05004841486612954, 0.13356239332093134, 0.13356239332093134, 0.13356239332093134, 0.13356239332093134, 0.13356239332093134, 0.13356239332093134, 0.13356239332093134, 0.13356239332093134, 0.13356239332093134, 0.13356239332093134, 0.0, 0.0, 0.060802459716796875, 0.060802459716796875, -0.15362044175465894, -0.15362044175465894, -0.15362044175465894, 0.0, 0.0, 0.13538181285063422, 0.13538181285063422, 0.13538181285063422, 0.13538181285063422, -0.2249186515808106, -0.2249186515808106, -0.2249186515808106, -0.2249186515808106, -0.2249186515808106, 0.8295824229717255, 0.8295824229717255, 0.07426540553569794, 0.07426540553569794, 0.07426540553569794, 0.07426540553569794, -1.423996925354004, -1.423996925354004, -0.22757268448670698, -0.22757268448670698, -0.22757268448670698, -0.22757268448670698, 0.4999045133590698, 0.4999045133590698, 0.22702819108963013, 0.22702819108963013, 0.22702819108963013, 0.22702819108963013, 0.22702819108963013, 0.08267402648925781, 0.08267402648925781, 0.08267402648925781, 0.0, 0.0, -0.5057243506113689, -0.5057243506113689, -0.5057243506113689, -0.5057243506113689, -0.29325583577156067, -0.29325583577156067, -0.29325583577156067, -0.29325583577156067, -0.29325583577156067, -0.007467802365620946, -0.007467802365620946, -0.007467802365620946, -0.007467802365620946, -0.007467802365620946, -0.007467802365620946, 0.7958751916885376, 0.7958751916885376, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15242309123277664, 0.15242309123277664, 0.15242309123277664, 0.15242309123277664, 0.15242309123277664, 0.15242309123277664, 0.15242309123277664, 0.15242309123277664, 0.15242309123277664, 0.15242309123277664, 0.16285993478127891, 0.16285993478127891, 0.16285993478127891, 0.16285993478127891, 0.16285993478127891, 0.16285993478127891, 0.16285993478127891, 0.16285993478127891, 0.2688808898131053, 0.2688808898131053, 0.2688808898131053, 0.2688808898131053, 0.2688808898131053, 0.2688808898131053, 0.2531284069021543, 0.2531284069021543, 0.2531284069021543, 0.0, 0.0, 0.3896669546763102, 0.3896669546763102, 0.3896669546763102, 0.4886441642329806, 0.4886441642329806, 0.4886441642329806, 0.4886441642329806, 0.4886441642329806, 0.4886441642329806, 0.4886441642329806, 0.46109387278556824, 0.46109387278556824, 0.46109387278556824, 0.46109387278556824, 0.46109387278556824, 0.46109387278556824, 0.29919731616973877, 0.29919731616973877, 0.2877986431121826, 0.2877986431121826, 0.2877986431121826, 0.2877986431121826, 0.29919731616973877, 0.29919731616973877, 0.0, 0.0, 0.16244568427403772, 0.16244568427403772, 0.16244568427403772, 0.43629830777645107, 0.43629830777645107, 0.43629830777645107, 0.43629830777645107, 0.43629830777645107, 0.39186679323514306, 0.39186679323514306, 0.39186679323514306, 0.39186679323514306, 0.498043417930603, 0.498043417930603, 0.498043417930603, 0.7527552445729574, 0.7527552445729574, 0.7527552445729574, 0.7527552445729574, 0.7527552445729574, 0.7527552445729574, 0.7958751916885376, 0.7958751916885376, 0.7958751916885376, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.6030125617980957, -0.6030125617980957, -0.6030125617980957, -0.3435824314753215, -0.3435824314753215, -0.3435824314753215, -0.3435824314753215, -0.545198639233907, -0.545198639233907, -0.545198639233907, -0.6490781307220459, -0.6490781307220459, 0.498043417930603, 0.498043417930603, 0.498043417930603, 0.0, 0.0, 0.05489734808603919, 0.05489734808603919, 0.05489734808603919, 0.0, 0.0, 0.048094719648361206, 0.048094719648361206, 0.048094719648361206, 0.048094719648361206, 0.048094719648361206, 0.048094719648361206, 0.048094719648361206, 0.048094719648361206, 0.048094719648361206, 0.048094719648361206, 0.6145440846681595, 0.6145440846681595, 0.6145440846681595, 0.6145440846681595, 0.6145440846681595, -0.9999966621398926, -0.9999966621398926, -0.22296973069508863, -0.22296973069508863, -0.22296973069508863, -0.665665864944458, -0.665665864944458, -0.22212421894073486, -0.22212421894073486, 0.7958751916885376, 0.7958751916885376, 0.7958751916885376, 0.15448691844940188, 0.15448691844940188, 0.15448691844940188, 0.15448691844940188, 0.15448691844940188, 0.05530595779418945, 0.05530595779418945, -0.41509600480397535, -0.41509600480397535, -0.41509600480397535, 0.6636858880519867, 0.6636858880519867, 0.6636858880519867, 0.6636858880519867, 0.0, 0.0, -1.3333323001861572, -1.3333323001861572, 0.1658993512392044, 0.1658993512392044, 0.1658993512392044, 0.1658993512392044, -0.668383816878001, -0.668383816878001, -0.668383816878001, -0.5714231729507446, -0.5714231729507446, -0.21161150932312012, -0.21161150932312012, 0.0, 0.0, 0.0, 0.0, 0.19998186826705933, 0.19998186826705933, 0.6636858880519867, 0.6636858880519867, 0.6636858880519867, 0.6636858880519867, 0.6636858880519867, 0.6636858880519867, 0.0, 0.0, 0.498043417930603, 0.498043417930603, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.06731975078582764, -0.06731975078582764, -0.06731975078582764, 0.6063895225524902, 0.6063895225524902, 0.6063895225524902, 0.6063895225524902, 0.6063895225524902, 0.6063895225524902, 0.0, 0.0, 0.49796783924102783, 0.49796783924102783, 0.49796783924102783, 0.49796783924102783, 0.49796783924102783, 0.0, 0.0, -0.4108901023864746, -0.4108901023864746, -0.4108901023864746, -0.4108901023864746, 0.041147371133168575, 0.041147371133168575, 0.041147371133168575, 0.041147371133168575, -1.5, -1.5, -0.2600871523221333, -0.2600871523221333, -0.2600871523221333, -0.4300769567489624, -0.4300769567489624, -0.4300769567489624, 0.0, 0.0, -0.22637557983398438, -0.22637557983398438, -0.6951000293095906, -0.6951000293095906, -0.6951000293095906, 0.0, 0.0, 0.0, 0.0, -0.09711321194966627, -0.09711321194966627, -0.09711321194966627, 0.14835643768310547, 0.14835643768310547, 0.14835643768310547, 0.1595190564791361, 0.1595190564791361, 0.1595190564791361, 0.1595190564791361, 0.020692342519760154, 0.020692342519760154, 0.020692342519760154, 0.020692342519760154, 0.020692342519760154, -0.2221635182698567, -0.2221635182698567, -0.2221635182698567, -0.8181737661361694, -0.8181737661361694, -0.3042761484781902, -0.3042761484781902, -0.3042761484781902, -0.11365022261937452, -0.11365022261937452, -0.11365022261937452, 0.0, 0.0, -0.15362044175465894, -0.15362044175465894, -0.15362044175465894, 0.056376059850057, 0.056376059850057, 0.056376059850057, 0.09560028711954749, 0.09560028711954749, 0.09560028711954749, 0.09560028711954749, 0.09560028711954749, 0.09560028711954749, 0.09560028711954749, 0.09560028711954749, 0.09560028711954749, 0.09560028711954749, 0.09560028711954749, 0.09560028711954749, 0.09560028711954749, 0.09560028711954749, 0.09560028711954749, 0.09560028711954749, 0.09560028711954749, 0.09560028711954749, -0.06675465901692701, -0.06675465901692701, -0.06675465901692701, -0.06675465901692701, 0.16368413964907325, 0.16368413964907325, 0.16368413964907325, 0.16368413964907325, 0.3096126317977905, 0.3096126317977905, 0.3096126317977905, 0.17305094003677368, 0.17305094003677368, 0.06649927496910091, 0.06649927496910091, 0.06649927496910091, 0.06649927496910091, 0.06649927496910091, 0.17305094003677368, 0.17305094003677368, 0.002986502647399858, 0.002986502647399858, 0.002986502647399858, 0.002986502647399858, 0.002986502647399858, 0.002986502647399858, -0.0069361130396525805, -0.0069361130396525805, -0.0069361130396525805, -0.0069361130396525805, -0.0069361130396525805, -0.0069361130396525805, -0.0069361130396525805, 0.11905629932880402, 0.11905629932880402, 0.11905629932880402, 0.11905629932880402, 0.17632364233334863, 0.17632364233334863, 0.17632364233334863, 0.17632364233334863, 0.498043417930603, 0.498043417930603, 0.0, 0.0, 0.0, 0.0, -1.3142857551574707, -1.3142857551574707, 0.0, 0.0, 0.8838143274188042, 0.8838143274188042, -0.7978010177612305, -0.7978010177612305, -0.45286786556243896, -0.45286786556243896, -0.2200005054473877, -0.2200005054473877, 0.0, 0.0, -1.0476176738739014, -1.0476176738739014, 0.498043417930603, 0.498043417930603, 0.498043417930603, 0.16642908255259192, 0.16642908255259192, 0.16642908255259192, 0.4155613034963608, 0.4155613034963608, 0.4155613034963608, 0.4155613034963608, 0.0, 0.0, 0.0, 0.0, -0.30258822441101074, -0.30258822441101074, -0.30258822441101074, 0.0, 0.0, 0.3327399094899496, 0.3327399094899496, 0.3327399094899496, 0.38821653525034583, 0.38821653525034583, 0.38821653525034583, 0.0, 0.0, 0.7026755909125011, 0.7026755909125011, 0.7026755909125011, 0.5504827871918678, 0.5504827871918678, 0.5504827871918678, 0.5504827871918678, 0.5504827871918678, 0.5504827871918678, -0.4988920291264851, -0.4988920291264851, -0.4988920291264851, -0.4988920291264851, 0.0, 0.0, 0.0, 0.0, -0.9166173934936523, -0.9166173934936523, -1.1111111640930176, -1.1111111640930176, -0.4656070272127788, -0.4656070272127788, -0.4656070272127788, -0.4656070272127788, 0.0, 0.0, 0.19534993171691895, 0.19534993171691895, -0.08934572339057922, -0.08934572339057922, -0.08934572339057922, -0.08934572339057922, 0.0, 0.0, 0.1835593183835348, 0.1835593183835348, 0.1835593183835348, 0.19534993171691895, 0.19534993171691895, -0.14954624573389697, -0.14954624573389697, -0.14954624573389697, -0.21791672706604004, -0.21791672706604004, 0.19534993171691895, 0.19534993171691895, 0.19534993171691895, 0.19534993171691895, 0.0, 0.0, -0.5502291321754456, -0.5502291321754456, -0.5502291321754456, -0.9863014221191406, -0.9863014221191406, -0.9863014221191406, -0.9863014221191406, -0.9863014221191406, -0.9863014221191406, -0.40024030208587646, -0.40024030208587646, 0.28878694772720337, 0.28878694772720337, -0.18050068616867065, -0.18050068616867065, -0.18050068616867065, 0.0, 0.0, -0.9863014221191406, -0.9863014221191406, 0.0, 0.0, 0.1835593183835348, 0.1835593183835348, 0.1835593183835348, 0.10726996262868249, 0.10726996262868249, 0.10726996262868249, -0.4216675360997517, -0.4216675360997517, -0.4216675360997517, -0.4216675360997517, -0.20051206648349762, -0.20051206648349762, -0.20051206648349762, -0.20051206648349762, -1.4870200157165527, -1.4870200157165527, -0.12219291925430298, -0.12219291925430298, -0.12219291925430298, 0.0, 0.0, -1.772155523300171, -1.772155523300171, 0.19534993171691895, 0.19534993171691895, -0.09191250801086426, -0.09191250801086426, -0.7499561309814453, -0.7499561309814453, -0.21072923143704725, -0.21072923143704725, -0.21072923143704725, 0.4203120917081833, 0.4203120917081833, 0.4203120917081833, 0.4203120917081833, 0.4203120917081833, -0.721105694770813, -0.721105694770813, -0.721105694770813, 0.15343401829401648, 0.15343401829401648, 0.15343401829401648, -1.516671895980835, -1.516671895980835, 0.0, 0.0, 0.0, 0.0, -0.6030620733896892, -0.6030620733896892, -0.6030620733896892, 0.0, 0.0, 0.0, 0.0, -0.4306003053983052, -0.4306003053983052, -0.4306003053983052, -0.40874127546946215, -0.40874127546946215, -0.40874127546946215, -0.555590033531189, -0.555590033531189, 0.0, 0.0, -0.22211500008900953, -0.22211500008900953, -0.22211500008900953, 0.006943603356679318, 0.006943603356679318, 0.006943603356679318, -0.6386917233467102, -0.6386917233467102, -0.6386917233467102, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5211096058289211, 0.5211096058289211, 0.5211096058289211, 0.5211096058289211, 0.5211096058289211, 0.5211096058289211, 0.5211096058289211, 0.5211096058289211, 0.5211096058289211, 0.5211096058289211, 0.5211096058289211, 0.5211096058289211, -1.3333332538604736, -1.3333332538604736, -0.2799595594406128, -0.2799595594406128, -0.10682964324951172, -0.10682964324951172, -0.10682964324951172, 0.19665535986423488, 0.19665535986423488, 0.19665535986423488, 0.19665535986423488, 0.19665535986423488, -0.2799595594406128, -0.2799595594406128, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.6640510161717732, -0.6640510161717732, -0.6640510161717732, -0.3237569332122803, -0.3237569332122803, -0.3135212858517964, -0.3135212858517964, -0.3135212858517964, -0.10434520244598389, -0.10434520244598389, -0.41982483863830566, -0.41982483863830566, -0.41982483863830566, -0.8509475787480671, -0.8509475787480671, -0.8509475787480671, -0.41973563035329176, -0.41973563035329176, -0.41973563035329176, -0.3842006325721741, -0.3842006325721741, -0.3842006325721741, -0.16144082943598437, -0.16144082943598437, -0.16144082943598437, -0.8249796628952026, -0.8249796628952026, 0.0, 0.0, 0.4989301413297653, 0.4989301413297653, 0.4989301413297653, 0.4989301413297653, 0.4989301413297653, 0.4989301413297653, 0.4989301413297653, 0.4989301413297653, -0.34425878524780273, -0.34425878524780273, -0.34425878524780273, -0.794670581817627, -0.794670581817627, -0.23812460899353027, -0.23812460899353027, 0.06831659873326623, 0.06831659873326623, 0.06831659873326623, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3201695829629898, 0.3201695829629898, 0.3201695829629898, 0.3201695829629898, 0.3201695829629898, 0.0, 0.0, -1.3142857551574707, -1.3142857551574707, -0.6001346111297607, -0.6001346111297607, 0.0, 0.0, -0.3811739683151245, -0.3811739683151245, -0.4429209232330322, -0.4429209232330322, 0.0, 0.0, 0.0, 0.0, -0.09589143594106031, -0.09589143594106031, -0.09589143594106031, 0.0, 0.0, 0.0, 0.0, 0.0003707607587178918, 0.0003707607587178918, 0.0003707607587178918, 0.0, 0.0, 0.1302153468132019, 0.1302153468132019, 0.1302153468132019, -0.23744500279426584, -0.23744500279426584, -0.23744500279426584, -0.23744500279426584, -0.23744500279426584, 0.03212376435597741, 0.03212376435597741, 0.03212376435597741, -0.09643653531869245, -0.09643653531869245, -0.09643653531869245, -0.09643653531869245, -0.13270288705825806, -0.13270288705825806, -0.13270288705825806, 0.10307574272155762, 0.10307574272155762, 0.10307574272155762, 0.06797162691752112, 0.06797162691752112, 0.06797162691752112, 0.0, 0.0, 0.0, 0.0, -0.1185157030820847, -0.1185157030820847, -0.1185157030820847, -0.1185157030820847, -0.1185157030820847, 0.24431488414605462, 0.24431488414605462, 0.24431488414605462, 0.24431488414605462, -0.6494624614715576, -0.6494624614715576, -0.4382007122039795, -0.4382007122039795, -0.3869582414627075, -0.3869582414627075, 0.0, 0.0, 0.20961851573416168, 0.20961851573416168, 0.20961851573416168, 0.20961851573416168, 0.20961851573416168, 0.20961851573416168, 0.20961851573416168, 0.20961851573416168, -0.17016552090644832, -0.17016552090644832, -0.17016552090644832, -0.17016552090644832, -0.17016552090644832, 0.36107329527537024, 0.36107329527537024, 0.36107329527537024, -0.8329541683197021, -0.8329541683197021, -0.8329541683197021, -0.8329541683197021, 0.24997729063034058, 0.24997729063034058, -1.0712916851043701, -1.0712916851043701, -0.3914051055908203, -0.3914051055908203, -0.3914051055908203, -0.3914051055908203, 0.0, 0.0, -1.2467644214630127, -1.2467644214630127, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011186063289642334, 0.011186063289642334, -0.24370487928390494, -0.24370487928390494, -0.24370487928390494, -0.24370487928390494, -0.24370487928390494, 0.0, 0.0, 0.498043417930603, 0.498043417930603, 0.498043417930603, 0.0, 0.0, 0.0, 0.0, -0.46665287017822266, -0.46665287017822266, -0.46665287017822266, 0.498043417930603, 0.498043417930603, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.3636362552642822, -1.3636362552642822, 0.0, 0.0, 0.0, 0.0, -0.1662846406300862, -0.1662846406300862, -0.1662846406300862, 0.0, 0.0, -0.7777220010757446, -0.7777220010757446, 0.2714069833358129, 0.2714069833358129, 0.2714069833358129, 0.2714069833358129, -0.39285898208618164, -0.39285898208618164, 0.0, 0.0, -0.07554864883422852, -0.07554864883422852, -0.33101630210876465, -0.33101630210876465, -0.33101630210876465, -0.33101630210876465, -0.4999969005584717, -0.4999969005584717, -0.4999969005584717, -0.4999969005584717, 0.0, 0.0, 0.0, 0.0, -0.10063334703445426, -0.10063334703445426, -0.10063334703445426, -0.10063334703445426, -0.10063334703445426, 0.0, 0.0, -0.532244086265564, -0.532244086265564, -0.532244086265564, -0.36592634518941236, -0.36592634518941236, -0.36592634518941236, -0.0001291036605834961, -0.0001291036605834961, 0.14284539222717285, 0.14284539222717285, -0.2448550065358479, -0.2448550065358479, -0.2448550065358479, -0.13727601369221998, -0.13727601369221998, -0.13727601369221998, 0.0, 0.0, 0.5889370143413544, 0.5889370143413544, 0.5889370143413544, 0.2407098114490509, 0.2407098114490509, 0.2407098114490509, 0.2407098114490509, -0.11018718679745998, -0.11018718679745998, -0.11018718679745998, -0.11018718679745998, -0.11018718679745998, -0.11018718679745998, 0.0, 0.0, 0.0, 0.0, 0.6636858880519867, 0.6636858880519867, 0.6636858880519867, 0.6636858880519867, -0.4878045320510864, -0.4878045320510864, -0.4878045320510864, -0.09516639014085126, -0.09516639014085126, -0.09516639014085126, -0.09516639014085126, 0.0, 0.0, 0.0, 0.0, -0.6636738777160645, -0.6636738777160645, 0.0, 0.0, -0.9999686479568481, -0.9999686479568481, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.20708839098612475, -0.20708839098612475, -0.20708839098612475, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.9578678607940674, -0.9578678607940674, -1.3142857551574707, -1.3142857551574707, 0.0, 0.0, -1.487375020980835, -1.487375020980835, 0.2221483588218689, 0.2221483588218689, 0.0, 0.0, 0.2649950385093689, 0.2649950385093689, 0.19762247800827026, 0.19762247800827026, 0.2682698249816895, 0.2682698249816895, 0.2682698249816895, 0.2682698249816895, 0.2682698249816895, -0.10172887643178297, -0.10172887643178297, -0.10172887643178297, 0.0, 0.0, 0.6234898343682289, 0.6234898343682289, 0.6234898343682289, 0.6234898343682289, 0.6234898343682289, 0.20092910528182983, 0.20092910528182983, -0.30230283737182617, -0.30230283737182617, -0.17127730449040723, -0.17127730449040723, -0.17127730449040723, -0.49281735420227046, -0.49281735420227046, -0.49281735420227046, -0.49281735420227046, -0.49281735420227046, -0.49281735420227046, -0.30230283737182617, -0.30230283737182617, 0.19540365537007653, 0.19540365537007653, 0.19540365537007653, 0.0, 0.0, 0.0, 0.0, 0.15381258726119995, 0.15381258726119995, 0.0770387053489685, 0.0770387053489685, -0.2690889835357666, -0.2690889835357666, 0.0770387053489685, 0.0770387053489685, -0.5735290050506592, -0.5735290050506592, -0.37322584788004565, -0.37322584788004565, -0.37322584788004565, -0.37322584788004565, -0.30129035313924146, -0.30129035313924146, -0.30129035313924146, -0.7651842435201008, -0.7651842435201008, -0.7651842435201008, -1.0195600986480713, -1.0195600986480713, -1.140087604522705, -1.140087604522705, -0.30129035313924146, -0.30129035313924146, -0.30129035313924146, -1.0355267524719238, -1.0355267524719238, -1.0355267524719238, -1.1800024509429932, -1.1800024509429932, 0.350957989692688, 0.350957989692688, 0.0553591251373291, 0.0553591251373291, 0.0, 0.0, -0.923076868057251, -0.923076868057251, -0.32986998558044434, -0.32986998558044434, 0.0, 0.0, 0.0, 0.0, 0.6636858880519867, 0.6636858880519867, -1.0832891464233398, -1.0832891464233398, 0.6234622731804847, 0.6234622731804847, 0.6234622731804847, 0.6234622731804847, 0.6234622731804847, 0.6097759554783504, 0.6097759554783504, 0.6097759554783504, 0.3333030939102173, 0.3333030939102173, 0.0, 0.0, 0.0, 0.0, 0.08267414569854736, 0.08267414569854736, 0.08267414569854736, 0.06755502223968501, 0.06755502223968501, 0.06755502223968501, 0.06755502223968501, 0.06755502223968501, 0.06755502223968501, -0.43847882747650146, -0.43847882747650146, -0.4985022942225139, -0.4985022942225139, -0.4985022942225139, -0.11513088146845507, -0.11513088146845507, -0.11513088146845507, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.9333065748214722, -0.9333065748214722, 0.25022484858830774, 0.25022484858830774, 0.25022484858830774, 0.0, 0.0, -0.33101630210876465, -0.33101630210876465, -0.04480016231536865, -0.04480016231536865, -0.04480016231536865, -0.04480016231536865, 0.0, 0.0, 0.0, 0.0, -0.9984625975290935, -0.9984625975290935, -0.9984625975290935, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0669609010219574, 0.0669609010219574, 0.0669609010219574, 0.0669609010219574, 0.0, 0.0, 0.0, 0.0, 0.15428215265274048, 0.15428215265274048, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.27223241329193115, -0.27223241329193115, -0.27223241329193115, 0.0, 0.0, -0.38174528876940417, -0.38174528876940417, -0.38174528876940417, -0.38174528876940417, 0.23326337337493896, 0.23326337337493896, 0.23326337337493896, 0.0, 0.0, 0.498043417930603, 0.498043417930603, 0.498043417930603, 0.498043417930603, 0.498043417930603, 0.498043417930603, 0.498043417930603, -0.33332276344299316, -0.33332276344299316, -0.9965664148330688, -0.9965664148330688, 0.6234622731804847, 0.6234622731804847, 0.6234622731804847, 0.6234622731804847, 0.6234622731804847, -0.9999966621398926, -0.9999966621398926, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.7499561309814453, -0.7499561309814453, 0.498043417930603, 0.498043417930603, 0.498043417930603, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.089033842086792, -1.089033842086792, 0.05757713317871094, 0.05757713317871094, -0.30607903003692627, -0.30607903003692627, -0.08352744579315186, -0.08352744579315186, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.4203471302986146, -0.4203471302986146, -0.4203471302986146, -0.4203471302986146, -0.4203471302986146, -0.6679909427960713, -0.6679909427960713, -0.6679909427960713, -0.6679909427960713, 0.0, 0.0, -0.33101630210876465, -0.33101630210876465, -0.6674402554829915, -0.6674402554829915, -0.6674402554829915, 0.0, 0.0, 0.0, 0.0, -0.7631683349609375, -0.7631683349609375, -0.7631683349609375, -0.42939817905426025, -0.42939817905426025, 0.0, 0.0, -1.2221672534942627, -1.2221672534942627, -0.2938229888677597, -0.2938229888677597, -0.2938229888677597, -0.2938229888677597, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.130741755167643, -1.130741755167643, -1.130741755167643, 0.0, 0.0, 0.8296424597501755, 0.8296424597501755, 0.0, 0.0, -4.5418739318847656e-05, -4.5418739318847656e-05, -0.9333065748214722, -0.9333065748214722, 0.0, 0.0, -0.7809271812438965, -0.7809271812438965, -1.4348604679107666, -1.4348604679107666, 0.0, 0.0, -0.7809271812438965, -0.7809271812438965, -0.7809271812438965, -0.7809271812438965, 0.498043417930603, 0.498043417930603, -1.2333333492279053, -1.2333333492279053, 0.0, 0.0, -0.6666690905888875, -0.6666690905888875, -0.6666690905888875, 0.498043417930603, 0.498043417930603, 0.498043417930603, 0.498043417930603, 0.498043417930603, 0.498043417930603, 0.3444699148337046, 0.3444699148337046, 0.3444699148337046, 0.5816285490989686, 0.5816285490989686, 0.5816285490989686, 0.5816285490989686, 0.5816285490989686, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.498043417930603, 0.498043417930603, 0.498043417930603, 0.498043417930603, 0.498043417930603, 0.498043417930603, 0.498043417930603, 0.0, 0.0, -0.7140998840332031, -0.7140998840332031, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.8999999761581421, -0.8999999761581421, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.33347443739573157, 0.33347443739573157, 0.33347443739573157, -0.03274218241373705, -0.03274218241373705, -0.03274218241373705, 0.0, 0.0]
the hyperedges are {0: [415, 514, 585, 690, 784], 953: [415, 684], 384: [514], 323: [514, 133, 109], 173: [514, 108, 384], 383: [514, 108, 384], 86: [585, 626, 460, 26, 262, 272, 769, 797, 1006], 691: [690, 250, 134, 499, 1009, 651, 935, 898, 519], 271: [690], 290: [784, 260, 157], 540: [784], 1: [153, 732, 1937], 291: [153, 683, 106, 356, 470, 516, 152, 154, 1714, 694, 673, 501, 159, 445, 67, 167, 216, 317, 336, 462, 482, 590, 640, 708, 746, 3130, 927], 422: [153, 469, 48, 54, 105, 106, 633, 987, 586, 714, 194, 28, 1899], 390: [153, 680, 303, 455, 668, 899, 54, 106, 697, 180, 573, 182, 444, 355, 356, 470, 481, 516, 597, 802, 176, 103, 619, 154, 382, 669, 694, 1000, 673, 28, 94, 445, 896, 167, 317, 336, 462, 482, 708, 746, 233, 504, 804, 51, 363, 234, 244, 253, 259, 483, 517, 523, 525, 550, 701, 706, 910, 7, 10, 22, 58, 82, 85, 98, 147, 151, 155, 156, 200, 212, 219, 231, 248, 391, 414, 456, 507, 557, 576, 700, 702, 711, 727, 744, 773, 812, 880, 906], 522: [153, 469, 54, 105, 106, 987, 697, 619, 1249, 999, 1000, 28, 956, 1374, 1464, 616], 714: [153, 54, 105, 106, 987, 999, 1000, 28, 956, 67], 748: [153, 48, 54, 105, 106, 987, 697, 1761, 714, 999, 1000, 28, 956, 67, 998], 275: [153, 469, 54, 105, 106, 987, 697, 619, 1249, 1000, 28, 956], 699: [153, 668, 54, 105, 106, 987, 697, 366, 310, 516, 619, 669, 714, 999, 1000, 746, 251, 575, 701, 22, 85, 98, 155, 156, 700, 906, 755, 748], 28: [153, 48, 54, 105, 106, 633, 987], 646: [732, 2064, 1282, 1650, 2149, 549, 1895, 2150], 905: [732, 685, 686, 687, 697, 3040, 802, 1969, 673, 768, 88, 3191, 2081, 1231, 1234, 2669, 2670, 2728, 2161, 2686], 266: [732, 685, 297, 152, 65, 88, 3191, 2081], 780: [732, 2064, 1203, 1239, 1282, 1650, 1788, 1991, 2149, 262, 272, 1006, 267, 1921, 88, 1895], 533: [732, 534], 534: [732], 284: [732, 1513], 31: [732, 779, 2665, 1282, 1357, 297, 1489, 2774, 2812, 646, 3028, 882, 2082], 289: [732], 295: [732, 634, 882, 228, 232, 236, 475, 535, 771], 429: [732, 1721, 1172, 305, 1409, 1512], 430: [732], 78: [732, 534], 471: [732, 127, 1075, 1357, 2812, 1169, 697, 802, 262, 272, 797, 1199, 541, 122, 566, 285, 1232, 504, 3015, 30, 556], 218: [732, 1489, 1724, 2104, 2401, 2638, 725, 65, 1247, 3303], 862: [732, 1937, 1212, 1969, 534, 1850, 90, 1129, 474, 1968], 876: [732, 1489, 1724, 2104, 2401, 2638, 2400, 725, 65, 1247, 3303], 505: [732, 69, 534, 56, 30], 1850: [1937, 1212, 1281, 1998, 1999, 1951], 1129: [1937, 1969], 1011: [1034, 2028, 2029], 2007: [1034], 2040: [2028, 2029], 2: [962], 3151: [962, 3146], 648: [2181], 2865: [2181, 946], 1012: [2181, 2031], 1349: [2031], 1500: [1417, 2024, 1236, 1089], 1013: [1417, 1427, 1545, 2024], 1725: [1417, 1427, 1545, 2024], 1545: [1427, 80], 1058: [2024, 1236], 1445: [2024], 2118: [2024, 774], 1801: [2024, 1531], 1236: [2024, 774], 1046: [2024, 1286, 1302, 1532, 1891], 1744: [1059, 1378, 1014], 1380: [1059, 1378, 1014], 1733: [1059, 1744], 1014: [1059, 1365, 1378], 1059: [1365, 1378, 1014], 1680: [1378, 1014, 1194], 1088: [1236], 1891: [1236, 1439], 1219: [1236], 1860: [1236, 1132, 1668], 1067: [1236], 1070: [1236], 1015: [1236, 1439, 1440, 2093], 1396: [1236], 1687: [1236, 421, 1508], 441: [1236, 2665, 1723, 1801, 2168, 774, 880, 2447, 1478, 1531, 2765, 645], 1242: [1236, 2118], 1150: [1236, 1262, 1294, 421, 1508, 2118], 1444: [1439, 1440], 1478: [1439, 1440], 1083: [1439, 1440, 2093, 1083, 1196], 1439: [1440], 1397: [2093], 2183: [2677, 845, 3156], 79: [2677, 2460, 828, 3162, 1724, 1019, 103, 278, 460], 32: [845, 592, 665, 686, 687, 697], 685: [845, 592, 665, 686, 687, 697], 460: [3156, 685, 1006, 154, 263, 3159], 1016: [2138], 1620: [2138], 3: [634], 749: [634], 304: [634], 819: [634], 503: [634], 120: [634], 2184: [2306, 2356], 2306: [2306, 2356], 2261: [2306, 2356], 2185: [2539], 3042: [2539], 2812: [2539], 1624: [1549], 1017: [1549, 591], 978: [1549, 917], 1549: [1549, 1624, 978], 326: [591, 324, 350, 917], 739: [588, 235, 592, 464], 4: [588], 294: [588], 2091: [2064], 1019: [2064], 30: [2064, 1203, 1239, 1282, 1650, 1788, 1991, 2149], 97: [36, 63, 949, 5], 5: [36, 63, 97, 603, 949], 329: [36, 63, 97, 661, 821, 948], 274: [36, 63, 83, 203, 735], 916: [36, 63, 813, 5, 617, 661, 400, 77, 595], 661: [36, 63, 97, 603, 617, 203, 735, 948, 191, 313, 314, 877, 328, 835, 852], 63: [949], 1896: [1176], 1021: [1176], 643: [64, 682, 239, 866, 840, 83, 661, 271, 808, 35, 121, 136, 313, 314, 348, 807, 814, 994], 6: [64, 682], 1024: [1527], 1077: [1527], 514: [868, 383], 867: [868, 1747], 677: [868, 1747, 1394, 1984, 867], 8: [868, 1986], 44: [868, 632], 14: [868], 270: [868], 431: [868], 308: [868, 1986], 341: [868], 925: [868], 478: [868], 2105: [1100, 1579, 1723, 311, 1144], 1634: [1100, 1138, 1828, 1893, 2139, 2165, 2244, 26, 2257], 1029: [1100, 1109, 1248], 1695: [1100, 1828], 1401: [1109], 1538: [1248], 1141: [1248], 1911: [1248], 2128: [1244], 1941: [1244], 1030: [1244], 1794: [1487], 2003: [1487, 2002], 1031: [1487, 1488, 1872], 1930: [1487], 961: [1488], 1645: [1488], 1488: [1872], 568: [158], 9: [158, 202, 260, 559, 820, 970], 583: [158], 385: [202, 386], 386: [202, 240], 387: [202, 650], 932: [202, 869, 386, 240], 933: [202, 970, 247], 584: [202, 386, 240], 654: [202, 255, 240, 214], 847: [202, 59, 723, 191, 506, 138, 394, 843, 775, 593, 594, 583, 980, 814, 40, 519, 730, 729, 712, 816, 242, 186, 509, 595, 826, 983, 984], 33: [260], 753: [559, 628, 579, 160, 874], 398: [820], 824: [970], 158: [970], 434: [387, 506], 11: [387, 945], 204: [945, 650], 1033: [1759, 2014, 2015], 1946: [1759], 1363: [1759, 1946], 2014: [1759, 2015], 2015: [2014], 1035: [1491], 1054: [1491], 1037: [1042, 1309, 1679], 1615: [1042, 1037], 1040: [1309, 1679, 1076, 1078], 1411: [1679], 1475: [1679, 2036], 1042: [1679, 1037], 1047: [1679, 1636, 1127, 1613, 1776], 2196: [55, 3279], 972: [55, 3279, 971], 2901: [3279], 55: [3279, 971], 1776: [1107, 1174, 1404], 1769: [1107, 1174], 1090: [1107, 1174, 1404, 1608], 1038: [1107, 1174, 1404, 1536, 1636, 1682, 1769], 1924: [1404, 1275, 1911], 1107: [1404, 1200, 1425], 1174: [1404, 1200, 1425], 57: [1536, 1636, 1682, 1769, 1127, 1253, 1608, 157, 1981], 2119: [1536], 1636: [1682, 1253, 1289], 1937: [1212], 1039: [1212], 1599: [1076, 1078, 1613, 1600, 1950], 1371: [1078], 1076: [1078], 1041: [2036], 1307: [2036, 2037], 1119: [2036], 1667: [2460, 828, 3028, 3153, 1326], 1543: [2460, 828, 2293, 1326], 2202: [2460, 779, 828, 3162], 3153: [2460], 3028: [2460, 779, 828, 3162], 3162: [2460], 828: [2460, 1326], 1664: [779, 1991, 1921], 1279: [779, 1976], 1895: [779, 1991], 1006: [779], 882: [779], 179: [779, 1282, 1650, 1991, 1075, 297, 646, 573, 438, 267, 1921], 887: [779, 686, 687, 697, 262, 1281, 1998, 1999, 2728, 1951], 88: [779, 152, 154, 1281, 1998, 1999, 2073], 609: [828, 3162, 462], 34: [828, 697, 180, 366, 1587, 573, 829], 37: [828, 1267, 182, 2464, 310, 444, 1724], 462: [828, 167], 278: [828, 1267, 182, 2464, 310, 444, 1724], 1043: [1138, 1828, 1893, 2139, 2165], 2139: [1828, 1498, 1255], 1894: [1893], 1351: [1893], 1116: [1893, 2139], 1481: [2139, 1144], 1892: [1274], 1044: [1274], 1045: [1453, 1704, 1705, 1944, 1954, 1955, 1980, 2002], 1470: [1453, 1704, 1705, 1944, 1954, 1955, 1980, 2002], 2080: [1704, 1705, 1954, 1300], 1705: [1704, 1617], 1916: [1705, 1422, 853], 1886: [1944], 1870: [1944, 1178, 1945, 1337], 2002: [1954, 1955, 1452], 2050: [1980, 1648, 2112, 1332, 1285, 1542, 2032, 2033, 853], 1542: [1980, 1422], 1423: [1980, 1285, 1542, 1845], 1143: [1980, 2103], 2657: [1286, 1302, 2447], 1125: [1286, 1302, 1891], 1902: [1286, 1302], 1532: [1286, 1302, 1532], 2658: [1532, 2447, 1531], 1690: [1891, 1478, 1814, 1122], 2034: [1776, 1981, 1289], 1775: [1776], 3099: [2675], 2206: [2675], 1050: [1168, 1254], 172: [1168, 56, 1934], 1524: [1168, 1254, 1934], 1057: [1254], 1787: [1254, 1288], 12: [508, 1741, 777], 502: [508, 107, 501, 1007, 755], 655: [508, 1007], 1740: [1741, 1282], 364: [777, 697, 355, 470, 481, 516, 302, 103, 382, 673, 167, 462, 482, 746, 251, 804, 911, 51, 234, 244, 253, 259, 365, 483, 517, 523, 525, 550, 701, 706, 910, 918], 799: [777, 639, 167, 640, 708, 365, 701, 391, 332, 707], 1890: [1253], 1253: [1253], 1051: [1253], 2210: [2275, 1707], 1707: [2275, 1229, 1511], 3085: [2275, 2647], 969: [1707, 684], 1063: [1707, 1229], 1066: [1961, 1962, 1963], 1055: [1961, 1962, 1963], 13: [250], 16: [145], 497: [145], 822: [145, 146, 418, 810, 144, 879, 281, 360, 473, 242], 472: [145, 142], 542: [145], 2213: [2244], 3235: [2845], 2214: [2845], 18: [59], 604: [59], 20: [6], 663: [6, 17, 858], 1864: [1123], 1060: [1123], 100: [680], 934: [680], 679: [680], 551: [680], 1005: [680, 481, 2860, 51, 1336, 920], 21: [680], 439: [680, 668, 697, 444, 355, 438, 481, 516, 597, 302, 382, 669, 94, 445, 167, 482, 804, 51, 244, 253, 259, 365, 483, 517, 525, 918, 58, 212, 414, 702, 332, 755, 871], 698: [303, 668, 697, 180, 481, 619, 694, 574, 575, 701, 85, 456, 700, 805, 572, 95, 549, 571], 22: [303, 455, 469, 599, 668, 899], 1008: [455], 554: [469, 106, 422, 1000, 58], 598: [599, 336, 151], 616: [668, 481, 1714, 694, 714, 999, 1000, 67, 51, 574, 575, 7, 572, 549, 571, 748, 998], 268: [668], 123: [668, 310, 669], 701: [668], 766: [668], 2489: [2391], 2219: [2391], 2242: [2665], 738: [2665, 789], 3080: [2665], 2221: [2665, 3115], 2745: [2665, 3115, 122, 464], 53: [309, 161], 23: [309, 704], 855: [704, 733, 432, 2699, 3107, 2282, 2441], 24: [127, 685, 947, 952], 297: [127, 802, 797, 541, 122, 1232, 504, 3015], 769: [685, 686, 687, 796, 1231, 1234, 2157, 1273, 2686], 903: [685], 263: [685, 26, 769, 122, 311, 426, 566, 768, 796, 840], 378: [947, 2596, 1633], 99: [952, 723], 164: [952], 518: [952], 423: [952], 552: [952], 588: [952], 2222: [2567], 2566: [2567], 1764: [1811], 1064: [1811], 1900: [1811], 1230: [1811, 1191, 1514, 2097], 795: [1811, 91, 2382, 1794], 1918: [1811, 2151, 1298, 1188, 1276, 1308, 2175], 1727: [1811, 1191, 1514, 2097], 296: [69, 655, 25, 1007], 265: [69, 655, 888, 107, 194, 501, 502], 461: [69, 888, 25], 338: [69, 655, 888, 107, 827, 463, 555, 580, 581, 901, 902], 276: [69, 655, 888, 107, 827, 463, 555, 580, 581, 901, 902], 372: [69, 655, 888, 107, 194, 501, 502, 1007], 25: [69, 357, 655, 683, 888], 888: [357], 561: [655, 669], 537: [655], 2224: [2780], 2558: [2780, 2224], 3022: [2780, 2224], 27: [393], 940: [393, 939, 394, 938], 1065: [1093], 2027: [1093], 1568: [235, 592], 1226: [235, 592], 1068: [235, 592], 592: [235], 615: [592, 306, 350, 3217, 217, 3205], 392: [592, 914], 2232: [2540, 3033, 3203], 3033: [2540], 3202: [2540, 3033, 3203, 2468], 3057: [3033], 2540: [3033], 2322: [3203, 2605], 2233: [2698, 3158], 2591: [2698, 821], 339: [3158, 813, 1858], 1071: [1744], 67: [54, 105, 176], 999: [54, 105, 106, 987, 714, 1000, 28, 956, 67, 998], 998: [54, 105, 106, 176, 1000, 67, 998], 184: [54, 106, 697, 586, 1249, 422, 1761, 694, 714, 3252, 999, 1000], 956: [54, 633, 1894], 48: [106], 442: [106], 469: [633, 697, 586], 2236: [2193], 1431: [2193], 2148: [1203, 2150], 1788: [1239], 1766: [1282], 1383: [1650, 1272], 1649: [1650, 1272], 567: [1650], 2066: [1788], 1165: [1788], 1374: [2149, 1061, 1177, 1979], 1989: [2149, 1514, 1241], 1979: [2149, 1514], 1073: [1075], 1220: [1075, 1169, 1199], 793: [740, 793, 818, 859], 29: [740, 793, 818, 859], 792: [793, 859], 1708: [818, 1214], 1863: [1489, 1336], 1357: [2774, 1976], 2082: [2082], 2083: [2082, 1369, 1356, 1637], 1075: [1169], 1387: [1169], 1613: [1190], 1078: [1190, 1600], 768: [665, 727, 728, 1876], 2081: [665, 2161, 1876], 2494: [686, 1198, 1469, 3040], 2278: [686, 1198, 1469, 3040], 2669: [686, 3191, 2463], 1678: [686, 1198, 1469, 1281, 1999, 2073], 1876: [686, 687], 1469: [686], 2686: [686, 687, 1714], 285: [697, 356, 94, 159, 2482, 445, 896], 547: [697, 573, 159, 590, 244, 253, 259, 572], 700: [697, 366, 619], 58: [697, 180, 302], 571: [697, 234, 95], 572: [697, 180, 573, 841, 549], 574: [697, 310, 572, 95, 549], 459: [697, 573], 85: [697, 366, 619], 482: [697, 481, 51], 2076: [1245], 1079: [1245, 1651, 1823], 3262: [1651, 1823, 2629], 1823: [1651], 1080: [1189, 1583], 1657: [1189, 1583, 1080, 1314, 1359, 1584, 1736, 1737, 1735], 1583: [1189], 1314: [1583], 1731: [1583, 1080, 1314, 1359, 1364, 1584, 1658, 1731, 1736, 1737, 1735, 1194, 1071, 1657, 1680, 1985], 1736: [1583, 1314, 1359, 1735], 1359: [1583], 1081: [1141, 1950], 1866: [1141], 2243: [2324, 3074], 2324: [2324, 3074], 3175: [2324, 3074], 2507: [3074], 1912: [1083, 1150], 1373: [1083], 1180: [1196], 105: [180, 176], 842: [180, 896], 717: [180], 303: [180, 841], 688: [180, 841], 987: [180, 176], 1586: [1587, 1336], 1197: [1587], 689: [573], 152: [573], 556: [829, 462, 2266, 257], 573: [829, 257], 2245: [2310, 2390, 2483], 3125: [2310, 2390, 2483, 2534], 1086: [1480], 1479: [1480, 1661, 2012], 35: [500], 892: [500, 703], 1392: [1608], 3035: [3034], 2246: [3034], 1626: [2009, 1224], 1092: [2009], 1772: [2009], 1724: [1267], 130: [182, 1498], 523: [182], 2858: [2464], 517: [310, 673, 251, 525, 871], 272: [310, 516, 103, 262, 285, 827], 373: [444, 262, 94, 445, 3015, 251, 2463, 374, 3014], 374: [444, 94, 445], 445: [444, 194, 94], 1677: [1724], 1977: [2026, 1974], 1093: [2026], 1637: [2026, 1181, 1375], 2248: [2746], 3195: [2746, 2349], 2733: [2746, 2349], 1108: [1317, 1676], 1094: [1317, 1676], 644: [1010, 42, 271, 611], 39: [1010], 488: [1010, 569, 42, 271, 950, 17, 834, 955, 964], 298: [1010, 764], 886: [1010], 919: [1010, 764], 416: [146, 418, 453, 499, 144, 137, 351, 142, 879, 1003], 418: [146, 1002, 141, 453], 230: [146, 148, 177, 411, 480, 621, 625], 40: [146, 148, 177, 411, 480, 621, 625], 144: [146, 1002, 141, 453], 410: [411, 622, 174, 241, 570, 975], 631: [411, 750, 1009, 403, 241, 1003, 492, 979, 70, 226, 495, 980, 995], 620: [621, 623], 623: [621, 623], 624: [621, 625, 623], 1096: [1490], 2052: [1490, 2072, 1590], 1709: [1490, 2072], 1098: [133], 41: [418], 300: [418], 281: [418, 584, 861, 144], 1100: [1579, 1723], 1528: [1723, 1255, 1478, 2765, 1814, 2970], 1972: [1609, 1964, 1968], 1101: [1609], 1102: [1673, 1958], 1959: [1673, 1958], 1673: [1958, 1639], 354: [1958, 2259, 2407, 351, 2579, 2770, 353, 2585, 2586], 868: [632], 1103: [2103], 45: [733, 751], 734: [733, 751], 704: [751, 239, 752], 996: [751, 994, 752], 2736: [2476, 2178], 2258: [2476], 2590: [2476, 2178], 1106: [1132, 1339], 1560: [1339, 1818, 1668, 341], 1273: [1339, 1668, 1714], 2833: [2744], 2259: [2744], 2724: [2744], 3078: [2744], 2955: [1813, 1173, 2251], 1812: [1813, 1173], 2957: [1813, 2959], 2263: [1813, 2959], 2264: [2223], 3171: [2223], 2491: [2223], 2265: [2204], 2762: [2204], 293: [213, 778, 397, 651], 872: [213, 778], 396: [213, 778, 723, 397], 397: [213, 778], 47: [213, 778], 447: [213, 778, 535, 744, 93, 800], 1114: [2104], 2084: [2104], 483: [355, 481, 516, 597, 154, 708, 51, 244, 253, 259, 525, 706, 918, 702, 189], 772: [355, 516, 896, 746, 804, 911, 706], 69: [355, 356, 516], 747: [355, 356, 481, 516, 746, 504, 804, 911, 51, 910, 805], 1004: [355, 481, 516, 802, 619, 122, 501, 502, 901, 365, 7, 1336, 920], 976: [355, 516], 977: [355, 516], 51: [355, 356, 438, 470, 481, 516, 597], 355: [356], 773: [356, 706, 773, 451], 745: [356, 438, 516, 746, 251, 804, 911, 910, 805], 302: [438, 746, 251, 804, 911], 437: [438, 516, 711], 318: [481, 597, 51], 805: [516, 746, 804], 408: [516], 608: [597, 639, 896, 167, 706, 773], 332: [597, 2342], 87: [597], 309: [161], 2664: [2044], 2274: [2044], 1222: [2044, 1540, 1807], 1799: [2044, 1540, 1807], 1120: [1210, 1256, 1604, 1606, 1818], 1818: [1210, 1256], 1206: [1210, 1256, 1604, 1606, 1818], 1606: [1604], 1256: [1818], 362: [971], 524: [971], 56: [1824], 1512: [1824, 305, 1409], 641: [157], 2843: [626], 2276: [626], 443: [586], 60: [586], 61: [944], 943: [944], 673: [802, 377, 762, 711], 801: [802, 754], 754: [802], 566: [802, 152, 771, 770], 668: [802, 251], 62: [802], 1128: [1588], 1456: [1588, 1232, 1235], 1131: [1885], 1883: [1885], 1884: [1885], 1646: [1885, 1262, 421, 1321], 64: [91, 210, 239, 569], 682: [91, 210, 239, 569], 279: [210, 2292, 358, 3005], 203: [239], 498: [239], 866: [569, 121], 612: [569, 271, 955, 964, 965, 583], 115: [569, 671], 600: [569, 671], 65: [152], 830: [176], 1134: [1422], 2006: [1422, 1214, 1571, 2112, 2005, 1258, 2110], 1655: [1422], 1984: [1747], 1986: [1747], 678: [1747, 1984, 867, 379], 1135: [1747], 1720: [1747], 2136: [1747, 1394], 1630: [1747, 14], 1136: [1494], 201: [1494, 383, 2125], 108: [1494, 383, 2125], 1577: [1494], 2049: [1964, 1845, 1703, 2048, 2032, 2033, 1308, 2175, 2075, 1252, 1846], 1223: [1964], 2121: [1964], 1137: [1964], 1203: [1964, 1432, 2151], 2285: [2616], 2615: [2616], 1632: [1801, 2168], 1633: [1801, 774, 645], 1138: [1801, 2168], 2168: [1801, 2168], 1851: [1801, 2168, 1633, 3009], 70: [750], 659: [750, 722, 1009, 241, 570, 1003, 208, 979, 70, 495, 980, 877, 986], 979: [750, 935, 980, 242], 986: [750, 548, 651, 935, 226, 495, 980, 995, 875], 930: [939], 71: [939], 938: [939, 506, 394], 506: [939, 171, 394, 721, 843], 77: [813], 735: [813, 77], 1145: [1139], 1842: [1139, 1738], 1716: [1139, 1717], 2891: [2253], 2299: [2253, 2259], 1147: [1874], 1743: [1874, 1019], 1312: [1019], 1354: [1019], 1938: [1019], 1175: [278, 1360, 1385, 2173], 539: [460], 1148: [1687, 710], 1533: [1687, 710], 450: [710, 896, 451], 1509: [1262, 149, 326, 324, 150, 325, 327, 1639], 421: [1262], 1506: [1294], 353: [421], 1151: [1301, 1960], 695: [1301, 1960], 2312: [3149], 3147: [3149, 3150, 3146], 3150: [3149, 3146], 84: [743], 495: [743, 139, 208, 667, 935, 979], 349: [619, 1177, 574, 575], 1153: [1329, 1342, 1629], 1516: [1329, 1342, 2470], 1438: [1329, 1342], 1585: [1629], 262: [797, 541, 673], 1158: [1214, 1461, 1571], 1994: [1214], 2004: [1214, 1461, 1571], 628: [1461, 1243], 1998: [1281], 1999: [1281], 1976: [1999], 1880: [1257], 1852: [1257, 1295], 1159: [1257, 1287], 1451: [1287], 1573: [1287], 1160: [1369], 89: [546, 548], 788: [546, 579, 935, 875], 546: [548], 2877: [2561, 2319], 2878: [2561, 2319, 3201], 2319: [2561], 2320: [2401, 2638], 2919: [2638], 2509: [2638], 2613: [2605], 2595: [2594, 2325], 2325: [2594], 1168: [1511, 2106], 1751: [2106], 2328: [3153], 2948: [1173], 2332: [1173], 1905: [1360], 1385: [1385], 2174: [1385, 2173], 1176: [1897], 2969: [1897, 2292], 2832: [3200, 2295], 2335: [3200], 1944: [1178, 1945], 1178: [1178], 264: [723], 814: [723, 866, 188, 808, 35, 121, 136, 807, 519, 816], 816: [723, 866, 121], 764: [723], 1838: [1836, 1837], 1835: [1836], 1755: [1836, 1837], 1183: [1836, 1837], 1834: [1837], 2339: [2596, 974], 947: [974], 102: [267], 725: [2400], 2340: [2400, 725], 992: [725, 65], 104: [129, 131, 134, 140], 589: [129, 131, 134, 140], 1185: [1945], 1186: [1648], 1461: [1648, 1243, 628, 2005], 2344: [2456, 2624], 2552: [2456], 2624: [2456], 3228: [2456, 2624, 2037], 3293: [2624], 2345: [2230], 3269: [2230], 838: [383, 108, 384], 1494: [2125], 2347: [2860], 1193: [1548, 2112], 1966: [1548, 2112], 1980: [2112, 1285, 2032, 2033], 2110: [2112, 2005], 2111: [2112], 389: [584, 579, 196, 334, 467, 936], 110: [584], 2348: [2548], 2598: [2548], 391: [382, 189], 113: [382], 3274: [3013], 821: [3013], 1858: [3013, 3274], 2351: [3013], 1841: [1183], 1202: [1183], 2151: [1432], 2808: [2354], 2357: [2354], 1209: [1748], 1515: [1748], 448: [810], 417: [810, 453], 900: [810], 531: [810, 188], 119: [810], 2360: [2839], 2840: [2839], 2361: [130, 3255], 1498: [130], 2556: [3255], 1521: [3255], 696: [614], 2362: [614], 2753: [2418], 3012: [2418], 2364: [2418], 121: [866], 1541: [1540], 2847: [1807], 2371: [377], 1001: [377, 762, 7], 672: [2420, 2437], 1931: [2420], 2374: [2420], 2375: [2269, 3213, 3216], 3071: [2269], 3212: [3213, 3216], 2376: [2965], 2573: [2965], 328: [617], 243: [617], 126: [617], 1227: [2792], 2623: [2792], 2378: [2682, 2683], 2684: [2682, 2683], 2682: [2682, 2683], 1241: [1514], 880: [774, 645], 817: [1002, 171], 141: [1002], 2866: [3181], 2395: [3181], 146: [907], 731: [907, 139, 979, 166], 493: [907, 492], 325: [149, 326], 1701: [149, 326, 324, 150, 325, 327], 149: [149, 326], 150: [149, 326], 917: [149, 326, 150, 325, 327], 637: [149, 326, 324, 150, 325, 327], 1255: [1255], 993: [1721, 605], 1254: [1721, 754, 2043], 1919: [1721, 1873], 782: [754, 2639], 1580: [2043, 2873], 1277: [2043], 1366: [2043, 1727], 1264: [1284], 1642: [1284], 1416: [1284, 1249, 1144, 1231, 1234, 2157], 167: [639], 640: [639], 170: [306, 494], 1003: [494, 247], 767: [494, 467, 473], 2416: [2629], 2721: [2629], 1492: [56], 2420: [2437], 3165: [2437], 3286: [2437, 2864], 3140: [2371], 2422: [2371], 2426: [2756], 851: [2756, 2336], 3119: [2756, 2336], 305: [228, 232, 236], 178: [228, 232, 236], 1474: [1921, 2065], 2149: [1921], 1991: [1921], 553: [1921, 673, 2728, 2791, 1951, 2161], 1419: [1921], 2064: [1921, 2065], 2161: [1921], 3043: [185, 3117, 2419, 2980], 3267: [185, 3266], 2855: [185, 199], 2641: [185, 3117], 183: [185, 605, 3117], 605: [185], 452: [605], 381: [605, 684, 713], 1460: [1761, 1061, 1979, 1374, 1464], 1526: [1761], 1464: [1761], 3253: [3252], 1000: [1000, 956, 67, 998], 3272: [2767], 2434: [2767], 806: [2767, 247], 2813: [2465, 2438], 2438: [2465], 2711: [2465, 2438, 2531, 2532, 2667], 192: [607], 849: [607, 40], 1889: [1344], 1291: [1344], 1382: [1344, 1889], 1926: [1344], 1760: [1854, 1953], 1292: [1854, 1953], 202: [255], 1297: [1013, 1089, 1725], 1967: [1013, 1725], 1347: [1089], 2448: [2563], 2562: [2563], 207: [499], 209: [347], 719: [347], 860: [861, 214], 2455: [861], 217: [724], 591: [724, 978, 217], 2976: [3303, 2399], 1483: [2116], 1310: [2116], 2467: [2470], 245: [722, 1009], 787: [722], 333: [722], 320: [1009], 741: [1009], 729: [1009, 730], 2112: [1332], 1338: [1332], 2005: [1332], 1343: [404], 1859: [404, 3274], 693: [404, 405, 400, 401], 3183: [404, 401], 428: [673], 538: [673], 352: [2583, 353], 2486: [2583], 283: [311, 263], 424: [426], 425: [426], 510: [566, 504], 3122: [194], 664: [502], 1353: [1965], 1348: [1965], 1903: [1965, 1624, 2032, 2033], 2696: [2578], 2490: [2578], 269: [80], 1352: [1207, 1449], 1539: [1207], 1978: [1449], 273: [432, 433], 1420: [432, 2260, 2699, 1770, 3107], 705: [432], 432: [433, 915], 901: [555, 902], 902: [555], 581: [580], 359: [358], 2550: [3005], 3006: [3005], 280: [324, 350, 917], 629: [350, 630, 515], 985: [350, 917, 515], 288: [917], 1759: [1946], 1816: [1727], 1797: [1727], 1710: [1513, 1172, 305, 1409, 1512], 444: [94], 504: [159], 2943: [2482], 922: [896], 1368: [1355, 1415, 1476], 1414: [1355, 1415, 1476], 1791: [1476], 2546: [2191], 2499: [2191], 544: [545, 14], 286: [545], 1376: [762], 1990: [762], 520: [170], 348: [170, 630, 982], 846: [170, 287, 604], 287: [170, 675, 692], 337: [675, 287], 2651: [692, 914], 1381: [1162, 1268, 2079], 1517: [1162, 1268, 2079], 2021: [2079], 684: [216], 525: [590], 639: [640], 707: [708], 709: [708, 707], 804: [746, 911], 3131: [3130], 926: [927, 244, 253, 259, 483, 918], 420: [233], 292: [233], 2916: [1889], 395: [397], 652: [397], 928: [651], 369: [651], 558: [475], 476: [475], 742: [475], 342: [771], 770: [771], 501: [1007], 1386: [341], 3258: [341], 1388: [1982], 1437: [1982], 1457: [1232], 2720: [3015], 3169: [3015, 3014], 3014: [3015], 1853: [1582], 1391: [1582], 2514: [2599], 2979: [2599], 2515: [3133], 3134: [3133], 1956: [251], 438: [251], 669: [251], 2521: [2557], 2726: [2557], 1406: [1080, 1314, 1359, 1364, 1584, 1658, 1715, 1731, 1736, 1737], 1735: [1314, 1359, 1736, 1737], 1737: [1314, 1359, 1736], 1584: [1314], 1985: [1584, 1071], 1408: [1715], 1732: [1731, 1657, 1985], 1996: [1179], 1997: [1179], 1407: [1179], 865: [1216, 1240, 1933], 1413: [1216, 1240], 1675: [1216, 1240, 1933], 1421: [1216, 1933], 1519: [1216], 1848: [1216, 1849], 2525: [2446], 365: [2446], 2528: [2342], 2530: [2529], 311: [2529], 2157: [1231], 312: [315], 313: [315], 314: [315], 2531: [2533], 3020: [2533, 936, 2531, 2532], 316: [622], 982: [622], 2535: [2534], 481: [51], 610: [135], 715: [135, 610], 716: [135, 2381, 2961], 370: [135, 1337], 319: [135], 3124: [2260, 2337, 1849], 3107: [2699, 2441, 2840], 1771: [1770, 2123], 1704: [1285], 1503: [1285], 2000: [1845], 1424: [1628], 1753: [1628, 1949], 1949: [1628], 1752: [1628, 1949], 1450: [2119], 1426: [2119], 848: [109, 515, 980, 348, 814, 816, 509, 983, 984], 2545: [3274], 3103: [3274], 330: [137], 1530: [2008], 1435: [2008], 2784: [2402, 2432, 2553], 2553: [2402, 2405], 2785: [2402, 2405, 2553], 3031: [2402, 2432], 1857: [1858], 810: [246], 2559: [246], 343: [869], 344: [2570], 2571: [2570], 2053: [1273], 1454: [1273], 1827: [2072], 2054: [2072], 1455: [2072], 1878: [2072, 1684], 2574: [3217], 2025: [1235], 1550: [1235], 1588: [1235], 1749: [1235], 1458: [1850], 1648: [1243], 1706: [1243, 1224, 1258, 610], 375: [630, 515], 981: [982], 2579: [2407, 351], 2580: [2579, 2770], 1589: [1590, 1128], 1463: [1590], 350: [515], 1465: [1298], 1770: [1298, 2123], 2586: [2585, 2586, 2584], 781: [2585, 858], 2584: [2586], 2585: [2584], 363: [363], 765: [363], 521: [701, 616], 946: [706, 1238], 2592: [3296], 853: [3296, 753, 1252], 2594: [2325], 1601: [1569], 1473: [1569], 763: [374, 93, 800, 805], 897: [3093, 557], 2599: [3093], 2600: [3109], 2998: [3109], 3087: [3109], 1660: [1661], 1525: [1661], 2010: [2012], 2011: [2012], 1879: [2012], 2603: [2271, 2602], 2652: [2271], 2704: [2602], 798: [713], 1843: [1738], 1493: [1738], 2616: [2617, 2618], 2618: [2617], 2614: [2617], 2619: [2618], 618: [240], 794: [240], 388: [579], 973: [196], 756: [334], 440: [10], 414: [58], 658: [82], 456: [147], 927: [147, 456], 582: [200], 963: [212, 2129], 681: [219, 231, 248, 2461], 541: [507], 873: [576], 960: [711, 2415], 665: [727, 728], 968: [727], 407: [812], 400: [402, 405], 401: [402, 405, 400], 3185: [402, 2864], 403: [405], 404: [405, 401, 403], 405: [2134], 1511: [2134], 406: [789], 409: [191], 1501: [1502], 1877: [1502], 509: [174], 492: [570], 564: [975, 775], 2868: [2466], 413: [2466], 496: [142], 530: [879], 419: [281, 360], 1898: [1899], 1507: [1150], 1957: [1409], 1681: [1409], 2640: [2639], 2643: [2645], 2644: [2645], 433: [915], 2646: [2282], 2995: [2765], 3210: [2970], 3021: [1529, 2166], 2169: [1529, 2166, 1133, 1170], 3305: [1529, 2166], 2654: [1529, 2166], 800: [93], 1537: [1394], 2668: [2669, 2670], 1540: [1266], 1861: [1266], 2774: [2293], 1796: [1326], 1662: [1326], 3156: [3159], 2678: [2382], 2894: [556, 2266], 2688: [2687], 2689: [2687], 1553: [1554], 1831: [1554, 1336], 474: [90], 1631: [90, 1129], 477: [753], 1562: [1321], 2699: [2441], 486: [138], 487: [42, 271, 950], 611: [950], 601: [17, 834], 832: [17], 831: [834], 613: [955, 611], 834: [964], 3058: [3231], 2700: [3231], 2702: [2647], 490: [805], 2705: [2864], 2863: [2864], 666: [667], 995: [935], 712: [935], 657: [935], 743: [979], 3244: [2438, 2667], 722: [721], 507: [206], 1628: [206], 3256: [2653], 2713: [2653], 2827: [2873], 1929: [2078, 1698], 1582: [2078], 1590: [1128], 1592: [1593], 1594: [1593], 1692: [1593], 1602: [1829], 3037: [1829], 2731: [2337], 791: [188], 2739: [2707], 2982: [2707], 2742: [1122], 1618: [1452], 1619: [1188], 1622: [1275], 1625: [1315], 1810: [1315], 2009: [1224], 2752: [2889], 2820: [2889], 3207: [2257], 1638: [1181, 1375, 1356, 1637], 2759: [2610], 2823: [2610], 1640: [1810], 1694: [1810], 3174: [2728], 891: [2728], 3208: [2791], 857: [775], 3040: [770], 3041: [770, 2415], 1856: [1868], 1652: [1868], 2923: [95], 575: [549], 577: [965], 2777: [2272], 3007: [2272], 2135: [1194, 1680], 809: [808, 136, 807], 587: [808], 2790: [2461], 1683: [1325, 1684], 1839: [1325], 2795: [1241], 1689: [2176], 1908: [2176], 595: [593, 594], 1691: [1992], 1815: [1992], 980: [160, 990], 596: [160], 2805: [2807], 2806: [2807], 606: [199], 1699: [1334], 1804: [1334], 1702: [1703], 1820: [1617], 3161: [2295], 3234: [3205], 619: [920], 2839: [2840], 621: [623], 625: [623], 1729: [1717], 1717: [1717, 1887], 1888: [1887], 1722: [2068], 2069: [2068], 850: [313, 604], 898: [197], 647: [197, 898], 2031: [946, 1238], 2874: [2972, 3105], 3220: [2972, 3105], 2022: [1784, 1790, 1927], 1927: [1784, 1785, 1790], 1928: [1784, 1785, 1790, 1927], 1786: [1784], 1756: [1784, 1785], 3046: [3201], 656: [166], 1762: [1911], 1910: [1911], 937: [877], 718: [877], 662: [877], 856: [877], 660: [986, 40, 875], 948: [328], 837: [835], 908: [852], 727: [728], 1768: [1099], 1767: [1099], 954: [379], 1780: [1288], 1782: [1576], 1935: [1576], 1789: [2363], 2927: [2363], 1792: [1794], 2048: [1777, 1846], 1793: [1777], 2929: [2867], 3019: [2867], 2930: [2419, 2980], 994: [752, 990], 751: [752], 2934: [2308], 3270: [2308], 2107: [2108], 1798: [2108], 2936: [2869], 3170: [2869], 2145: [1805, 2054], 1806: [1805], 1809: [1974], 2953: [2950], 2956: [2950], 3173: [2251], 3186: [2381, 2961], 1906: [2048, 2826], 1819: [2048], 3303: [2399], 737: [730, 320, 729], 942: [730], 736: [320], 730: [320], 3138: [2549], 2981: [2549], 2984: [2454], 2985: [2454, 2984, 2986], 2983: [2454, 2984, 2986], 1847: [1790], 1915: [1790], 3008: [3009], 2018: [1295], 3232: [3233], 2996: [3233], 750: [712], 3155: [874, 2429], 760: [193], 758: [193], 759: [193], 1869: [1873], 1920: [1663], 1873: [1663], 3032: [2477], 3259: [2477], 1893: [1894], 3051: [2234, 2510], 3052: [2234, 2510], 1901: [1300], 2033: [2032], 3065: [3066], 3067: [3066], 3123: [2826, 1698], 3081: [3082], 3084: [3082], 1913: [2143], 2142: [2143], 2098: [1276], 2077: [2175], 2159: [2175], 3096: [3097, 3182], 3095: [3097, 3182], 1925: [2075], 3100: [3101], 3102: [3101], 3106: [2428], 3178: [2428], 966: [186], 912: [826], 1968: [474], 2160: [1968], 1970: [1398, 1400], 1971: [1398, 1400], 2019: [1398, 1400], 3146: [3150], 3149: [3150, 3146], 3148: [3146], 874: [2429], 2109: [2110], 2008: [1341], 2056: [1341], 3290: [2334, 2444, 3168], 3167: [2334, 2444, 3168], 3168: [2444], 893: [703], 3188: [2247], 3189: [2247], 3273: [446], 909: [446], 2046: [1133], 3204: [2468], 2055: [2054], 2089: [2087], 2085: [2087], 2100: [2102], 2101: [2102], 3240: [3222], 3281: [3222], 941: [938], 3260: [3263], 3261: [3263], 3268: [3266], 2130: [2129], 2131: [2129], 989: [990], 2166: [1170]}
The hypergraph features for node 0, index 0 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.13333321  0.2382891  -0.31035982  0.2382891   0.67195495]]
The hypergraph features for node 953, index 1 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.13333321 -0.33952797 -0.73643059 -0.73643059  0.39690262]]
The hypergraph features for node 384, index 2 are 
 [[0. 1. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.13333321 -1.13333321 -1.13333321 -1.13333321  0.        ]]
The hypergraph features for node 323, index 3 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.13333321  0.31569135 -0.32811296 -0.16669703  0.60247233]]
The hypergraph features for node 173, index 4 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.13333321  0.         -0.37777774  0.          0.5342584 ]]
The hypergraph features for node 383, index 5 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.13333321  0.         -0.37777774  0.          0.5342584 ]]
The hypergraph features for node 86, index 6 are 
 [[1. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.03248266  0.38290265  0.07331237 -0.03248266  0.14896547]]
The hypergraph features for node 691, index 7 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.41377223  0.49804342  0.10404045  0.12383125  0.22509235]]
The hypergraph features for node 271, index 8 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.2382891 0.2382891 0.2382891 0.2382891 0.       ]]
The hypergraph features for node 290, index 9 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.2382891  0.38290265 0.31815822 0.33328291 0.0599991 ]]
The hypergraph features for node 540, index 10 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.2382891 0.2382891 0.2382891 0.2382891 0.       ]]
The hypergraph features for node 1, index 11 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.66666675  0.2382891  -0.39669618  0.2382891   0.8980048 ]]
The hypergraph features for node 291, index 12 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.03578424  0.51352639  0.18054807  0.3784898   0.38833222]]
The hypergraph features for node 422, index 13 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.63684154  0.51352639  0.0533582  -0.01246057  0.28961069]]
The hypergraph features for node 390, index 14 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.00021172  0.54552976 -0.01545819 -0.01614304  0.35744193]]
The hypergraph features for node 522, index 15 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.63684154  0.49804342 -0.01034919 -0.00769789  0.2612929 ]]
The hypergraph features for node 714, index 16 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.63684154  0.3784898  -0.02895554 -0.01284447  0.2806138 ]]
The hypergraph features for node 748, index 17 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.63684154  0.51352639  0.06058986  0.16337156  0.32019024]]
The hypergraph features for node 275, index 18 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.63684154  0.49804342 -0.03542888 -0.01284447  0.28729464]]
The hypergraph features for node 699, index 19 are 
 [[0. 1. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.34452717  0.51352639  0.03339733 -0.01284447  0.24069948]]
The hypergraph features for node 28, index 20 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.01284447  0.2382891   0.09855291  0.16337156  0.09954088]]
The hypergraph features for node 646, index 21 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.09883769  0.38765102  0.16340077  0.15836866  0.16178412]]
The hypergraph features for node 905, index 22 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.84751308  0.90345629 -0.01833612  0.05124215  0.39085473]]
The hypergraph features for node 266, index 23 are 
 [[0. 1. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.18913507  0.65104993  0.06740604 -0.0890531   0.2724921 ]]
The hypergraph features for node 780, index 24 are 
 [[0. 1. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.33304143  0.38765102  0.09029314  0.03785768  0.19954497]]
The hypergraph features for node 533, index 25 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.22036536 0.2382891  0.22932723 0.22932723 0.00896187]]
The hypergraph features for node 534, index 26 are 
 [[0. 1. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.2382891 0.2382891 0.2382891 0.2382891 0.       ]]
The hypergraph features for node 284, index 27 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.16608846 0.2382891  0.20218878 0.20218878 0.03610032]]
The hypergraph features for node 31, index 28 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.2382891  0.65104993 0.58032235 0.65104993 0.12596798]]
The hypergraph features for node 289, index 29 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.2382891 0.2382891 0.2382891 0.2382891 0.       ]]
The hypergraph features for node 295, index 30 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.         0.65104993 0.18315581 0.22778262 0.1943996 ]]
The hypergraph features for node 429, index 31 are 
 [[0. 1. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.181585   0.39841863 0.22717462 0.181585   0.07933231]]
The hypergraph features for node 430, index 32 are 
 [[0. 1. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.2382891 0.2382891 0.2382891 0.2382891 0.       ]]
The hypergraph features for node 78, index 33 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.22036536 0.2382891  0.22932723 0.22932723 0.00896187]]
The hypergraph features for node 471, index 34 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.84751308  0.65104993  0.16024391  0.10076948  0.36371458]]
The hypergraph features for node 218, index 35 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.18913507  0.65104993  0.17109889  0.19568865  0.26113424]]
The hypergraph features for node 862, index 36 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.66666675  0.54901731 -0.09501149  0.          0.56892313]]
The hypergraph features for node 876, index 37 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.18913507  0.65104993  0.14373718  0.1530882   0.26358781]]
The hypergraph features for node 505, index 38 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.         0.41503509 0.17793985 0.22036536 0.15462217]]
The hypergraph features for node 1850, index 39 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.66666675  0.54901731 -0.0086419   0.27745622  0.75044235]]
The hypergraph features for node 1129, index 40 are 
 [[0. 1. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.66666675 -0.29719605 -0.9819314  -0.9819314   0.68473535]]
The hypergraph features for node 1011, index 41 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.66666675 -0.66297483 -0.99753881 -0.66297483  0.4731449 ]]
The hypergraph features for node 2007, index 42 are 
 [[0. 1. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.66666675 -1.66666675 -1.66666675 -1.66666675  0.        ]]
The hypergraph features for node 2040, index 43 are 
 [[0. 1. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.66297483 -0.66297483 -0.66297483 -0.66297483  0.        ]]
The hypergraph features for node 2, index 44 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.66297483 -0.66297483 -0.66297483 -0.66297483  0.        ]]
The hypergraph features for node 3151, index 45 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.66297483  0.07747366 -0.29275059 -0.29275059  0.37022425]]
The hypergraph features for node 648, index 46 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.39896945 -0.39896945 -0.39896945 -0.39896945  0.        ]]
The hypergraph features for node 2865, index 47 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.39896945  0.09827193 -0.15034876 -0.15034876  0.24862069]]
The hypergraph features for node 1012, index 48 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.39896945 -0.39896945 -0.39896945 -0.39896945  0.        ]]
The hypergraph features for node 1349, index 49 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.39896945 -0.39896945 -0.39896945 -0.39896945  0.        ]]
The hypergraph features for node 1500, index 50 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.39896945  0.10370222 -0.07825437 -0.00887512  0.19078347]]
The hypergraph features for node 1013, index 51 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.39896945 -0.00887512 -0.10639871 -0.00887512  0.1689158 ]]
The hypergraph features for node 1725, index 52 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.39896945 -0.00887512 -0.10639871 -0.00887512  0.1689158 ]]
The hypergraph features for node 1545, index 53 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.0890531  -0.00887512 -0.04896411 -0.04896411  0.04008899]]
The hypergraph features for node 1058, index 54 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.00887512 -0.00887512 -0.00887512 -0.00887512  0.        ]]
The hypergraph features for node 1445, index 55 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.00887512 -0.00887512 -0.00887512 -0.00887512  0.        ]]
The hypergraph features for node 2118, index 56 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.14388468 -0.00887512 -0.0763799  -0.0763799   0.06750478]]
The hypergraph features for node 1801, index 57 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.00887512  0.181585    0.08635494  0.08635494  0.09523006]]
The hypergraph features for node 1236, index 58 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.14388468 -0.00887512 -0.0763799  -0.0763799   0.06750478]]
The hypergraph features for node 1046, index 59 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.00887512  0.43558432  0.10745248  0.03685106  0.16501897]]
The hypergraph features for node 1744, index 60 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.00887512  0.44969376  0.14398117 -0.00887512  0.21617145]]
The hypergraph features for node 1380, index 61 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.00887512  0.44969376  0.14398117 -0.00887512  0.21617145]]
The hypergraph features for node 1733, index 62 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.00887512  0.16337156  0.07724822  0.07724822  0.08612334]]
The hypergraph features for node 1014, index 63 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.00887512 -0.00887512 -0.00887512 -0.00887512  0.        ]]
The hypergraph features for node 1059, index 64 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.00887512  0.44969376  0.14398117 -0.00887512  0.21617145]]
The hypergraph features for node 1680, index 65 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.09883769  0.44969376  0.11399365 -0.00887512  0.24020025]]
The hypergraph features for node 1088, index 66 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.00887512 -0.00887512 -0.00887512 -0.00887512  0.        ]]
The hypergraph features for node 1891, index 67 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.00887512 -0.00887512 -0.00887512 -0.00887512  0.        ]]
The hypergraph features for node 1219, index 68 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.00887512 -0.00887512 -0.00887512 -0.00887512  0.        ]]
The hypergraph features for node 1860, index 69 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.00887512  0.08347921  0.02486803  0.          0.04160244]]
The hypergraph features for node 1067, index 70 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.00887512 -0.00887512 -0.00887512 -0.00887512  0.        ]]
The hypergraph features for node 1070, index 71 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.00887512 -0.00887512 -0.00887512 -0.00887512  0.        ]]
The hypergraph features for node 1015, index 72 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.00887512 -0.00887512 -0.00887512 -0.00887512  0.        ]]
The hypergraph features for node 1396, index 73 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.00887512 -0.00887512 -0.00887512 -0.00887512  0.        ]]
The hypergraph features for node 1687, index 74 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.00887512  0.00251163 -0.00128395  0.00251163  0.00536777]]
The hypergraph features for node 441, index 75 are 
 [[0. 1. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.66367388  0.49804342  0.08617162  0.181585    0.27179359]]
The hypergraph features for node 1242, index 76 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.00887512  0.00251163 -0.00318175 -0.00318175  0.00569338]]
The hypergraph features for node 1150, index 77 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.00887512  0.00251163  0.00061384  0.00251163  0.00424359]]
The hypergraph features for node 1444, index 78 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.00887512 -0.00887512 -0.00887512 -0.00887512  0.        ]]
The hypergraph features for node 1478, index 79 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.00887512 -0.00887512 -0.00887512 -0.00887512  0.        ]]
The hypergraph features for node 1083, index 80 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.00887512  0.49804342  0.19389229 -0.00887512  0.24833835]]
The hypergraph features for node 1439, index 81 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.00887512 -0.00887512 -0.00887512 -0.00887512  0.        ]]
The hypergraph features for node 1397, index 82 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.00887512 -0.00887512 -0.00887512 -0.00887512  0.        ]]
The hypergraph features for node 2183, index 83 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.07571537 0.07571537 0.07571537 0.07571537 0.        ]]
The hypergraph features for node 79, index 84 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.00251163 0.60016119 0.28960028 0.22036536 0.21261353]]
The hypergraph features for node 32, index 85 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.34452717  0.90345629  0.22615753  0.11954346  0.5156556 ]]
The hypergraph features for node 685, index 86 are 
 [[1. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.34452717  0.90345629  0.22615753  0.11954346  0.5156556 ]]
The hypergraph features for node 460, index 87 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.07571537 0.25866617 0.17625041 0.17925562 0.05451988]]
The hypergraph features for node 1016, index 88 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.07571537 0.07571537 0.07571537 0.07571537 0.        ]]
The hypergraph features for node 1620, index 89 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.07571537 0.07571537 0.07571537 0.07571537 0.        ]]
The hypergraph features for node 3, index 90 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.07571537 0.07571537 0.07571537 0.07571537 0.        ]]
The hypergraph features for node 749, index 91 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.07571537 0.07571537 0.07571537 0.07571537 0.        ]]
The hypergraph features for node 304, index 92 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.07571537 0.07571537 0.07571537 0.07571537 0.        ]]
The hypergraph features for node 819, index 93 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.07571537 0.07571537 0.07571537 0.07571537 0.        ]]
The hypergraph features for node 503, index 94 are 
 [[0. 1. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.07571537 0.07571537 0.07571537 0.07571537 0.        ]]
The hypergraph features for node 120, index 95 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.07571537 0.07571537 0.07571537 0.07571537 0.        ]]
The hypergraph features for node 2184, index 96 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.07571537 0.07571537 0.07571537 0.07571537 0.        ]]
The hypergraph features for node 2306, index 97 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.07571537 0.07571537 0.07571537 0.07571537 0.        ]]
The hypergraph features for node 2261, index 98 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.07571537 0.07571537 0.07571537 0.07571537 0.        ]]
The hypergraph features for node 2185, index 99 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.07571537 0.07571537 0.07571537 0.07571537 0.        ]]
The hypergraph features for node 3042, index 100 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.07571537 0.07571537 0.07571537 0.07571537 0.        ]]
The hypergraph features for node 2812, index 101 are 
 [[1. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.07571537 0.07571537 0.07571537 0.07571537 0.        ]]
The hypergraph features for node 1624, index 102 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.07571537 0.07571537 0.07571537 0.07571537 0.        ]]
The hypergraph features for node 1017, index 103 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.07571537 0.07571537 0.07571537 0.07571537 0.        ]]
The hypergraph features for node 978, index 104 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.07571537 0.36496251 0.22033894 0.22033894 0.14462357]]
The hypergraph features for node 1549, index 105 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.07571537 0.41503509 0.30192852 0.41503509 0.15995685]]
The hypergraph features for node 326, index 106 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.07571537 0.36496251 0.29265072 0.36496251 0.12524769]]
The hypergraph features for node 739, index 107 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.         0.16337156 0.10061462 0.11954346 0.06822782]]
The hypergraph features for node 4, index 108 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.07571537 0.07571537 0.07571537 0.07571537 0.        ]]
The hypergraph features for node 294, index 109 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.07571537 0.07571537 0.07571537 0.07571537 0.        ]]
The hypergraph features for node 2091, index 110 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.07571537 0.07571537 0.07571537 0.07571537 0.        ]]
The hypergraph features for node 1019, index 111 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.07571537 0.07571537 0.07571537 0.07571537 0.        ]]
The hypergraph features for node 30, index 112 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.33304143  0.38765102  0.11320337  0.03785768  0.24107813]]
The hypergraph features for node 97, index 113 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.07571537 0.12383125 0.08774434 0.07571537 0.02083479]]
The hypergraph features for node 5, index 114 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.07571537 0.07571537 0.07571537 0.07571537 0.        ]]
The hypergraph features for node 329, index 115 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.07571537 0.31569135 0.17898763 0.14555617 0.10854871]]
The hypergraph features for node 274, index 116 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.777722    0.07571537 -0.37999487 -0.63684154  0.37562506]]
The hypergraph features for node 916, index 117 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.         0.46595803 0.17027664 0.12383125 0.13534333]]
The hypergraph features for node 661, index 118 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.63684154  0.89484407 -0.09183579  0.          0.39650164]]
The hypergraph features for node 63, index 119 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.07571537 0.07571537 0.07571537 0.07571537 0.        ]]
The hypergraph features for node 1896, index 120 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.07571537 0.07571537 0.07571537 0.07571537 0.        ]]
The hypergraph features for node 1021, index 121 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.07571537 0.07571537 0.07571537 0.07571537 0.        ]]
The hypergraph features for node 643, index 122 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.777722    0.89484407  0.01404179 -0.01156116  0.45408814]]
The hypergraph features for node 6, index 123 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.16293981  0.07571537 -0.04361222 -0.04361222  0.11932759]]
The hypergraph features for node 1024, index 124 are 
 [[0. 1. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.16293981 -0.16293981 -0.16293981 -0.16293981  0.        ]]
The hypergraph features for node 1077, index 125 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.16293981 -0.16293981 -0.16293981 -0.16293981  0.        ]]
The hypergraph features for node 514, index 126 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.16293981 -0.12711139 -0.1450256  -0.1450256   0.01791421]]
The hypergraph features for node 867, index 127 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.16293981 -0.12486577 -0.14390279 -0.14390279  0.01903702]]
The hypergraph features for node 677, index 128 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.16293981  0.19466809  0.03372698  0.13088619  0.14738477]]
The hypergraph features for node 8, index 129 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.16293981 -0.16293981 -0.16293981 -0.16293981  0.        ]]
The hypergraph features for node 44, index 130 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.16293981  0.5272129   0.18213655  0.18213655  0.34507635]]
The hypergraph features for node 14, index 131 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.16293981 -0.16293981 -0.16293981 -0.16293981  0.        ]]
The hypergraph features for node 270, index 132 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.16293981 -0.16293981 -0.16293981 -0.16293981  0.        ]]
The hypergraph features for node 431, index 133 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.16293981 -0.16293981 -0.16293981 -0.16293981  0.        ]]
The hypergraph features for node 308, index 134 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.16293981 -0.16293981 -0.16293981 -0.16293981  0.        ]]
The hypergraph features for node 341, index 135 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.16293981 -0.16293981 -0.16293981 -0.16293981  0.        ]]
The hypergraph features for node 925, index 136 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.16293981 -0.16293981 -0.16293981 -0.16293981  0.        ]]
The hypergraph features for node 478, index 137 are 
 [[1. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.16293981 -0.16293981 -0.16293981 -0.16293981  0.        ]]
The hypergraph features for node 2105, index 138 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.00571215  0.26621402  0.07111184  0.          0.10564581]]
The hypergraph features for node 1634, index 139 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.30618523  0.56075762  0.21826967  0.43558432  0.28663054]]
The hypergraph features for node 1029, index 140 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 1695, index 141 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.         0.43558432 0.21779216 0.21779216 0.21779216]]
The hypergraph features for node 1401, index 142 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 1538, index 143 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 1141, index 144 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 1911, index 145 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 2128, index 146 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 1941, index 147 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 1030, index 148 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 1794, index 149 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 2003, index 150 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.         0.43558432 0.21779216 0.21779216 0.21779216]]
The hypergraph features for node 1031, index 151 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 1930, index 152 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 961, index 153 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 1645, index 154 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 1488, index 155 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 568, index 156 are 
 [[0. 1. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 9, index 157 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.         0.33328291 0.16664145 0.16664145 0.16664145]]
The hypergraph features for node 583, index 158 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 385, index 159 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.33328291 0.44343065 0.38835678 0.38835678 0.05507387]]
The hypergraph features for node 386, index 160 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.0009821   0.33328291  0.16615041  0.16615041  0.1671325 ]]
The hypergraph features for node 387, index 161 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.10370222 0.33328291 0.21849256 0.21849256 0.11479034]]
The hypergraph features for node 932, index 162 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.0009821   0.44343065  0.24778211  0.27433994  0.16471275]]
The hypergraph features for node 933, index 163 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.         0.33328291 0.15416856 0.12922278 0.13720081]]
The hypergraph features for node 584, index 164 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.0009821   0.44343065  0.25857715  0.33328291  0.18896452]]
The hypergraph features for node 654, index 165 are 
 [[0. 1. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.0009821   0.33328291  0.10900076  0.05185111  0.13629742]]
The hypergraph features for node 847, index 166 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.53037278  0.66368589  0.15563579  0.14707265  0.28375513]]
The hypergraph features for node 33, index 167 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.33328291 0.33328291 0.33328291 0.33328291 0.        ]]
The hypergraph features for node 753, index 168 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.2982744   0.33328291  0.07130312  0.          0.2360613 ]]
The hypergraph features for node 398, index 169 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 824, index 170 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 158, index 171 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 434, index 172 are 
 [[0. 1. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.181585 0.187497 0.184541 0.184541 0.002956]]
The hypergraph features for node 11, index 173 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.187497 0.187497 0.187497 0.187497 0.      ]]
The hypergraph features for node 204, index 174 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.10370222 0.187497   0.14559961 0.14559961 0.04189739]]
The hypergraph features for node 1033, index 175 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.07460831 0.187497   0.11223788 0.07460831 0.05321624]]
The hypergraph features for node 1946, index 176 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.187497 0.187497 0.187497 0.187497 0.      ]]
The hypergraph features for node 1363, index 177 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.187497   0.36496251 0.27622976 0.27622976 0.08873275]]
The hypergraph features for node 2014, index 178 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.07460831 0.187497   0.13105266 0.13105266 0.05644435]]
The hypergraph features for node 2015, index 179 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.07460831 0.07460831 0.07460831 0.07460831 0.        ]]
The hypergraph features for node 1035, index 180 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.07460831 0.07460831 0.07460831 0.07460831 0.        ]]
The hypergraph features for node 1054, index 181 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.07460831 0.07460831 0.07460831 0.07460831 0.        ]]
The hypergraph features for node 1037, index 182 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.31399939 0.88388427 0.69392264 0.88388427 0.26864631]]
The hypergraph features for node 1615, index 183 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.56075762 0.88388427 0.72232094 0.72232094 0.16156332]]
The hypergraph features for node 1040, index 184 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.31399939 0.88388427 0.57397957 0.54901731 0.20302515]]
The hypergraph features for node 1411, index 185 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.31399939 0.31399939 0.31399939 0.31399939 0.        ]]
The hypergraph features for node 1475, index 186 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.31399939 0.60016119 0.45708029 0.45708029 0.1430809 ]]
The hypergraph features for node 1042, index 187 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.31399939 0.56075762 0.4373785  0.4373785  0.12337912]]
The hypergraph features for node 1047, index 188 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.03685106 0.31399939 0.17800122 0.18830519 0.12405153]]
The hypergraph features for node 2196, index 189 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.31399939 0.31399939 0.31399939 0.31399939 0.        ]]
The hypergraph features for node 972, index 190 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.31399939 0.38290265 0.33696714 0.31399939 0.03248131]]
The hypergraph features for node 2901, index 191 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.31399939 0.31399939 0.31399939 0.31399939 0.        ]]
The hypergraph features for node 55, index 192 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.31399939 0.38290265 0.34845102 0.34845102 0.03445163]]
The hypergraph features for node 1776, index 193 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.31399939 0.31399939 0.31399939 0.31399939 0.        ]]
The hypergraph features for node 1769, index 194 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.31399939 0.31399939 0.31399939 0.31399939 0.        ]]
The hypergraph features for node 1090, index 195 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.04231091  0.31399939  0.22492181  0.31399939  0.15428689]]
The hypergraph features for node 1038, index 196 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.31399939 0.54901731 0.34757338 0.31399939 0.08223914]]
The hypergraph features for node 1924, index 197 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.14795573  0.31399939  0.05534789  0.          0.19261044]]
The hypergraph features for node 1107, index 198 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.1666553  0.31399939 0.25768047 0.29238671 0.06496647]]
The hypergraph features for node 1174, index 199 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.1666553  0.31399939 0.25768047 0.29238671 0.06496647]]
The hypergraph features for node 57, index 200 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.16511349  0.54901731  0.23180527  0.31399939  0.22041458]]
The hypergraph features for node 2119, index 201 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.31399939 0.31399939 0.31399939 0.31399939 0.        ]]
The hypergraph features for node 1636, index 202 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.30618523  0.31399939 -0.05243311 -0.16511349  0.26543033]]
The hypergraph features for node 1937, index 203 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.54901731 0.54901731 0.54901731 0.54901731 0.        ]]
The hypergraph features for node 1039, index 204 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.54901731 0.54901731 0.54901731 0.54901731 0.        ]]
The hypergraph features for node 1599, index 205 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.03685106 0.65104993 0.42831598 0.54901731 0.21784123]]
The hypergraph features for node 1371, index 206 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.54901731 0.54901731 0.54901731 0.54901731 0.        ]]
The hypergraph features for node 1076, index 207 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.54901731 0.54901731 0.54901731 0.54901731 0.        ]]
The hypergraph features for node 1041, index 208 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.60016119 0.60016119 0.60016119 0.60016119 0.        ]]
The hypergraph features for node 1307, index 209 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.11421325  0.60016119  0.24297397  0.24297397  0.35718722]]
The hypergraph features for node 1119, index 210 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.60016119 0.60016119 0.60016119 0.60016119 0.        ]]
The hypergraph features for node 1667, index 211 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.19185595 0.65104993 0.45625618 0.56075762 0.18517598]]
The hypergraph features for node 1543, index 212 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.19185595 0.60016119 0.38615768 0.37630678 0.19480052]]
The hypergraph features for node 2202, index 213 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.56075762 0.60016119 0.57060851 0.56075762 0.01706225]]
The hypergraph features for node 3153, index 214 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.60016119 0.60016119 0.60016119 0.60016119 0.        ]]
The hypergraph features for node 3028, index 215 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.56075762 0.60016119 0.57060851 0.56075762 0.01706225]]
The hypergraph features for node 3162, index 216 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.60016119 0.60016119 0.60016119 0.60016119 0.        ]]
The hypergraph features for node 828, index 217 are 
 [[0. 0. 1. ... 0. 0. 0.]]
We add the encoding:
 [[0.19185595 0.60016119 0.39600857 0.39600857 0.20415262]]
The hypergraph features for node 1664, index 218 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.         0.56075762 0.26284675 0.22778262 0.23026709]]
The hypergraph features for node 1279, index 219 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.         0.56075762 0.28037881 0.28037881 0.28037881]]
The hypergraph features for node 1895, index 220 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.         0.56075762 0.28037881 0.28037881 0.28037881]]
The hypergraph features for node 1006, index 221 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.56075762 0.56075762 0.56075762 0.56075762 0.        ]]
The hypergraph features for node 882, index 222 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.56075762 0.56075762 0.56075762 0.56075762 0.        ]]
The hypergraph features for node 179, index 223 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.12987991  0.65104993  0.31731873  0.38765102  0.26002162]]
The hypergraph features for node 887, index 224 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.34452717  0.90345629  0.21527052  0.27745622  0.36392971]]
The hypergraph features for node 88, index 225 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.15246251  0.56075762  0.18364077  0.1666553   0.21086895]]
The hypergraph features for node 609, index 226 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.3784898  0.56075762 0.50000168 0.56075762 0.08592187]]
The hypergraph features for node 34, index 227 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.34452717  0.56075762  0.19436321  0.24445819  0.24941739]]
The hypergraph features for node 37, index 228 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.14540301 0.56075762 0.20473939 0.14540301 0.14534384]]
The hypergraph features for node 462, index 229 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.3784898  0.56075762 0.46962371 0.46962371 0.09113391]]
The hypergraph features for node 278, index 230 are 
 [[0. 1. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.14540301 0.56075762 0.20473939 0.14540301 0.14534384]]
The hypergraph features for node 1043, index 231 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.43558432 0.56075762 0.46061898 0.43558432 0.05006932]]
The hypergraph features for node 2139, index 232 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.08223394 0.43558432 0.30541229 0.39841863 0.15853865]]
The hypergraph features for node 1894, index 233 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.43558432 0.43558432 0.43558432 0.43558432 0.        ]]
The hypergraph features for node 1351, index 234 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.43558432 0.43558432 0.43558432 0.43558432 0.        ]]
The hypergraph features for node 1116, index 235 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.43558432 0.43558432 0.43558432 0.43558432 0.        ]]
The hypergraph features for node 1481, index 236 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.26621402 0.43558432 0.35089917 0.35089917 0.08468515]]
The hypergraph features for node 1892, index 237 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.43558432 0.43558432 0.43558432 0.43558432 0.        ]]
The hypergraph features for node 1044, index 238 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.43558432 0.43558432 0.43558432 0.43558432 0.        ]]
The hypergraph features for node 1045, index 239 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.43558432 0.43558432 0.43558432 0.43558432 0.        ]]
The hypergraph features for node 1470, index 240 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.43558432 0.43558432 0.43558432 0.43558432 0.        ]]
The hypergraph features for node 2080, index 241 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.17545551 0.43558432 0.37055212 0.43558432 0.11263908]]
The hypergraph features for node 1705, index 242 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.33328291 0.43558432 0.38443361 0.38443361 0.05115071]]
The hypergraph features for node 1916, index 243 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.12486577  0.43558432  0.17911988  0.22664109  0.23125711]]
The hypergraph features for node 1886, index 244 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.43558432 0.43558432 0.43558432 0.43558432 0.        ]]
The hypergraph features for node 1870, index 245 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.1731715   0.43558432  0.12751883  0.12383125  0.21525927]]
The hypergraph features for node 2002, index 246 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.         0.43558432 0.29038955 0.43558432 0.20533642]]
The hypergraph features for node 2050, index 247 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.12711139  0.43558432  0.19217903  0.19180139  0.15380195]]
The hypergraph features for node 1542, index 248 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.12486577  0.43558432  0.15535927  0.15535927  0.28022504]]
The hypergraph features for node 1423, index 249 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.31569135 0.43558432 0.34566459 0.31569135 0.05191518]]
The hypergraph features for node 1143, index 250 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.         0.43558432 0.21779216 0.21779216 0.21779216]]
The hypergraph features for node 2657, index 251 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.03685106 0.43558432 0.21800679 0.181585   0.16480688]]
The hypergraph features for node 1125, index 252 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.03685106 0.43558432 0.16976215 0.03685106 0.18796466]]
The hypergraph features for node 1902, index 253 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.03685106 0.43558432 0.23621769 0.23621769 0.19936663]]
The hypergraph features for node 1532, index 254 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.03685106 0.43558432 0.16976215 0.03685106 0.18796466]]
The hypergraph features for node 2658, index 255 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.03685106 0.181585   0.13334035 0.181585   0.06822823]]
The hypergraph features for node 1690, index 256 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.1221567   0.181585    0.06946609  0.10921803  0.12542361]]
The hypergraph features for node 2034, index 257 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.30618523  0.38290265  0.08834087  0.18830519  0.29006341]]
The hypergraph features for node 1775, index 258 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.18830519 0.18830519 0.18830519 0.18830519 0.        ]]
The hypergraph features for node 3099, index 259 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.18830519 0.18830519 0.18830519 0.18830519 0.        ]]
The hypergraph features for node 2206, index 260 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.18830519 0.18830519 0.18830519 0.18830519 0.        ]]
The hypergraph features for node 1050, index 261 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.18830519 0.18830519 0.18830519 0.18830519 0.        ]]
The hypergraph features for node 172, index 262 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.         0.18830519 0.0627684  0.         0.08876792]]
The hypergraph features for node 1524, index 263 are 
 [[0. 1. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.         0.18830519 0.12553679 0.18830519 0.08876792]]
The hypergraph features for node 1057, index 264 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.18830519 0.18830519 0.18830519 0.18830519 0.        ]]
The hypergraph features for node 1787, index 265 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.         0.18830519 0.09415259 0.09415259 0.09415259]]
The hypergraph features for node 12, index 266 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.16511349  0.18830519 -0.04730726 -0.16511349  0.16660316]]
The hypergraph features for node 502, index 267 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.24365318  0.54552976  0.13435335  0.181585    0.25890053]]
The hypergraph features for node 655, index 268 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.18830519 0.54552976 0.36691747 0.36691747 0.17861228]]
The hypergraph features for node 1740, index 269 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.16511349  0.38765102  0.11126877  0.11126877  0.27638226]]
The hypergraph features for node 364, index 270 are 
 [[0. 1. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.34452717  0.50998556  0.10058555  0.02745987  0.23446325]]
The hypergraph features for node 799, index 271 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.03578424  0.49804342 -0.05583869  0.10452243  0.52112741]]
The hypergraph features for node 1890, index 272 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.16511349 -0.16511349 -0.16511349 -0.16511349  0.        ]]
The hypergraph features for node 1253, index 273 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.16511349 -0.16511349 -0.16511349 -0.16511349  0.        ]]
The hypergraph features for node 1051, index 274 are 
 [[0. 1. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.16511349 -0.16511349 -0.16511349 -0.16511349  0.        ]]
The hypergraph features for node 2210, index 275 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.65327156 -0.65327156 -0.65327156 -0.65327156  0.        ]]
The hypergraph features for node 1707, index 276 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.65327156  0.2184887  -0.29475728 -0.449489    0.37233311]]
The hypergraph features for node 3085, index 277 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.65327156  0.         -0.32663578 -0.32663578  0.32663578]]
The hypergraph features for node 969, index 278 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.65327156 -0.33952797 -0.49639977 -0.49639977  0.15687179]]
The hypergraph features for node 1063, index 279 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.65327156 -0.449489   -0.55138028 -0.55138028  0.10189128]]
The hypergraph features for node 1066, index 280 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.10136391 0.10136391 0.10136391 0.10136391 0.        ]]
The hypergraph features for node 1055, index 281 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.10136391 0.10136391 0.10136391 0.10136391 0.        ]]
The hypergraph features for node 13, index 282 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.41377223 -0.41377223 -0.41377223 -0.41377223  0.        ]]
The hypergraph features for node 16, index 283 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.41377223 -0.41377223 -0.41377223 -0.41377223  0.        ]]
The hypergraph features for node 497, index 284 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.41377223 -0.41377223 -0.41377223 -0.41377223  0.        ]]
The hypergraph features for node 822, index 285 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.56069112  0.36496251 -0.10742266 -0.01246057  0.533752  ]]
The hypergraph features for node 472, index 286 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.41377223 -0.01246057 -0.2131164  -0.2131164   0.20065583]]
The hypergraph features for node 542, index 287 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.41377223 -0.41377223 -0.41377223 -0.41377223  0.        ]]
The hypergraph features for node 2213, index 288 are 
 [[0. 0. 1. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 3235, index 289 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 2214, index 290 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 18, index 291 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.44969376 0.44969376 0.44969376 0.44969376 0.        ]]
The hypergraph features for node 604, index 292 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.44969376 0.44969376 0.44969376 0.44969376 0.        ]]
The hypergraph features for node 20, index 293 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.44969376 0.44969376 0.44969376 0.44969376 0.        ]]
The hypergraph features for node 663, index 294 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.         0.66368589 0.37112655 0.44969376 0.27658554]]
The hypergraph features for node 1864, index 295 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.44969376 0.44969376 0.44969376 0.44969376 0.        ]]
The hypergraph features for node 1060, index 296 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.44969376 0.44969376 0.44969376 0.44969376 0.        ]]
The hypergraph features for node 100, index 297 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.44969376 0.44969376 0.44969376 0.44969376 0.        ]]
The hypergraph features for node 934, index 298 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.44969376 0.44969376 0.44969376 0.44969376 0.        ]]
The hypergraph features for node 679, index 299 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.44969376 0.44969376 0.44969376 0.44969376 0.        ]]
The hypergraph features for node 551, index 300 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.44969376 0.44969376 0.44969376 0.44969376 0.        ]]
The hypergraph features for node 1005, index 301 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.12711139  0.50998556  0.23382804  0.2864758   0.23211959]]
The hypergraph features for node 21, index 302 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.44969376 0.44969376 0.44969376 0.44969376 0.        ]]
The hypergraph features for node 439, index 303 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.43492627  0.50998556  0.09837026  0.02745987  0.28027281]]
The hypergraph features for node 698, index 304 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.43492627  0.51352639  0.08066811  0.          0.28024554]]
The hypergraph features for node 22, index 305 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.44969376 0.49804342 0.48192687 0.49804342 0.02279225]]
The hypergraph features for node 1008, index 306 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.44969376 0.44969376 0.44969376 0.44969376 0.        ]]
The hypergraph features for node 554, index 307 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.27547288  0.49804342  0.0175722  -0.01284447  0.25977095]]
The hypergraph features for node 598, index 308 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.23723134  0.49804342  0.21310063  0.3784898   0.32215155]]
The hypergraph features for node 616, index 309 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.39093649  0.51352639  0.13997045  0.04299955  0.26785898]]
The hypergraph features for node 268, index 310 are 
 [[0. 1. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.49804342 0.49804342 0.49804342 0.49804342 0.        ]]
The hypergraph features for node 123, index 311 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.22306526  0.49804342  0.14012706  0.14540301  0.29441502]]
The hypergraph features for node 701, index 312 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.49804342 0.49804342 0.49804342 0.49804342 0.        ]]
The hypergraph features for node 766, index 313 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.49804342 0.49804342 0.49804342 0.49804342 0.        ]]
The hypergraph features for node 2489, index 314 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.49804342 0.49804342 0.49804342 0.49804342 0.        ]]
The hypergraph features for node 2219, index 315 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.49804342 0.49804342 0.49804342 0.49804342 0.        ]]
The hypergraph features for node 2242, index 316 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.49804342 0.49804342 0.49804342 0.49804342 0.        ]]
The hypergraph features for node 738, index 317 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.14707265 0.49804342 0.32255803 0.32255803 0.17548539]]
The hypergraph features for node 3080, index 318 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.49804342 0.49804342 0.49804342 0.49804342 0.        ]]
The hypergraph features for node 2221, index 319 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.49804342 0.49804342 0.49804342 0.49804342 0.        ]]
The hypergraph features for node 2745, index 320 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.         0.49804342 0.27421408 0.29940645 0.22664705]]
The hypergraph features for node 53, index 321 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.35921465 0.49804342 0.42862904 0.42862904 0.06941438]]
The hypergraph features for node 23, index 322 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.49804342 0.49804342 0.49804342 0.49804342 0.        ]]
The hypergraph features for node 855, index 323 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-4.54187393e-05  6.63685888e-01  2.54879982e-01  2.20445496e-01
   2.28017892e-01]]
The hypergraph features for node 24, index 324 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.25866617 0.25866617 0.25866617 0.25866617 0.        ]]
The hypergraph features for node 297, index 325 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.84751308  0.54552976  0.07874919  0.10076948  0.411377  ]]
The hypergraph features for node 769, index 326 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.34452717  0.90345629  0.21895042  0.2204455   0.29879582]]
The hypergraph features for node 903, index 327 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.25866617 0.25866617 0.25866617 0.25866617 0.        ]]
The hypergraph features for node 263, index 328 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.03248266  0.25866617  0.08990872  0.10076948  0.07698316]]
The hypergraph features for node 378, index 329 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.32253191  0.25866617 -0.00762691  0.04098501  0.23975007]]
The hypergraph features for node 99, index 330 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.12383125 0.25866617 0.19124871 0.19124871 0.06741746]]
The hypergraph features for node 164, index 331 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.25866617 0.25866617 0.25866617 0.25866617 0.        ]]
The hypergraph features for node 518, index 332 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.25866617 0.25866617 0.25866617 0.25866617 0.        ]]
The hypergraph features for node 423, index 333 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.25866617 0.25866617 0.25866617 0.25866617 0.        ]]
The hypergraph features for node 552, index 334 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.25866617 0.25866617 0.25866617 0.25866617 0.        ]]
The hypergraph features for node 588, index 335 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.25866617 0.25866617 0.25866617 0.25866617 0.        ]]
The hypergraph features for node 2222, index 336 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.449489 -0.449489 -0.449489 -0.449489  0.      ]]
The hypergraph features for node 2566, index 337 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.449489 -0.449489 -0.449489 -0.449489  0.      ]]
The hypergraph features for node 1764, index 338 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.01600969 0.01600969 0.01600969 0.01600969 0.        ]]
The hypergraph features for node 1064, index 339 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.01600969 0.01600969 0.01600969 0.01600969 0.        ]]
The hypergraph features for node 1900, index 340 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.01600969 0.01600969 0.01600969 0.01600969 0.        ]]
The hypergraph features for node 1230, index 341 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.01600969 0.53961465 0.40871341 0.53961465 0.2267276 ]]
The hypergraph features for node 795, index 342 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.4461019   0.41503509 -0.07579279 -0.13605218  0.32846189]]
The hypergraph features for node 1918, index 343 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.         0.33725635 0.15374127 0.22664109 0.12217729]]
The hypergraph features for node 1727, index 344 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.01600969 0.53961465 0.40871341 0.53961465 0.2267276 ]]
The hypergraph features for node 296, index 345 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.         0.54552976 0.15264264 0.03252041 0.22752141]]
The hypergraph features for node 265, index 346 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.24365318  0.04903113 -0.0533192   0.          0.12182985]]
The hypergraph features for node 461, index 347 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.         0.04903113 0.02168027 0.01600969 0.02041453]]
The hypergraph features for node 338, index 348 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.32681682  0.04903113 -0.19004796 -0.32681682  0.16732825]]
The hypergraph features for node 276, index 349 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.32681682  0.04903113 -0.19004796 -0.32681682  0.16732825]]
The hypergraph features for node 372, index 350 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.24365318  0.54552976  0.02153692  0.00800484  0.22849786]]
The hypergraph features for node 25, index 351 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.01600969 0.04903113 0.03582255 0.04903113 0.01617713]]
The hypergraph features for node 888, index 352 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.01600969 0.01600969 0.01600969 0.01600969 0.        ]]
The hypergraph features for node 561, index 353 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.22306526  0.04903113 -0.08701707 -0.08701707  0.13604819]]
The hypergraph features for node 537, index 354 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.04903113 0.04903113 0.04903113 0.04903113 0.        ]]
The hypergraph features for node 2224, index 355 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.04903113 0.04903113 0.04903113 0.04903113 0.        ]]
The hypergraph features for node 2558, index 356 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.04903113 0.21539697 0.13221405 0.13221405 0.08318292]]
The hypergraph features for node 3022, index 357 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.04903113 0.21539697 0.13221405 0.13221405 0.08318292]]
The hypergraph features for node 27, index 358 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.04903113 0.04903113 0.04903113 0.04903113 0.        ]]
The hypergraph features for node 940, index 359 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.04903113 0.32299052 0.20765616 0.22930149 0.09938706]]
The hypergraph features for node 1065, index 360 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.16337156 0.16337156 0.16337156 0.16337156 0.        ]]
The hypergraph features for node 2027, index 361 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.16337156 0.16337156 0.16337156 0.16337156 0.        ]]
The hypergraph features for node 1568, index 362 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.16337156 0.16337156 0.16337156 0.16337156 0.        ]]
The hypergraph features for node 1226, index 363 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.16337156 0.16337156 0.16337156 0.16337156 0.        ]]
The hypergraph features for node 1068, index 364 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.16337156 0.16337156 0.16337156 0.16337156 0.        ]]
The hypergraph features for node 592, index 365 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.16337156 0.16337156 0.16337156 0.16337156 0.        ]]
The hypergraph features for node 615, index 366 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.15347104  0.36496251  0.13563781  0.18938427  0.16804442]]
The hypergraph features for node 392, index 367 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.08102639  0.16337156  0.04117259  0.04117259  0.12219898]]
The hypergraph features for node 2232, index 368 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.16337156 0.16337156 0.16337156 0.16337156 0.        ]]
The hypergraph features for node 3033, index 369 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.16337156 0.16337156 0.16337156 0.16337156 0.        ]]
The hypergraph features for node 3202, index 370 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.16337156 0.23672436 0.18170976 0.16337156 0.03176269]]
The hypergraph features for node 3057, index 371 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.16337156 0.16337156 0.16337156 0.16337156 0.        ]]
The hypergraph features for node 2540, index 372 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.16337156 0.16337156 0.16337156 0.16337156 0.        ]]
The hypergraph features for node 2322, index 373 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.16337156 0.2184887  0.19093013 0.19093013 0.02755857]]
The hypergraph features for node 2233, index 374 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.16337156 0.16337156 0.16337156 0.16337156 0.        ]]
The hypergraph features for node 2591, index 375 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.16337156 0.31569135 0.23953146 0.23953146 0.0761599 ]]
The hypergraph features for node 339, index 376 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.16337156 0.22187862 0.20021572 0.21539697 0.02618679]]
The hypergraph features for node 1071, index 377 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.16337156 0.16337156 0.16337156 0.16337156 0.        ]]
The hypergraph features for node 67, index 378 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.15246251  0.16337156  0.05809354  0.16337156  0.14888561]]
The hypergraph features for node 999, index 379 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.63684154  0.51352639  0.0460053   0.07526354  0.33030252]]
The hypergraph features for node 998, index 380 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.15246251  0.3784898   0.110081    0.16337156  0.19796527]]
The hypergraph features for node 184, index 381 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.34452717  0.51352639  0.12512892  0.          0.29656258]]
The hypergraph features for node 956, index 382 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.01284447  0.17545551  0.10866087  0.16337156  0.08605876]]
The hypergraph features for node 48, index 383 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.01284447 -0.01284447 -0.01284447 -0.01284447  0.        ]]
The hypergraph features for node 442, index 384 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.01284447 -0.01284447 -0.01284447 -0.01284447  0.        ]]
The hypergraph features for node 469, index 385 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.34452717 -0.01284447 -0.15739997 -0.11482827  0.13871461]]
The hypergraph features for node 2236, index 386 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.33304143 -0.33304143 -0.33304143 -0.33304143  0.        ]]
The hypergraph features for node 1431, index 387 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.33304143 -0.33304143 -0.33304143 -0.33304143  0.        ]]
The hypergraph features for node 2148, index 388 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.33304143  0.15836866 -0.08733638 -0.08733638  0.24570505]]
The hypergraph features for node 1788, index 389 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.38765102 0.38765102 0.38765102 0.38765102 0.        ]]
The hypergraph features for node 1766, index 390 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.38765102 0.38765102 0.38765102 0.38765102 0.        ]]
The hypergraph features for node 1383, index 391 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.38765102 0.49804342 0.44284722 0.44284722 0.0551962 ]]
The hypergraph features for node 1649, index 392 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.38765102 0.49804342 0.44284722 0.44284722 0.0551962 ]]
The hypergraph features for node 567, index 393 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.38765102 0.38765102 0.38765102 0.38765102 0.        ]]
The hypergraph features for node 2066, index 394 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 1165, index 395 are 
 [[0. 1. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 1374, index 396 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.         0.16685881 0.12514411 0.16685881 0.07225198]]
The hypergraph features for node 1989, index 397 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.15347104  0.53961465  0.12871454  0.          0.29722889]]
The hypergraph features for node 1979, index 398 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.         0.53961465 0.26980732 0.26980732 0.26980732]]
The hypergraph features for node 1073, index 399 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 1220, index 400 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.22306526  0.65104993  0.14266156  0.          0.37084011]]
The hypergraph features for node 793, index 401 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 29, index 402 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 792, index 403 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 1708, index 404 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.        0.1985212 0.0992606 0.0992606 0.0992606]]
The hypergraph features for node 1863, index 405 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.00255132  0.65104993  0.32424931  0.32424931  0.32680062]]
The hypergraph features for node 1357, index 406 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.         0.65104993 0.32552497 0.32552497 0.32552497]]
The hypergraph features for node 2082, index 407 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.65104993 0.65104993 0.65104993 0.65104993 0.        ]]
The hypergraph features for node 2083, index 408 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.01103275  0.65104993  0.24318927  0.16636995  0.27301076]]
The hypergraph features for node 1075, index 409 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.65104993 0.65104993 0.65104993 0.65104993 0.        ]]
The hypergraph features for node 1387, index 410 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.65104993 0.65104993 0.65104993 0.65104993 0.        ]]
The hypergraph features for node 1613, index 411 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.65104993 0.65104993 0.65104993 0.65104993 0.        ]]
The hypergraph features for node 1078, index 412 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.65104993 0.65104993 0.65104993 0.65104993 0.        ]]
The hypergraph features for node 768, index 413 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.03036596  0.90345629  0.42805793  0.4195707   0.37265419]]
The hypergraph features for node 2081, index 414 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.17719673  0.90345629  0.30057169  0.17545551  0.44995799]]
The hypergraph features for node 2494, index 415 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.84751308  0.90345629 -0.04342833 -0.11482827  0.62316504]]
The hypergraph features for node 2278, index 416 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.84751308  0.90345629 -0.04342833 -0.11482827  0.62316504]]
The hypergraph features for node 2669, index 417 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.1731715   0.90345629  0.2137439  -0.0890531   0.48890788]]
The hypergraph features for node 1678, index 418 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.11482827  0.90345629  0.18447976  0.07781128  0.35196717]]
The hypergraph features for node 1876, index 419 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.34452717  0.90345629  0.27946456  0.27946456  0.62399173]]
The hypergraph features for node 1469, index 420 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.90345629 0.90345629 0.90345629 0.90345629 0.        ]]
The hypergraph features for node 2686, index 421 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.34452717  0.90345629  0.21413611  0.08347921  0.51779603]]
The hypergraph features for node 285, index 422 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.34452717  0.40007007  0.11982353  0.26107393  0.23382841]]
The hypergraph features for node 547, index 423 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.03578424  0.24445819 -0.16964434 -0.07376717  0.36010397]]
The hypergraph features for node 700, index 424 are 
 [[0. 1. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.34452717  0.24445819 -0.04418388 -0.03248266  0.24059458]]
The hypergraph features for node 58, index 425 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.34452717  0.38290265  0.09427789  0.24445819  0.31538773]]
The hypergraph features for node 571, index 426 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.34452717  0.34423026 -0.02468802 -0.07376717  0.28331757]]
The hypergraph features for node 572, index 427 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.34452717  0.34440913  0.07799213  0.24445819  0.2593034 ]]
The hypergraph features for node 574, index 428 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.34452717  0.34423026  0.00925368  0.          0.23137684]]
The hypergraph features for node 459, index 429 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.34452717  0.24445819 -0.05003449 -0.05003449  0.29449268]]
The hypergraph features for node 85, index 430 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.34452717  0.24445819 -0.04418388 -0.03248266  0.24059458]]
The hypergraph features for node 482, index 431 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.34452717  0.50998556  0.12863463  0.2204455   0.35484262]]
The hypergraph features for node 2076, index 432 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.34452717 -0.34452717 -0.34452717 -0.34452717  0.        ]]
The hypergraph features for node 1079, index 433 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.34452717  0.         -0.22968478 -0.34452717  0.16241166]]
The hypergraph features for node 3262, index 434 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.34452717  0.08347921 -0.08701599  0.          0.18524974]]
The hypergraph features for node 1823, index 435 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.34452717 -0.34452717 -0.34452717 -0.34452717  0.        ]]
The hypergraph features for node 1080, index 436 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.         0.35564427 0.17782214 0.17782214 0.17782214]]
The hypergraph features for node 1657, index 437 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.00571215  0.46570232  0.26999626  0.34440913  0.19950337]]
The hypergraph features for node 1583, index 438 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 1314, index 439 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.35564427 0.35564427 0.35564427 0.35564427 0.        ]]
The hypergraph features for node 1731, index 440 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.09883769  0.89484407  0.4166636   0.4106733   0.32676057]]
The hypergraph features for node 1736, index 441 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.34423026 0.46570232 0.40781979 0.4106733  0.05802303]]
The hypergraph features for node 1359, index 442 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.35564427 0.35564427 0.35564427 0.35564427 0.        ]]
The hypergraph features for node 1081, index 443 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.35564427 0.35564427 0.35564427 0.35564427 0.        ]]
The hypergraph features for node 1866, index 444 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.35564427 0.35564427 0.35564427 0.35564427 0.        ]]
The hypergraph features for node 2243, index 445 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.35564427 0.49804342 0.42684384 0.42684384 0.07119957]]
The hypergraph features for node 2324, index 446 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.35564427 0.49804342 0.42684384 0.42684384 0.07119957]]
The hypergraph features for node 3175, index 447 are 
 [[1. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.35564427 0.49804342 0.42684384 0.42684384 0.07119957]]
The hypergraph features for node 2507, index 448 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.49804342 0.49804342 0.49804342 0.49804342 0.        ]]
The hypergraph features for node 1912, index 449 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.181585   0.49804342 0.33981421 0.33981421 0.15822921]]
The hypergraph features for node 1373, index 450 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.49804342 0.49804342 0.49804342 0.49804342 0.        ]]
The hypergraph features for node 1180, index 451 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.49804342 0.49804342 0.49804342 0.49804342 0.        ]]
The hypergraph features for node 105, index 452 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.15246251  0.24445819  0.04599784  0.04599784  0.19846035]]
The hypergraph features for node 842, index 453 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.24445819 0.26107393 0.25276606 0.25276606 0.00830787]]
The hypergraph features for node 717, index 454 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.24445819 0.24445819 0.24445819 0.24445819 0.        ]]
The hypergraph features for node 303, index 455 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.24445819 0.34440913 0.29443366 0.29443366 0.04997547]]
The hypergraph features for node 688, index 456 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.24445819 0.34440913 0.29443366 0.29443366 0.04997547]]
The hypergraph features for node 987, index 457 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.15246251  0.24445819  0.04599784  0.04599784  0.19846035]]
The hypergraph features for node 1586, index 458 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.00255132  0.24445819  0.12095344  0.12095344  0.12350475]]
The hypergraph features for node 1197, index 459 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.24445819 0.24445819 0.24445819 0.24445819 0.        ]]
The hypergraph features for node 689, index 460 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.24445819 0.24445819 0.24445819 0.24445819 0.        ]]
The hypergraph features for node 152, index 461 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.24445819 0.24445819 0.24445819 0.24445819 0.        ]]
The hypergraph features for node 556, index 462 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.17719673  0.3784898   0.0476439  -0.00535874  0.23700605]]
The hypergraph features for node 573, index 463 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.17719673  0.16647925 -0.00535874 -0.00535874  0.17183799]]
The hypergraph features for node 2245, index 464 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.         0.16647925 0.11098617 0.16647925 0.07847907]]
The hypergraph features for node 3125, index 465 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.         0.2204455  0.138351   0.16647925 0.08285967]]
The hypergraph features for node 1086, index 466 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 1479, index 467 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.33952797  0.         -0.22068663 -0.32253191  0.1562032 ]]
The hypergraph features for node 35, index 468 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.04231091 -0.04231091 -0.04231091 -0.04231091  0.        ]]
The hypergraph features for node 892, index 469 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.04231091  0.03851919 -0.00189586 -0.00189586  0.04041505]]
The hypergraph features for node 1392, index 470 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.04231091 -0.04231091 -0.04231091 -0.04231091  0.        ]]
The hypergraph features for node 3035, index 471 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.04231091 -0.04231091 -0.04231091 -0.04231091  0.        ]]
The hypergraph features for node 2246, index 472 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.04231091 -0.04231091 -0.04231091 -0.04231091  0.        ]]
The hypergraph features for node 1626, index 473 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.         0.14540301 0.07270151 0.07270151 0.07270151]]
The hypergraph features for node 1092, index 474 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.14540301 0.14540301 0.14540301 0.14540301 0.        ]]
The hypergraph features for node 1772, index 475 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.14540301 0.14540301 0.14540301 0.14540301 0.        ]]
The hypergraph features for node 1724, index 476 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.14540301 0.14540301 0.14540301 0.14540301 0.        ]]
The hypergraph features for node 130, index 477 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.08223394 0.14540301 0.11381847 0.11381847 0.03158454]]
The hypergraph features for node 523, index 478 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.14540301 0.14540301 0.14540301 0.14540301 0.        ]]
The hypergraph features for node 2858, index 479 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.14540301 0.14540301 0.14540301 0.14540301 0.        ]]
The hypergraph features for node 517, index 480 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.04127541  0.181585    0.08278839  0.10076948  0.08057016]]
The hypergraph features for node 272, index 481 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.03248266  0.50998556  0.18143895  0.18288419  0.17960502]]
The hypergraph features for node 373, index 482 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.32253191  0.26107393 -0.05853243 -0.04127541  0.18286963]]
The hypergraph features for node 374, index 483 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.         0.26107393 0.13549231 0.14540301 0.10681313]]
The hypergraph features for node 445, index 484 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.24365318  0.14540301 -0.03275005  0.          0.16051086]]
The hypergraph features for node 1677, index 485 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.14540301 0.14540301 0.14540301 0.14540301 0.        ]]
The hypergraph features for node 1977, index 486 are 
 [[1. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.35416472  0.14540301 -0.10438085 -0.10438085  0.24978387]]
The hypergraph features for node 1093, index 487 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.14540301 0.14540301 0.14540301 0.14540301 0.        ]]
The hypergraph features for node 1637, index 488 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.30618523  0.14540301 -0.05359407  0.          0.18821482]]
The hypergraph features for node 2248, index 489 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.14540301 0.14540301 0.14540301 0.14540301 0.        ]]
The hypergraph features for node 3195, index 490 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.1221567   0.14540301  0.01162316  0.01162316  0.13377986]]
The hypergraph features for node 2733, index 491 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.1221567   0.14540301  0.01162316  0.01162316  0.13377986]]
The hypergraph features for node 1108, index 492 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.49969922 -0.49969922 -0.49969922 -0.49969922  0.        ]]
The hypergraph features for node 1094, index 493 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.49969922 -0.49969922 -0.49969922 -0.49969922  0.        ]]
The hypergraph features for node 644, index 494 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.49969922  0.66368589  0.20691814  0.33184294  0.489744  ]]
The hypergraph features for node 39, index 495 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.49969922 -0.49969922 -0.49969922 -0.49969922  0.        ]]
The hypergraph features for node 488, index 496 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.49969922  0.66368589  0.26744515  0.66368589  0.45524169]]
The hypergraph features for node 298, index 497 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.49969922 -0.04127541 -0.27048731 -0.27048731  0.2292119 ]]
The hypergraph features for node 886, index 498 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.49969922 -0.49969922 -0.49969922 -0.49969922  0.        ]]
The hypergraph features for node 919, index 499 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.49969922 -0.04127541 -0.27048731 -0.27048731  0.2292119 ]]
The hypergraph features for node 416, index 500 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.56069112  0.36496251 -0.1582842  -0.01246057  0.56460797]]
The hypergraph features for node 418, index 501 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.56069112 -0.14388468 -0.66827087 -0.48425383  0.58541046]]
The hypergraph features for node 230, index 502 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.56069112  0.11135557 -0.39818793  0.          0.73667577]]
The hypergraph features for node 40, index 503 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.56069112  0.11135557 -0.39818793  0.          0.73667577]]
The hypergraph features for node 144, index 504 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.56069112 -0.14388468 -0.66827087 -0.48425383  0.58541046]]
The hypergraph features for node 410, index 505 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.         0.2204455  0.13478935 0.14707265 0.06596547]]
The hypergraph features for node 631, index 506 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.16162951  0.35250611  0.08819966  0.13491015  0.17057542]]
The hypergraph features for node 620, index 507 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.11135557 0.35250611 0.23193084 0.23193084 0.12057527]]
The hypergraph features for node 623, index 508 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.11135557 0.35250611 0.23193084 0.23193084 0.12057527]]
The hypergraph features for node 624, index 509 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.11135557 0.35250611 0.19173908 0.11135557 0.11367945]]
The hypergraph features for node 1096, index 510 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.11135557 0.11135557 0.11135557 0.11135557 0.        ]]
The hypergraph features for node 2052, index 511 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.         0.21539697 0.10891751 0.11135557 0.08795234]]
The hypergraph features for node 1709, index 512 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.11135557 0.21539697 0.16337627 0.16337627 0.0520207 ]]
The hypergraph features for node 1098, index 513 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.16669703 -0.16669703 -0.16669703 -0.16669703  0.        ]]
The hypergraph features for node 41, index 514 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.16669703 -0.16669703 -0.16669703 -0.16669703  0.        ]]
The hypergraph features for node 300, index 515 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.16669703 -0.16669703 -0.16669703 -0.16669703  0.        ]]
The hypergraph features for node 281, index 516 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.32961957  0.36496251 -0.06139183 -0.14045514  0.258652  ]]
The hypergraph features for node 1100, index 517 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.00571215 -0.00571215 -0.00571215 -0.00571215  0.        ]]
The hypergraph features for node 1528, index 518 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.00571215  0.39841863  0.18650774  0.181585    0.11687004]]
The hypergraph features for node 1972, index 519 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.09295591  0.5272129   0.22087175  0.22835826  0.2532382 ]]
The hypergraph features for node 1101, index 520 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.5272129 0.5272129 0.5272129 0.5272129 0.       ]]
The hypergraph features for node 1102, index 521 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.5272129 0.5272129 0.5272129 0.5272129 0.       ]]
The hypergraph features for node 1959, index 522 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.5272129 0.5272129 0.5272129 0.5272129 0.       ]]
The hypergraph features for node 1673, index 523 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.181585   0.5272129  0.35439895 0.35439895 0.17281395]]
The hypergraph features for node 354, index 524 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.01069256  0.5272129   0.2228286   0.3224892   0.18134718]]
The hypergraph features for node 868, index 525 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.5272129 0.5272129 0.5272129 0.5272129 0.       ]]
The hypergraph features for node 1103, index 526 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 45, index 527 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.         0.49993193 0.24996597 0.24996597 0.24996597]]
The hypergraph features for node 734, index 528 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.         0.49993193 0.24996597 0.24996597 0.24996597]]
The hypergraph features for node 704, index 529 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.15246251  0.49993193  0.15751202  0.12506664  0.26732521]]
The hypergraph features for node 996, index 530 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.12506664 0.49993193 0.26112241 0.15836866 0.16941024]]
The hypergraph features for node 2736, index 531 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.01944161  0.49993193  0.24024516  0.24024516  0.25968677]]
The hypergraph features for node 2258, index 532 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.49993193 0.49993193 0.49993193 0.49993193 0.        ]]
The hypergraph features for node 2590, index 533 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.01944161  0.49993193  0.24024516  0.24024516  0.25968677]]
The hypergraph features for node 1106, index 534 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 1560, index 535 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.         0.54552976 0.2470559  0.22134693 0.21763512]]
The hypergraph features for node 1273, index 536 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.         0.08347921 0.0556528  0.08347921 0.03935247]]
The hypergraph features for node 2833, index 537 are 
 [[0. 1. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.1666553 0.1666553 0.1666553 0.1666553 0.       ]]
The hypergraph features for node 2259, index 538 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.1666553 0.1666553 0.1666553 0.1666553 0.       ]]
The hypergraph features for node 2724, index 539 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.1666553 0.1666553 0.1666553 0.1666553 0.       ]]
The hypergraph features for node 3078, index 540 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.1666553 0.1666553 0.1666553 0.1666553 0.       ]]
The hypergraph features for node 2955, index 541 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.15290536  0.29238671  0.13897919  0.27745622  0.20648353]]
The hypergraph features for node 1812, index 542 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.27745622 0.29238671 0.28492147 0.28492147 0.00746524]]
The hypergraph features for node 2957, index 543 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.27800241 0.29238671 0.28519456 0.28519456 0.00719215]]
The hypergraph features for node 2263, index 544 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.27800241 0.29238671 0.28519456 0.28519456 0.00719215]]
The hypergraph features for node 2264, index 545 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.27800241 0.27800241 0.27800241 0.27800241 0.        ]]
The hypergraph features for node 3171, index 546 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.27800241 0.27800241 0.27800241 0.27800241 0.        ]]
The hypergraph features for node 2491, index 547 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.27800241 0.27800241 0.27800241 0.27800241 0.        ]]
The hypergraph features for node 2265, index 548 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.27800241 0.27800241 0.27800241 0.27800241 0.        ]]
The hypergraph features for node 2762, index 549 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.27800241 0.27800241 0.27800241 0.27800241 0.        ]]
The hypergraph features for node 293, index 550 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.27800241 0.49804342 0.38802292 0.38802292 0.1100205 ]]
The hypergraph features for node 872, index 551 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.27800241 0.27800241 0.27800241 0.27800241 0.        ]]
The hypergraph features for node 396, index 552 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.12383125 0.49804342 0.29446987 0.27800241 0.13332487]]
The hypergraph features for node 397, index 553 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.27800241 0.27800241 0.27800241 0.27800241 0.        ]]
The hypergraph features for node 47, index 554 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.27800241 0.27800241 0.27800241 0.27800241 0.        ]]
The hypergraph features for node 447, index 555 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.         0.27800241 0.09266747 0.         0.13105159]]
The hypergraph features for node 1114, index 556 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.27800241 0.27800241 0.27800241 0.27800241 0.        ]]
The hypergraph features for node 2084, index 557 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.27800241 0.27800241 0.27800241 0.27800241 0.        ]]
The hypergraph features for node 483, index 558 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.08102639  0.50998556  0.15176571  0.02745987  0.22087213]]
The hypergraph features for node 772, index 559 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.04127541  0.50998556  0.2176262   0.26107393  0.2160899 ]]
The hypergraph features for node 69, index 560 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.40007007 0.50998556 0.43670857 0.40007007 0.05181466]]
The hypergraph features for node 747, index 561 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.04127541  0.54552976  0.26285808  0.40007007  0.23280085]]
The hypergraph features for node 1004, index 562 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.84751308  0.50998556  0.01532686  0.          0.37169156]]
The hypergraph features for node 976, index 563 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.40007007 0.50998556 0.45502781 0.45502781 0.05495774]]
The hypergraph features for node 977, index 564 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.40007007 0.50998556 0.45502781 0.45502781 0.05495774]]
The hypergraph features for node 51, index 565 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.35921465 0.50998556 0.45704243 0.50998556 0.06242009]]
The hypergraph features for node 355, index 566 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.40007007 0.40007007 0.40007007 0.40007007 0.        ]]
The hypergraph features for node 773, index 567 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.         0.40007007 0.15554951 0.11106398 0.15962705]]
The hypergraph features for node 745, index 568 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.04127541  0.50998556  0.18712423  0.          0.24415523]]
The hypergraph features for node 302, index 569 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.04127541  0.50998556  0.15870082 -0.04127541  0.24706123]]
The hypergraph features for node 437, index 570 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.03036596  0.50998556  0.32986838  0.50998556  0.25472415]]
The hypergraph features for node 318, index 571 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.2204455  0.50998556 0.36321524 0.35921465 0.11823808]]
The hypergraph features for node 805, index 572 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.04127541  0.50998556  0.2920183   0.40734475  0.23937043]]
The hypergraph features for node 408, index 573 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.50998556 0.50998556 0.50998556 0.50998556 0.        ]]
The hypergraph features for node 608, index 574 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.         0.3784898  0.20830085 0.24232039 0.14767645]]
The hypergraph features for node 332, index 575 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.26621402 0.35921465 0.31271434 0.31271434 0.04650032]]
The hypergraph features for node 87, index 576 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.35921465 0.35921465 0.35921465 0.35921465 0.        ]]
The hypergraph features for node 309, index 577 are 
 [[1. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.35921465 0.35921465 0.35921465 0.35921465 0.        ]]
The hypergraph features for node 2664, index 578 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.35921465 0.35921465 0.35921465 0.35921465 0.        ]]
The hypergraph features for node 2274, index 579 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.35921465 0.35921465 0.35921465 0.35921465 0.        ]]
The hypergraph features for node 1222, index 580 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.999313    0.35921465 -0.54647045 -0.999313    0.64041608]]
The hypergraph features for node 1799, index 581 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.999313    0.35921465 -0.54647045 -0.999313    0.64041608]]
The hypergraph features for node 1120, index 582 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.35921465 0.35921465 0.35921465 0.35921465 0.        ]]
The hypergraph features for node 1818, index 583 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.35921465 0.35921465 0.35921465 0.35921465 0.        ]]
The hypergraph features for node 1206, index 584 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.35921465 0.35921465 0.35921465 0.35921465 0.        ]]
The hypergraph features for node 1606, index 585 are 
 [[0. 1. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.35921465 0.35921465 0.35921465 0.35921465 0.        ]]
The hypergraph features for node 1256, index 586 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.35921465 0.35921465 0.35921465 0.35921465 0.        ]]
The hypergraph features for node 362, index 587 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.38290265 0.38290265 0.38290265 0.38290265 0.        ]]
The hypergraph features for node 524, index 588 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.38290265 0.38290265 0.38290265 0.38290265 0.        ]]
The hypergraph features for node 56, index 589 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.38290265 0.38290265 0.38290265 0.38290265 0.        ]]
The hypergraph features for node 1512, index 590 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.181585   0.38290265 0.24869088 0.181585   0.09490205]]
The hypergraph features for node 641, index 591 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.38290265 0.38290265 0.38290265 0.38290265 0.        ]]
The hypergraph features for node 2843, index 592 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.38290265 0.38290265 0.38290265 0.38290265 0.        ]]
The hypergraph features for node 2276, index 593 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.38290265 0.38290265 0.38290265 0.38290265 0.        ]]
The hypergraph features for node 443, index 594 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.11482827 -0.11482827 -0.11482827 -0.11482827  0.        ]]
The hypergraph features for node 60, index 595 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.11482827 -0.11482827 -0.11482827 -0.11482827  0.        ]]
The hypergraph features for node 61, index 596 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.84751308 -0.84751308 -0.84751308 -0.84751308  0.        ]]
The hypergraph features for node 943, index 597 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.84751308 -0.84751308 -0.84751308 -0.84751308  0.        ]]
The hypergraph features for node 673, index 598 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.84751308  0.16685881 -0.35342592 -0.3665247   0.43045525]]
The hypergraph features for node 801, index 599 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.84751308  0.39841863 -0.22454723 -0.22454723  0.62296585]]
The hypergraph features for node 754, index 600 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.84751308 -0.84751308 -0.84751308 -0.84751308  0.        ]]
The hypergraph features for node 566, index 601 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.84751308  0.         -0.29429308 -0.16482962  0.32653034]]
The hypergraph features for node 668, index 602 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.84751308 -0.04127541 -0.44439424 -0.44439424  0.40311884]]
The hypergraph features for node 62, index 603 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.84751308 -0.84751308 -0.84751308 -0.84751308  0.        ]]
The hypergraph features for node 1128, index 604 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.29719605 -0.29719605 -0.29719605 -0.29719605  0.        ]]
The hypergraph features for node 1456, index 605 are 
 [[0. 1. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.29719605  0.54552976  0.15457689  0.21539697  0.34671891]]
The hypergraph features for node 1131, index 606 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.29719605 -0.29719605 -0.29719605 -0.29719605  0.        ]]
The hypergraph features for node 1883, index 607 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.29719605 -0.29719605 -0.29719605 -0.29719605  0.        ]]
The hypergraph features for node 1884, index 608 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.29719605 -0.29719605 -0.29719605 -0.29719605  0.        ]]
The hypergraph features for node 1646, index 609 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.66367388  0.00251163 -0.23896167 -0.14734221  0.27403942]]
The hypergraph features for node 64, index 610 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.4461019  -0.15246251 -0.29928221 -0.29928221  0.1468197 ]]
The hypergraph features for node 682, index 611 are 
 [[1. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.4461019  -0.15246251 -0.29928221 -0.29928221  0.1468197 ]]
The hypergraph features for node 279, index 612 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.20000005  0.36496251 -0.62028487 -0.82305098  0.64675951]]
The hypergraph features for node 203, index 613 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.15246251 -0.15246251 -0.15246251 -0.15246251  0.        ]]
The hypergraph features for node 498, index 614 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.15246251 -0.15246251 -0.15246251 -0.15246251  0.        ]]
The hypergraph features for node 866, index 615 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.43821917 -0.15246251 -0.29534084 -0.29534084  0.14287833]]
The hypergraph features for node 612, index 616 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.15246251  0.66368589  0.10852796 -0.11423419  0.33775687]]
The hypergraph features for node 115, index 617 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.15246251  0.33725635  0.09239692  0.09239692  0.24485943]]
The hypergraph features for node 600, index 618 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.15246251  0.33725635  0.09239692  0.09239692  0.24485943]]
The hypergraph features for node 65, index 619 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.15246251 -0.15246251 -0.15246251 -0.15246251  0.        ]]
The hypergraph features for node 830, index 620 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.15246251 -0.15246251 -0.15246251 -0.15246251  0.        ]]
The hypergraph features for node 1134, index 621 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.12486577 -0.12486577 -0.12486577 -0.12486577  0.        ]]
The hypergraph features for node 2006, index 622 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.12711139  0.33328291  0.11990808  0.1985212   0.17987495]]
The hypergraph features for node 1655, index 623 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.12486577 -0.12486577 -0.12486577 -0.12486577  0.        ]]
The hypergraph features for node 1984, index 624 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.12486577 -0.12486577 -0.12486577 -0.12486577  0.        ]]
The hypergraph features for node 1986, index 625 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.12486577 -0.12486577 -0.12486577 -0.12486577  0.        ]]
The hypergraph features for node 678, index 626 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.12486577  0.13088619  0.0669482   0.13088619  0.11074385]]
The hypergraph features for node 1135, index 627 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.12486577 -0.12486577 -0.12486577 -0.12486577  0.        ]]
The hypergraph features for node 1720, index 628 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.12486577 -0.12486577 -0.12486577 -0.12486577  0.        ]]
The hypergraph features for node 2136, index 629 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.12486577  0.19466809  0.03490116  0.03490116  0.15976693]]
The hypergraph features for node 1630, index 630 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.12486577  0.         -0.06243289 -0.06243289  0.06243289]]
The hypergraph features for node 1136, index 631 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.22835826 0.22835826 0.22835826 0.22835826 0.        ]]
The hypergraph features for node 201, index 632 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.12711139  0.22835826 -0.00862151 -0.12711139  0.16757   ]]
The hypergraph features for node 108, index 633 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.12711139  0.22835826 -0.00862151 -0.12711139  0.16757   ]]
The hypergraph features for node 1577, index 634 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.22835826 0.22835826 0.22835826 0.22835826 0.        ]]
The hypergraph features for node 2049, index 635 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.09295591  0.66368589  0.22465302  0.22664109  0.1808848 ]]
The hypergraph features for node 1223, index 636 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.22835826 0.22835826 0.22835826 0.22835826 0.        ]]
The hypergraph features for node 2121, index 637 are 
 [[0. 0. 1. ... 0. 0. 0.]]
We add the encoding:
 [[0.22835826 0.22835826 0.22835826 0.22835826 0.        ]]
The hypergraph features for node 1137, index 638 are 
 [[0. 0. 1. ... 0. 0. 0.]]
We add the encoding:
 [[0.22835826 0.22835826 0.22835826 0.22835826 0.        ]]
The hypergraph features for node 1203, index 639 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.22835826 0.33725635 0.30095699 0.33725635 0.05133505]]
The hypergraph features for node 2285, index 640 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.22835826 0.22835826 0.22835826 0.22835826 0.        ]]
The hypergraph features for node 2615, index 641 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.22835826 0.22835826 0.22835826 0.22835826 0.        ]]
The hypergraph features for node 1632, index 642 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.22187862 0.22835826 0.22511844 0.22511844 0.00323982]]
The hypergraph features for node 1633, index 643 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.14388468  0.22835826  0.08868619  0.181585    0.16555733]]
The hypergraph features for node 1138, index 644 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.22187862 0.22835826 0.22511844 0.22511844 0.00323982]]
The hypergraph features for node 2168, index 645 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.22187862 0.22835826 0.22511844 0.22511844 0.00323982]]
The hypergraph features for node 1851, index 646 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.32253191  0.66368589  0.19784772  0.22511844  0.34975296]]
The hypergraph features for node 70, index 647 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.22187862 0.22187862 0.22187862 0.22187862 0.        ]]
The hypergraph features for node 659, index 648 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.16506422  0.35250611  0.05989836  0.13491015  0.16647457]]
The hypergraph features for node 979, index 649 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.16162951  0.22664109  0.10545009  0.17839439  0.15846314]]
The hypergraph features for node 986, index 650 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.53037278  0.49804342  0.06319224  0.13491015  0.31716356]]
The hypergraph features for node 930, index 651 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.22187862 0.22187862 0.22187862 0.22187862 0.        ]]
The hypergraph features for node 71, index 652 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.22187862 0.22187862 0.22187862 0.22187862 0.        ]]
The hypergraph features for node 938, index 653 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.181585   0.32299052 0.24215138 0.22187862 0.05948176]]
The hypergraph features for node 506, index 654 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.22187862 0.32299052 0.30276814 0.32299052 0.04044476]]
The hypergraph features for node 77, index 655 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.22187862 0.22187862 0.22187862 0.22187862 0.        ]]
The hypergraph features for node 735, index 656 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.22187862 0.46595803 0.34391833 0.34391833 0.1220397 ]]
The hypergraph features for node 1145, index 657 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.22036536 0.22036536 0.22036536 0.22036536 0.        ]]
The hypergraph features for node 1842, index 658 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.22036536 0.66371593 0.44204064 0.44204064 0.22167529]]
The hypergraph features for node 1716, index 659 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.16162951  0.22036536  0.02936793  0.02936793  0.19099743]]
The hypergraph features for node 2891, index 660 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.22036536 0.22036536 0.22036536 0.22036536 0.        ]]
The hypergraph features for node 2299, index 661 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.22036536 0.22036536 0.22036536 0.22036536 0.        ]]
The hypergraph features for node 1147, index 662 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.22036536 0.22036536 0.22036536 0.22036536 0.        ]]
The hypergraph features for node 1743, index 663 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.22036536 0.22036536 0.22036536 0.22036536 0.        ]]
The hypergraph features for node 1312, index 664 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.22036536 0.22036536 0.22036536 0.22036536 0.        ]]
The hypergraph features for node 1354, index 665 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.22036536 0.22036536 0.22036536 0.22036536 0.        ]]
The hypergraph features for node 1938, index 666 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.22036536 0.22036536 0.22036536 0.22036536 0.        ]]
The hypergraph features for node 1175, index 667 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.         0.91114353 0.28287722 0.11018268 0.37371959]]
The hypergraph features for node 539, index 668 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.00251163 0.00251163 0.00251163 0.00251163 0.        ]]
The hypergraph features for node 1148, index 669 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.00251163 0.00251163 0.00251163 0.00251163 0.        ]]
The hypergraph features for node 1533, index 670 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.00251163 0.00251163 0.00251163 0.00251163 0.        ]]
The hypergraph features for node 450, index 671 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.00251163 0.26107393 0.15275122 0.19466809 0.10963997]]
The hypergraph features for node 1509, index 672 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.50459472  0.36496251  0.01057809  0.181585    0.31093265]]
The hypergraph features for node 421, index 673 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.00251163 0.00251163 0.00251163 0.00251163 0.        ]]
The hypergraph features for node 1506, index 674 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.00251163 0.00251163 0.00251163 0.00251163 0.        ]]
The hypergraph features for node 353, index 675 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.00251163 0.00251163 0.00251163 0.00251163 0.        ]]
The hypergraph features for node 1151, index 676 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.00251163 0.00251163 0.00251163 0.00251163 0.        ]]
The hypergraph features for node 695, index 677 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.00251163 0.00251163 0.00251163 0.00251163 0.        ]]
The hypergraph features for node 2312, index 678 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.00251163 0.00251163 0.00251163 0.00251163 0.        ]]
The hypergraph features for node 3147, index 679 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.00251163 0.07747366 0.05248632 0.07747366 0.03533744]]
The hypergraph features for node 3150, index 680 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.00251163 0.07747366 0.03999265 0.03999265 0.03748101]]
The hypergraph features for node 84, index 681 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.03248266 -0.03248266 -0.03248266 -0.03248266  0.        ]]
The hypergraph features for node 495, index 682 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.03248266  0.13491015  0.10701135  0.13491015  0.06238362]]
The hypergraph features for node 349, index 683 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.03248266  0.16685881  0.04434392  0.02149977  0.07563136]]
The hypergraph features for node 1153, index 684 are 
 [[0. 1. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.03248266 -0.03248266 -0.03248266 -0.03248266  0.        ]]
The hypergraph features for node 1516, index 685 are 
 [[0. 1. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.03248266  0.1530882   0.02937429 -0.03248266  0.08747894]]
The hypergraph features for node 1438, index 686 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.03248266 -0.03248266 -0.03248266 -0.03248266  0.        ]]
The hypergraph features for node 1585, index 687 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.03248266 -0.03248266 -0.03248266 -0.03248266  0.        ]]
The hypergraph features for node 262, index 688 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.03248266  0.10076948  0.0563521   0.10076948  0.06281566]]
The hypergraph features for node 1158, index 689 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.1985212 0.1985212 0.1985212 0.1985212 0.       ]]
The hypergraph features for node 1994, index 690 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.1985212 0.1985212 0.1985212 0.1985212 0.       ]]
The hypergraph features for node 2004, index 691 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.1985212 0.1985212 0.1985212 0.1985212 0.       ]]
The hypergraph features for node 628, index 692 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.19198794 0.1985212  0.19525457 0.19525457 0.00326663]]
The hypergraph features for node 1998, index 693 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.1666553 0.1666553 0.1666553 0.1666553 0.       ]]
The hypergraph features for node 1999, index 694 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.1666553 0.1666553 0.1666553 0.1666553 0.       ]]
The hypergraph features for node 1976, index 695 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.27745622 0.27745622 0.27745622 0.27745622 0.        ]]
The hypergraph features for node 1880, index 696 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.01103275 -0.01103275 -0.01103275 -0.01103275  0.        ]]
The hypergraph features for node 1852, index 697 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.01103275  0.49804342  0.24350533  0.24350533  0.25453808]]
The hypergraph features for node 1159, index 698 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.01103275 -0.01103275 -0.01103275 -0.01103275  0.        ]]
The hypergraph features for node 1451, index 699 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.01103275 -0.01103275 -0.01103275 -0.01103275  0.        ]]
The hypergraph features for node 1573, index 700 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.01103275 -0.01103275 -0.01103275 -0.01103275  0.        ]]
The hypergraph features for node 1160, index 701 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.01103275 -0.01103275 -0.01103275 -0.01103275  0.        ]]
The hypergraph features for node 89, index 702 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.37665315 0.37665315 0.37665315 0.37665315 0.        ]]
The hypergraph features for node 788, index 703 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.53037278  0.37665315 -0.00494789  0.06696403  0.33213953]]
The hypergraph features for node 546, index 704 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.37665315 0.37665315 0.37665315 0.37665315 0.        ]]
The hypergraph features for node 2877, index 705 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.14795573  0.37665315  0.11434871  0.11434871  0.26230444]]
The hypergraph features for node 2878, index 706 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.14795573  0.37665315  0.0269139  -0.14795573  0.247303  ]]
The hypergraph features for node 2319, index 707 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.37665315 0.37665315 0.37665315 0.37665315 0.        ]]
The hypergraph features for node 2320, index 708 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.37665315 0.37665315 0.37665315 0.37665315 0.        ]]
The hypergraph features for node 2919, index 709 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.37665315 0.37665315 0.37665315 0.37665315 0.        ]]
The hypergraph features for node 2509, index 710 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.37665315 0.37665315 0.37665315 0.37665315 0.        ]]
The hypergraph features for node 2613, index 711 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.2184887 0.2184887 0.2184887 0.2184887 0.       ]]
The hypergraph features for node 2595, index 712 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.1731715  0.2184887  0.0226586  0.0226586  0.1958301]]
The hypergraph features for node 2325, index 713 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.2184887 0.2184887 0.2184887 0.2184887 0.       ]]
The hypergraph features for node 1168, index 714 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.2184887 0.2184887 0.2184887 0.2184887 0.       ]]
The hypergraph features for node 1751, index 715 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.2184887 0.2184887 0.2184887 0.2184887 0.       ]]
The hypergraph features for node 2328, index 716 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.27745622 0.27745622 0.27745622 0.27745622 0.        ]]
The hypergraph features for node 2948, index 717 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.27745622 0.27745622 0.27745622 0.27745622 0.        ]]
The hypergraph features for node 2332, index 718 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.27745622 0.27745622 0.27745622 0.27745622 0.        ]]
The hypergraph features for node 1905, index 719 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 1385, index 720 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 2174, index 721 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.         0.91114353 0.45557177 0.45557177 0.45557177]]
The hypergraph features for node 1176, index 722 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.91114353 0.91114353 0.91114353 0.91114353 0.        ]]
The hypergraph features for node 2969, index 723 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.20000005  0.91114353 -0.14442826 -0.14442826  1.05557179]]
The hypergraph features for node 2832, index 724 are 
 [[0. 0. 1. ... 0. 0. 0.]]
We add the encoding:
 [[0.12383125 0.49804342 0.31093733 0.31093733 0.18710608]]
The hypergraph features for node 2335, index 725 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.12383125 0.12383125 0.12383125 0.12383125 0.        ]]
The hypergraph features for node 1944, index 726 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.12383125 0.12383125 0.12383125 0.12383125 0.        ]]
The hypergraph features for node 1178, index 727 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.12383125 0.12383125 0.12383125 0.12383125 0.        ]]
The hypergraph features for node 264, index 728 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.12383125 0.12383125 0.12383125 0.12383125 0.        ]]
The hypergraph features for node 814, index 729 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.43821917  0.22664109 -0.11302708 -0.0506945   0.24488189]]
The hypergraph features for node 816, index 730 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.43821917  0.12383125 -0.17915106 -0.22306526  0.23154771]]
The hypergraph features for node 764, index 731 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.12383125 0.12383125 0.12383125 0.12383125 0.        ]]
The hypergraph features for node 1838, index 732 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.04098501 0.04098501 0.04098501 0.04098501 0.        ]]
The hypergraph features for node 1835, index 733 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.04098501 0.04098501 0.04098501 0.04098501 0.        ]]
The hypergraph features for node 1755, index 734 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.04098501 0.04098501 0.04098501 0.04098501 0.        ]]
The hypergraph features for node 1183, index 735 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.04098501 0.04098501 0.04098501 0.04098501 0.        ]]
The hypergraph features for node 1834, index 736 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.04098501 0.04098501 0.04098501 0.04098501 0.        ]]
The hypergraph features for node 2339, index 737 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.04098501 0.04098501 0.04098501 0.04098501 0.        ]]
The hypergraph features for node 947, index 738 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.04098501 0.04098501 0.04098501 0.04098501 0.        ]]
The hypergraph features for node 102, index 739 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.12987991 -0.12987991 -0.12987991 -0.12987991  0.        ]]
The hypergraph features for node 725, index 740 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.12987991 -0.12987991 -0.12987991 -0.12987991  0.        ]]
The hypergraph features for node 2340, index 741 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.12987991 -0.12987991 -0.12987991 -0.12987991  0.        ]]
The hypergraph features for node 992, index 742 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.18913507 -0.12987991 -0.15950749 -0.15950749  0.02962758]]
The hypergraph features for node 104, index 743 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.12987991  0.12383125  0.06040346  0.12383125  0.10986016]]
The hypergraph features for node 589, index 744 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.12987991  0.12383125  0.06040346  0.12383125  0.10986016]]
The hypergraph features for node 1185, index 745 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.12383125 0.12383125 0.12383125 0.12383125 0.        ]]
The hypergraph features for node 1186, index 746 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.19180139 0.19180139 0.19180139 0.19180139 0.        ]]
The hypergraph features for node 1461, index 747 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.19180139 0.3224892  0.25719193 0.25723857 0.0652973 ]]
The hypergraph features for node 2344, index 748 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.19180139 0.19180139 0.19180139 0.19180139 0.        ]]
The hypergraph features for node 2552, index 749 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.19180139 0.19180139 0.19180139 0.19180139 0.        ]]
The hypergraph features for node 2624, index 750 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.19180139 0.19180139 0.19180139 0.19180139 0.        ]]
The hypergraph features for node 3228, index 751 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.11421325  0.19180139  0.08979651  0.19180139  0.14425668]]
The hypergraph features for node 3293, index 752 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.19180139 0.19180139 0.19180139 0.19180139 0.        ]]
The hypergraph features for node 2345, index 753 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.12711139 -0.12711139 -0.12711139 -0.12711139  0.        ]]
The hypergraph features for node 3269, index 754 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.12711139 -0.12711139 -0.12711139 -0.12711139  0.        ]]
The hypergraph features for node 838, index 755 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.12711139  0.         -0.04237046  0.          0.05992088]]
The hypergraph features for node 1494, index 756 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.12711139 -0.12711139 -0.12711139 -0.12711139  0.        ]]
The hypergraph features for node 2347, index 757 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.12711139 -0.12711139 -0.12711139 -0.12711139  0.        ]]
The hypergraph features for node 1193, index 758 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.12711139 -0.12711139 -0.12711139 -0.12711139  0.        ]]
The hypergraph features for node 1966, index 759 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.12711139 -0.12711139 -0.12711139 -0.12711139  0.        ]]
The hypergraph features for node 1980, index 760 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.12711139  0.31569135  0.12331456  0.15233914  0.16005903]]
The hypergraph features for node 2110, index 761 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.12711139  0.3224892   0.09768891  0.09768891  0.2248003 ]]
The hypergraph features for node 2111, index 762 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.12711139 -0.12711139 -0.12711139 -0.12711139  0.        ]]
The hypergraph features for node 389, index 763 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.32961957  0.44343065  0.16645137  0.22122428  0.29785536]]
The hypergraph features for node 110, index 764 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.32961957 -0.32961957 -0.32961957 -0.32961957  0.        ]]
The hypergraph features for node 2348, index 765 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.32961957 -0.32961957 -0.32961957 -0.32961957  0.        ]]
The hypergraph features for node 2598, index 766 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.32961957 -0.32961957 -0.32961957 -0.32961957  0.        ]]
The hypergraph features for node 391, index 767 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.32961957 -0.08102639 -0.20532298 -0.20532298  0.12429659]]
The hypergraph features for node 113, index 768 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.32961957 -0.32961957 -0.32961957 -0.32961957  0.        ]]
The hypergraph features for node 3274, index 769 are 
 [[1. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.33725635 0.33725635 0.33725635 0.33725635 0.        ]]
The hypergraph features for node 821, index 770 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.33725635 0.33725635 0.33725635 0.33725635 0.        ]]
The hypergraph features for node 1858, index 771 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.31569135 0.33725635 0.32647385 0.32647385 0.0107825 ]]
The hypergraph features for node 2351, index 772 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.33725635 0.33725635 0.33725635 0.33725635 0.        ]]
The hypergraph features for node 1841, index 773 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.33725635 0.33725635 0.33725635 0.33725635 0.        ]]
The hypergraph features for node 1202, index 774 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.33725635 0.33725635 0.33725635 0.33725635 0.        ]]
The hypergraph features for node 2151, index 775 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.33725635 0.33725635 0.33725635 0.33725635 0.        ]]
The hypergraph features for node 2808, index 776 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.33725635 0.33725635 0.33725635 0.33725635 0.        ]]
The hypergraph features for node 2357, index 777 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.33725635 0.33725635 0.33725635 0.33725635 0.        ]]
The hypergraph features for node 1209, index 778 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.33725635 0.33725635 0.33725635 0.33725635 0.        ]]
The hypergraph features for node 1515, index 779 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.33725635 0.33725635 0.33725635 0.33725635 0.        ]]
The hypergraph features for node 448, index 780 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.33725635 0.33725635 0.33725635 0.33725635 0.        ]]
The hypergraph features for node 417, index 781 are 
 [[0. 0. 1. ... 0. 0. 0.]]
We add the encoding:
 [[-0.82462299  0.33725635 -0.24368332 -0.24368332  0.58093967]]
The hypergraph features for node 900, index 782 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.33725635 0.33725635 0.33725635 0.33725635 0.        ]]
The hypergraph features for node 531, index 783 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.00255132  0.33725635  0.16735252  0.16735252  0.16990383]]
The hypergraph features for node 119, index 784 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.33725635 0.33725635 0.33725635 0.33725635 0.        ]]
The hypergraph features for node 2360, index 785 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.33725635 0.33725635 0.33725635 0.33725635 0.        ]]
The hypergraph features for node 2840, index 786 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.33725635 0.33725635 0.33725635 0.33725635 0.        ]]
The hypergraph features for node 2361, index 787 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.11978716 0.33725635 0.22852176 0.22852176 0.1087346 ]]
The hypergraph features for node 1498, index 788 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.33725635 0.33725635 0.33725635 0.33725635 0.        ]]
The hypergraph features for node 2556, index 789 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.11978716 0.11978716 0.11978716 0.11978716 0.        ]]
The hypergraph features for node 1521, index 790 are 
 [[0. 1. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.11978716 0.11978716 0.11978716 0.11978716 0.        ]]
The hypergraph features for node 696, index 791 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.11978716 0.11978716 0.11978716 0.11978716 0.        ]]
The hypergraph features for node 2362, index 792 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.11978716 0.11978716 0.11978716 0.11978716 0.        ]]
The hypergraph features for node 2753, index 793 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.11978716 0.11978716 0.11978716 0.11978716 0.        ]]
The hypergraph features for node 3012, index 794 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.11978716 0.11978716 0.11978716 0.11978716 0.        ]]
The hypergraph features for node 2364, index 795 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.11978716 0.11978716 0.11978716 0.11978716 0.        ]]
The hypergraph features for node 121, index 796 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.22306526 -0.22306526 -0.22306526 -0.22306526  0.        ]]
The hypergraph features for node 1541, index 797 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.999313 -0.999313 -0.999313 -0.999313  0.      ]]
The hypergraph features for node 2847, index 798 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.999313 -0.999313 -0.999313 -0.999313  0.      ]]
The hypergraph features for node 2371, index 799 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.70268345 -0.70268345 -0.70268345 -0.70268345  0.        ]]
The hypergraph features for node 1001, index 800 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.70268345  0.16685881 -0.30892038 -0.39093649  0.35969516]]
The hypergraph features for node 672, index 801 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.70268345  0.         -0.35134172 -0.35134172  0.35134172]]
The hypergraph features for node 1931, index 802 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.70268345 -0.70268345 -0.70268345 -0.70268345  0.        ]]
The hypergraph features for node 2374, index 803 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.70268345 -0.70268345 -0.70268345 -0.70268345  0.        ]]
The hypergraph features for node 2375, index 804 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.01589288 -0.01589288 -0.01589288 -0.01589288  0.        ]]
The hypergraph features for node 3071, index 805 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.01589288 -0.01589288 -0.01589288 -0.01589288  0.        ]]
The hypergraph features for node 3212, index 806 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.01589288 -0.01589288 -0.01589288 -0.01589288  0.        ]]
The hypergraph features for node 2376, index 807 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 2573, index 808 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 328, index 809 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 243, index 810 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 126, index 811 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 1227, index 812 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.08223394 0.08223394 0.08223394 0.08223394 0.        ]]
The hypergraph features for node 2623, index 813 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.08223394 0.08223394 0.08223394 0.08223394 0.        ]]
The hypergraph features for node 2378, index 814 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.08223394 0.53961465 0.31092429 0.31092429 0.22869036]]
The hypergraph features for node 2684, index 815 are 
 [[0. 1. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.08223394 0.53961465 0.31092429 0.31092429 0.22869036]]
The hypergraph features for node 2682, index 816 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.08223394 0.53961465 0.31092429 0.31092429 0.22869036]]
The hypergraph features for node 1241, index 817 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.53961465 0.53961465 0.53961465 0.53961465 0.        ]]
The hypergraph features for node 880, index 818 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.14388468  0.181585    0.01885016  0.01885016  0.16273484]]
The hypergraph features for node 817, index 819 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.14388468  0.32299052  0.08955292  0.08955292  0.2334376 ]]
The hypergraph features for node 141, index 820 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.14388468 -0.14388468 -0.14388468 -0.14388468  0.        ]]
The hypergraph features for node 2866, index 821 are 
 [[0. 1. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.82462299 -0.82462299 -0.82462299 -0.82462299  0.        ]]
The hypergraph features for node 2395, index 822 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.82462299 -0.82462299 -0.82462299 -0.82462299  0.        ]]
The hypergraph features for node 146, index 823 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.50459472 -0.50459472 -0.50459472 -0.50459472  0.        ]]
The hypergraph features for node 731, index 824 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.50459472  0.13491015 -0.09568254 -0.00652279  0.26281535]]
The hypergraph features for node 493, index 825 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.50459472  0.13491015 -0.18484229 -0.18484229  0.31975244]]
The hypergraph features for node 325, index 826 are 
 [[0. 1. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.50459472 -0.50459472 -0.50459472 -0.50459472  0.        ]]
The hypergraph features for node 1701, index 827 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.50459472  0.36496251 -0.01657866  0.181585    0.35111715]]
The hypergraph features for node 149, index 828 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.50459472 -0.50459472 -0.50459472 -0.50459472  0.        ]]
The hypergraph features for node 150, index 829 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.50459472 -0.50459472 -0.50459472 -0.50459472  0.        ]]
The hypergraph features for node 917, index 830 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.50459472  0.181585   -0.09288689  0.181585    0.33615804]]
The hypergraph features for node 637, index 831 are 
 [[0. 1. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.50459472  0.36496251 -0.01657866  0.181585    0.35111715]]
The hypergraph features for node 1255, index 832 are 
 [[0. 1. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.39841863 0.39841863 0.39841863 0.39841863 0.        ]]
The hypergraph features for node 993, index 833 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.         0.39841863 0.19920931 0.19920931 0.19920931]]
The hypergraph features for node 1254, index 834 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[3.98418626e-01 3.98418626e-01 3.98418626e-01 3.98418626e-01
  5.55111512e-17]]
The hypergraph features for node 1919, index 835 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.17545551 0.39841863 0.28693707 0.28693707 0.11148156]]
The hypergraph features for node 782, index 836 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.181585   0.39841863 0.29000181 0.29000181 0.10841681]]
The hypergraph features for node 1580, index 837 are 
 [[0. 1. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.36561408  0.39841863  0.01640227  0.01640227  0.38201635]]
The hypergraph features for node 1277, index 838 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.39841863 0.39841863 0.39841863 0.39841863 0.        ]]
The hypergraph features for node 1366, index 839 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.36496251 0.39841863 0.38169057 0.38169057 0.01672806]]
The hypergraph features for node 1264, index 840 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.39841863 0.39841863 0.39841863 0.39841863 0.        ]]
The hypergraph features for node 1642, index 841 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.39841863 0.39841863 0.39841863 0.39841863 0.        ]]
The hypergraph features for node 1416, index 842 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.         0.39841863 0.22099486 0.2204455  0.11716342]]
The hypergraph features for node 167, index 843 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.22356685 0.22356685 0.22356685 0.22356685 0.        ]]
The hypergraph features for node 640, index 844 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.22356685 0.22356685 0.22356685 0.22356685 0.        ]]
The hypergraph features for node 170, index 845 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.22356685 0.22356685 0.22356685 0.22356685 0.        ]]
The hypergraph features for node 1003, index 846 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.12922278 0.22356685 0.17639481 0.17639481 0.04717204]]
The hypergraph features for node 767, index 847 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.17545551 0.44343065 0.28081767 0.22356685 0.11665022]]
The hypergraph features for node 2416, index 848 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.08347921 0.08347921 0.08347921 0.08347921 0.        ]]
The hypergraph features for node 2721, index 849 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.08347921 0.08347921 0.08347921 0.08347921 0.        ]]
The hypergraph features for node 1492, index 850 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 2420, index 851 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 3165, index 852 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 3286, index 853 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.         0.13491015 0.06745507 0.06745507 0.06745507]]
The hypergraph features for node 3140, index 854 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 2422, index 855 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 2426, index 856 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 851, index 857 are 
 [[0. 1. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.09295591  0.         -0.04647796 -0.04647796  0.04647796]]
The hypergraph features for node 3119, index 858 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.09295591  0.         -0.04647796 -0.04647796  0.04647796]]
The hypergraph features for node 305, index 859 are 
 [[1. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.22778262 0.22778262 0.22778262 0.22778262 0.        ]]
The hypergraph features for node 178, index 860 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.22778262 0.22778262 0.22778262 0.22778262 0.        ]]
The hypergraph features for node 1474, index 861 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.1731715   0.22778262  0.02730556  0.02730556  0.20047706]]
The hypergraph features for node 2149, index 862 are 
 [[0. 1. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.22778262 0.22778262 0.22778262 0.22778262 0.        ]]
The hypergraph features for node 1991, index 863 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.22778262 0.22778262 0.22778262 0.22778262 0.        ]]
The hypergraph features for node 553, index 864 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.17719673  0.34423026  0.19734103  0.28600644  0.18941164]]
The hypergraph features for node 1419, index 865 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.22778262 0.22778262 0.22778262 0.22778262 0.        ]]
The hypergraph features for node 2064, index 866 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.1731715   0.22778262  0.02730556  0.02730556  0.20047706]]
The hypergraph features for node 2161, index 867 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.22778262 0.22778262 0.22778262 0.22778262 0.        ]]
The hypergraph features for node 3043, index 868 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.         0.22778262 0.11947898 0.12506664 0.08072693]]
The hypergraph features for node 3267, index 869 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.22778262 0.23672436 0.23225349 0.23225349 0.00447087]]
The hypergraph features for node 2855, index 870 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.2982744   0.22778262 -0.03524589 -0.03524589  0.26302851]]
The hypergraph features for node 2641, index 871 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.         0.22778262 0.11389131 0.11389131 0.11389131]]
The hypergraph features for node 183, index 872 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.         0.22778262 0.07592754 0.         0.10737776]]
The hypergraph features for node 605, index 873 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.22778262 0.22778262 0.22778262 0.22778262 0.        ]]
The hypergraph features for node 452, index 874 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 381, index 875 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.33952797  0.66371593  0.10806265  0.          0.41663952]]
The hypergraph features for node 1460, index 876 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.16685881 0.51352639 0.24624398 0.19198794 0.13411289]]
The hypergraph features for node 1526, index 877 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.51352639 0.51352639 0.51352639 0.51352639 0.        ]]
The hypergraph features for node 1464, index 878 are 
 [[0. 0. 0. ... 0. 0. 1.]]
We add the encoding:
 [[0.51352639 0.51352639 0.51352639 0.51352639 0.        ]]
The hypergraph features for node 3253, index 879 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.51352639 0.51352639 0.51352639 0.51352639 0.        ]]
The hypergraph features for node 1000, index 880 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.32681682  0.3784898   0.0705785   0.11532052  0.30382977]]
The hypergraph features for node 3272, index 881 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.12186507 -0.12186507 -0.12186507 -0.12186507  0.        ]]
The hypergraph features for node 2434, index 882 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.12186507 -0.12186507 -0.12186507 -0.12186507  0.        ]]
The hypergraph features for node 806, index 883 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.12186507  0.12922278  0.00367885  0.00367885  0.12554393]]
The hypergraph features for node 2813, index 884 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.         0.13491015 0.06745507 0.06745507 0.06745507]]
The hypergraph features for node 2438, index 885 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 2711, index 886 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.         0.13491015 0.10792812 0.13491015 0.05396406]]
The hypergraph features for node 192, index 887 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 849, index 888 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.53037278  0.         -0.26518639 -0.26518639  0.26518639]]
The hypergraph features for node 1889, index 889 are 
 [[0. 1. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 1291, index 890 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 1382, index 891 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.         0.40734475 0.20367238 0.20367238 0.20367238]]
The hypergraph features for node 1926, index 892 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 1760, index 893 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.         0.10370222 0.05185111 0.05185111 0.05185111]]
The hypergraph features for node 1292, index 894 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.         0.10370222 0.05185111 0.05185111 0.05185111]]
The hypergraph features for node 202, index 895 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.10370222 0.10370222 0.10370222 0.10370222 0.        ]]
The hypergraph features for node 1297, index 896 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.10370222 0.10370222 0.10370222 0.10370222 0.        ]]
The hypergraph features for node 1967, index 897 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.10370222 0.10370222 0.10370222 0.10370222 0.        ]]
The hypergraph features for node 1347, index 898 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.10370222 0.10370222 0.10370222 0.10370222 0.        ]]
The hypergraph features for node 2448, index 899 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.10370222 0.10370222 0.10370222 0.10370222 0.        ]]
The hypergraph features for node 2562, index 900 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.10370222 0.10370222 0.10370222 0.10370222 0.        ]]
The hypergraph features for node 207, index 901 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.10370222 0.10370222 0.10370222 0.10370222 0.        ]]
The hypergraph features for node 209, index 902 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.11421325 -0.11421325 -0.11421325 -0.11421325  0.        ]]
The hypergraph features for node 719, index 903 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.11421325 -0.11421325 -0.11421325 -0.11421325  0.        ]]
The hypergraph features for node 860, index 904 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.11421325  0.         -0.05710662 -0.05710662  0.05710662]]
The hypergraph features for node 2455, index 905 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.11421325 -0.11421325 -0.11421325 -0.11421325  0.        ]]
The hypergraph features for node 217, index 906 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.18913507 -0.18913507 -0.18913507 -0.18913507  0.        ]]
The hypergraph features for node 591, index 907 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.18913507  0.41503509  0.02414299 -0.15347104  0.27678567]]
The hypergraph features for node 2976, index 908 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.1530882  0.66368589 0.40838704 0.40838704 0.25529884]]
The hypergraph features for node 1483, index 909 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.1530882 0.1530882 0.1530882 0.1530882 0.       ]]
The hypergraph features for node 1310, index 910 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.1530882 0.1530882 0.1530882 0.1530882 0.       ]]
The hypergraph features for node 2467, index 911 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.1530882 0.1530882 0.1530882 0.1530882 0.       ]]
The hypergraph features for node 245, index 912 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.1530882 0.1530882 0.1530882 0.1530882 0.       ]]
The hypergraph features for node 787, index 913 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.1530882 0.1530882 0.1530882 0.1530882 0.       ]]
The hypergraph features for node 333, index 914 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.1530882 0.1530882 0.1530882 0.1530882 0.       ]]
The hypergraph features for node 320, index 915 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.1530882 0.1530882 0.1530882 0.1530882 0.       ]]
The hypergraph features for node 741, index 916 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.1530882 0.1530882 0.1530882 0.1530882 0.       ]]
The hypergraph features for node 729, index 917 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.1530882  0.66368589 0.40838704 0.40838704 0.25529884]]
The hypergraph features for node 2112, index 918 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.06663489 0.06663489 0.06663489 0.06663489 0.        ]]
The hypergraph features for node 1338, index 919 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.06663489 0.06663489 0.06663489 0.06663489 0.        ]]
The hypergraph features for node 2005, index 920 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.06663489 0.06663489 0.06663489 0.06663489 0.        ]]
The hypergraph features for node 1343, index 921 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.06663489 0.06663489 0.06663489 0.06663489 0.        ]]
The hypergraph features for node 1859, index 922 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.06663489 0.31569135 0.19116312 0.19116312 0.12452823]]
The hypergraph features for node 693, index 923 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.06663489 0.14707265 0.12696321 0.14707265 0.03483057]]
The hypergraph features for node 3183, index 924 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.06663489 0.14707265 0.10685377 0.10685377 0.04021888]]
The hypergraph features for node 428, index 925 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.10076948 0.10076948 0.10076948 0.10076948 0.        ]]
The hypergraph features for node 538, index 926 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.10076948 0.10076948 0.10076948 0.10076948 0.        ]]
The hypergraph features for node 352, index 927 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.01069256  0.10076948  0.04503846  0.04503846  0.05573102]]
The hypergraph features for node 2486, index 928 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.10076948 0.10076948 0.10076948 0.10076948 0.        ]]
The hypergraph features for node 283, index 929 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.10076948 0.16608846 0.13342897 0.13342897 0.03265949]]
The hypergraph features for node 424, index 930 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.10076948 0.10076948 0.10076948 0.10076948 0.        ]]
The hypergraph features for node 425, index 931 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.10076948 0.10076948 0.10076948 0.10076948 0.        ]]
The hypergraph features for node 510, index 932 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.10076948 0.54552976 0.32314962 0.32314962 0.22238014]]
The hypergraph features for node 3122, index 933 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.24365318 -0.24365318 -0.24365318 -0.24365318  0.        ]]
The hypergraph features for node 664, index 934 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 1353, index 935 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.0890531 -0.0890531 -0.0890531 -0.0890531  0.       ]]
The hypergraph features for node 1348, index 936 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.0890531 -0.0890531 -0.0890531 -0.0890531  0.       ]]
The hypergraph features for node 1903, index 937 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.0890531   0.41503509  0.15766507  0.15233914  0.17904933]]
The hypergraph features for node 2696, index 938 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.0890531 -0.0890531 -0.0890531 -0.0890531  0.       ]]
The hypergraph features for node 2490, index 939 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.0890531 -0.0890531 -0.0890531 -0.0890531  0.       ]]
The hypergraph features for node 269, index 940 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.0890531 -0.0890531 -0.0890531 -0.0890531  0.       ]]
The hypergraph features for node 1352, index 941 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.0890531   0.24540788  0.07817739  0.07817739  0.16723049]]
The hypergraph features for node 1539, index 942 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.0890531 -0.0890531 -0.0890531 -0.0890531  0.       ]]
The hypergraph features for node 1978, index 943 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.24540788 0.24540788 0.24540788 0.24540788 0.        ]]
The hypergraph features for node 273, index 944 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-7.77722001e-01 -4.54187393e-05 -3.88883710e-01 -3.88883710e-01
   3.88838291e-01]]
The hypergraph features for node 1420, index 945 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-4.54187393e-05  2.20445496e-01  1.76347313e-01  2.20445496e-01
   8.81963660e-02]]
The hypergraph features for node 705, index 946 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-4.54187393e-05 -4.54187393e-05 -4.54187393e-05 -4.54187393e-05
   0.00000000e+00]]
The hypergraph features for node 432, index 947 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.777722   0.181585  -0.2980685 -0.2980685  0.4796535]]
The hypergraph features for node 901, index 948 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.32681682 -0.32681682 -0.32681682 -0.32681682  0.        ]]
The hypergraph features for node 902, index 949 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.32681682 -0.32681682 -0.32681682 -0.32681682  0.        ]]
The hypergraph features for node 581, index 950 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.32681682 -0.32681682 -0.32681682 -0.32681682  0.        ]]
The hypergraph features for node 359, index 951 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.20000005 -1.20000005 -1.20000005 -1.20000005  0.        ]]
The hypergraph features for node 2550, index 952 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.36496251 0.36496251 0.36496251 0.36496251 0.        ]]
The hypergraph features for node 3006, index 953 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.36496251 0.36496251 0.36496251 0.36496251 0.        ]]
The hypergraph features for node 280, index 954 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[3.64962510e-01 3.64962510e-01 3.64962510e-01 3.64962510e-01
  5.55111512e-17]]
The hypergraph features for node 629, index 955 are 
 [[0. 1. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.04299955 0.36496251 0.24348375 0.3224892  0.14282024]]
The hypergraph features for node 985, index 956 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.04299955 0.36496251 0.25764152 0.36496251 0.1517748 ]]
The hypergraph features for node 288, index 957 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.36496251 0.36496251 0.36496251 0.36496251 0.        ]]
The hypergraph features for node 1759, index 958 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.36496251 0.36496251 0.36496251 0.36496251 0.        ]]
The hypergraph features for node 1816, index 959 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.36496251 0.36496251 0.36496251 0.36496251 0.        ]]
The hypergraph features for node 1797, index 960 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.36496251 0.36496251 0.36496251 0.36496251 0.        ]]
The hypergraph features for node 1710, index 961 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.16608846 0.181585   0.17848569 0.181585   0.00619861]]
The hypergraph features for node 444, index 962 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 504, index 963 are 
 [[0. 1. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 2943, index 964 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.26107393 0.26107393 0.26107393 0.26107393 0.        ]]
The hypergraph features for node 922, index 965 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.26107393 0.26107393 0.26107393 0.26107393 0.        ]]
The hypergraph features for node 1368, index 966 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.26107393 0.26107393 0.26107393 0.26107393 0.        ]]
The hypergraph features for node 1414, index 967 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.26107393 0.26107393 0.26107393 0.26107393 0.        ]]
The hypergraph features for node 1791, index 968 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.26107393 0.26107393 0.26107393 0.26107393 0.        ]]
The hypergraph features for node 2546, index 969 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.26107393 0.26107393 0.26107393 0.26107393 0.        ]]
The hypergraph features for node 2499, index 970 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.26107393 0.26107393 0.26107393 0.26107393 0.        ]]
The hypergraph features for node 544, index 971 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.         0.16685881 0.0834294  0.0834294  0.0834294 ]]
The hypergraph features for node 286, index 972 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.16685881 0.16685881 0.16685881 0.16685881 0.        ]]
The hypergraph features for node 1376, index 973 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.16685881 0.16685881 0.16685881 0.16685881 0.        ]]
The hypergraph features for node 1990, index 974 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.16685881 0.16685881 0.16685881 0.16685881 0.        ]]
The hypergraph features for node 520, index 975 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.16685881 0.16685881 0.16685881 0.16685881 0.        ]]
The hypergraph features for node 348, index 976 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.16685881 0.3224892  0.2706124  0.3224892  0.07336487]]
The hypergraph features for node 846, index 977 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.10662713 0.21539697 0.16296097 0.16685881 0.04449056]]
The hypergraph features for node 287, index 978 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.16685881 0.16685881 0.16685881 0.16685881 0.        ]]
The hypergraph features for node 337, index 979 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.16685881 0.21539697 0.19112789 0.19112789 0.02426908]]
The hypergraph features for node 2651, index 980 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.08102639  0.16685881  0.04291621  0.04291621  0.1239426 ]]
The hypergraph features for node 1381, index 981 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.         0.3784898  0.12616327 0.         0.1784218 ]]
The hypergraph features for node 1517, index 982 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.         0.3784898  0.12616327 0.         0.1784218 ]]
The hypergraph features for node 2021, index 983 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.3784898 0.3784898 0.3784898 0.3784898 0.       ]]
The hypergraph features for node 684, index 984 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.3784898 0.3784898 0.3784898 0.3784898 0.       ]]
The hypergraph features for node 525, index 985 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.03578424 -1.03578424 -1.03578424 -1.03578424  0.        ]]
The hypergraph features for node 639, index 986 are 
 [[1. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.03578424 -1.03578424 -1.03578424 -1.03578424  0.        ]]
The hypergraph features for node 707, index 987 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.40734475 0.40734475 0.40734475 0.40734475 0.        ]]
The hypergraph features for node 709, index 988 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.40734475 0.49804342 0.45269408 0.45269408 0.04534933]]
The hypergraph features for node 804, index 989 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.04127541  0.40734475  0.18303467  0.18303467  0.22431008]]
The hypergraph features for node 3131, index 990 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.40734475 0.40734475 0.40734475 0.40734475 0.        ]]
The hypergraph features for node 926, index 991 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.07376717  0.40734475  0.03234358 -0.04660439  0.17180896]]
The hypergraph features for node 420, index 992 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.40734475 0.40734475 0.40734475 0.40734475 0.        ]]
The hypergraph features for node 292, index 993 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.40734475 0.40734475 0.40734475 0.40734475 0.        ]]
The hypergraph features for node 2916, index 994 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.40734475 0.40734475 0.40734475 0.40734475 0.        ]]
The hypergraph features for node 395, index 995 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.49804342 0.49804342 0.49804342 0.49804342 0.        ]]
The hypergraph features for node 652, index 996 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.49804342 0.49804342 0.49804342 0.49804342 0.        ]]
The hypergraph features for node 928, index 997 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.49804342 0.49804342 0.49804342 0.49804342 0.        ]]
The hypergraph features for node 369, index 998 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.49804342 0.49804342 0.49804342 0.49804342 0.        ]]
The hypergraph features for node 558, index 999 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 476, index 1000 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 742, index 1001 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 342, index 1002 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 770, index 1003 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 501, index 1004 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.54552976 0.54552976 0.54552976 0.54552976 0.        ]]
The hypergraph features for node 1386, index 1005 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.54552976 0.54552976 0.54552976 0.54552976 0.        ]]
The hypergraph features for node 3258, index 1006 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.54552976 0.54552976 0.54552976 0.54552976 0.        ]]
The hypergraph features for node 1388, index 1007 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.54552976 0.54552976 0.54552976 0.54552976 0.        ]]
The hypergraph features for node 1437, index 1008 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.54552976 0.54552976 0.54552976 0.54552976 0.        ]]
The hypergraph features for node 1457, index 1009 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.54552976 0.54552976 0.54552976 0.54552976 0.        ]]
The hypergraph features for node 2720, index 1010 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.04127541 -0.04127541 -0.04127541 -0.04127541  0.        ]]
The hypergraph features for node 3169, index 1011 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.32253191 -0.04127541 -0.18190366 -0.18190366  0.14062825]]
The hypergraph features for node 3014, index 1012 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.04127541 -0.04127541 -0.04127541 -0.04127541  0.        ]]
The hypergraph features for node 1853, index 1013 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.04127541 -0.04127541 -0.04127541 -0.04127541  0.        ]]
The hypergraph features for node 1391, index 1014 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.04127541 -0.04127541 -0.04127541 -0.04127541  0.        ]]
The hypergraph features for node 2514, index 1015 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.04127541 -0.04127541 -0.04127541 -0.04127541  0.        ]]
The hypergraph features for node 2979, index 1016 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.04127541 -0.04127541 -0.04127541 -0.04127541  0.        ]]
The hypergraph features for node 2515, index 1017 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.04127541 -0.04127541 -0.04127541 -0.04127541  0.        ]]
The hypergraph features for node 3134, index 1018 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.04127541 -0.04127541 -0.04127541 -0.04127541  0.        ]]
The hypergraph features for node 1956, index 1019 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.04127541 -0.04127541 -0.04127541 -0.04127541  0.        ]]
The hypergraph features for node 438, index 1020 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.04127541 -0.04127541 -0.04127541 -0.04127541  0.        ]]
The hypergraph features for node 669, index 1021 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.04127541 -0.04127541 -0.04127541 -0.04127541  0.        ]]
The hypergraph features for node 2521, index 1022 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.34440913 0.34440913 0.34440913 0.34440913 0.        ]]
The hypergraph features for node 2726, index 1023 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.34440913 0.34440913 0.34440913 0.34440913 0.        ]]
The hypergraph features for node 1406, index 1024 are 
 [[0. 1. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.00571215  0.46570232  0.26314097  0.24480716  0.18894958]]
The hypergraph features for node 1735, index 1025 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.00571215  0.46570232  0.22999508  0.22999508  0.23570724]]
The hypergraph features for node 1737, index 1026 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.00571215  0.46570232  0.30856416  0.46570232  0.22222691]]
The hypergraph features for node 1584, index 1027 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.46570232 0.46570232 0.46570232 0.46570232 0.        ]]
The hypergraph features for node 1985, index 1028 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.46570232 0.89484407 0.6802732  0.6802732  0.21457087]]
The hypergraph features for node 1408, index 1029 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.1452052 0.1452052 0.1452052 0.1452052 0.       ]]
The hypergraph features for node 1732, index 1030 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.1452052  0.89484407 0.64496445 0.89484407 0.35338315]]
The hypergraph features for node 1996, index 1031 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.39884651 -0.39884651 -0.39884651 -0.39884651  0.        ]]
The hypergraph features for node 1997, index 1032 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.39884651 -0.39884651 -0.39884651 -0.39884651  0.        ]]
The hypergraph features for node 1407, index 1033 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.39884651 -0.39884651 -0.39884651 -0.39884651  0.        ]]
The hypergraph features for node 865, index 1034 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.39884651  0.31569135 -0.02771839  0.          0.29236657]]
The hypergraph features for node 1413, index 1035 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.39884651  0.         -0.19942325 -0.19942325  0.19942325]]
The hypergraph features for node 1675, index 1036 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.39884651  0.31569135 -0.02771839  0.          0.29236657]]
The hypergraph features for node 1421, index 1037 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.39884651  0.31569135 -0.04157758 -0.04157758  0.35726893]]
The hypergraph features for node 1519, index 1038 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.39884651 -0.39884651 -0.39884651 -0.39884651  0.        ]]
The hypergraph features for node 1848, index 1039 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.39884651  0.66368589  0.13241969  0.13241969  0.5312662 ]]
The hypergraph features for node 2525, index 1040 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 365, index 1041 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 2528, index 1042 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.26621402 0.26621402 0.26621402 0.26621402 0.        ]]
The hypergraph features for node 2530, index 1043 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.26621402 0.26621402 0.26621402 0.26621402 0.        ]]
The hypergraph features for node 311, index 1044 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.26621402 0.26621402 0.26621402 0.26621402 0.        ]]
The hypergraph features for node 2157, index 1045 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.2204455 0.2204455 0.2204455 0.2204455 0.       ]]
The hypergraph features for node 312, index 1046 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.2204455 0.2204455 0.2204455 0.2204455 0.       ]]
The hypergraph features for node 313, index 1047 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.2204455 0.2204455 0.2204455 0.2204455 0.       ]]
The hypergraph features for node 314, index 1048 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.2204455 0.2204455 0.2204455 0.2204455 0.       ]]
The hypergraph features for node 2531, index 1049 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.2204455 0.2204455 0.2204455 0.2204455 0.       ]]
The hypergraph features for node 3020, index 1050 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.13491015 0.44343065 0.23342411 0.17767782 0.12617566]]
The hypergraph features for node 316, index 1051 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.2204455 0.2204455 0.2204455 0.2204455 0.       ]]
The hypergraph features for node 982, index 1052 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.2204455 0.2204455 0.2204455 0.2204455 0.       ]]
The hypergraph features for node 2535, index 1053 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.2204455 0.2204455 0.2204455 0.2204455 0.       ]]
The hypergraph features for node 481, index 1054 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.2204455 0.2204455 0.2204455 0.2204455 0.       ]]
The hypergraph features for node 610, index 1055 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.2204455 0.2204455 0.2204455 0.2204455 0.       ]]
The hypergraph features for node 715, index 1056 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.2204455  0.49804342 0.35924446 0.35924446 0.13879896]]
The hypergraph features for node 716, index 1057 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.15290536  0.66368589  0.24374201  0.2204455   0.33377873]]
The hypergraph features for node 370, index 1058 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.1731715  0.2204455  0.023637   0.023637   0.1968085]]
The hypergraph features for node 319, index 1059 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.2204455 0.2204455 0.2204455 0.2204455 0.       ]]
The hypergraph features for node 3124, index 1060 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.00255132  0.66368589  0.29386002  0.2204455   0.27689983]]
The hypergraph features for node 3107, index 1061 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.2204455  0.66368589 0.4122125  0.35250611 0.185812  ]]
The hypergraph features for node 1771, index 1062 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.13088619 0.2204455  0.17566584 0.17566584 0.04477965]]
The hypergraph features for node 1704, index 1063 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.31569135 0.31569135 0.31569135 0.31569135 0.        ]]
The hypergraph features for node 1503, index 1064 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.31569135 0.31569135 0.31569135 0.31569135 0.        ]]
The hypergraph features for node 2000, index 1065 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.31569135 0.31569135 0.31569135 0.31569135 0.        ]]
The hypergraph features for node 1424, index 1066 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.31569135 0.31569135 0.31569135 0.31569135 0.        ]]
The hypergraph features for node 1753, index 1067 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.27775202 0.31569135 0.29672169 0.29672169 0.01896967]]
The hypergraph features for node 1949, index 1068 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.31569135 0.31569135 0.31569135 0.31569135 0.        ]]
The hypergraph features for node 1752, index 1069 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.27775202 0.31569135 0.29672169 0.29672169 0.01896967]]
The hypergraph features for node 1450, index 1070 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.31569135 0.31569135 0.31569135 0.31569135 0.        ]]
The hypergraph features for node 1426, index 1071 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.31569135 0.31569135 0.31569135 0.31569135 0.        ]]
The hypergraph features for node 848, index 1072 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.16162951  0.89484407  0.15529228  0.10662713  0.30118959]]
The hypergraph features for node 2545, index 1073 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.31569135 0.31569135 0.31569135 0.31569135 0.        ]]
The hypergraph features for node 3103, index 1074 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.31569135 0.31569135 0.31569135 0.31569135 0.        ]]
The hypergraph features for node 330, index 1075 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.21539697 0.21539697 0.21539697 0.21539697 0.        ]]
The hypergraph features for node 1530, index 1076 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.21539697 0.21539697 0.21539697 0.21539697 0.        ]]
The hypergraph features for node 1435, index 1077 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.21539697 0.21539697 0.21539697 0.21539697 0.        ]]
The hypergraph features for node 2784, index 1078 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.09883769  0.21539697  0.0059072  -0.09883769  0.14813164]]
The hypergraph features for node 2553, index 1079 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.21539697 0.21539697 0.21539697 0.21539697 0.        ]]
The hypergraph features for node 2785, index 1080 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.09883769  0.21539697  0.11065209  0.21539697  0.14813164]]
The hypergraph features for node 3031, index 1081 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.09883769  0.21539697  0.05827964  0.05827964  0.15711733]]
The hypergraph features for node 1857, index 1082 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.21539697 0.21539697 0.21539697 0.21539697 0.        ]]
The hypergraph features for node 810, index 1083 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.21539697 0.21539697 0.21539697 0.21539697 0.        ]]
The hypergraph features for node 2559, index 1084 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.21539697 0.21539697 0.21539697 0.21539697 0.        ]]
The hypergraph features for node 343, index 1085 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.21539697 0.21539697 0.21539697 0.21539697 0.        ]]
The hypergraph features for node 344, index 1086 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.21539697 0.21539697 0.21539697 0.21539697 0.        ]]
The hypergraph features for node 2571, index 1087 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.21539697 0.21539697 0.21539697 0.21539697 0.        ]]
The hypergraph features for node 2053, index 1088 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.21539697 0.21539697 0.21539697 0.21539697 0.        ]]
The hypergraph features for node 1454, index 1089 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.21539697 0.21539697 0.21539697 0.21539697 0.        ]]
The hypergraph features for node 1827, index 1090 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.21539697 0.21539697 0.21539697 0.21539697 0.        ]]
The hypergraph features for node 2054, index 1091 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.21539697 0.21539697 0.21539697 0.21539697 0.        ]]
The hypergraph features for node 1455, index 1092 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.21539697 0.21539697 0.21539697 0.21539697 0.        ]]
The hypergraph features for node 1878, index 1093 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.15347104  0.21539697  0.03096297  0.03096297  0.18443401]]
The hypergraph features for node 2574, index 1094 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.21539697 0.21539697 0.21539697 0.21539697 0.        ]]
The hypergraph features for node 2025, index 1095 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.21539697 0.21539697 0.21539697 0.21539697 0.        ]]
The hypergraph features for node 1550, index 1096 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.21539697 0.21539697 0.21539697 0.21539697 0.        ]]
The hypergraph features for node 1588, index 1097 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.21539697 0.21539697 0.21539697 0.21539697 0.        ]]
The hypergraph features for node 1749, index 1098 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.21539697 0.21539697 0.21539697 0.21539697 0.        ]]
The hypergraph features for node 1458, index 1099 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.19198794 0.19198794 0.19198794 0.19198794 0.        ]]
The hypergraph features for node 1648, index 1100 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.19198794 0.19198794 0.19198794 0.19198794 0.        ]]
The hypergraph features for node 1706, index 1101 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.         0.49804342 0.25582857 0.26263542 0.18316051]]
The hypergraph features for node 375, index 1102 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.04299955 0.3224892  0.18274437 0.18274437 0.13974483]]
The hypergraph features for node 981, index 1103 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.3224892 0.3224892 0.3224892 0.3224892 0.       ]]
The hypergraph features for node 2579, index 1104 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.3224892 0.3224892 0.3224892 0.3224892 0.       ]]
The hypergraph features for node 2580, index 1105 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.3224892 0.3224892 0.3224892 0.3224892 0.       ]]
The hypergraph features for node 1589, index 1106 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.00255132  0.         -0.00127566 -0.00127566  0.00127566]]
The hypergraph features for node 1463, index 1107 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 350, index 1108 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.04299955 0.04299955 0.04299955 0.04299955 0.        ]]
The hypergraph features for node 1465, index 1109 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.04299955 0.04299955 0.04299955 0.04299955 0.        ]]
The hypergraph features for node 1770, index 1110 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.04299955 0.13088619 0.08694287 0.08694287 0.04394332]]
The hypergraph features for node 2586, index 1111 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.07376717 -0.01069256 -0.03171743 -0.01069256  0.02973366]]
The hypergraph features for node 781, index 1112 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.01069256  0.         -0.00534628 -0.00534628  0.00534628]]
The hypergraph features for node 2584, index 1113 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.01069256 -0.01069256 -0.01069256 -0.01069256  0.        ]]
The hypergraph features for node 2585, index 1114 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.07376717 -0.07376717 -0.07376717 -0.07376717  0.        ]]
The hypergraph features for node 363, index 1115 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.07376717 -0.07376717 -0.07376717 -0.07376717  0.        ]]
The hypergraph features for node 765, index 1116 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.07376717 -0.07376717 -0.07376717 -0.07376717  0.        ]]
The hypergraph features for node 521, index 1117 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.00255132  0.02745987  0.01245428  0.01245428  0.01500559]]
The hypergraph features for node 946, index 1118 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.02745987 0.03851919 0.03298953 0.03298953 0.00552966]]
The hypergraph features for node 2592, index 1119 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.01944161 -0.01944161 -0.01944161 -0.01944161  0.        ]]
The hypergraph features for node 853, index 1120 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.66367388 -0.01944161 -0.25869047 -0.09295591  0.2879349 ]]
The hypergraph features for node 2594, index 1121 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.1731715 -0.1731715 -0.1731715 -0.1731715  0.       ]]
The hypergraph features for node 1601, index 1122 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.1731715 -0.1731715 -0.1731715 -0.1731715  0.       ]]
The hypergraph features for node 1473, index 1123 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.1731715 -0.1731715 -0.1731715 -0.1731715  0.       ]]
The hypergraph features for node 763, index 1124 are 
 [[0. 0. 0. ... 0. 1. 0.]]
We add the encoding:
 [[-0.32253191  0.         -0.08063298  0.          0.13966042]]
The hypergraph features for node 897, index 1125 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.44478365 -0.32253191 -0.38365778 -0.38365778  0.06112587]]
The hypergraph features for node 2599, index 1126 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.32253191 -0.32253191 -0.32253191 -0.32253191  0.        ]]
The hypergraph features for node 2600, index 1127 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.32253191 -0.32253191 -0.32253191 -0.32253191  0.        ]]
The hypergraph features for node 2998, index 1128 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.32253191 -0.32253191 -0.32253191 -0.32253191  0.        ]]
The hypergraph features for node 3087, index 1129 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.32253191 -0.32253191 -0.32253191 -0.32253191  0.        ]]
The hypergraph features for node 1660, index 1130 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.32253191 -0.32253191 -0.32253191 -0.32253191  0.        ]]
The hypergraph features for node 1525, index 1131 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.32253191 -0.32253191 -0.32253191 -0.32253191  0.        ]]
The hypergraph features for node 2010, index 1132 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.33952797 -0.33952797 -0.33952797 -0.33952797  0.        ]]
The hypergraph features for node 2011, index 1133 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.33952797 -0.33952797 -0.33952797 -0.33952797  0.        ]]
The hypergraph features for node 1879, index 1134 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.33952797 -0.33952797 -0.33952797 -0.33952797  0.        ]]
The hypergraph features for node 2603, index 1135 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.33952797 -0.33952797 -0.33952797 -0.33952797  0.        ]]
The hypergraph features for node 2652, index 1136 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.33952797 -0.33952797 -0.33952797 -0.33952797  0.        ]]
The hypergraph features for node 2704, index 1137 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.33952797 -0.33952797 -0.33952797 -0.33952797  0.        ]]
The hypergraph features for node 798, index 1138 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.66371593 0.66371593 0.66371593 0.66371593 0.        ]]
The hypergraph features for node 1843, index 1139 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.66371593 0.66371593 0.66371593 0.66371593 0.        ]]
The hypergraph features for node 1493, index 1140 are 
 [[0. 0. 0. ... 0. 1. 0.]]
We add the encoding:
 [[0.66371593 0.66371593 0.66371593 0.66371593 0.        ]]
The hypergraph features for node 2616, index 1141 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.44343065 0.44343065 0.44343065 0.44343065 0.        ]]
The hypergraph features for node 2618, index 1142 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.44343065 0.44343065 0.44343065 0.44343065 0.        ]]
The hypergraph features for node 2614, index 1143 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.44343065 0.44343065 0.44343065 0.44343065 0.        ]]
The hypergraph features for node 2619, index 1144 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.44343065 0.44343065 0.44343065 0.44343065 0.        ]]
The hypergraph features for node 618, index 1145 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.0009821 -0.0009821 -0.0009821 -0.0009821  0.       ]]
The hypergraph features for node 794, index 1146 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.0009821 -0.0009821 -0.0009821 -0.0009821  0.       ]]
The hypergraph features for node 388, index 1147 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.0009821 -0.0009821 -0.0009821 -0.0009821  0.       ]]
The hypergraph features for node 973, index 1148 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.0009821 -0.0009821 -0.0009821 -0.0009821  0.       ]]
The hypergraph features for node 756, index 1149 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.44343065 0.44343065 0.44343065 0.44343065 0.        ]]
The hypergraph features for node 440, index 1150 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.39093649 -0.39093649 -0.39093649 -0.39093649  0.        ]]
The hypergraph features for node 414, index 1151 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.27547288 -0.27547288 -0.27547288 -0.27547288  0.        ]]
The hypergraph features for node 658, index 1152 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.27547288 -0.27547288 -0.27547288 -0.27547288  0.        ]]
The hypergraph features for node 456, index 1153 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.23723134 -0.23723134 -0.23723134 -0.23723134  0.        ]]
The hypergraph features for node 927, index 1154 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.43492627 -0.23723134 -0.33607881 -0.33607881  0.09884747]]
The hypergraph features for node 582, index 1155 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.39093649 -0.39093649 -0.39093649 -0.39093649  0.        ]]
The hypergraph features for node 963, index 1156 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.39093649  0.23672436 -0.07710607 -0.07710607  0.31383042]]
The hypergraph features for node 681, index 1157 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-1.00021172 -0.15347104 -0.42326601 -0.26969063  0.33646191]]
The hypergraph features for node 541, index 1158 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.44478365 -0.44478365 -0.44478365 -0.44478365  0.        ]]
The hypergraph features for node 873, index 1159 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.44478365 -0.44478365 -0.44478365 -0.44478365  0.        ]]
The hypergraph features for node 960, index 1160 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.03036596  0.17545551  0.07254477  0.07254477  0.10291073]]
The hypergraph features for node 665, index 1161 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.03036596  0.66368589  0.31665996  0.31665996  0.34702592]]
The hypergraph features for node 968, index 1162 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.03036596 -0.03036596 -0.03036596 -0.03036596  0.        ]]
The hypergraph features for node 407, index 1163 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.66367388 -0.66367388 -0.66367388 -0.66367388  0.        ]]
The hypergraph features for node 400, index 1164 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.08102639  0.14707265  0.03302313  0.03302313  0.11404952]]
The hypergraph features for node 401, index 1165 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.08102639  0.14707265  0.07103963  0.14707265  0.10752692]]
The hypergraph features for node 3185, index 1166 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.08102639  0.13491015  0.02694188  0.02694188  0.10796827]]
The hypergraph features for node 403, index 1167 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.14707265 0.14707265 0.14707265 0.14707265 0.        ]]
The hypergraph features for node 404, index 1168 are 
 [[0. 1. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.14707265 0.14707265 0.14707265 0.14707265 0.        ]]
The hypergraph features for node 405, index 1169 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.14707265 0.14707265 0.14707265 0.14707265 0.        ]]
The hypergraph features for node 1511, index 1170 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.14707265 0.14707265 0.14707265 0.14707265 0.        ]]
The hypergraph features for node 406, index 1171 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.14707265 0.14707265 0.14707265 0.14707265 0.        ]]
The hypergraph features for node 409, index 1172 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.14707265 0.14707265 0.14707265 0.14707265 0.        ]]
The hypergraph features for node 1501, index 1173 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.14707265 0.14707265 0.14707265 0.14707265 0.        ]]
The hypergraph features for node 1877, index 1174 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.14707265 0.14707265 0.14707265 0.14707265 0.        ]]
The hypergraph features for node 509, index 1175 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.14707265 0.14707265 0.14707265 0.14707265 0.        ]]
The hypergraph features for node 492, index 1176 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.14707265 0.14707265 0.14707265 0.14707265 0.        ]]
The hypergraph features for node 564, index 1177 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.17719673  0.14707265 -0.01506204 -0.01506204  0.16213469]]
The hypergraph features for node 2868, index 1178 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.01246057 -0.01246057 -0.01246057 -0.01246057  0.        ]]
The hypergraph features for node 413, index 1179 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.01246057 -0.01246057 -0.01246057 -0.01246057  0.        ]]
The hypergraph features for node 496, index 1180 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.01246057 -0.01246057 -0.01246057 -0.01246057  0.        ]]
The hypergraph features for node 530, index 1181 are 
 [[0. 1. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.01246057 -0.01246057 -0.01246057 -0.01246057  0.        ]]
The hypergraph features for node 419, index 1182 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.01246057 -0.01246057 -0.01246057 -0.01246057  0.        ]]
The hypergraph features for node 1898, index 1183 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.01246057 -0.01246057 -0.01246057 -0.01246057  0.        ]]
The hypergraph features for node 1507, index 1184 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.181585 0.181585 0.181585 0.181585 0.      ]]
The hypergraph features for node 1957, index 1185 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.181585 0.181585 0.181585 0.181585 0.      ]]
The hypergraph features for node 1681, index 1186 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.181585 0.181585 0.181585 0.181585 0.      ]]
The hypergraph features for node 2640, index 1187 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.181585 0.181585 0.181585 0.181585 0.      ]]
The hypergraph features for node 2643, index 1188 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.181585 0.181585 0.181585 0.181585 0.      ]]
The hypergraph features for node 2644, index 1189 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.181585 0.181585 0.181585 0.181585 0.      ]]
The hypergraph features for node 433, index 1190 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.181585 0.181585 0.181585 0.181585 0.      ]]
The hypergraph features for node 2646, index 1191 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.181585 0.181585 0.181585 0.181585 0.      ]]
The hypergraph features for node 2995, index 1192 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.181585 0.181585 0.181585 0.181585 0.      ]]
The hypergraph features for node 3210, index 1193 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.181585 0.181585 0.181585 0.181585 0.      ]]
The hypergraph features for node 3021, index 1194 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.181585 0.181585 0.181585 0.181585 0.      ]]
The hypergraph features for node 2169, index 1195 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.03851919 0.23672436 0.15960338 0.181585   0.07344285]]
The hypergraph features for node 3305, index 1196 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.181585 0.181585 0.181585 0.181585 0.      ]]
The hypergraph features for node 2654, index 1197 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.181585 0.181585 0.181585 0.181585 0.      ]]
The hypergraph features for node 800, index 1198 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 1537, index 1199 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.19466809 0.19466809 0.19466809 0.19466809 0.        ]]
The hypergraph features for node 2668, index 1200 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.00171483 0.19466809 0.09819146 0.09819146 0.09647663]]
The hypergraph features for node 1540, index 1201 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.00171483 0.00171483 0.00171483 0.00171483 0.        ]]
The hypergraph features for node 1861, index 1202 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.00171483 0.00171483 0.00171483 0.00171483 0.        ]]
The hypergraph features for node 2774, index 1203 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.19185595 0.19185595 0.19185595 0.19185595 0.        ]]
The hypergraph features for node 1796, index 1204 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.19185595 0.19185595 0.19185595 0.19185595 0.        ]]
The hypergraph features for node 1662, index 1205 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.19185595 0.19185595 0.19185595 0.19185595 0.        ]]
The hypergraph features for node 3156, index 1206 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.19185595 0.19185595 0.19185595 0.19185595 0.        ]]
The hypergraph features for node 2678, index 1207 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.41503509 0.41503509 0.41503509 0.41503509 0.        ]]
The hypergraph features for node 2894, index 1208 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.17719673  0.41503509  0.11891918  0.11891918  0.29611591]]
The hypergraph features for node 2688, index 1209 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.41503509 0.41503509 0.41503509 0.41503509 0.        ]]
The hypergraph features for node 2689, index 1210 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.41503509 0.41503509 0.41503509 0.41503509 0.        ]]
The hypergraph features for node 1553, index 1211 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 1831, index 1212 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.00255132  0.         -0.00127566 -0.00127566  0.00127566]]
The hypergraph features for node 474, index 1213 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 1631, index 1214 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 477, index 1215 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.66367388 -0.66367388 -0.66367388 -0.66367388  0.        ]]
The hypergraph features for node 1562, index 1216 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.66367388 -0.66367388 -0.66367388 -0.66367388  0.        ]]
The hypergraph features for node 2699, index 1217 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.66368589 0.66368589 0.66368589 0.66368589 0.        ]]
The hypergraph features for node 486, index 1218 are 
 [[0. 0. 0. ... 0. 1. 0.]]
We add the encoding:
 [[0.66368589 0.66368589 0.66368589 0.66368589 0.        ]]
The hypergraph features for node 487, index 1219 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.66368589 0.66368589 0.66368589 0.66368589 0.        ]]
The hypergraph features for node 611, index 1220 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.66368589 0.66368589 0.66368589 0.66368589 0.        ]]
The hypergraph features for node 601, index 1221 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.66368589 0.66368589 0.66368589 0.66368589 0.        ]]
The hypergraph features for node 832, index 1222 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.66368589 0.66368589 0.66368589 0.66368589 0.        ]]
The hypergraph features for node 831, index 1223 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.66368589 0.66368589 0.66368589 0.66368589 0.        ]]
The hypergraph features for node 613, index 1224 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.12963068  0.         -0.06481534 -0.06481534  0.06481534]]
The hypergraph features for node 834, index 1225 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.12963068 -0.12963068 -0.12963068 -0.12963068  0.        ]]
The hypergraph features for node 3058, index 1226 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.12963068 -0.12963068 -0.12963068 -0.12963068  0.        ]]
The hypergraph features for node 2700, index 1227 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.12963068 -0.12963068 -0.12963068 -0.12963068  0.        ]]
The hypergraph features for node 2702, index 1228 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 490, index 1229 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 2705, index 1230 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.13491015 0.13491015 0.13491015 0.13491015 0.        ]]
The hypergraph features for node 2863, index 1231 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.13491015 0.13491015 0.13491015 0.13491015 0.        ]]
The hypergraph features for node 666, index 1232 are 
 [[0. 0. 0. ... 1. 0. 0.]]
We add the encoding:
 [[0.13491015 0.13491015 0.13491015 0.13491015 0.        ]]
The hypergraph features for node 995, index 1233 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.13491015 0.13491015 0.13491015 0.13491015 0.        ]]
The hypergraph features for node 712, index 1234 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.13491015 0.13491015 0.13491015 0.13491015 0.        ]]
The hypergraph features for node 657, index 1235 are 
 [[0. 0. 0. ... 1. 0. 0.]]
We add the encoding:
 [[0.13491015 0.13491015 0.13491015 0.13491015 0.        ]]
The hypergraph features for node 743, index 1236 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.13491015 0.13491015 0.13491015 0.13491015 0.        ]]
The hypergraph features for node 3244, index 1237 are 
 [[0. 1. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.13491015 0.13491015 0.13491015 0.13491015 0.        ]]
The hypergraph features for node 722, index 1238 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.32299052 0.32299052 0.32299052 0.32299052 0.        ]]
The hypergraph features for node 507, index 1239 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.36561408 -0.36561408 -0.36561408 -0.36561408  0.        ]]
The hypergraph features for node 1628, index 1240 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.36561408 -0.36561408 -0.36561408 -0.36561408  0.        ]]
The hypergraph features for node 3256, index 1241 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.36561408 -0.36561408 -0.36561408 -0.36561408  0.        ]]
The hypergraph features for node 2713, index 1242 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.36561408 -0.36561408 -0.36561408 -0.36561408  0.        ]]
The hypergraph features for node 2827, index 1243 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.36561408 -0.36561408 -0.36561408 -0.36561408  0.        ]]
The hypergraph features for node 1929, index 1244 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.00255132  0.10662713  0.05203791  0.05203791  0.05458922]]
The hypergraph features for node 1582, index 1245 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.00255132 -0.00255132 -0.00255132 -0.00255132  0.        ]]
The hypergraph features for node 1590, index 1246 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.00255132 -0.00255132 -0.00255132 -0.00255132  0.        ]]
The hypergraph features for node 1592, index 1247 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.00255132 -0.00255132 -0.00255132 -0.00255132  0.        ]]
The hypergraph features for node 1594, index 1248 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.00255132 -0.00255132 -0.00255132 -0.00255132  0.        ]]
The hypergraph features for node 1692, index 1249 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.00255132 -0.00255132 -0.00255132 -0.00255132  0.        ]]
The hypergraph features for node 1602, index 1250 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.00255132 -0.00255132 -0.00255132 -0.00255132  0.        ]]
The hypergraph features for node 3037, index 1251 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.00255132 -0.00255132 -0.00255132 -0.00255132  0.        ]]
The hypergraph features for node 2731, index 1252 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.00255132 -0.00255132 -0.00255132 -0.00255132  0.        ]]
The hypergraph features for node 791, index 1253 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.00255132 -0.00255132 -0.00255132 -0.00255132  0.        ]]
The hypergraph features for node 2739, index 1254 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.1221567 -0.1221567 -0.1221567 -0.1221567  0.       ]]
The hypergraph features for node 2982, index 1255 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.1221567 -0.1221567 -0.1221567 -0.1221567  0.       ]]
The hypergraph features for node 2742, index 1256 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.1221567 -0.1221567 -0.1221567 -0.1221567  0.       ]]
The hypergraph features for node 1618, index 1257 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 1619, index 1258 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 1622, index 1259 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 1625, index 1260 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 1810, index 1261 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 2009, index 1262 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 2752, index 1263 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 2820, index 1264 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 3207, index 1265 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.30618523 -0.30618523 -0.30618523 -0.30618523  0.        ]]
The hypergraph features for node 1638, index 1266 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.30618523  0.33273991  0.00663867  0.          0.22599168]]
The hypergraph features for node 2759, index 1267 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.33273991 0.33273991 0.33273991 0.33273991 0.        ]]
The hypergraph features for node 2823, index 1268 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.33273991 0.33273991 0.33273991 0.33273991 0.        ]]
The hypergraph features for node 1640, index 1269 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.33273991 0.33273991 0.33273991 0.33273991 0.        ]]
The hypergraph features for node 1694, index 1270 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.33273991 0.33273991 0.33273991 0.33273991 0.        ]]
The hypergraph features for node 3174, index 1271 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.34423026 0.34423026 0.34423026 0.34423026 0.        ]]
The hypergraph features for node 891, index 1272 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.34423026 0.34423026 0.34423026 0.34423026 0.        ]]
The hypergraph features for node 3208, index 1273 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.34423026 0.34423026 0.34423026 0.34423026 0.        ]]
The hypergraph features for node 857, index 1274 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.17719673 -0.17719673 -0.17719673 -0.17719673  0.        ]]
The hypergraph features for node 3040, index 1275 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.17719673 -0.17719673 -0.17719673 -0.17719673  0.        ]]
The hypergraph features for node 3041, index 1276 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.17719673  0.17545551 -0.00087061 -0.00087061  0.17632612]]
The hypergraph features for node 1856, index 1277 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.34423026 0.34423026 0.34423026 0.34423026 0.        ]]
The hypergraph features for node 1652, index 1278 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.34423026 0.34423026 0.34423026 0.34423026 0.        ]]
The hypergraph features for node 2923, index 1279 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.34423026 0.34423026 0.34423026 0.34423026 0.        ]]
The hypergraph features for node 575, index 1280 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.09883769 -0.09883769 -0.09883769 -0.09883769  0.        ]]
The hypergraph features for node 577, index 1281 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.09883769 -0.09883769 -0.09883769 -0.09883769  0.        ]]
The hypergraph features for node 2777, index 1282 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.09883769 -0.09883769 -0.09883769 -0.09883769  0.        ]]
The hypergraph features for node 3007, index 1283 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.09883769 -0.09883769 -0.09883769 -0.09883769  0.        ]]
The hypergraph features for node 2135, index 1284 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.09883769  0.89484407  0.39800319  0.39800319  0.49684088]]
The hypergraph features for node 809, index 1285 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.43821917  0.15836866 -0.1262294  -0.09883769  0.24432491]]
The hypergraph features for node 587, index 1286 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.09883769 -0.09883769 -0.09883769 -0.09883769  0.        ]]
The hypergraph features for node 2790, index 1287 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.15347104 -0.15347104 -0.15347104 -0.15347104  0.        ]]
The hypergraph features for node 1683, index 1288 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.15347104 -0.15347104 -0.15347104 -0.15347104  0.        ]]
The hypergraph features for node 1839, index 1289 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.15347104 -0.15347104 -0.15347104 -0.15347104  0.        ]]
The hypergraph features for node 2795, index 1290 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.15347104 -0.15347104 -0.15347104 -0.15347104  0.        ]]
The hypergraph features for node 1689, index 1291 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.15347104 -0.15347104 -0.15347104 -0.15347104  0.        ]]
The hypergraph features for node 1908, index 1292 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.15347104 -0.15347104 -0.15347104 -0.15347104  0.        ]]
The hypergraph features for node 595, index 1293 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.2982744  -0.15347104 -0.22587272 -0.22587272  0.07240168]]
The hypergraph features for node 1691, index 1294 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.2982744 -0.2982744 -0.2982744 -0.2982744  0.       ]]
The hypergraph features for node 1815, index 1295 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.2982744 -0.2982744 -0.2982744 -0.2982744  0.       ]]
The hypergraph features for node 980, index 1296 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.2982744   0.23672436 -0.03077502 -0.03077502  0.26749938]]
The hypergraph features for node 596, index 1297 are 
 [[0. 1. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.2982744 -0.2982744 -0.2982744 -0.2982744  0.       ]]
The hypergraph features for node 2805, index 1298 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.2982744 -0.2982744 -0.2982744 -0.2982744  0.       ]]
The hypergraph features for node 2806, index 1299 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.2982744 -0.2982744 -0.2982744 -0.2982744  0.       ]]
The hypergraph features for node 606, index 1300 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.2982744 -0.2982744 -0.2982744 -0.2982744  0.       ]]
The hypergraph features for node 1699, index 1301 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.2982744 -0.2982744 -0.2982744 -0.2982744  0.       ]]
The hypergraph features for node 1804, index 1302 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.2982744 -0.2982744 -0.2982744 -0.2982744  0.       ]]
The hypergraph features for node 1702, index 1303 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.33328291 0.33328291 0.33328291 0.33328291 0.        ]]
The hypergraph features for node 1820, index 1304 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.33328291 0.33328291 0.33328291 0.33328291 0.        ]]
The hypergraph features for node 3161, index 1305 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.49804342 0.49804342 0.49804342 0.49804342 0.        ]]
The hypergraph features for node 3234, index 1306 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 619, index 1307 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.35250611 0.35250611 0.35250611 0.35250611 0.        ]]
The hypergraph features for node 2839, index 1308 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.35250611 0.35250611 0.35250611 0.35250611 0.        ]]
The hypergraph features for node 621, index 1309 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.35250611 0.35250611 0.35250611 0.35250611 0.        ]]
The hypergraph features for node 625, index 1310 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.35250611 0.35250611 0.35250611 0.35250611 0.        ]]
The hypergraph features for node 1729, index 1311 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.16162951 -0.16162951 -0.16162951 -0.16162951  0.        ]]
The hypergraph features for node 1717, index 1312 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.16162951 -0.16162951 -0.16162951 -0.16162951  0.        ]]
The hypergraph features for node 1888, index 1313 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.16162951 -0.16162951 -0.16162951 -0.16162951  0.        ]]
The hypergraph features for node 1722, index 1314 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.16162951 -0.16162951 -0.16162951 -0.16162951  0.        ]]
The hypergraph features for node 2069, index 1315 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.16162951 -0.16162951 -0.16162951 -0.16162951  0.        ]]
The hypergraph features for node 850, index 1316 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.43821917  0.10662713 -0.16579602 -0.16579602  0.27242315]]
The hypergraph features for node 898, index 1317 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.09827193 0.09827193 0.09827193 0.09827193 0.        ]]
The hypergraph features for node 647, index 1318 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.09827193 0.09827193 0.09827193 0.09827193 0.        ]]
The hypergraph features for node 2031, index 1319 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.03851919 0.09827193 0.06839556 0.06839556 0.02987637]]
The hypergraph features for node 2874, index 1320 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.27775202 0.27775202 0.27775202 0.27775202 0.        ]]
The hypergraph features for node 3220, index 1321 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.27775202 0.27775202 0.27775202 0.27775202 0.        ]]
The hypergraph features for node 2022, index 1322 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.         0.46595803 0.19086172 0.10662713 0.19933359]]
The hypergraph features for node 1927, index 1323 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.14795573  0.46595803  0.10600077  0.          0.26159721]]
The hypergraph features for node 1928, index 1324 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.14795573  0.46595803  0.10615736  0.05331356  0.22654999]]
The hypergraph features for node 1786, index 1325 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 1756, index 1326 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.14795573  0.         -0.07397786 -0.07397786  0.07397786]]
The hypergraph features for node 3046, index 1327 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.14795573 -0.14795573 -0.14795573 -0.14795573  0.        ]]
The hypergraph features for node 656, index 1328 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.14795573 -0.14795573 -0.14795573 -0.14795573  0.        ]]
The hypergraph features for node 1762, index 1329 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.14795573 -0.14795573 -0.14795573 -0.14795573  0.        ]]
The hypergraph features for node 1910, index 1330 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.14795573 -0.14795573 -0.14795573 -0.14795573  0.        ]]
The hypergraph features for node 937, index 1331 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.16506422 -0.16506422 -0.16506422 -0.16506422  0.        ]]
The hypergraph features for node 718, index 1332 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.16506422 -0.16506422 -0.16506422 -0.16506422  0.        ]]
The hypergraph features for node 662, index 1333 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.16506422 -0.16506422 -0.16506422 -0.16506422  0.        ]]
The hypergraph features for node 856, index 1334 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.16506422 -0.16506422 -0.16506422 -0.16506422  0.        ]]
The hypergraph features for node 660, index 1335 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.53037278 -0.16506422 -0.40860326 -0.53037278  0.17220811]]
The hypergraph features for node 948, index 1336 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.53037278 -0.53037278 -0.53037278 -0.53037278  0.        ]]
The hypergraph features for node 837, index 1337 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.53037278 -0.53037278 -0.53037278 -0.53037278  0.        ]]
The hypergraph features for node 908, index 1338 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 727, index 1339 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.66368589 0.66368589 0.66368589 0.66368589 0.        ]]
The hypergraph features for node 1768, index 1340 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.66368589 0.66368589 0.66368589 0.66368589 0.        ]]
The hypergraph features for node 1767, index 1341 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.66368589 0.66368589 0.66368589 0.66368589 0.        ]]
The hypergraph features for node 954, index 1342 are 
 [[0. 0. 0. ... 0. 1. 0.]]
We add the encoding:
 [[0.13088619 0.13088619 0.13088619 0.13088619 0.        ]]
The hypergraph features for node 1780, index 1343 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 1782, index 1344 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 1935, index 1345 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 1789, index 1346 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 2927, index 1347 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 1792, index 1348 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.28811404 -0.28811404 -0.28811404 -0.28811404  0.        ]]
The hypergraph features for node 2048, index 1349 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.28811404  0.03851919 -0.12479743 -0.12479743  0.16331662]]
The hypergraph features for node 1793, index 1350 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.28811404 -0.28811404 -0.28811404 -0.28811404  0.        ]]
The hypergraph features for node 2929, index 1351 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.28811404 -0.28811404 -0.28811404 -0.28811404  0.        ]]
The hypergraph features for node 3019, index 1352 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.28811404 -0.28811404 -0.28811404 -0.28811404  0.        ]]
The hypergraph features for node 2930, index 1353 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.12506664 0.12506664 0.12506664 0.12506664 0.        ]]
The hypergraph features for node 994, index 1354 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.12506664 0.23672436 0.1808955  0.1808955  0.05582886]]
The hypergraph features for node 751, index 1355 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.12506664 0.12506664 0.12506664 0.12506664 0.        ]]
The hypergraph features for node 2934, index 1356 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.12506664 0.12506664 0.12506664 0.12506664 0.        ]]
The hypergraph features for node 3270, index 1357 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.12506664 0.12506664 0.12506664 0.12506664 0.        ]]
The hypergraph features for node 2107, index 1358 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.49804342 0.49804342 0.49804342 0.49804342 0.        ]]
The hypergraph features for node 1798, index 1359 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.49804342 0.49804342 0.49804342 0.49804342 0.        ]]
The hypergraph features for node 2936, index 1360 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.49804342 0.49804342 0.49804342 0.49804342 0.        ]]
The hypergraph features for node 3170, index 1361 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.49804342 0.49804342 0.49804342 0.49804342 0.        ]]
The hypergraph features for node 2145, index 1362 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.35416472  0.23672436 -0.05872018 -0.05872018  0.29544454]]
The hypergraph features for node 1806, index 1363 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.35416472 -0.35416472 -0.35416472 -0.35416472  0.        ]]
The hypergraph features for node 1809, index 1364 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.35416472 -0.35416472 -0.35416472 -0.35416472  0.        ]]
The hypergraph features for node 2953, index 1365 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.15290536 -0.15290536 -0.15290536 -0.15290536  0.        ]]
The hypergraph features for node 2956, index 1366 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.15290536 -0.15290536 -0.15290536 -0.15290536  0.        ]]
The hypergraph features for node 3173, index 1367 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.15290536 -0.15290536 -0.15290536 -0.15290536  0.        ]]
The hypergraph features for node 3186, index 1368 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.15290536  0.66368589  0.25539026  0.25539026  0.40829563]]
The hypergraph features for node 1906, index 1369 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.12922278 0.66368589 0.39645433 0.39645433 0.26723155]]
The hypergraph features for node 1819, index 1370 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.66368589 0.66368589 0.66368589 0.66368589 0.        ]]
The hypergraph features for node 3303, index 1371 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.66368589 0.66368589 0.66368589 0.66368589 0.        ]]
The hypergraph features for node 737, index 1372 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.46595803 0.79587519 0.6418397  0.66368589 0.13557108]]
The hypergraph features for node 942, index 1373 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.66368589 0.66368589 0.66368589 0.66368589 0.        ]]
The hypergraph features for node 736, index 1374 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.79587519 0.79587519 0.79587519 0.79587519 0.        ]]
The hypergraph features for node 730, index 1375 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.79587519 0.79587519 0.79587519 0.79587519 0.        ]]
The hypergraph features for node 3138, index 1376 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.79587519 0.79587519 0.79587519 0.79587519 0.        ]]
The hypergraph features for node 2981, index 1377 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.79587519 0.79587519 0.79587519 0.79587519 0.        ]]
The hypergraph features for node 2984, index 1378 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.79587519 0.79587519 0.79587519 0.79587519 0.        ]]
The hypergraph features for node 2985, index 1379 are 
 [[0. 1. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.46595803 0.79587519 0.6859028  0.79587519 0.15552444]]
The hypergraph features for node 2983, index 1380 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.46595803 0.79587519 0.6859028  0.79587519 0.15552444]]
The hypergraph features for node 1847, index 1381 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.46595803 0.46595803 0.46595803 0.46595803 0.        ]]
The hypergraph features for node 1915, index 1382 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.46595803 0.46595803 0.46595803 0.46595803 0.        ]]
The hypergraph features for node 3008, index 1383 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.66368589 0.66368589 0.66368589 0.66368589 0.        ]]
The hypergraph features for node 2018, index 1384 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.49804342 0.49804342 0.49804342 0.49804342 0.        ]]
The hypergraph features for node 3232, index 1385 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.49804342 0.49804342 0.49804342 0.49804342 0.        ]]
The hypergraph features for node 2996, index 1386 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.49804342 0.49804342 0.49804342 0.49804342 0.        ]]
The hypergraph features for node 750, index 1387 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.49804342 0.49804342 0.49804342 0.49804342 0.        ]]
The hypergraph features for node 3155, index 1388 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.         0.03851919 0.0192596  0.0192596  0.0192596 ]]
The hypergraph features for node 760, index 1389 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 758, index 1390 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 759, index 1391 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0. 0. 0. 0. 0.]]
The hypergraph features for node 1869, index 1392 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.17545551 0.17545551 0.17545551 0.17545551 0.        ]]
The hypergraph features for node 1920, index 1393 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.17545551 0.17545551 0.17545551 0.17545551 0.        ]]
The hypergraph features for node 1873, index 1394 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.17545551 0.17545551 0.17545551 0.17545551 0.        ]]
The hypergraph features for node 3032, index 1395 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.17545551 0.17545551 0.17545551 0.17545551 0.        ]]
The hypergraph features for node 3259, index 1396 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.17545551 0.17545551 0.17545551 0.17545551 0.        ]]
The hypergraph features for node 1893, index 1397 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.17545551 0.17545551 0.17545551 0.17545551 0.        ]]
The hypergraph features for node 3051, index 1398 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.17545551 0.17545551 0.17545551 0.17545551 0.        ]]
The hypergraph features for node 3052, index 1399 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.17545551 0.17545551 0.17545551 0.17545551 0.        ]]
The hypergraph features for node 1901, index 1400 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.17545551 0.17545551 0.17545551 0.17545551 0.        ]]
The hypergraph features for node 2033, index 1401 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.17545551 0.17545551 0.17545551 0.17545551 0.        ]]
The hypergraph features for node 3065, index 1402 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.12922278 0.12922278 0.12922278 0.12922278 0.        ]]
The hypergraph features for node 3067, index 1403 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.12922278 0.12922278 0.12922278 0.12922278 0.        ]]
The hypergraph features for node 3123, index 1404 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.10662713 0.12922278 0.11792495 0.11792495 0.01129783]]
The hypergraph features for node 3081, index 1405 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.22664109 0.22664109 0.22664109 0.22664109 0.        ]]
The hypergraph features for node 3084, index 1406 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.22664109 0.22664109 0.22664109 0.22664109 0.        ]]
The hypergraph features for node 1913, index 1407 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.22664109 0.22664109 0.22664109 0.22664109 0.        ]]
The hypergraph features for node 2142, index 1408 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.22664109 0.22664109 0.22664109 0.22664109 0.        ]]
The hypergraph features for node 2098, index 1409 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.22664109 0.22664109 0.22664109 0.22664109 0.        ]]
The hypergraph features for node 2077, index 1410 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.22664109 0.22664109 0.22664109 0.22664109 0.        ]]
The hypergraph features for node 2159, index 1411 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.22664109 0.22664109 0.22664109 0.22664109 0.        ]]
The hypergraph features for node 3096, index 1412 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.22664109 0.22664109 0.22664109 0.22664109 0.        ]]
The hypergraph features for node 3095, index 1413 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.22664109 0.22664109 0.22664109 0.22664109 0.        ]]
The hypergraph features for node 1925, index 1414 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.22664109 0.22664109 0.22664109 0.22664109 0.        ]]
The hypergraph features for node 3100, index 1415 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.10662713 0.10662713 0.10662713 0.10662713 0.        ]]
The hypergraph features for node 3102, index 1416 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.10662713 0.10662713 0.10662713 0.10662713 0.        ]]
The hypergraph features for node 3106, index 1417 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.10662713 0.10662713 0.10662713 0.10662713 0.        ]]
The hypergraph features for node 3178, index 1418 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.10662713 0.10662713 0.10662713 0.10662713 0.        ]]
The hypergraph features for node 966, index 1419 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.10662713 0.10662713 0.10662713 0.10662713 0.        ]]
The hypergraph features for node 912, index 1420 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.10662713 0.10662713 0.10662713 0.10662713 0.        ]]
The hypergraph features for node 1968, index 1421 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.09295591 -0.09295591 -0.09295591 -0.09295591  0.        ]]
The hypergraph features for node 2160, index 1422 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[-0.09295591 -0.09295591 -0.09295591 -0.09295591  0.        ]]
The hypergraph features for node 1970, index 1423 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.07747366 0.07747366 0.07747366 0.07747366 0.        ]]
The hypergraph features for node 1971, index 1424 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.07747366 0.07747366 0.07747366 0.07747366 0.        ]]
The hypergraph features for node 2019, index 1425 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.07747366 0.07747366 0.07747366 0.07747366 0.        ]]
The hypergraph features for node 3146, index 1426 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.07747366 0.07747366 0.07747366 0.07747366 0.        ]]
The hypergraph features for node 3149, index 1427 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.07747366 0.07747366 0.07747366 0.07747366 0.        ]]
The hypergraph features for node 3148, index 1428 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.07747366 0.07747366 0.07747366 0.07747366 0.        ]]
The hypergraph features for node 874, index 1429 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.03851919 0.03851919 0.03851919 0.03851919 0.        ]]
The hypergraph features for node 2109, index 1430 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.03851919 0.03851919 0.03851919 0.03851919 0.        ]]
The hypergraph features for node 2008, index 1431 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.03851919 0.03851919 0.03851919 0.03851919 0.        ]]
The hypergraph features for node 2056, index 1432 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.03851919 0.03851919 0.03851919 0.03851919 0.        ]]
The hypergraph features for node 3290, index 1433 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.03851919 0.03851919 0.03851919 0.03851919 0.        ]]
The hypergraph features for node 3167, index 1434 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.03851919 0.03851919 0.03851919 0.03851919 0.        ]]
The hypergraph features for node 3168, index 1435 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.03851919 0.03851919 0.03851919 0.03851919 0.        ]]
The hypergraph features for node 893, index 1436 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.03851919 0.03851919 0.03851919 0.03851919 0.        ]]
The hypergraph features for node 3188, index 1437 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.03851919 0.03851919 0.03851919 0.03851919 0.        ]]
The hypergraph features for node 3189, index 1438 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.03851919 0.03851919 0.03851919 0.03851919 0.        ]]
The hypergraph features for node 3273, index 1439 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.03851919 0.03851919 0.03851919 0.03851919 0.        ]]
The hypergraph features for node 909, index 1440 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.03851919 0.03851919 0.03851919 0.03851919 0.        ]]
The hypergraph features for node 2046, index 1441 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.03851919 0.03851919 0.03851919 0.03851919 0.        ]]
The hypergraph features for node 3204, index 1442 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.23672436 0.23672436 0.23672436 0.23672436 0.        ]]
The hypergraph features for node 2055, index 1443 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.23672436 0.23672436 0.23672436 0.23672436 0.        ]]
The hypergraph features for node 2089, index 1444 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.23672436 0.23672436 0.23672436 0.23672436 0.        ]]
The hypergraph features for node 2085, index 1445 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.23672436 0.23672436 0.23672436 0.23672436 0.        ]]
The hypergraph features for node 2100, index 1446 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.23672436 0.23672436 0.23672436 0.23672436 0.        ]]
The hypergraph features for node 2101, index 1447 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.23672436 0.23672436 0.23672436 0.23672436 0.        ]]
The hypergraph features for node 3240, index 1448 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.23672436 0.23672436 0.23672436 0.23672436 0.        ]]
The hypergraph features for node 3281, index 1449 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.23672436 0.23672436 0.23672436 0.23672436 0.        ]]
The hypergraph features for node 941, index 1450 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.23672436 0.23672436 0.23672436 0.23672436 0.        ]]
The hypergraph features for node 3260, index 1451 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.23672436 0.23672436 0.23672436 0.23672436 0.        ]]
The hypergraph features for node 3261, index 1452 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.23672436 0.23672436 0.23672436 0.23672436 0.        ]]
The hypergraph features for node 3268, index 1453 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.23672436 0.23672436 0.23672436 0.23672436 0.        ]]
The hypergraph features for node 2130, index 1454 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.23672436 0.23672436 0.23672436 0.23672436 0.        ]]
The hypergraph features for node 2131, index 1455 are 
 [[0. 0. 0. ... 0. 0. 0.]]
We add the encoding:
 [[0.23672436 0.23672436 0.23672436 0.23672436 0.        ]]
The hypergraph features for node 989, index 1456 are 
 [[0. 0. 0. ... 1. 0. 0.]]
We add the encoding:
 [[0.23672436 0.23672436 0.23672436 0.23672436 0.        ]]
The hypergraph features for node 2166, index 1457 are 
 [[0. 0. 0. ... 1. 0. 0.]]
We add the encoding:
 [[0.23672436 0.23672436 0.23672436 0.23672436 0.        ]]
X are the features 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0345, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]) 
 with shape torch.Size([3312, 3703])
Y are the labels 
 tensor([1, 4, 1,  ..., 4, 0, 3])
G is the hg
Namespace(add_encodings=True, encodings='LCP', data='cocitation', dataset='citeseer', model_name='UniGAT', first_aggregate='mean', second_aggregate='sum', add_self_loop=True, use_norm=True, activation='relu', nlayer=2, nhid=8, nhead=8, dropout=0.6, input_drop=0.6, attn_drop=0.6, lr=0.01, wd=0.0005, epochs=200, n_runs=10, gpu=0, seed=1, patience=200, nostdout=False, split=1, out_dir='runs/test', dataset_dict={'hypergraph': {415: {0, 953}, 514: {0, 384, 323, 173, 383}, 585: {0, 86}, 690: {0, 691, 271}, 784: {0, 290, 540}, 153: {1, 291, 422, 390, 522, 714, 748, 275, 699, 28}, 732: {1, 646, 905, 266, 780, 533, 534, 284, 31, 289, 295, 429, 430, 78, 471, 218, 862, 876, 505}, 1937: {1, 1850, 862, 1129}, 1034: {1011, 2007}, 2028: {2040, 1011}, 2029: {2040, 1011}, 962: {2, 3151}, 2181: {648, 2865, 1012}, 2031: {1012, 1349}, 1417: {1500, 1013, 1725}, 1427: {1545, 1013, 1725}, 1545: {1013, 1725}, 2024: {1058, 1445, 2118, 1801, 1236, 1013, 1046, 1500, 1725}, 1059: {1744, 1380, 1733, 1014}, 1365: {1059, 1014}, 1378: {1059, 1380, 1680, 1744, 1014}, 1236: {1088, 1058, 1891, 1219, 1860, 1067, 1070, 1015, 1396, 1687, 441, 1242, 1500, 1150}, 1439: {1891, 1444, 1478, 1015, 1083}, 1440: {1444, 1478, 1015, 1083, 1439}, 2093: {1083, 1397, 1015}, 2677: {2183, 79}, 845: {32, 685, 2183}, 3156: {460, 2183}, 2138: {1016, 1620}, 634: {3, 295, 749, 304, 819, 503, 120}, 2306: {2184, 2306, 2261}, 2356: {2184, 2306, 2261}, 2539: {2185, 3042, 2812}, 1549: {1624, 1017, 978, 1549}, 591: {1017, 326}, 588: {739, 4, 294}, 2064: {646, 2091, 780, 1019, 30}, 36: {97, 5, 329, 274, 916, 661}, 63: {97, 5, 329, 274, 916, 661}, 97: {329, 5, 661}, 603: {5, 661}, 949: {97, 5, 63}, 1176: {1896, 1021}, 64: {643, 6}, 682: {643, 6}, 1527: {1024, 1077}, 868: {514, 867, 677, 8, 44, 14, 270, 431, 308, 341, 925, 478}, 1986: {8, 308}, 1100: {2105, 1634, 1029, 1695}, 1109: {1401, 1029}, 1248: {1538, 1029, 1141, 1911}, 1244: {2128, 1941, 1030}, 1487: {1794, 2003, 1031, 1930}, 1488: {961, 1645, 1031}, 1872: {1488, 1031}, 158: {568, 9, 583}, 202: {385, 386, 387, 932, 933, 584, 9, 654, 847}, 260: {9, 290, 33}, 559: {9, 753}, 820: {9, 398}, 970: {824, 9, 933, 158}, 387: {434, 11}, 945: {11, 204}, 1759: {1033, 1946, 1363, 2014}, 2014: {1033, 2015}, 2015: {1033, 2014}, 1491: {1035, 1054}, 1042: {1037, 1615}, 1309: {1040, 1037}, 1679: {1411, 1475, 1037, 1040, 1042, 1047}, 55: {2196, 972}, 3279: {2196, 972, 2901, 55}, 1107: {1776, 1769, 1090, 1038}, 1174: {1776, 1769, 1090, 1038}, 1404: {1090, 1924, 1038, 1776, 1107, 1174}, 1536: {57, 1038, 2119}, 1636: {57, 1038, 1047}, 1682: {57, 1636, 1038}, 1769: {57, 1038}, 1212: {1937, 1850, 862, 1039}, 1076: {1040, 1599}, 1078: {1040, 1371, 1076, 1599}, 2036: {1041, 1307, 1475, 1119}, 2460: {1667, 1543, 2202, 79, 3153, 3028, 3162, 828}, 779: {1664, 1279, 1895, 1006, 882, 179, 3028, 887, 88, 2202, 31}, 828: {609, 34, 1667, 37, 1543, 462, 79, 3028, 278, 2202}, 3162: {609, 2202, 3028, 79}, 1037: {1042, 1615}, 1138: {1634, 1043}, 1828: {1634, 1043, 2139, 1695}, 1893: {1634, 1894, 1351, 1043, 1116}, 2139: {1481, 1634, 1043, 1116}, 2165: {1634, 1043}, 1274: {1892, 1044}, 1453: {1045, 1470}, 1704: {2080, 1705, 1045, 1470}, 1705: {2080, 1916, 1045, 1470}, 1944: {1886, 1470, 1045, 1870}, 1954: {2080, 2002, 1045, 1470}, 1955: {2002, 1045, 1470}, 1980: {2050, 1542, 1423, 1045, 1143, 1470}, 2002: {2003, 1045, 1470}, 1286: {2657, 1125, 1902, 1046, 1532}, 1302: {2657, 1125, 1902, 1046, 1532}, 1532: {2658, 1532, 1046}, 1891: {1690, 1125, 1046}, 1127: {57, 1047}, 1613: {1047, 1599}, 1776: {2034, 1047, 1775}, 2675: {3099, 2206}, 1168: {1050, 172, 1524}, 1254: {1057, 1050, 1787, 1524}, 508: {12, 502, 655}, 1741: {1740, 12}, 777: {364, 12, 799}, 1253: {1890, 1636, 1253, 57, 1051}, 2275: {2210, 1707, 3085}, 1707: {969, 2210, 1063}, 1961: {1066, 1055}, 1962: {1066, 1055}, 1963: {1066, 1055}, 250: {691, 13}, 145: {16, 497, 822, 472, 542}, 2244: {1634, 2213}, 2845: {3235, 2214}, 1014: {1680, 1059, 1380, 1744}, 59: {18, 604, 847}, 6: {20, 663}, 1123: {1864, 1060}, 680: {100, 934, 679, 390, 551, 1005, 21, 439}, 303: {698, 390, 22}, 455: {1008, 390, 22}, 469: {422, 522, 554, 275, 22}, 599: {598, 22}, 668: {390, 616, 268, 699, 22, 439, 698, 123, 701, 766}, 899: {390, 22}, 2391: {2489, 2219}, 2665: {2242, 738, 3080, 441, 2221, 2745, 31}, 3115: {2745, 2221}, 309: {53, 23}, 704: {23, 855}, 127: {24, 297, 471}, 685: {769, 903, 263, 905, 266, 460, 24}, 947: {24, 378}, 952: {99, 164, 518, 423, 552, 588, 24}, 1229: {1707, 1063}, 2567: {2222, 2566}, 1811: {1764, 1064, 1900, 1230, 795, 1918, 1727}, 69: {296, 265, 461, 338, 276, 372, 505, 25}, 357: {888, 25}, 655: {296, 265, 561, 338, 276, 372, 537, 25}, 683: {25, 291}, 888: {265, 461, 338, 276, 372, 25}, 2780: {2224, 2558, 3022}, 393: {27, 940}, 1093: {1065, 2027}, 235: {1568, 739, 1226, 1068, 592}, 592: {32, 1568, 739, 615, 392, 1226, 1068, 685}, 2540: {2232, 3033, 3202}, 3033: {2232, 3057, 3202, 2540}, 3203: {2232, 2322, 3202}, 2698: {2233, 2591}, 3158: {2233, 339}, 1744: {1733, 1071}, 48: {28, 748, 422}, 54: {67, 422, 999, 390, 998, 522, 714, 748, 28, 275, 184, 699, 956}, 105: {67, 422, 999, 998, 522, 714, 748, 275, 699, 28}, 106: {291, 422, 999, 390, 998, 522, 554, 714, 748, 48, 275, 184, 442, 699, 28}, 633: {469, 956, 28, 422}, 987: {422, 999, 714, 522, 748, 275, 699, 28}, 2193: {2236, 1431}, 1203: {2148, 780, 30}, 1239: {1788, 780, 30}, 1282: {1766, 646, 1740, 780, 179, 30, 31}, 1650: {646, 1383, 780, 1649, 179, 567, 30}, 1788: {2066, 780, 1165, 30}, 1991: {1664, 1895, 780, 179, 30}, 2149: {1374, 1989, 646, 780, 1979, 30}, 1075: {1073, 179, 1220, 471}, 740: {793, 29}, 793: {792, 793, 29}, 818: {793, 1708, 29}, 859: {792, 793, 29}, 1357: {31, 471}, 297: {266, 179, 31}, 1489: {218, 876, 31, 1863}, 2774: {1357, 31}, 2812: {31, 471}, 646: {179, 31}, 3028: {1667, 31}, 882: {31, 295}, 2082: {2082, 2083, 31}, 1169: {1075, 1387, 1220, 471}, 1190: {1613, 1078}, 1600: {1078, 1599}, 665: {32, 768, 685, 2081}, 686: {32, 769, 2494, 2278, 905, 2669, 1678, 685, 1876, 887, 1469, 2686}, 687: {32, 769, 905, 685, 1876, 887, 2686}, 697: {390, 905, 522, 275, 285, 32, 34, 547, 685, 439, 184, 698, 699, 700, 58, 571, 572, 574, 459, 85, 469, 471, 482, 364, 748, 887}, 1245: {2076, 1079}, 1651: {1079, 3262, 1823}, 1823: {3262, 1079}, 1189: {1080, 1657, 1583}, 1583: {1314, 1731, 1736, 1359, 1080, 1657}, 1141: {1081, 1866}, 1950: {1081, 1599}, 2324: {2243, 2324, 3175}, 3074: {2243, 2324, 2507, 3175}, 1083: {1912, 1083, 1373}, 1196: {1083, 1180}, 180: {34, 58, 390, 105, 842, 717, 303, 688, 698, 987, 572}, 366: {34, 699, 700, 85}, 1587: {34, 1586, 1197}, 573: {34, 547, 390, 459, 689, 179, 152, 572}, 829: {34, 556, 573}, 2310: {2245, 3125}, 2390: {2245, 3125}, 2483: {2245, 3125}, 1480: {1086, 1479}, 500: {35, 892}, 1608: {1392, 57, 1090}, 3034: {3035, 2246}, 2009: {1626, 1092, 1772}, 1267: {1724, 37, 278}, 182: {130, 37, 390, 523, 278}, 2464: {2858, 37, 278}, 310: {37, 517, 272, 278, 123, 699, 574}, 444: {37, 390, 373, 374, 439, 278, 445}, 1724: {37, 876, 1677, 79, 278, 218}, 2026: {1977, 1093, 1637}, 2746: {2248, 3195, 2733}, 1317: {1108, 1094}, 1676: {1108, 1094}, 1010: {644, 39, 488, 298, 886, 919}, 146: {416, 418, 230, 40, 144, 822}, 148: {40, 230}, 177: {40, 230}, 411: {40, 410, 230, 631}, 480: {40, 230}, 621: {230, 40, 620, 623, 624}, 625: {40, 624, 230}, 1490: {1096, 2052, 1709}, 133: {1098, 323}, 418: {416, 41, 300, 822, 281}, 1579: {2105, 1100}, 1723: {1528, 441, 1100, 2105}, 1609: {1972, 1101}, 1673: {1102, 1959}, 1958: {1673, 354, 1102, 1959}, 632: {868, 44}, 2103: {1103, 1143}, 733: {45, 734, 855}, 751: {704, 996, 45, 734}, 2476: {2736, 2258, 2590}, 1132: {1106, 1860}, 1339: {1560, 1273, 1106}, 2744: {2833, 2259, 2724, 3078}, 1200: {1107, 1174}, 1425: {1107, 1174}, 1813: {2955, 1812, 2957, 2263}, 2959: {2957, 2263}, 2223: {2264, 3171, 2491}, 2204: {2265, 2762}, 213: {293, 872, 396, 397, 47, 447}, 778: {293, 872, 396, 397, 47, 447}, 2104: {218, 876, 1114, 2084}, 355: {483, 772, 69, 390, 747, 364, 1004, 976, 977, 51, 439}, 356: {291, 355, 773, 69, 390, 745, 747, 51, 285}, 438: {745, 302, 51, 179, 437, 439}, 470: {51, 291, 364, 390}, 481: {482, 483, 390, 616, 747, 364, 1004, 1005, 51, 439, 698, 318}, 516: {291, 772, 69, 390, 483, 805, 745, 747, 364, 1004, 272, 976, 977, 51, 437, 439, 408, 699}, 597: {608, 483, 390, 332, 51, 87, 439, 318}, 161: {53, 309}, 2044: {2664, 2274, 1222, 1799}, 1210: {1120, 1818, 1206}, 1256: {1120, 1818, 1206}, 1604: {1120, 1206, 1606}, 1606: {1120, 1206}, 1818: {1120, 1560, 1256, 1206}, 971: {362, 972, 524, 55}, 1824: {56, 1512}, 157: {57, 290, 641}, 1981: {57, 2034}, 302: {58, 364, 439}, 626: {2843, 2276, 86}, 586: {422, 469, 184, 443, 60}, 1198: {2278, 2494, 1678}, 1469: {2278, 2494, 1678}, 3040: {905, 2494, 2278}, 944: {61, 943}, 802: {673, 801, 390, 905, 297, 1004, 754, 566, 471, 668, 62}, 1588: {1128, 1456}, 1969: {1129, 905, 862}, 1885: {1131, 1883, 1884, 1646}, 91: {64, 682, 795}, 210: {64, 682, 279}, 239: {64, 704, 643, 682, 203, 498}, 569: {64, 866, 612, 488, 682, 115, 600}, 152: {65, 291, 266, 566, 88}, 176: {67, 390, 998, 105, 987, 830}, 1422: {1542, 1134, 2006, 1655, 1916}, 1747: {1984, 1986, 867, 677, 678, 1135, 1720, 2136, 1630}, 1494: {1136, 201, 108, 1577}, 1964: {2049, 1223, 2121, 1137, 1203, 1972}, 2616: {2285, 2615}, 1801: {1632, 1633, 1138, 2168, 441, 1851}, 2168: {1632, 1138, 2168, 441, 1851}, 750: {70, 659, 979, 631, 986}, 939: {930, 71, 938, 940, 506}, 813: {339, 916, 77, 735}, 1139: {1145, 1842, 1716}, 534: {505, 533, 78, 862}, 2253: {2891, 2299}, 2259: {354, 2299}, 1874: {1147, 1743}, 1019: {1312, 1354, 79, 1743, 1938}, 103: {272, 364, 390, 79}, 278: {79, 1175}, 460: {539, 86, 79}, 1687: {1148, 1533}, 710: {450, 1148, 1533}, 1262: {1150, 1509, 1646, 421}, 1294: {1506, 1150}, 421: {353, 1150, 1646, 1687}, 1508: {1150, 1687}, 2118: {1242, 1150}, 1301: {1151, 695}, 1960: {1151, 695}, 3149: {2312, 3147, 3150}, 743: {84, 495}, 619: {390, 522, 1004, 275, 85, 698, 699, 700, 349}, 1329: {1153, 1516, 1438}, 1342: {1153, 1516, 1438}, 1629: {1153, 1585}, 26: {1634, 86, 263}, 262: {471, 780, 272, 373, 86, 887}, 272: {780, 86, 471}, 769: {86, 263}, 797: {297, 262, 86, 471}, 1006: {460, 780, 86}, 1214: {1158, 1994, 1708, 2004, 2006}, 1461: {628, 2004, 1158}, 1571: {1158, 2004, 2006}, 154: {291, 483, 390, 460, 88}, 1281: {1678, 1998, 1999, 887, 88, 1850}, 1998: {88, 1850, 887}, 1999: {1678, 887, 1976, 1850, 88}, 2073: {88, 1678}, 1257: {1880, 1852, 1159}, 1287: {1451, 1573, 1159}, 1369: {1160, 2083}, 546: {89, 788}, 548: {89, 546, 986}, 2561: {2877, 2878, 2319}, 2401: {2320, 218, 876}, 2638: {2919, 876, 2509, 2320, 218}, 2605: {2322, 2613}, 2594: {2595, 2325}, 1511: {1168, 1707}, 2106: {1168, 1751}, 3153: {2328, 1667}, 1173: {1812, 2955, 2948, 2332}, 1360: {1905, 1175}, 1385: {1385, 2174, 1175}, 2173: {2174, 1175}, 1897: {1176, 2969}, 3200: {2832, 2335}, 5: {97, 916}, 1178: {1944, 1178, 1870}, 723: {99, 264, 396, 814, 847, 816, 764}, 1836: {1838, 1835, 1755, 1183}, 1837: {1834, 1755, 1838, 1183}, 2596: {378, 2339}, 974: {2339, 947}, 267: {179, 780, 102}, 2400: {876, 725, 2340}, 725: {992, 218, 876, 2340}, 129: {104, 589}, 131: {104, 589}, 134: {104, 691, 589}, 140: {104, 589}, 1945: {1944, 1185, 1870}, 1648: {1186, 2050, 1461}, 2456: {2344, 2552, 2624, 3228}, 2624: {2344, 3228, 3293}, 2230: {2345, 3269}, 383: {201, 514, 108, 838}, 2125: {201, 108, 1494}, 2860: {2347, 1005}, 1548: {1193, 1966}, 2112: {2050, 1193, 1966, 2006, 1980, 2110, 2111}, 584: {281, 389, 110}, 2548: {2348, 2598}, 382: {390, 391, 364, 113, 439}, 3013: {3274, 821, 1858, 2351}, 1183: {1841, 1202}, 671: {600, 115}, 1432: {1203, 2151}, 2151: {1203, 1918}, 2354: {2808, 2357}, 1748: {1209, 1515}, 810: {448, 417, 900, 531, 822, 119}, 2839: {2360, 2840}, 130: {2361, 1498}, 3255: {2361, 2556, 1521}, 614: {696, 2362}, 2418: {2753, 3012, 2364}, 866: {816, 121, 643, 814}, 669: {390, 561, 439, 123, 699}, 1199: {1220, 471}, 1540: {1541, 1222, 1799}, 1807: {2847, 1222, 1799}, 377: {673, 2371, 1001}, 2420: {672, 1931, 2374}, 2269: {2375, 3071}, 3213: {3212, 2375}, 3216: {3212, 2375}, 2965: {2376, 2573}, 617: {328, 243, 916, 661, 126}, 2792: {1227, 2623}, 1498: {130, 2139}, 2682: {2378, 2684, 2682}, 2683: {2378, 2684, 2682}, 1191: {1230, 1727}, 1514: {1989, 1230, 1241, 1979, 1727}, 2097: {1230, 1727}, 774: {1633, 2118, 880, 1236, 441}, 1002: {144, 817, 418, 141}, 141: {144, 418}, 453: {144, 417, 418, 416}, 3181: {2866, 2395}, 907: {146, 731, 493}, 149: {325, 1509, 1701, 149, 150, 917, 637}, 326: {325, 1509, 1701, 149, 150, 917, 637}, 1255: {1528, 2139, 1255}, 1721: {993, 429, 1254, 1919}, 754: {801, 1254, 782}, 2043: {1254, 1580, 1277, 1366}, 1284: {1264, 1642, 1416}, 639: {608, 167, 640, 799}, 306: {170, 615}, 494: {170, 1003, 767}, 1668: {1560, 1273, 1860}, 1714: {616, 1273, 291, 2686}, 2629: {2416, 2721, 3262}, 56: {505, 172, 1492}, 1934: {172, 1524}, 108: {173, 838, 383}, 384: {173, 838, 383}, 2437: {672, 2420, 3165, 3286}, 2371: {3140, 2422}, 1976: {1357, 1279}, 2756: {2426, 851, 3119}, 228: {305, 178, 295}, 232: {305, 178, 295}, 236: {305, 178, 295}, 1921: {1664, 1474, 2149, 1991, 553, 1419, 780, 2064, 2161, 179}, 185: {3043, 3267, 2855, 2641, 183, 605}, 605: {993, 452, 381, 183}, 3117: {2641, 3043, 183}, 1249: {184, 522, 275, 1416}, 422: {184, 554}, 1761: {748, 1460, 1526, 1464, 184}, 694: {291, 390, 616, 184, 698}, 714: {422, 999, 616, 748, 184, 699}, 3252: {184, 3253}, 999: {616, 714, 522, 748, 184, 699}, 1000: {390, 999, 616, 998, 522, 554, 714, 748, 1000, 275, 184, 699}, 2767: {3272, 2434, 806}, 2465: {2813, 2438, 2711}, 607: {192, 849}, 1344: {1889, 1291, 1382, 1926}, 1854: {1760, 1292}, 1953: {1760, 1292}, 255: {202, 654}, 1013: {1297, 1967}, 1089: {1297, 1347, 1500}, 1725: {1297, 1967}, 650: {387, 204}, 2563: {2448, 2562}, 499: {416, 691, 207}, 347: {209, 719}, 861: {281, 860, 2455}, 2037: {1307, 3228}, 724: {217, 591}, 65: {992, 218, 266, 876}, 1247: {218, 876}, 3303: {2976, 218, 876}, 2116: {1483, 1310}, 2470: {2467, 1516}, 722: {245, 787, 659, 333}, 1009: {320, 741, 691, 659, 245, 631, 729}, 1332: {2112, 1338, 2050, 2005}, 404: {1343, 1859, 693, 3183}, 541: {297, 262, 471}, 673: {291, 517, 390, 262, 553, 905, 428, 364, 538}, 2583: {352, 2486}, 122: {263, 297, 1004, 471, 2745}, 311: {2105, 283, 263}, 426: {424, 425, 263}, 566: {263, 510, 471}, 768: {905, 263}, 796: {769, 263}, 840: {643, 263}, 107: {265, 338, 372, 276, 502}, 194: {422, 265, 3122, 372, 445}, 501: {291, 265, 1004, 372, 502}, 502: {664, 265, 372, 1004}, 88: {905, 266, 780}, 3191: {905, 266, 2669}, 2081: {905, 266}, 1965: {1353, 1348, 1903}, 2578: {2696, 2490}, 80: {1545, 269}, 1207: {1352, 1539}, 1449: {1352, 1978}, 285: {272, 471}, 827: {272, 338, 276}, 432: {273, 1420, 705, 855}, 433: {432, 273}, 83: {274, 643}, 203: {274, 661}, 735: {274, 661}, 28: {390, 422, 999, 522, 714, 748, 275}, 956: {999, 1000, 714, 522, 748, 275}, 463: {338, 276}, 555: {338, 276, 901, 902}, 580: {338, 276, 581}, 581: {338, 276}, 901: {338, 276, 1004}, 902: {338, 276, 901}, 2292: {2969, 279}, 358: {279, 359}, 3005: {2550, 3006, 279}, 324: {1509, 326, 1701, 280, 637}, 350: {326, 615, 629, 280, 985}, 917: {288, 326, 978, 280, 985}, 144: {416, 281, 822}, 1946: {1363, 1759}, 1727: {1816, 1797, 1366}, 263: {283, 460}, 1513: {284, 1710}, 94: {390, 373, 374, 439, 444, 285, 445}, 159: {504, 291, 547, 285}, 2482: {285, 2943}, 445: {291, 390, 373, 374, 439, 285}, 896: {608, 450, 772, 390, 842, 922, 285}, 1355: {1368, 1414}, 1415: {1368, 1414}, 1476: {1368, 1414, 1791}, 2191: {2546, 2499}, 1061: {1460, 1374}, 1177: {349, 1374}, 1979: {1460, 1374}, 545: {544, 286}, 762: {1376, 673, 1001, 1990}, 170: {520, 348, 846, 287}, 675: {337, 287}, 692: {2651, 287}, 1162: {1381, 1517}, 1268: {1381, 1517}, 2079: {2021, 1381, 1517}, 67: {291, 998, 999, 616, 1000, 714, 748}, 167: {608, 291, 390, 364, 462, 439, 799}, 216: {291, 684}, 317: {291, 390}, 336: {390, 291, 598}, 462: {609, 291, 390, 364, 556}, 482: {291, 364, 390, 439}, 590: {291, 547, 525}, 640: {291, 639, 799}, 708: {291, 707, 709, 390, 483, 799}, 746: {291, 772, 805, 390, 804, 745, 747, 364, 302, 699}, 3130: {291, 3131}, 927: {291, 926}, 233: {390, 420, 292}, 1889: {2916, 1382}, 397: {396, 395, 652, 293}, 651: {928, 293, 369, 691, 986}, 1272: {1649, 1383}, 475: {558, 476, 742, 295}, 535: {295, 447}, 771: {342, 770, 566, 295}, 25: {296, 461}, 1007: {296, 655, 372, 501, 502}, 341: {1560, 1386, 3258}, 1982: {1388, 1437}, 1232: {1456, 297, 1457, 471}, 504: {390, 297, 747, 471, 510}, 3015: {2720, 3169, 3014, 297, 373, 471}, 764: {298, 919}, 1582: {1853, 1391}, 2599: {2514, 2979}, 3133: {2515, 3134}, 251: {1956, 517, 745, 364, 302, 373, 438, 699, 668, 669}, 804: {772, 805, 390, 745, 747, 364, 302, 439}, 911: {772, 804, 745, 747, 364, 302}, 841: {688, 572, 303}, 2557: {2521, 2726}, 1080: {1657, 1731, 1406}, 1314: {1731, 1735, 1736, 1737, 1584, 1657, 1406}, 1359: {1731, 1735, 1736, 1737, 1657, 1406}, 1364: {1731, 1406}, 1584: {1657, 1731, 1985, 1406}, 1658: {1731, 1406}, 1715: {1408, 1406}, 1731: {1731, 1732, 1406}, 1736: {1731, 1735, 1737, 1657, 1406}, 1737: {1657, 1731, 1406, 1735}, 1179: {1996, 1997, 1407}, 1216: {865, 1413, 1675, 1421, 1519, 1848}, 1240: {865, 1675, 1413}, 2446: {2525, 365}, 2342: {2528, 332}, 2529: {2530, 311}, 1144: {1416, 1481, 2105}, 1231: {1416, 769, 905, 2157}, 1234: {1416, 769, 905}, 2157: {1416, 769}, 315: {312, 313, 314}, 2533: {2531, 3020}, 622: {410, 316, 982}, 2534: {3125, 2535}, 51: {481, 482, 483, 390, 616, 747, 364, 1005, 439, 318}, 135: {610, 715, 716, 370, 319}, 2260: {1420, 3124}, 2699: {3107, 1420, 855}, 1770: {1771, 1420}, 3107: {1420, 855}, 1933: {865, 1675, 1421}, 1285: {2050, 1704, 1423, 1980, 1503}, 1542: {2050, 1423}, 1845: {2000, 2049, 1423}, 1628: {1424, 1753, 1949, 1752}, 2119: {1450, 1426}, 109: {848, 323}, 3274: {2545, 1858, 1859, 3103}, 661: {329, 643, 916}, 821: {329, 2591}, 948: {329, 661}, 137: {416, 330}, 2008: {1530, 1435}, 287: {337, 846}, 2402: {2784, 2553, 2785, 3031}, 2405: {2553, 2785}, 1858: {1857, 339}, 2224: {2558, 3022}, 246: {810, 2559}, 869: {932, 343}, 2570: {344, 2571}, 1273: {769, 2053, 1454}, 2072: {1827, 2052, 2054, 1709, 1455, 1878}, 3217: {2574, 615}, 1235: {2025, 1550, 1456, 1588, 1749}, 1850: {1458, 862}, 1374: {522, 1460}, 1464: {522, 1460}, 1243: {1648, 1706, 628, 1461}, 628: {753, 1461}, 2005: {2006, 1461, 2110}, 630: {348, 629, 375}, 982: {348, 981}, 2407: {354, 2579}, 351: {416, 354, 2579}, 2579: {354, 2580}, 2770: {354, 2580}, 1590: {2052, 1589, 1463}, 574: {616, 698, 349}, 575: {616, 698, 699, 349}, 515: {848, 629, 375, 985, 350}, 1298: {1465, 1770, 1918}, 353: {352, 354}, 2585: {354, 2586, 781}, 2586: {2584, 354, 2586}, 2584: {2585, 2586}, 363: {363, 765, 390}, 234: {571, 364, 390}, 244: {547, 483, 390, 364, 439, 926}, 253: {547, 483, 390, 364, 439, 926}, 259: {547, 483, 390, 364, 439, 926}, 365: {364, 439, 1004, 799}, 483: {390, 364, 926, 439}, 517: {364, 390, 439}, 523: {364, 390}, 525: {483, 517, 390, 364, 439}, 550: {364, 390}, 701: {390, 521, 364, 698, 699, 799}, 706: {608, 483, 772, 773, 390, 364, 946}, 910: {745, 747, 364, 390}, 918: {483, 364, 926, 439}, 2178: {2736, 2590}, 3296: {2592, 853}, 2325: {2594, 2595}, 1337: {370, 1870}, 1569: {1601, 1473}, 2065: {2064, 1474}, 2463: {373, 2669}, 374: {763, 373}, 3014: {3169, 373}, 3093: {897, 2599}, 1633: {378, 1851}, 3109: {2600, 2998, 3087}, 1661: {1660, 1525, 1479}, 2012: {2010, 2011, 1479, 1879}, 2271: {2603, 2652}, 2602: {2704, 2603}, 684: {953, 969, 381}, 713: {381, 798}, 1738: {1842, 1843, 1493}, 2617: {2616, 2618, 2614}, 2618: {2616, 2619}, 386: {584, 385, 932}, 240: {386, 932, 584, 618, 654, 794}, 579: {753, 788, 389, 388}, 196: {973, 389}, 334: {756, 389}, 467: {389, 767}, 936: {3020, 389}, 7: {616, 1001, 1004, 390}, 10: {440, 390}, 22: {699, 390}, 58: {554, 390, 414, 439}, 82: {658, 390}, 85: {698, 699, 390}, 98: {699, 390}, 147: {456, 390, 927}, 151: {390, 598}, 155: {699, 390}, 156: {699, 390}, 200: {390, 582}, 212: {963, 390, 439}, 219: {681, 390}, 231: {681, 390}, 248: {681, 390}, 391: {390, 799}, 414: {390, 439}, 456: {698, 390, 927}, 507: {541, 390}, 557: {897, 390}, 576: {873, 390}, 700: {698, 699, 390}, 702: {483, 390, 439}, 711: {960, 673, 437, 390}, 727: {768, 665, 390, 968}, 744: {390, 447}, 773: {608, 773, 390}, 812: {390, 407}, 880: {441, 390}, 906: {699, 390}, 189: {483, 391}, 914: {392, 2651}, 402: {400, 401, 3185}, 405: {400, 401, 403, 404, 693}, 400: {401, 916, 693}, 401: {404, 693, 3183}, 403: {404, 631}, 2134: {405, 1511}, 789: {738, 406}, 191: {409, 661, 847}, 1502: {1501, 1877}, 174: {410, 509}, 241: {410, 659, 631}, 570: {410, 659, 492}, 975: {410, 564}, 2466: {2868, 413}, 142: {416, 496, 472}, 879: {416, 530, 822}, 1003: {416, 659, 631}, 281: {419, 822}, 360: {419, 822}, 1899: {1898, 422}, 1150: {1912, 1507}, 150: {1509, 637, 917, 1701}, 325: {1509, 637, 917, 1701}, 327: {1509, 637, 917, 1701}, 1639: {1673, 1509}, 1172: {429, 1710}, 305: {1512, 429, 1710}, 1409: {1957, 1512, 429, 1710, 1681}, 1512: {429, 1710}, 2639: {2640, 782}, 2645: {2643, 2644}, 915: {432, 433}, 2282: {2646, 855}, 506: {434, 938, 847}, 332: {439, 799}, 755: {699, 502, 439}, 871: {517, 439}, 2447: {2657, 2658, 441}, 1478: {1528, 441, 1690}, 1531: {441, 2658, 1801}, 2765: {1528, 441, 2995}, 645: {880, 441, 1633}, 1814: {1528, 1690}, 2970: {1528, 3210}, 1529: {3021, 2169, 3305, 2654}, 2166: {3021, 2169, 3305, 2654}, 93: {800, 763, 447}, 800: {763, 447}, 451: {450, 773}, 1394: {2136, 1537, 677}, 2669: {905, 2668}, 2670: {905, 2668}, 1266: {1540, 1861}, 2293: {2774, 1543}, 1326: {1667, 1796, 1543, 828, 1662}, 3159: {460, 3156}, 1624: {1549, 1903}, 978: {1549, 591}, 2382: {795, 2678}, 30: {505, 471}, 556: {2894, 471}, 2687: {2688, 2689}, 1554: {1553, 1831}, 90: {474, 862, 1631}, 753: {477, 853}, 1321: {1562, 1646}, 2441: {3107, 2699, 855}, 138: {486, 847}, 42: {488, 644, 487}, 271: {643, 612, 644, 487, 488}, 950: {488, 611, 487}, 17: {488, 601, 832, 663}, 834: {488, 601, 831}, 955: {488, 612, 613}, 964: {488, 834, 612}, 3231: {3058, 2700}, 2647: {3085, 2702}, 805: {745, 490, 747, 698, 763}, 2864: {2705, 2863, 3286, 3185}, 492: {493, 631}, 139: {731, 495}, 208: {659, 495}, 667: {666, 495}, 935: {995, 712, 495, 657, 691, 788, 979, 986}, 979: {743, 495, 659, 631, 731}, 2438: {3244, 2813, 2711}, 2531: {3020, 2711}, 2532: {3020, 2711}, 2667: {3244, 2711}, 171: {817, 506}, 394: {938, 506, 940, 847}, 721: {722, 506}, 843: {506, 847}, 206: {507, 1628}, 2653: {3256, 2713}, 2873: {2827, 1580}, 2078: {1929, 1582}, 616: {521, 522}, 1336: {1831, 1863, 1004, 1005, 1586}, 1128: {1589, 1590}, 1593: {1592, 1594, 1692}, 1829: {1602, 3037}, 2337: {2731, 3124}, 188: {531, 814, 791}, 2349: {3195, 2733}, 2707: {2739, 2982}, 1122: {1690, 2742}, 1452: {2002, 1618}, 1188: {1619, 1918}, 464: {2745, 739}, 1275: {1924, 1622}, 1315: {1625, 1810}, 1224: {2009, 1706, 1626}, 2889: {2752, 2820}, 14: {544, 1630}, 572: {616, 698, 547, 574}, 1129: {862, 1631}, 2257: {1634, 3207}, 1289: {2034, 1636}, 1181: {1637, 1638}, 1375: {1637, 1638}, 1356: {2083, 1638}, 1637: {2083, 1638}, 2610: {2759, 2823}, 1810: {1640, 1694}, 2728: {3174, 553, 905, 887, 891}, 2791: {3208, 553}, 1951: {553, 1850, 887}, 2161: {553, 905, 2081}, 2266: {556, 2894}, 257: {556, 573}, 775: {857, 564, 847}, 770: {3040, 3041, 566}, 1868: {1856, 1652}, 1735: {1736, 1657, 1731}, 95: {571, 698, 2923, 574}, 549: {646, 616, 698, 572, 574, 575}, 965: {577, 612}, 2272: {2777, 3007}, 2432: {2784, 3031}, 2553: {2784, 2785}, 1194: {1680, 1731, 2135}, 808: {809, 587, 643, 814}, 2461: {681, 2790}, 1325: {1683, 1839}, 1684: {1683, 1878}, 1241: {2795, 1989}, 217: {591, 615}, 2176: {1689, 1908}, 593: {595, 847}, 594: {595, 847}, 1992: {1691, 1815}, 160: {753, 980, 596}, 2807: {2805, 2806}, 199: {606, 2855}, 1334: {1699, 1804}, 1703: {2049, 1702}, 1617: {1705, 1820}, 1258: {1706, 2006}, 610: {1706, 715}, 2295: {2832, 3161}, 583: {612, 847}, 611: {644, 613}, 3205: {3234, 615}, 571: {616, 698}, 748: {616, 699}, 998: {998, 999, 616, 1000, 748}, 920: {619, 1004, 1005}, 2840: {3107, 2839}, 623: {620, 621, 623, 624, 625}, 70: {659, 631}, 226: {986, 631}, 495: {986, 659, 631}, 980: {847, 848, 659, 979, 631, 986}, 995: {986, 631}, 1717: {1729, 1716, 1717}, 1887: {1888, 1717}, 2068: {1722, 2069}, 1071: {1985, 1731}, 1657: {1731, 1732}, 1680: {1731, 2135}, 1985: {1731, 1732}, 35: {643, 814}, 121: {816, 866, 643, 814}, 136: {809, 643, 814}, 313: {850, 643, 661}, 314: {643, 661}, 348: {848, 643}, 807: {809, 643, 814}, 814: {848, 643, 847}, 994: {643, 996}, 1895: {780, 646}, 2150: {2148, 646}, 197: {898, 647}, 898: {691, 647}, 946: {2865, 2031}, 2972: {2874, 3220}, 3105: {2874, 3220}, 1949: {1752, 1753}, 214: {860, 654}, 1784: {2022, 1927, 1928, 1786, 1756}, 1785: {1928, 1756, 1927}, 2319: {2877, 2878}, 3201: {2878, 3046}, 166: {656, 731}, 1911: {1762, 1924, 1910}, 877: {937, 718, 659, 661, 662, 856}, 986: {659, 660}, 40: {849, 660, 847}, 875: {986, 660, 788}, 328: {948, 661}, 835: {837, 661}, 852: {908, 661}, 858: {781, 663}, 728: {768, 665, 727}, 1099: {1768, 1767}, 2123: {1770, 1771}, 1984: {677, 678}, 867: {677, 678}, 379: {954, 678}, 1288: {1787, 1780}, 519: {691, 814, 847}, 1576: {1782, 1935}, 2363: {1789, 2927}, 1794: {1792, 795}, 1777: {2048, 1793}, 2867: {2929, 3019}, 2419: {2930, 3043}, 2980: {2930, 3043}, 752: {704, 994, 996, 751}, 2308: {2934, 3270}, 2108: {2107, 1798}, 707: {709, 799}, 2869: {2936, 3170}, 1805: {2145, 1806}, 1974: {1809, 1977}, 2950: {2953, 2956}, 2251: {2955, 3173}, 2381: {3186, 716}, 2961: {3186, 716}, 2048: {2049, 1906, 1819}, 2399: {2976, 3303}, 730: {729, 737, 942, 847}, 320: {736, 737, 730}, 2549: {3138, 2981}, 2454: {2984, 2985, 2983}, 2984: {2985, 2983}, 2986: {2985, 2983}, 77: {916, 735}, 729: {737, 847}, 1790: {2022, 1927, 1928, 1847, 1915}, 1849: {1848, 3124}, 3009: {3008, 1851}, 1295: {2018, 1852}, 3233: {3232, 2996}, 712: {750, 847}, 874: {753, 3155}, 193: {760, 758, 759}, 1873: {1869, 1919}, 1663: {1920, 1873}, 2477: {3032, 3259}, 473: {822, 767}, 1876: {768, 2081}, 2686: {769, 905}, 2415: {960, 3041}, 1894: {956, 1893}, 2234: {3051, 3052}, 2510: {3051, 3052}, 1300: {2080, 1901}, 2032: {2049, 2050, 1903, 2033, 1980}, 2033: {2049, 2050, 1980, 1903}, 3066: {3065, 3067}, 2826: {1906, 3123}, 247: {1003, 933, 806}, 3082: {3081, 3084}, 2143: {1913, 2142}, 816: {848, 814, 847}, 853: {2050, 1916}, 1276: {2098, 1918}, 1308: {2049, 1918}, 2175: {2049, 2077, 1918, 2159}, 3097: {3096, 3095}, 3182: {3096, 3095}, 2075: {2049, 1925}, 242: {979, 822, 847}, 3101: {3100, 3102}, 1927: {1928, 2022}, 2428: {3106, 3178}, 1698: {1929, 3123}, 604: {850, 846}, 186: {966, 847}, 509: {848, 847}, 595: {916, 847}, 826: {912, 847}, 983: {848, 847}, 984: {848, 847}, 2336: {851, 3119}, 1252: {2049, 853}, 474: {1968, 862}, 1968: {2160, 1972, 862}, 1398: {1970, 1971, 2019}, 1400: {1970, 1971, 2019}, 3150: {3146, 3147, 3149}, 3146: {3147, 3148, 3149, 3150, 3151}, 2429: {874, 3155}, 2110: {2109, 2006}, 1341: {2008, 2056}, 2334: {3290, 3167}, 2444: {3168, 3290, 3167}, 3168: {3290, 3167}, 703: {892, 893}, 1238: {946, 2031}, 2247: {3188, 3189}, 446: {3273, 909}, 1133: {2169, 2046}, 1846: {2048, 2049}, 2468: {3202, 3204}, 2054: {2145, 2055}, 2087: {2089, 2085}, 2102: {2100, 2101}, 3222: {3240, 3281}, 938: {940, 941}, 3263: {3260, 3261}, 3266: {3267, 3268}, 2129: {2130, 2131, 963}, 990: {994, 980, 989}, 1170: {2169, 2166}}, 'features': matrix([[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 1., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), 'labels': array([[0, 1, 0, 0, 0, 0],
       [0, 0, 0, 0, 1, 0],
       [0, 1, 0, 0, 0, 0],
       ...,
       [0, 0, 0, 0, 1, 0],
       [1, 0, 0, 0, 0, 0],
       [0, 0, 0, 1, 0, 0]], dtype=int32), 'n': 3312, 'feature': array([[[ 0.        ,  0.        ,  0.        , ..., -0.31035982,
          0.2382891 ,  0.67195495]],

       [[ 0.        ,  0.        ,  0.        , ..., -0.39669618,
          0.2382891 ,  0.8980048 ]],

       [[ 0.        ,  1.        ,  0.        , ..., -0.66297483,
         -0.66297483,  0.        ]],

       ...,

       [[ 0.        ,  0.        ,  0.        , ...,  0.19180139,
          0.19180139,  0.        ]],

       [[ 0.        ,  0.        ,  0.        , ...,  0.66368589,
          0.66368589,  0.        ]],

       [[ 0.        ,  0.        ,  0.        , ...,  0.181585  ,
          0.181585  ,  0.        ]]])})
Namespace(add_encodings=True, encodings='LCP', data='cocitation', dataset='citeseer', model_name='UniGAT', first_aggregate='mean', second_aggregate='sum', add_self_loop=True, use_norm=True, activation='relu', nlayer=2, nhid=8, nhead=8, dropout=0.6, input_drop=0.6, attn_drop=0.6, lr=0.01, wd=0.0005, epochs=200, n_runs=10, gpu=0, seed=1, patience=200, nostdout=False, split=1, out_dir='runs/test', dataset_dict={'hypergraph': {415: {0, 953}, 514: {0, 384, 323, 173, 383}, 585: {0, 86}, 690: {0, 691, 271}, 784: {0, 290, 540}, 153: {1, 291, 422, 390, 522, 714, 748, 275, 699, 28}, 732: {1, 646, 905, 266, 780, 533, 534, 284, 31, 289, 295, 429, 430, 78, 471, 218, 862, 876, 505}, 1937: {1, 1850, 862, 1129}, 1034: {1011, 2007}, 2028: {2040, 1011}, 2029: {2040, 1011}, 962: {2, 3151}, 2181: {648, 2865, 1012}, 2031: {1012, 1349}, 1417: {1500, 1013, 1725}, 1427: {1545, 1013, 1725}, 1545: {1013, 1725}, 2024: {1058, 1445, 2118, 1801, 1236, 1013, 1046, 1500, 1725}, 1059: {1744, 1380, 1733, 1014}, 1365: {1059, 1014}, 1378: {1059, 1380, 1680, 1744, 1014}, 1236: {1088, 1058, 1891, 1219, 1860, 1067, 1070, 1015, 1396, 1687, 441, 1242, 1500, 1150}, 1439: {1891, 1444, 1478, 1015, 1083}, 1440: {1444, 1478, 1015, 1083, 1439}, 2093: {1083, 1397, 1015}, 2677: {2183, 79}, 845: {32, 685, 2183}, 3156: {460, 2183}, 2138: {1016, 1620}, 634: {3, 295, 749, 304, 819, 503, 120}, 2306: {2184, 2306, 2261}, 2356: {2184, 2306, 2261}, 2539: {2185, 3042, 2812}, 1549: {1624, 1017, 978, 1549}, 591: {1017, 326}, 588: {739, 4, 294}, 2064: {646, 2091, 780, 1019, 30}, 36: {97, 5, 329, 274, 916, 661}, 63: {97, 5, 329, 274, 916, 661}, 97: {329, 5, 661}, 603: {5, 661}, 949: {97, 5, 63}, 1176: {1896, 1021}, 64: {643, 6}, 682: {643, 6}, 1527: {1024, 1077}, 868: {514, 867, 677, 8, 44, 14, 270, 431, 308, 341, 925, 478}, 1986: {8, 308}, 1100: {2105, 1634, 1029, 1695}, 1109: {1401, 1029}, 1248: {1538, 1029, 1141, 1911}, 1244: {2128, 1941, 1030}, 1487: {1794, 2003, 1031, 1930}, 1488: {961, 1645, 1031}, 1872: {1488, 1031}, 158: {568, 9, 583}, 202: {385, 386, 387, 932, 933, 584, 9, 654, 847}, 260: {9, 290, 33}, 559: {9, 753}, 820: {9, 398}, 970: {824, 9, 933, 158}, 387: {434, 11}, 945: {11, 204}, 1759: {1033, 1946, 1363, 2014}, 2014: {1033, 2015}, 2015: {1033, 2014}, 1491: {1035, 1054}, 1042: {1037, 1615}, 1309: {1040, 1037}, 1679: {1411, 1475, 1037, 1040, 1042, 1047}, 55: {2196, 972}, 3279: {2196, 972, 2901, 55}, 1107: {1776, 1769, 1090, 1038}, 1174: {1776, 1769, 1090, 1038}, 1404: {1090, 1924, 1038, 1776, 1107, 1174}, 1536: {57, 1038, 2119}, 1636: {57, 1038, 1047}, 1682: {57, 1636, 1038}, 1769: {57, 1038}, 1212: {1937, 1850, 862, 1039}, 1076: {1040, 1599}, 1078: {1040, 1371, 1076, 1599}, 2036: {1041, 1307, 1475, 1119}, 2460: {1667, 1543, 2202, 79, 3153, 3028, 3162, 828}, 779: {1664, 1279, 1895, 1006, 882, 179, 3028, 887, 88, 2202, 31}, 828: {609, 34, 1667, 37, 1543, 462, 79, 3028, 278, 2202}, 3162: {609, 2202, 3028, 79}, 1037: {1042, 1615}, 1138: {1634, 1043}, 1828: {1634, 1043, 2139, 1695}, 1893: {1634, 1894, 1351, 1043, 1116}, 2139: {1481, 1634, 1043, 1116}, 2165: {1634, 1043}, 1274: {1892, 1044}, 1453: {1045, 1470}, 1704: {2080, 1705, 1045, 1470}, 1705: {2080, 1916, 1045, 1470}, 1944: {1886, 1470, 1045, 1870}, 1954: {2080, 2002, 1045, 1470}, 1955: {2002, 1045, 1470}, 1980: {2050, 1542, 1423, 1045, 1143, 1470}, 2002: {2003, 1045, 1470}, 1286: {2657, 1125, 1902, 1046, 1532}, 1302: {2657, 1125, 1902, 1046, 1532}, 1532: {2658, 1532, 1046}, 1891: {1690, 1125, 1046}, 1127: {57, 1047}, 1613: {1047, 1599}, 1776: {2034, 1047, 1775}, 2675: {3099, 2206}, 1168: {1050, 172, 1524}, 1254: {1057, 1050, 1787, 1524}, 508: {12, 502, 655}, 1741: {1740, 12}, 777: {364, 12, 799}, 1253: {1890, 1636, 1253, 57, 1051}, 2275: {2210, 1707, 3085}, 1707: {969, 2210, 1063}, 1961: {1066, 1055}, 1962: {1066, 1055}, 1963: {1066, 1055}, 250: {691, 13}, 145: {16, 497, 822, 472, 542}, 2244: {1634, 2213}, 2845: {3235, 2214}, 1014: {1680, 1059, 1380, 1744}, 59: {18, 604, 847}, 6: {20, 663}, 1123: {1864, 1060}, 680: {100, 934, 679, 390, 551, 1005, 21, 439}, 303: {698, 390, 22}, 455: {1008, 390, 22}, 469: {422, 522, 554, 275, 22}, 599: {598, 22}, 668: {390, 616, 268, 699, 22, 439, 698, 123, 701, 766}, 899: {390, 22}, 2391: {2489, 2219}, 2665: {2242, 738, 3080, 441, 2221, 2745, 31}, 3115: {2745, 2221}, 309: {53, 23}, 704: {23, 855}, 127: {24, 297, 471}, 685: {769, 903, 263, 905, 266, 460, 24}, 947: {24, 378}, 952: {99, 164, 518, 423, 552, 588, 24}, 1229: {1707, 1063}, 2567: {2222, 2566}, 1811: {1764, 1064, 1900, 1230, 795, 1918, 1727}, 69: {296, 265, 461, 338, 276, 372, 505, 25}, 357: {888, 25}, 655: {296, 265, 561, 338, 276, 372, 537, 25}, 683: {25, 291}, 888: {265, 461, 338, 276, 372, 25}, 2780: {2224, 2558, 3022}, 393: {27, 940}, 1093: {1065, 2027}, 235: {1568, 739, 1226, 1068, 592}, 592: {32, 1568, 739, 615, 392, 1226, 1068, 685}, 2540: {2232, 3033, 3202}, 3033: {2232, 3057, 3202, 2540}, 3203: {2232, 2322, 3202}, 2698: {2233, 2591}, 3158: {2233, 339}, 1744: {1733, 1071}, 48: {28, 748, 422}, 54: {67, 422, 999, 390, 998, 522, 714, 748, 28, 275, 184, 699, 956}, 105: {67, 422, 999, 998, 522, 714, 748, 275, 699, 28}, 106: {291, 422, 999, 390, 998, 522, 554, 714, 748, 48, 275, 184, 442, 699, 28}, 633: {469, 956, 28, 422}, 987: {422, 999, 714, 522, 748, 275, 699, 28}, 2193: {2236, 1431}, 1203: {2148, 780, 30}, 1239: {1788, 780, 30}, 1282: {1766, 646, 1740, 780, 179, 30, 31}, 1650: {646, 1383, 780, 1649, 179, 567, 30}, 1788: {2066, 780, 1165, 30}, 1991: {1664, 1895, 780, 179, 30}, 2149: {1374, 1989, 646, 780, 1979, 30}, 1075: {1073, 179, 1220, 471}, 740: {793, 29}, 793: {792, 793, 29}, 818: {793, 1708, 29}, 859: {792, 793, 29}, 1357: {31, 471}, 297: {266, 179, 31}, 1489: {218, 876, 31, 1863}, 2774: {1357, 31}, 2812: {31, 471}, 646: {179, 31}, 3028: {1667, 31}, 882: {31, 295}, 2082: {2082, 2083, 31}, 1169: {1075, 1387, 1220, 471}, 1190: {1613, 1078}, 1600: {1078, 1599}, 665: {32, 768, 685, 2081}, 686: {32, 769, 2494, 2278, 905, 2669, 1678, 685, 1876, 887, 1469, 2686}, 687: {32, 769, 905, 685, 1876, 887, 2686}, 697: {390, 905, 522, 275, 285, 32, 34, 547, 685, 439, 184, 698, 699, 700, 58, 571, 572, 574, 459, 85, 469, 471, 482, 364, 748, 887}, 1245: {2076, 1079}, 1651: {1079, 3262, 1823}, 1823: {3262, 1079}, 1189: {1080, 1657, 1583}, 1583: {1314, 1731, 1736, 1359, 1080, 1657}, 1141: {1081, 1866}, 1950: {1081, 1599}, 2324: {2243, 2324, 3175}, 3074: {2243, 2324, 2507, 3175}, 1083: {1912, 1083, 1373}, 1196: {1083, 1180}, 180: {34, 58, 390, 105, 842, 717, 303, 688, 698, 987, 572}, 366: {34, 699, 700, 85}, 1587: {34, 1586, 1197}, 573: {34, 547, 390, 459, 689, 179, 152, 572}, 829: {34, 556, 573}, 2310: {2245, 3125}, 2390: {2245, 3125}, 2483: {2245, 3125}, 1480: {1086, 1479}, 500: {35, 892}, 1608: {1392, 57, 1090}, 3034: {3035, 2246}, 2009: {1626, 1092, 1772}, 1267: {1724, 37, 278}, 182: {130, 37, 390, 523, 278}, 2464: {2858, 37, 278}, 310: {37, 517, 272, 278, 123, 699, 574}, 444: {37, 390, 373, 374, 439, 278, 445}, 1724: {37, 876, 1677, 79, 278, 218}, 2026: {1977, 1093, 1637}, 2746: {2248, 3195, 2733}, 1317: {1108, 1094}, 1676: {1108, 1094}, 1010: {644, 39, 488, 298, 886, 919}, 146: {416, 418, 230, 40, 144, 822}, 148: {40, 230}, 177: {40, 230}, 411: {40, 410, 230, 631}, 480: {40, 230}, 621: {230, 40, 620, 623, 624}, 625: {40, 624, 230}, 1490: {1096, 2052, 1709}, 133: {1098, 323}, 418: {416, 41, 300, 822, 281}, 1579: {2105, 1100}, 1723: {1528, 441, 1100, 2105}, 1609: {1972, 1101}, 1673: {1102, 1959}, 1958: {1673, 354, 1102, 1959}, 632: {868, 44}, 2103: {1103, 1143}, 733: {45, 734, 855}, 751: {704, 996, 45, 734}, 2476: {2736, 2258, 2590}, 1132: {1106, 1860}, 1339: {1560, 1273, 1106}, 2744: {2833, 2259, 2724, 3078}, 1200: {1107, 1174}, 1425: {1107, 1174}, 1813: {2955, 1812, 2957, 2263}, 2959: {2957, 2263}, 2223: {2264, 3171, 2491}, 2204: {2265, 2762}, 213: {293, 872, 396, 397, 47, 447}, 778: {293, 872, 396, 397, 47, 447}, 2104: {218, 876, 1114, 2084}, 355: {483, 772, 69, 390, 747, 364, 1004, 976, 977, 51, 439}, 356: {291, 355, 773, 69, 390, 745, 747, 51, 285}, 438: {745, 302, 51, 179, 437, 439}, 470: {51, 291, 364, 390}, 481: {482, 483, 390, 616, 747, 364, 1004, 1005, 51, 439, 698, 318}, 516: {291, 772, 69, 390, 483, 805, 745, 747, 364, 1004, 272, 976, 977, 51, 437, 439, 408, 699}, 597: {608, 483, 390, 332, 51, 87, 439, 318}, 161: {53, 309}, 2044: {2664, 2274, 1222, 1799}, 1210: {1120, 1818, 1206}, 1256: {1120, 1818, 1206}, 1604: {1120, 1206, 1606}, 1606: {1120, 1206}, 1818: {1120, 1560, 1256, 1206}, 971: {362, 972, 524, 55}, 1824: {56, 1512}, 157: {57, 290, 641}, 1981: {57, 2034}, 302: {58, 364, 439}, 626: {2843, 2276, 86}, 586: {422, 469, 184, 443, 60}, 1198: {2278, 2494, 1678}, 1469: {2278, 2494, 1678}, 3040: {905, 2494, 2278}, 944: {61, 943}, 802: {673, 801, 390, 905, 297, 1004, 754, 566, 471, 668, 62}, 1588: {1128, 1456}, 1969: {1129, 905, 862}, 1885: {1131, 1883, 1884, 1646}, 91: {64, 682, 795}, 210: {64, 682, 279}, 239: {64, 704, 643, 682, 203, 498}, 569: {64, 866, 612, 488, 682, 115, 600}, 152: {65, 291, 266, 566, 88}, 176: {67, 390, 998, 105, 987, 830}, 1422: {1542, 1134, 2006, 1655, 1916}, 1747: {1984, 1986, 867, 677, 678, 1135, 1720, 2136, 1630}, 1494: {1136, 201, 108, 1577}, 1964: {2049, 1223, 2121, 1137, 1203, 1972}, 2616: {2285, 2615}, 1801: {1632, 1633, 1138, 2168, 441, 1851}, 2168: {1632, 1138, 2168, 441, 1851}, 750: {70, 659, 979, 631, 986}, 939: {930, 71, 938, 940, 506}, 813: {339, 916, 77, 735}, 1139: {1145, 1842, 1716}, 534: {505, 533, 78, 862}, 2253: {2891, 2299}, 2259: {354, 2299}, 1874: {1147, 1743}, 1019: {1312, 1354, 79, 1743, 1938}, 103: {272, 364, 390, 79}, 278: {79, 1175}, 460: {539, 86, 79}, 1687: {1148, 1533}, 710: {450, 1148, 1533}, 1262: {1150, 1509, 1646, 421}, 1294: {1506, 1150}, 421: {353, 1150, 1646, 1687}, 1508: {1150, 1687}, 2118: {1242, 1150}, 1301: {1151, 695}, 1960: {1151, 695}, 3149: {2312, 3147, 3150}, 743: {84, 495}, 619: {390, 522, 1004, 275, 85, 698, 699, 700, 349}, 1329: {1153, 1516, 1438}, 1342: {1153, 1516, 1438}, 1629: {1153, 1585}, 26: {1634, 86, 263}, 262: {471, 780, 272, 373, 86, 887}, 272: {780, 86, 471}, 769: {86, 263}, 797: {297, 262, 86, 471}, 1006: {460, 780, 86}, 1214: {1158, 1994, 1708, 2004, 2006}, 1461: {628, 2004, 1158}, 1571: {1158, 2004, 2006}, 154: {291, 483, 390, 460, 88}, 1281: {1678, 1998, 1999, 887, 88, 1850}, 1998: {88, 1850, 887}, 1999: {1678, 887, 1976, 1850, 88}, 2073: {88, 1678}, 1257: {1880, 1852, 1159}, 1287: {1451, 1573, 1159}, 1369: {1160, 2083}, 546: {89, 788}, 548: {89, 546, 986}, 2561: {2877, 2878, 2319}, 2401: {2320, 218, 876}, 2638: {2919, 876, 2509, 2320, 218}, 2605: {2322, 2613}, 2594: {2595, 2325}, 1511: {1168, 1707}, 2106: {1168, 1751}, 3153: {2328, 1667}, 1173: {1812, 2955, 2948, 2332}, 1360: {1905, 1175}, 1385: {1385, 2174, 1175}, 2173: {2174, 1175}, 1897: {1176, 2969}, 3200: {2832, 2335}, 5: {97, 916}, 1178: {1944, 1178, 1870}, 723: {99, 264, 396, 814, 847, 816, 764}, 1836: {1838, 1835, 1755, 1183}, 1837: {1834, 1755, 1838, 1183}, 2596: {378, 2339}, 974: {2339, 947}, 267: {179, 780, 102}, 2400: {876, 725, 2340}, 725: {992, 218, 876, 2340}, 129: {104, 589}, 131: {104, 589}, 134: {104, 691, 589}, 140: {104, 589}, 1945: {1944, 1185, 1870}, 1648: {1186, 2050, 1461}, 2456: {2344, 2552, 2624, 3228}, 2624: {2344, 3228, 3293}, 2230: {2345, 3269}, 383: {201, 514, 108, 838}, 2125: {201, 108, 1494}, 2860: {2347, 1005}, 1548: {1193, 1966}, 2112: {2050, 1193, 1966, 2006, 1980, 2110, 2111}, 584: {281, 389, 110}, 2548: {2348, 2598}, 382: {390, 391, 364, 113, 439}, 3013: {3274, 821, 1858, 2351}, 1183: {1841, 1202}, 671: {600, 115}, 1432: {1203, 2151}, 2151: {1203, 1918}, 2354: {2808, 2357}, 1748: {1209, 1515}, 810: {448, 417, 900, 531, 822, 119}, 2839: {2360, 2840}, 130: {2361, 1498}, 3255: {2361, 2556, 1521}, 614: {696, 2362}, 2418: {2753, 3012, 2364}, 866: {816, 121, 643, 814}, 669: {390, 561, 439, 123, 699}, 1199: {1220, 471}, 1540: {1541, 1222, 1799}, 1807: {2847, 1222, 1799}, 377: {673, 2371, 1001}, 2420: {672, 1931, 2374}, 2269: {2375, 3071}, 3213: {3212, 2375}, 3216: {3212, 2375}, 2965: {2376, 2573}, 617: {328, 243, 916, 661, 126}, 2792: {1227, 2623}, 1498: {130, 2139}, 2682: {2378, 2684, 2682}, 2683: {2378, 2684, 2682}, 1191: {1230, 1727}, 1514: {1989, 1230, 1241, 1979, 1727}, 2097: {1230, 1727}, 774: {1633, 2118, 880, 1236, 441}, 1002: {144, 817, 418, 141}, 141: {144, 418}, 453: {144, 417, 418, 416}, 3181: {2866, 2395}, 907: {146, 731, 493}, 149: {325, 1509, 1701, 149, 150, 917, 637}, 326: {325, 1509, 1701, 149, 150, 917, 637}, 1255: {1528, 2139, 1255}, 1721: {993, 429, 1254, 1919}, 754: {801, 1254, 782}, 2043: {1254, 1580, 1277, 1366}, 1284: {1264, 1642, 1416}, 639: {608, 167, 640, 799}, 306: {170, 615}, 494: {170, 1003, 767}, 1668: {1560, 1273, 1860}, 1714: {616, 1273, 291, 2686}, 2629: {2416, 2721, 3262}, 56: {505, 172, 1492}, 1934: {172, 1524}, 108: {173, 838, 383}, 384: {173, 838, 383}, 2437: {672, 2420, 3165, 3286}, 2371: {3140, 2422}, 1976: {1357, 1279}, 2756: {2426, 851, 3119}, 228: {305, 178, 295}, 232: {305, 178, 295}, 236: {305, 178, 295}, 1921: {1664, 1474, 2149, 1991, 553, 1419, 780, 2064, 2161, 179}, 185: {3043, 3267, 2855, 2641, 183, 605}, 605: {993, 452, 381, 183}, 3117: {2641, 3043, 183}, 1249: {184, 522, 275, 1416}, 422: {184, 554}, 1761: {748, 1460, 1526, 1464, 184}, 694: {291, 390, 616, 184, 698}, 714: {422, 999, 616, 748, 184, 699}, 3252: {184, 3253}, 999: {616, 714, 522, 748, 184, 699}, 1000: {390, 999, 616, 998, 522, 554, 714, 748, 1000, 275, 184, 699}, 2767: {3272, 2434, 806}, 2465: {2813, 2438, 2711}, 607: {192, 849}, 1344: {1889, 1291, 1382, 1926}, 1854: {1760, 1292}, 1953: {1760, 1292}, 255: {202, 654}, 1013: {1297, 1967}, 1089: {1297, 1347, 1500}, 1725: {1297, 1967}, 650: {387, 204}, 2563: {2448, 2562}, 499: {416, 691, 207}, 347: {209, 719}, 861: {281, 860, 2455}, 2037: {1307, 3228}, 724: {217, 591}, 65: {992, 218, 266, 876}, 1247: {218, 876}, 3303: {2976, 218, 876}, 2116: {1483, 1310}, 2470: {2467, 1516}, 722: {245, 787, 659, 333}, 1009: {320, 741, 691, 659, 245, 631, 729}, 1332: {2112, 1338, 2050, 2005}, 404: {1343, 1859, 693, 3183}, 541: {297, 262, 471}, 673: {291, 517, 390, 262, 553, 905, 428, 364, 538}, 2583: {352, 2486}, 122: {263, 297, 1004, 471, 2745}, 311: {2105, 283, 263}, 426: {424, 425, 263}, 566: {263, 510, 471}, 768: {905, 263}, 796: {769, 263}, 840: {643, 263}, 107: {265, 338, 372, 276, 502}, 194: {422, 265, 3122, 372, 445}, 501: {291, 265, 1004, 372, 502}, 502: {664, 265, 372, 1004}, 88: {905, 266, 780}, 3191: {905, 266, 2669}, 2081: {905, 266}, 1965: {1353, 1348, 1903}, 2578: {2696, 2490}, 80: {1545, 269}, 1207: {1352, 1539}, 1449: {1352, 1978}, 285: {272, 471}, 827: {272, 338, 276}, 432: {273, 1420, 705, 855}, 433: {432, 273}, 83: {274, 643}, 203: {274, 661}, 735: {274, 661}, 28: {390, 422, 999, 522, 714, 748, 275}, 956: {999, 1000, 714, 522, 748, 275}, 463: {338, 276}, 555: {338, 276, 901, 902}, 580: {338, 276, 581}, 581: {338, 276}, 901: {338, 276, 1004}, 902: {338, 276, 901}, 2292: {2969, 279}, 358: {279, 359}, 3005: {2550, 3006, 279}, 324: {1509, 326, 1701, 280, 637}, 350: {326, 615, 629, 280, 985}, 917: {288, 326, 978, 280, 985}, 144: {416, 281, 822}, 1946: {1363, 1759}, 1727: {1816, 1797, 1366}, 263: {283, 460}, 1513: {284, 1710}, 94: {390, 373, 374, 439, 444, 285, 445}, 159: {504, 291, 547, 285}, 2482: {285, 2943}, 445: {291, 390, 373, 374, 439, 285}, 896: {608, 450, 772, 390, 842, 922, 285}, 1355: {1368, 1414}, 1415: {1368, 1414}, 1476: {1368, 1414, 1791}, 2191: {2546, 2499}, 1061: {1460, 1374}, 1177: {349, 1374}, 1979: {1460, 1374}, 545: {544, 286}, 762: {1376, 673, 1001, 1990}, 170: {520, 348, 846, 287}, 675: {337, 287}, 692: {2651, 287}, 1162: {1381, 1517}, 1268: {1381, 1517}, 2079: {2021, 1381, 1517}, 67: {291, 998, 999, 616, 1000, 714, 748}, 167: {608, 291, 390, 364, 462, 439, 799}, 216: {291, 684}, 317: {291, 390}, 336: {390, 291, 598}, 462: {609, 291, 390, 364, 556}, 482: {291, 364, 390, 439}, 590: {291, 547, 525}, 640: {291, 639, 799}, 708: {291, 707, 709, 390, 483, 799}, 746: {291, 772, 805, 390, 804, 745, 747, 364, 302, 699}, 3130: {291, 3131}, 927: {291, 926}, 233: {390, 420, 292}, 1889: {2916, 1382}, 397: {396, 395, 652, 293}, 651: {928, 293, 369, 691, 986}, 1272: {1649, 1383}, 475: {558, 476, 742, 295}, 535: {295, 447}, 771: {342, 770, 566, 295}, 25: {296, 461}, 1007: {296, 655, 372, 501, 502}, 341: {1560, 1386, 3258}, 1982: {1388, 1437}, 1232: {1456, 297, 1457, 471}, 504: {390, 297, 747, 471, 510}, 3015: {2720, 3169, 3014, 297, 373, 471}, 764: {298, 919}, 1582: {1853, 1391}, 2599: {2514, 2979}, 3133: {2515, 3134}, 251: {1956, 517, 745, 364, 302, 373, 438, 699, 668, 669}, 804: {772, 805, 390, 745, 747, 364, 302, 439}, 911: {772, 804, 745, 747, 364, 302}, 841: {688, 572, 303}, 2557: {2521, 2726}, 1080: {1657, 1731, 1406}, 1314: {1731, 1735, 1736, 1737, 1584, 1657, 1406}, 1359: {1731, 1735, 1736, 1737, 1657, 1406}, 1364: {1731, 1406}, 1584: {1657, 1731, 1985, 1406}, 1658: {1731, 1406}, 1715: {1408, 1406}, 1731: {1731, 1732, 1406}, 1736: {1731, 1735, 1737, 1657, 1406}, 1737: {1657, 1731, 1406, 1735}, 1179: {1996, 1997, 1407}, 1216: {865, 1413, 1675, 1421, 1519, 1848}, 1240: {865, 1675, 1413}, 2446: {2525, 365}, 2342: {2528, 332}, 2529: {2530, 311}, 1144: {1416, 1481, 2105}, 1231: {1416, 769, 905, 2157}, 1234: {1416, 769, 905}, 2157: {1416, 769}, 315: {312, 313, 314}, 2533: {2531, 3020}, 622: {410, 316, 982}, 2534: {3125, 2535}, 51: {481, 482, 483, 390, 616, 747, 364, 1005, 439, 318}, 135: {610, 715, 716, 370, 319}, 2260: {1420, 3124}, 2699: {3107, 1420, 855}, 1770: {1771, 1420}, 3107: {1420, 855}, 1933: {865, 1675, 1421}, 1285: {2050, 1704, 1423, 1980, 1503}, 1542: {2050, 1423}, 1845: {2000, 2049, 1423}, 1628: {1424, 1753, 1949, 1752}, 2119: {1450, 1426}, 109: {848, 323}, 3274: {2545, 1858, 1859, 3103}, 661: {329, 643, 916}, 821: {329, 2591}, 948: {329, 661}, 137: {416, 330}, 2008: {1530, 1435}, 287: {337, 846}, 2402: {2784, 2553, 2785, 3031}, 2405: {2553, 2785}, 1858: {1857, 339}, 2224: {2558, 3022}, 246: {810, 2559}, 869: {932, 343}, 2570: {344, 2571}, 1273: {769, 2053, 1454}, 2072: {1827, 2052, 2054, 1709, 1455, 1878}, 3217: {2574, 615}, 1235: {2025, 1550, 1456, 1588, 1749}, 1850: {1458, 862}, 1374: {522, 1460}, 1464: {522, 1460}, 1243: {1648, 1706, 628, 1461}, 628: {753, 1461}, 2005: {2006, 1461, 2110}, 630: {348, 629, 375}, 982: {348, 981}, 2407: {354, 2579}, 351: {416, 354, 2579}, 2579: {354, 2580}, 2770: {354, 2580}, 1590: {2052, 1589, 1463}, 574: {616, 698, 349}, 575: {616, 698, 699, 349}, 515: {848, 629, 375, 985, 350}, 1298: {1465, 1770, 1918}, 353: {352, 354}, 2585: {354, 2586, 781}, 2586: {2584, 354, 2586}, 2584: {2585, 2586}, 363: {363, 765, 390}, 234: {571, 364, 390}, 244: {547, 483, 390, 364, 439, 926}, 253: {547, 483, 390, 364, 439, 926}, 259: {547, 483, 390, 364, 439, 926}, 365: {364, 439, 1004, 799}, 483: {390, 364, 926, 439}, 517: {364, 390, 439}, 523: {364, 390}, 525: {483, 517, 390, 364, 439}, 550: {364, 390}, 701: {390, 521, 364, 698, 699, 799}, 706: {608, 483, 772, 773, 390, 364, 946}, 910: {745, 747, 364, 390}, 918: {483, 364, 926, 439}, 2178: {2736, 2590}, 3296: {2592, 853}, 2325: {2594, 2595}, 1337: {370, 1870}, 1569: {1601, 1473}, 2065: {2064, 1474}, 2463: {373, 2669}, 374: {763, 373}, 3014: {3169, 373}, 3093: {897, 2599}, 1633: {378, 1851}, 3109: {2600, 2998, 3087}, 1661: {1660, 1525, 1479}, 2012: {2010, 2011, 1479, 1879}, 2271: {2603, 2652}, 2602: {2704, 2603}, 684: {953, 969, 381}, 713: {381, 798}, 1738: {1842, 1843, 1493}, 2617: {2616, 2618, 2614}, 2618: {2616, 2619}, 386: {584, 385, 932}, 240: {386, 932, 584, 618, 654, 794}, 579: {753, 788, 389, 388}, 196: {973, 389}, 334: {756, 389}, 467: {389, 767}, 936: {3020, 389}, 7: {616, 1001, 1004, 390}, 10: {440, 390}, 22: {699, 390}, 58: {554, 390, 414, 439}, 82: {658, 390}, 85: {698, 699, 390}, 98: {699, 390}, 147: {456, 390, 927}, 151: {390, 598}, 155: {699, 390}, 156: {699, 390}, 200: {390, 582}, 212: {963, 390, 439}, 219: {681, 390}, 231: {681, 390}, 248: {681, 390}, 391: {390, 799}, 414: {390, 439}, 456: {698, 390, 927}, 507: {541, 390}, 557: {897, 390}, 576: {873, 390}, 700: {698, 699, 390}, 702: {483, 390, 439}, 711: {960, 673, 437, 390}, 727: {768, 665, 390, 968}, 744: {390, 447}, 773: {608, 773, 390}, 812: {390, 407}, 880: {441, 390}, 906: {699, 390}, 189: {483, 391}, 914: {392, 2651}, 402: {400, 401, 3185}, 405: {400, 401, 403, 404, 693}, 400: {401, 916, 693}, 401: {404, 693, 3183}, 403: {404, 631}, 2134: {405, 1511}, 789: {738, 406}, 191: {409, 661, 847}, 1502: {1501, 1877}, 174: {410, 509}, 241: {410, 659, 631}, 570: {410, 659, 492}, 975: {410, 564}, 2466: {2868, 413}, 142: {416, 496, 472}, 879: {416, 530, 822}, 1003: {416, 659, 631}, 281: {419, 822}, 360: {419, 822}, 1899: {1898, 422}, 1150: {1912, 1507}, 150: {1509, 637, 917, 1701}, 325: {1509, 637, 917, 1701}, 327: {1509, 637, 917, 1701}, 1639: {1673, 1509}, 1172: {429, 1710}, 305: {1512, 429, 1710}, 1409: {1957, 1512, 429, 1710, 1681}, 1512: {429, 1710}, 2639: {2640, 782}, 2645: {2643, 2644}, 915: {432, 433}, 2282: {2646, 855}, 506: {434, 938, 847}, 332: {439, 799}, 755: {699, 502, 439}, 871: {517, 439}, 2447: {2657, 2658, 441}, 1478: {1528, 441, 1690}, 1531: {441, 2658, 1801}, 2765: {1528, 441, 2995}, 645: {880, 441, 1633}, 1814: {1528, 1690}, 2970: {1528, 3210}, 1529: {3021, 2169, 3305, 2654}, 2166: {3021, 2169, 3305, 2654}, 93: {800, 763, 447}, 800: {763, 447}, 451: {450, 773}, 1394: {2136, 1537, 677}, 2669: {905, 2668}, 2670: {905, 2668}, 1266: {1540, 1861}, 2293: {2774, 1543}, 1326: {1667, 1796, 1543, 828, 1662}, 3159: {460, 3156}, 1624: {1549, 1903}, 978: {1549, 591}, 2382: {795, 2678}, 30: {505, 471}, 556: {2894, 471}, 2687: {2688, 2689}, 1554: {1553, 1831}, 90: {474, 862, 1631}, 753: {477, 853}, 1321: {1562, 1646}, 2441: {3107, 2699, 855}, 138: {486, 847}, 42: {488, 644, 487}, 271: {643, 612, 644, 487, 488}, 950: {488, 611, 487}, 17: {488, 601, 832, 663}, 834: {488, 601, 831}, 955: {488, 612, 613}, 964: {488, 834, 612}, 3231: {3058, 2700}, 2647: {3085, 2702}, 805: {745, 490, 747, 698, 763}, 2864: {2705, 2863, 3286, 3185}, 492: {493, 631}, 139: {731, 495}, 208: {659, 495}, 667: {666, 495}, 935: {995, 712, 495, 657, 691, 788, 979, 986}, 979: {743, 495, 659, 631, 731}, 2438: {3244, 2813, 2711}, 2531: {3020, 2711}, 2532: {3020, 2711}, 2667: {3244, 2711}, 171: {817, 506}, 394: {938, 506, 940, 847}, 721: {722, 506}, 843: {506, 847}, 206: {507, 1628}, 2653: {3256, 2713}, 2873: {2827, 1580}, 2078: {1929, 1582}, 616: {521, 522}, 1336: {1831, 1863, 1004, 1005, 1586}, 1128: {1589, 1590}, 1593: {1592, 1594, 1692}, 1829: {1602, 3037}, 2337: {2731, 3124}, 188: {531, 814, 791}, 2349: {3195, 2733}, 2707: {2739, 2982}, 1122: {1690, 2742}, 1452: {2002, 1618}, 1188: {1619, 1918}, 464: {2745, 739}, 1275: {1924, 1622}, 1315: {1625, 1810}, 1224: {2009, 1706, 1626}, 2889: {2752, 2820}, 14: {544, 1630}, 572: {616, 698, 547, 574}, 1129: {862, 1631}, 2257: {1634, 3207}, 1289: {2034, 1636}, 1181: {1637, 1638}, 1375: {1637, 1638}, 1356: {2083, 1638}, 1637: {2083, 1638}, 2610: {2759, 2823}, 1810: {1640, 1694}, 2728: {3174, 553, 905, 887, 891}, 2791: {3208, 553}, 1951: {553, 1850, 887}, 2161: {553, 905, 2081}, 2266: {556, 2894}, 257: {556, 573}, 775: {857, 564, 847}, 770: {3040, 3041, 566}, 1868: {1856, 1652}, 1735: {1736, 1657, 1731}, 95: {571, 698, 2923, 574}, 549: {646, 616, 698, 572, 574, 575}, 965: {577, 612}, 2272: {2777, 3007}, 2432: {2784, 3031}, 2553: {2784, 2785}, 1194: {1680, 1731, 2135}, 808: {809, 587, 643, 814}, 2461: {681, 2790}, 1325: {1683, 1839}, 1684: {1683, 1878}, 1241: {2795, 1989}, 217: {591, 615}, 2176: {1689, 1908}, 593: {595, 847}, 594: {595, 847}, 1992: {1691, 1815}, 160: {753, 980, 596}, 2807: {2805, 2806}, 199: {606, 2855}, 1334: {1699, 1804}, 1703: {2049, 1702}, 1617: {1705, 1820}, 1258: {1706, 2006}, 610: {1706, 715}, 2295: {2832, 3161}, 583: {612, 847}, 611: {644, 613}, 3205: {3234, 615}, 571: {616, 698}, 748: {616, 699}, 998: {998, 999, 616, 1000, 748}, 920: {619, 1004, 1005}, 2840: {3107, 2839}, 623: {620, 621, 623, 624, 625}, 70: {659, 631}, 226: {986, 631}, 495: {986, 659, 631}, 980: {847, 848, 659, 979, 631, 986}, 995: {986, 631}, 1717: {1729, 1716, 1717}, 1887: {1888, 1717}, 2068: {1722, 2069}, 1071: {1985, 1731}, 1657: {1731, 1732}, 1680: {1731, 2135}, 1985: {1731, 1732}, 35: {643, 814}, 121: {816, 866, 643, 814}, 136: {809, 643, 814}, 313: {850, 643, 661}, 314: {643, 661}, 348: {848, 643}, 807: {809, 643, 814}, 814: {848, 643, 847}, 994: {643, 996}, 1895: {780, 646}, 2150: {2148, 646}, 197: {898, 647}, 898: {691, 647}, 946: {2865, 2031}, 2972: {2874, 3220}, 3105: {2874, 3220}, 1949: {1752, 1753}, 214: {860, 654}, 1784: {2022, 1927, 1928, 1786, 1756}, 1785: {1928, 1756, 1927}, 2319: {2877, 2878}, 3201: {2878, 3046}, 166: {656, 731}, 1911: {1762, 1924, 1910}, 877: {937, 718, 659, 661, 662, 856}, 986: {659, 660}, 40: {849, 660, 847}, 875: {986, 660, 788}, 328: {948, 661}, 835: {837, 661}, 852: {908, 661}, 858: {781, 663}, 728: {768, 665, 727}, 1099: {1768, 1767}, 2123: {1770, 1771}, 1984: {677, 678}, 867: {677, 678}, 379: {954, 678}, 1288: {1787, 1780}, 519: {691, 814, 847}, 1576: {1782, 1935}, 2363: {1789, 2927}, 1794: {1792, 795}, 1777: {2048, 1793}, 2867: {2929, 3019}, 2419: {2930, 3043}, 2980: {2930, 3043}, 752: {704, 994, 996, 751}, 2308: {2934, 3270}, 2108: {2107, 1798}, 707: {709, 799}, 2869: {2936, 3170}, 1805: {2145, 1806}, 1974: {1809, 1977}, 2950: {2953, 2956}, 2251: {2955, 3173}, 2381: {3186, 716}, 2961: {3186, 716}, 2048: {2049, 1906, 1819}, 2399: {2976, 3303}, 730: {729, 737, 942, 847}, 320: {736, 737, 730}, 2549: {3138, 2981}, 2454: {2984, 2985, 2983}, 2984: {2985, 2983}, 2986: {2985, 2983}, 77: {916, 735}, 729: {737, 847}, 1790: {2022, 1927, 1928, 1847, 1915}, 1849: {1848, 3124}, 3009: {3008, 1851}, 1295: {2018, 1852}, 3233: {3232, 2996}, 712: {750, 847}, 874: {753, 3155}, 193: {760, 758, 759}, 1873: {1869, 1919}, 1663: {1920, 1873}, 2477: {3032, 3259}, 473: {822, 767}, 1876: {768, 2081}, 2686: {769, 905}, 2415: {960, 3041}, 1894: {956, 1893}, 2234: {3051, 3052}, 2510: {3051, 3052}, 1300: {2080, 1901}, 2032: {2049, 2050, 1903, 2033, 1980}, 2033: {2049, 2050, 1980, 1903}, 3066: {3065, 3067}, 2826: {1906, 3123}, 247: {1003, 933, 806}, 3082: {3081, 3084}, 2143: {1913, 2142}, 816: {848, 814, 847}, 853: {2050, 1916}, 1276: {2098, 1918}, 1308: {2049, 1918}, 2175: {2049, 2077, 1918, 2159}, 3097: {3096, 3095}, 3182: {3096, 3095}, 2075: {2049, 1925}, 242: {979, 822, 847}, 3101: {3100, 3102}, 1927: {1928, 2022}, 2428: {3106, 3178}, 1698: {1929, 3123}, 604: {850, 846}, 186: {966, 847}, 509: {848, 847}, 595: {916, 847}, 826: {912, 847}, 983: {848, 847}, 984: {848, 847}, 2336: {851, 3119}, 1252: {2049, 853}, 474: {1968, 862}, 1968: {2160, 1972, 862}, 1398: {1970, 1971, 2019}, 1400: {1970, 1971, 2019}, 3150: {3146, 3147, 3149}, 3146: {3147, 3148, 3149, 3150, 3151}, 2429: {874, 3155}, 2110: {2109, 2006}, 1341: {2008, 2056}, 2334: {3290, 3167}, 2444: {3168, 3290, 3167}, 3168: {3290, 3167}, 703: {892, 893}, 1238: {946, 2031}, 2247: {3188, 3189}, 446: {3273, 909}, 1133: {2169, 2046}, 1846: {2048, 2049}, 2468: {3202, 3204}, 2054: {2145, 2055}, 2087: {2089, 2085}, 2102: {2100, 2101}, 3222: {3240, 3281}, 938: {940, 941}, 3263: {3260, 3261}, 3266: {3267, 3268}, 2129: {2130, 2131, 963}, 990: {994, 980, 989}, 1170: {2169, 2166}}, 'features': matrix([[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 1., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), 'labels': array([[0, 1, 0, 0, 0, 0],
       [0, 0, 0, 0, 1, 0],
       [0, 1, 0, 0, 0, 0],
       ...,
       [0, 0, 0, 0, 1, 0],
       [1, 0, 0, 0, 0, 0],
       [0, 0, 0, 1, 0, 0]], dtype=int32), 'n': 3312, 'feature': array([[[ 0.        ,  0.        ,  0.        , ..., -0.31035982,
          0.2382891 ,  0.67195495]],

       [[ 0.        ,  0.        ,  0.        , ..., -0.39669618,
          0.2382891 ,  0.8980048 ]],

       [[ 0.        ,  1.        ,  0.        , ..., -0.66297483,
         -0.66297483,  0.        ]],

       ...,

       [[ 0.        ,  0.        ,  0.        , ...,  0.19180139,
          0.19180139,  0.        ]],

       [[ 0.        ,  0.        ,  0.        , ...,  0.66368589,
          0.66368589,  0.        ]],

       [[ 0.        ,  0.        ,  0.        , ...,  0.181585  ,
          0.181585  ,  0.        ]]])})
The split is 1
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 1/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.7736 | train acc:34.06 | val acc:30.00 | test_acc_best_val: 32.70  | best_test_acc: 32.70 | test acc:32.70 | time:84.5ms
epoch:20 | loss:1.5360 | train acc:91.30 | val acc:64.29 | test_acc_best_val: 64.62  | best_test_acc: 64.62 | test acc:64.82 | time:21.8ms
epoch:40 | loss:1.5275 | train acc:97.83 | val acc:64.29 | test_acc_best_val: 65.37  | best_test_acc: 65.37 | test acc:66.00 | time:20.0ms
epoch:60 | loss:1.5193 | train acc:98.55 | val acc:65.56 | test_acc_best_val: 65.80  | best_test_acc: 65.80 | test acc:65.37 | time:24.3ms
epoch:80 | loss:1.4956 | train acc:98.55 | val acc:65.87 | test_acc_best_val: 64.86  | best_test_acc: 64.86 | test acc:65.29 | time:22.5ms
epoch:100 | loss:1.4371 | train acc:98.55 | val acc:67.78 | test_acc_best_val: 66.19  | best_test_acc: 66.19 | test acc:66.59 | time:22.6ms
epoch:120 | loss:1.5026 | train acc:97.83 | val acc:68.25 | test_acc_best_val: 66.75  | best_test_acc: 66.75 | test acc:66.59 | time:21.8ms
epoch:140 | loss:1.5003 | train acc:97.83 | val acc:68.10 | test_acc_best_val: 66.75  | best_test_acc: 66.75 | test acc:66.43 | time:20.7ms
epoch:160 | loss:1.5148 | train acc:97.83 | val acc:67.30 | test_acc_best_val: 66.75  | best_test_acc: 66.75 | test acc:66.39 | time:22.1ms
epoch:180 | loss:1.4651 | train acc:97.83 | val acc:68.57 | test_acc_best_val: 66.75  | best_test_acc: 66.75 | test acc:66.39 | time:21.5ms
Run 1/10, best test accuracy: 66.75, acc(last): 65.96, total time: 6.36s
The split is 2
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 2/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.8028 | train acc:44.20 | val acc:39.68 | test_acc_best_val: 41.94  | best_test_acc: 41.94 | test acc:41.94 | time:31.5ms
epoch:20 | loss:1.5472 | train acc:94.20 | val acc:63.33 | test_acc_best_val: 64.62  | best_test_acc: 64.62 | test acc:64.98 | time:21.4ms
epoch:40 | loss:1.4883 | train acc:99.28 | val acc:63.97 | test_acc_best_val: 66.47  | best_test_acc: 66.47 | test acc:66.78 | time:21.1ms
epoch:60 | loss:1.5234 | train acc:100.00 | val acc:63.65 | test_acc_best_val: 66.47  | best_test_acc: 66.47 | test acc:67.92 | time:20.6ms
epoch:80 | loss:1.4421 | train acc:100.00 | val acc:63.49 | test_acc_best_val: 66.47  | best_test_acc: 66.47 | test acc:67.57 | time:23.1ms
epoch:100 | loss:1.4448 | train acc:100.00 | val acc:63.65 | test_acc_best_val: 66.47  | best_test_acc: 66.47 | test acc:67.85 | time:21.4ms
epoch:120 | loss:1.5185 | train acc:100.00 | val acc:63.97 | test_acc_best_val: 66.47  | best_test_acc: 66.47 | test acc:67.33 | time:31.6ms
epoch:140 | loss:1.5396 | train acc:100.00 | val acc:63.33 | test_acc_best_val: 66.47  | best_test_acc: 66.47 | test acc:67.53 | time:21.7ms
epoch:160 | loss:1.4922 | train acc:100.00 | val acc:63.65 | test_acc_best_val: 66.47  | best_test_acc: 66.47 | test acc:67.14 | time:23.2ms
epoch:180 | loss:1.4959 | train acc:100.00 | val acc:63.49 | test_acc_best_val: 66.47  | best_test_acc: 66.47 | test acc:68.04 | time:25.2ms
Run 2/10, best test accuracy: 68.16, acc(last): 67.89, total time: 6.43s
The split is 3
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 3/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.8455 | train acc:22.46 | val acc:23.81 | test_acc_best_val: 21.58  | best_test_acc: 21.58 | test acc:21.58 | time:23.0ms
epoch:20 | loss:1.5614 | train acc:92.03 | val acc:63.17 | test_acc_best_val: 65.13  | best_test_acc: 65.13 | test acc:65.02 | time:24.1ms
epoch:40 | loss:1.5578 | train acc:97.83 | val acc:65.40 | test_acc_best_val: 64.90  | best_test_acc: 64.90 | test acc:64.90 | time:20.7ms
epoch:60 | loss:1.5304 | train acc:99.28 | val acc:65.08 | test_acc_best_val: 64.90  | best_test_acc: 64.90 | test acc:66.08 | time:20.6ms
epoch:80 | loss:1.5146 | train acc:99.28 | val acc:65.40 | test_acc_best_val: 67.14  | best_test_acc: 67.14 | test acc:67.06 | time:22.1ms
epoch:100 | loss:1.5335 | train acc:99.28 | val acc:66.51 | test_acc_best_val: 67.22  | best_test_acc: 67.22 | test acc:66.94 | time:22.5ms
epoch:120 | loss:1.5597 | train acc:99.28 | val acc:66.67 | test_acc_best_val: 67.22  | best_test_acc: 67.22 | test acc:67.22 | time:19.5ms
epoch:140 | loss:1.5403 | train acc:99.28 | val acc:66.35 | test_acc_best_val: 67.22  | best_test_acc: 67.22 | test acc:67.81 | time:22.6ms
epoch:160 | loss:1.5249 | train acc:99.28 | val acc:67.78 | test_acc_best_val: 67.92  | best_test_acc: 67.92 | test acc:68.04 | time:26.4ms
epoch:180 | loss:1.5665 | train acc:99.28 | val acc:68.41 | test_acc_best_val: 68.32  | best_test_acc: 68.32 | test acc:68.32 | time:20.0ms
Run 3/10, best test accuracy: 68.47, acc(last): 68.28, total time: 6.05s
The split is 4
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 4/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.8530 | train acc:40.58 | val acc:28.25 | test_acc_best_val: 32.31  | best_test_acc: 32.31 | test acc:32.31 | time:20.4ms
epoch:20 | loss:1.5262 | train acc:88.41 | val acc:59.37 | test_acc_best_val: 61.20  | best_test_acc: 61.20 | test acc:61.16 | time:22.4ms
epoch:40 | loss:1.5388 | train acc:94.93 | val acc:60.48 | test_acc_best_val: 61.36  | best_test_acc: 61.36 | test acc:61.36 | time:19.2ms
epoch:60 | loss:1.4735 | train acc:99.28 | val acc:62.70 | test_acc_best_val: 62.81  | best_test_acc: 62.81 | test acc:62.78 | time:21.6ms
epoch:80 | loss:1.4661 | train acc:98.55 | val acc:62.70 | test_acc_best_val: 63.25  | best_test_acc: 63.25 | test acc:63.56 | time:20.2ms
epoch:100 | loss:1.4710 | train acc:98.55 | val acc:62.86 | test_acc_best_val: 63.25  | best_test_acc: 63.25 | test acc:64.03 | time:21.7ms
epoch:120 | loss:1.4595 | train acc:98.55 | val acc:65.24 | test_acc_best_val: 64.74  | best_test_acc: 64.74 | test acc:64.74 | time:19.9ms
epoch:140 | loss:1.4782 | train acc:98.55 | val acc:64.44 | test_acc_best_val: 64.74  | best_test_acc: 64.74 | test acc:64.78 | time:23.8ms
epoch:160 | loss:1.5359 | train acc:98.55 | val acc:65.08 | test_acc_best_val: 65.17  | best_test_acc: 65.17 | test acc:65.13 | time:20.3ms
epoch:180 | loss:1.4587 | train acc:98.55 | val acc:64.29 | test_acc_best_val: 65.17  | best_test_acc: 65.17 | test acc:65.37 | time:21.1ms
Run 4/10, best test accuracy: 65.53, acc(last): 65.45, total time: 6.16s
The split is 5
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 5/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.8478 | train acc:37.68 | val acc:30.79 | test_acc_best_val: 28.14  | best_test_acc: 28.14 | test acc:28.14 | time:22.6ms
epoch:20 | loss:1.5448 | train acc:92.75 | val acc:64.76 | test_acc_best_val: 62.34  | best_test_acc: 62.34 | test acc:62.34 | time:21.8ms
epoch:40 | loss:1.5212 | train acc:99.28 | val acc:67.94 | test_acc_best_val: 64.78  | best_test_acc: 64.78 | test acc:65.06 | time:21.2ms
epoch:60 | loss:1.4952 | train acc:100.00 | val acc:68.73 | test_acc_best_val: 66.31  | best_test_acc: 66.31 | test acc:66.31 | time:19.1ms
epoch:80 | loss:1.4964 | train acc:100.00 | val acc:68.73 | test_acc_best_val: 66.55  | best_test_acc: 66.55 | test acc:66.39 | time:20.3ms
epoch:100 | loss:1.4855 | train acc:100.00 | val acc:69.84 | test_acc_best_val: 66.59  | best_test_acc: 66.59 | test acc:66.63 | time:20.2ms
epoch:120 | loss:1.4847 | train acc:100.00 | val acc:69.21 | test_acc_best_val: 66.59  | best_test_acc: 66.59 | test acc:66.31 | time:22.1ms
epoch:140 | loss:1.4917 | train acc:100.00 | val acc:70.00 | test_acc_best_val: 66.55  | best_test_acc: 66.55 | test acc:66.55 | time:21.4ms
epoch:160 | loss:1.4663 | train acc:100.00 | val acc:69.05 | test_acc_best_val: 66.55  | best_test_acc: 66.55 | test acc:66.94 | time:20.1ms
epoch:180 | loss:1.4749 | train acc:100.00 | val acc:69.52 | test_acc_best_val: 66.55  | best_test_acc: 66.55 | test acc:66.35 | time:19.0ms
Run 5/10, best test accuracy: 66.55, acc(last): 67.06, total time: 6.02s
The split is 6
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 6/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.8476 | train acc:26.09 | val acc:24.76 | test_acc_best_val: 23.35  | best_test_acc: 23.35 | test acc:23.35 | time:24.7ms
epoch:20 | loss:1.5310 | train acc:90.58 | val acc:64.60 | test_acc_best_val: 63.09  | best_test_acc: 63.09 | test acc:63.09 | time:21.1ms
epoch:40 | loss:1.5252 | train acc:95.65 | val acc:66.98 | test_acc_best_val: 64.58  | best_test_acc: 64.58 | test acc:64.70 | time:19.7ms
epoch:60 | loss:1.4992 | train acc:98.55 | val acc:66.98 | test_acc_best_val: 65.25  | best_test_acc: 65.25 | test acc:66.19 | time:24.7ms
epoch:80 | loss:1.5005 | train acc:99.28 | val acc:68.25 | test_acc_best_val: 66.12  | best_test_acc: 66.12 | test acc:66.04 | time:20.3ms
epoch:100 | loss:1.5454 | train acc:99.28 | val acc:67.30 | test_acc_best_val: 66.12  | best_test_acc: 66.12 | test acc:66.51 | time:19.5ms
epoch:120 | loss:1.5031 | train acc:99.28 | val acc:67.62 | test_acc_best_val: 66.12  | best_test_acc: 66.12 | test acc:66.47 | time:21.7ms
epoch:140 | loss:1.4836 | train acc:99.28 | val acc:66.51 | test_acc_best_val: 66.12  | best_test_acc: 66.12 | test acc:67.14 | time:21.6ms
epoch:160 | loss:1.4984 | train acc:100.00 | val acc:66.51 | test_acc_best_val: 66.12  | best_test_acc: 66.12 | test acc:66.67 | time:22.5ms
epoch:180 | loss:1.4730 | train acc:100.00 | val acc:66.98 | test_acc_best_val: 66.12  | best_test_acc: 66.12 | test acc:66.78 | time:22.2ms
Run 6/10, best test accuracy: 66.12, acc(last): 66.94, total time: 6.16s
The split is 7
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 7/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.8168 | train acc:49.28 | val acc:44.29 | test_acc_best_val: 43.32  | best_test_acc: 43.32 | test acc:43.32 | time:39.8ms
epoch:20 | loss:1.5732 | train acc:89.86 | val acc:62.38 | test_acc_best_val: 58.10  | best_test_acc: 58.10 | test acc:59.59 | time:48.6ms
epoch:40 | loss:1.5384 | train acc:97.83 | val acc:63.65 | test_acc_best_val: 61.01  | best_test_acc: 61.01 | test acc:61.36 | time:21.7ms
epoch:60 | loss:1.5469 | train acc:98.55 | val acc:62.70 | test_acc_best_val: 61.40  | best_test_acc: 61.40 | test acc:60.73 | time:27.4ms
epoch:80 | loss:1.5813 | train acc:98.55 | val acc:63.17 | test_acc_best_val: 61.40  | best_test_acc: 61.40 | test acc:60.53 | time:20.8ms
epoch:100 | loss:1.5068 | train acc:98.55 | val acc:63.65 | test_acc_best_val: 61.40  | best_test_acc: 61.40 | test acc:61.79 | time:20.8ms
epoch:120 | loss:1.4921 | train acc:97.83 | val acc:63.33 | test_acc_best_val: 61.83  | best_test_acc: 61.83 | test acc:61.52 | time:24.8ms
epoch:140 | loss:1.4920 | train acc:97.83 | val acc:62.54 | test_acc_best_val: 61.83  | best_test_acc: 61.83 | test acc:60.42 | time:28.7ms
epoch:160 | loss:1.5394 | train acc:97.83 | val acc:63.02 | test_acc_best_val: 61.83  | best_test_acc: 61.83 | test acc:61.75 | time:21.8ms
epoch:180 | loss:1.5353 | train acc:98.55 | val acc:63.49 | test_acc_best_val: 61.83  | best_test_acc: 61.83 | test acc:61.95 | time:28.4ms
Run 7/10, best test accuracy: 61.83, acc(last): 62.58, total time: 6.74s
The split is 8
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 8/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.8358 | train acc:44.93 | val acc:31.11 | test_acc_best_val: 32.55  | best_test_acc: 32.55 | test acc:32.55 | time:20.9ms
epoch:20 | loss:1.5568 | train acc:92.75 | val acc:60.79 | test_acc_best_val: 61.08  | best_test_acc: 61.08 | test acc:61.24 | time:21.8ms
epoch:40 | loss:1.5098 | train acc:99.28 | val acc:59.68 | test_acc_best_val: 61.08  | best_test_acc: 61.08 | test acc:62.62 | time:21.2ms
epoch:60 | loss:1.4965 | train acc:100.00 | val acc:60.32 | test_acc_best_val: 61.08  | best_test_acc: 61.08 | test acc:62.54 | time:24.6ms
epoch:80 | loss:1.5063 | train acc:100.00 | val acc:61.11 | test_acc_best_val: 62.97  | best_test_acc: 62.97 | test acc:64.11 | time:22.6ms
epoch:100 | loss:1.4605 | train acc:100.00 | val acc:61.75 | test_acc_best_val: 64.19  | best_test_acc: 64.19 | test acc:64.19 | time:27.7ms
epoch:120 | loss:1.4556 | train acc:100.00 | val acc:60.63 | test_acc_best_val: 63.88  | best_test_acc: 63.88 | test acc:64.03 | time:26.9ms
epoch:140 | loss:1.5018 | train acc:100.00 | val acc:61.43 | test_acc_best_val: 63.88  | best_test_acc: 63.88 | test acc:64.27 | time:21.2ms
epoch:160 | loss:1.4997 | train acc:100.00 | val acc:60.95 | test_acc_best_val: 63.88  | best_test_acc: 63.88 | test acc:63.88 | time:21.7ms
epoch:180 | loss:1.4719 | train acc:100.00 | val acc:62.06 | test_acc_best_val: 64.39  | best_test_acc: 64.39 | test acc:65.02 | time:78.5ms
Run 8/10, best test accuracy: 64.86, acc(last): 64.62, total time: 6.89s
The split is 9
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 9/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.8352 | train acc:59.42 | val acc:45.24 | test_acc_best_val: 47.13  | best_test_acc: 47.13 | test acc:47.13 | time:46.0ms
epoch:20 | loss:1.5194 | train acc:92.75 | val acc:62.54 | test_acc_best_val: 60.65  | best_test_acc: 60.65 | test acc:60.65 | time:24.6ms
epoch:40 | loss:1.4909 | train acc:97.83 | val acc:63.81 | test_acc_best_val: 63.09  | best_test_acc: 63.09 | test acc:63.09 | time:28.0ms
epoch:60 | loss:1.4570 | train acc:98.55 | val acc:64.29 | test_acc_best_val: 62.85  | best_test_acc: 62.85 | test acc:62.85 | time:20.6ms
epoch:80 | loss:1.5091 | train acc:99.28 | val acc:64.29 | test_acc_best_val: 62.85  | best_test_acc: 62.85 | test acc:63.01 | time:23.2ms
epoch:100 | loss:1.5053 | train acc:99.28 | val acc:65.24 | test_acc_best_val: 63.92  | best_test_acc: 63.92 | test acc:63.92 | time:23.6ms
epoch:120 | loss:1.4668 | train acc:99.28 | val acc:65.40 | test_acc_best_val: 63.76  | best_test_acc: 63.76 | test acc:63.68 | time:23.2ms
epoch:140 | loss:1.4711 | train acc:99.28 | val acc:66.03 | test_acc_best_val: 64.27  | best_test_acc: 64.27 | test acc:64.78 | time:22.9ms
epoch:160 | loss:1.4671 | train acc:97.83 | val acc:65.87 | test_acc_best_val: 64.27  | best_test_acc: 64.27 | test acc:63.88 | time:19.9ms
epoch:180 | loss:1.4869 | train acc:97.83 | val acc:65.87 | test_acc_best_val: 64.27  | best_test_acc: 64.27 | test acc:64.19 | time:22.6ms
Run 9/10, best test accuracy: 64.27, acc(last): 64.62, total time: 6.62s
The split is 10
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 10/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.8581 | train acc:47.83 | val acc:41.11 | test_acc_best_val: 41.31  | best_test_acc: 41.31 | test acc:41.31 | time:23.5ms
epoch:20 | loss:1.5397 | train acc:89.86 | val acc:61.75 | test_acc_best_val: 60.42  | best_test_acc: 60.42 | test acc:60.61 | time:21.9ms
epoch:40 | loss:1.4896 | train acc:97.83 | val acc:65.08 | test_acc_best_val: 61.64  | best_test_acc: 61.64 | test acc:62.11 | time:24.2ms
epoch:60 | loss:1.4879 | train acc:98.55 | val acc:63.97 | test_acc_best_val: 62.15  | best_test_acc: 62.15 | test acc:61.99 | time:21.5ms
epoch:80 | loss:1.4837 | train acc:98.55 | val acc:64.92 | test_acc_best_val: 62.15  | best_test_acc: 62.15 | test acc:62.15 | time:23.1ms
epoch:100 | loss:1.5018 | train acc:98.55 | val acc:65.71 | test_acc_best_val: 62.19  | best_test_acc: 62.19 | test acc:62.22 | time:22.7ms
epoch:120 | loss:1.4754 | train acc:99.28 | val acc:65.56 | test_acc_best_val: 62.19  | best_test_acc: 62.19 | test acc:63.09 | time:22.7ms
epoch:140 | loss:1.4911 | train acc:99.28 | val acc:66.35 | test_acc_best_val: 63.76  | best_test_acc: 63.76 | test acc:63.76 | time:25.2ms
epoch:160 | loss:1.4681 | train acc:99.28 | val acc:66.98 | test_acc_best_val: 63.99  | best_test_acc: 63.99 | test acc:63.76 | time:22.2ms
epoch:180 | loss:1.5059 | train acc:99.28 | val acc:67.46 | test_acc_best_val: 64.19  | best_test_acc: 64.19 | test acc:64.11 | time:21.6ms
Run 10/10, best test accuracy: 64.19, acc(last): 64.15, total time: 6.68s
The split is 1
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 1/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.8462 | train acc:38.41 | val acc:28.57 | test_acc_best_val: 31.56  | best_test_acc: 31.56 | test acc:31.56 | time:23.7ms
epoch:20 | loss:1.5432 | train acc:86.23 | val acc:66.03 | test_acc_best_val: 63.68  | best_test_acc: 63.68 | test acc:63.68 | time:21.5ms
epoch:40 | loss:1.5313 | train acc:97.10 | val acc:67.46 | test_acc_best_val: 64.27  | best_test_acc: 64.27 | test acc:65.21 | time:23.6ms
epoch:60 | loss:1.5130 | train acc:99.28 | val acc:67.46 | test_acc_best_val: 64.27  | best_test_acc: 64.27 | test acc:66.00 | time:21.7ms
epoch:80 | loss:1.4845 | train acc:99.28 | val acc:68.41 | test_acc_best_val: 66.27  | best_test_acc: 66.27 | test acc:66.75 | time:21.6ms
epoch:100 | loss:1.4796 | train acc:99.28 | val acc:68.10 | test_acc_best_val: 66.31  | best_test_acc: 66.31 | test acc:65.92 | time:23.9ms
epoch:120 | loss:1.4567 | train acc:99.28 | val acc:67.94 | test_acc_best_val: 66.31  | best_test_acc: 66.31 | test acc:66.63 | time:23.7ms
epoch:140 | loss:1.4986 | train acc:99.28 | val acc:67.94 | test_acc_best_val: 66.31  | best_test_acc: 66.31 | test acc:66.67 | time:23.2ms
epoch:160 | loss:1.4693 | train acc:99.28 | val acc:67.62 | test_acc_best_val: 66.31  | best_test_acc: 66.31 | test acc:66.23 | time:25.2ms
epoch:180 | loss:1.4904 | train acc:99.28 | val acc:68.25 | test_acc_best_val: 66.31  | best_test_acc: 66.31 | test acc:66.23 | time:21.9ms
Run 1/10, best test accuracy: 66.31, acc(last): 66.55, total time: 6.83s
The split is 2
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 2/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.8249 | train acc:34.06 | val acc:32.54 | test_acc_best_val: 31.72  | best_test_acc: 31.72 | test acc:31.72 | time:20.9ms
epoch:20 | loss:1.5235 | train acc:90.58 | val acc:61.90 | test_acc_best_val: 63.44  | best_test_acc: 63.44 | test acc:63.95 | time:21.7ms
epoch:40 | loss:1.4843 | train acc:99.28 | val acc:65.71 | test_acc_best_val: 65.88  | best_test_acc: 65.88 | test acc:65.88 | time:22.8ms
epoch:60 | loss:1.5120 | train acc:100.00 | val acc:65.08 | test_acc_best_val: 65.88  | best_test_acc: 65.88 | test acc:65.06 | time:23.2ms
epoch:80 | loss:1.4469 | train acc:100.00 | val acc:65.56 | test_acc_best_val: 65.88  | best_test_acc: 65.88 | test acc:65.45 | time:24.3ms
epoch:100 | loss:1.4530 | train acc:100.00 | val acc:66.83 | test_acc_best_val: 66.00  | best_test_acc: 66.00 | test acc:66.00 | time:20.8ms
epoch:120 | loss:1.4363 | train acc:100.00 | val acc:66.51 | test_acc_best_val: 65.96  | best_test_acc: 65.96 | test acc:66.55 | time:21.3ms
epoch:140 | loss:1.4795 | train acc:100.00 | val acc:67.46 | test_acc_best_val: 66.43  | best_test_acc: 66.43 | test acc:66.31 | time:22.3ms
epoch:160 | loss:1.4447 | train acc:100.00 | val acc:66.83 | test_acc_best_val: 66.43  | best_test_acc: 66.43 | test acc:66.75 | time:26.3ms
epoch:180 | loss:1.5051 | train acc:100.00 | val acc:66.98 | test_acc_best_val: 66.43  | best_test_acc: 66.43 | test acc:66.16 | time:32.2ms
Run 2/10, best test accuracy: 66.43, acc(last): 66.00, total time: 6.54s
The split is 3
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 3/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.8461 | train acc:42.75 | val acc:43.02 | test_acc_best_val: 41.23  | best_test_acc: 41.23 | test acc:41.23 | time:25.2ms
epoch:20 | loss:1.5764 | train acc:95.65 | val acc:65.87 | test_acc_best_val: 62.38  | best_test_acc: 62.38 | test acc:63.48 | time:23.4ms
epoch:40 | loss:1.4981 | train acc:97.83 | val acc:66.83 | test_acc_best_val: 65.17  | best_test_acc: 65.17 | test acc:65.06 | time:29.5ms
epoch:60 | loss:1.5344 | train acc:99.28 | val acc:65.40 | test_acc_best_val: 65.17  | best_test_acc: 65.17 | test acc:65.25 | time:20.7ms
epoch:80 | loss:1.5018 | train acc:99.28 | val acc:65.56 | test_acc_best_val: 65.17  | best_test_acc: 65.17 | test acc:66.04 | time:25.3ms
epoch:100 | loss:1.5197 | train acc:99.28 | val acc:66.67 | test_acc_best_val: 65.17  | best_test_acc: 65.17 | test acc:66.55 | time:21.5ms
epoch:120 | loss:1.5150 | train acc:99.28 | val acc:66.51 | test_acc_best_val: 65.17  | best_test_acc: 65.17 | test acc:66.63 | time:19.5ms
epoch:140 | loss:1.5231 | train acc:99.28 | val acc:66.03 | test_acc_best_val: 65.17  | best_test_acc: 65.17 | test acc:66.71 | time:21.6ms
epoch:160 | loss:1.5306 | train acc:99.28 | val acc:66.51 | test_acc_best_val: 65.17  | best_test_acc: 65.17 | test acc:66.63 | time:25.2ms
epoch:180 | loss:1.5085 | train acc:99.28 | val acc:67.30 | test_acc_best_val: 67.45  | best_test_acc: 67.45 | test acc:67.61 | time:20.5ms
Run 3/10, best test accuracy: 67.69, acc(last): 67.73, total time: 6.28s
The split is 4
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 4/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.8319 | train acc:35.51 | val acc:34.44 | test_acc_best_val: 34.04  | best_test_acc: 34.04 | test acc:34.04 | time:22.7ms
epoch:20 | loss:1.5170 | train acc:90.58 | val acc:59.68 | test_acc_best_val: 60.34  | best_test_acc: 60.34 | test acc:61.12 | time:23.1ms
epoch:40 | loss:1.4829 | train acc:95.65 | val acc:61.27 | test_acc_best_val: 60.34  | best_test_acc: 60.34 | test acc:63.09 | time:19.9ms
epoch:60 | loss:1.4870 | train acc:99.28 | val acc:63.02 | test_acc_best_val: 63.68  | best_test_acc: 63.68 | test acc:63.68 | time:19.1ms
epoch:80 | loss:1.4830 | train acc:99.28 | val acc:63.02 | test_acc_best_val: 63.64  | best_test_acc: 63.64 | test acc:63.95 | time:21.2ms
epoch:100 | loss:1.5005 | train acc:99.28 | val acc:64.44 | test_acc_best_val: 63.68  | best_test_acc: 63.68 | test acc:63.60 | time:22.9ms
epoch:120 | loss:1.4804 | train acc:99.28 | val acc:63.97 | test_acc_best_val: 63.68  | best_test_acc: 63.68 | test acc:63.95 | time:21.8ms
epoch:140 | loss:1.4829 | train acc:99.28 | val acc:64.76 | test_acc_best_val: 64.66  | best_test_acc: 64.66 | test acc:64.66 | time:26.0ms
epoch:160 | loss:1.5063 | train acc:98.55 | val acc:63.81 | test_acc_best_val: 64.66  | best_test_acc: 64.66 | test acc:64.66 | time:20.7ms
epoch:180 | loss:1.4979 | train acc:98.55 | val acc:63.65 | test_acc_best_val: 64.66  | best_test_acc: 64.66 | test acc:64.78 | time:18.9ms
Run 4/10, best test accuracy: 65.45, acc(last): 65.49, total time: 6.05s
The split is 5
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 5/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.8369 | train acc:30.43 | val acc:30.32 | test_acc_best_val: 29.56  | best_test_acc: 29.56 | test acc:29.56 | time:22.5ms
epoch:20 | loss:1.5349 | train acc:95.65 | val acc:63.17 | test_acc_best_val: 64.82  | best_test_acc: 64.82 | test acc:65.21 | time:20.3ms
epoch:40 | loss:1.5616 | train acc:100.00 | val acc:65.08 | test_acc_best_val: 65.72  | best_test_acc: 65.72 | test acc:65.68 | time:20.7ms
epoch:60 | loss:1.4658 | train acc:100.00 | val acc:64.60 | test_acc_best_val: 65.72  | best_test_acc: 65.72 | test acc:66.19 | time:20.9ms
epoch:80 | loss:1.5132 | train acc:100.00 | val acc:66.19 | test_acc_best_val: 66.75  | best_test_acc: 66.75 | test acc:66.75 | time:19.3ms
epoch:100 | loss:1.5061 | train acc:100.00 | val acc:66.03 | test_acc_best_val: 66.75  | best_test_acc: 66.75 | test acc:67.33 | time:24.7ms
epoch:120 | loss:1.5243 | train acc:100.00 | val acc:65.56 | test_acc_best_val: 66.75  | best_test_acc: 66.75 | test acc:66.98 | time:24.2ms
epoch:140 | loss:1.5018 | train acc:100.00 | val acc:65.40 | test_acc_best_val: 66.75  | best_test_acc: 66.75 | test acc:67.45 | time:19.3ms
epoch:160 | loss:1.5368 | train acc:100.00 | val acc:66.83 | test_acc_best_val: 67.53  | best_test_acc: 67.53 | test acc:67.41 | time:20.4ms
epoch:180 | loss:1.4814 | train acc:99.28 | val acc:65.71 | test_acc_best_val: 67.53  | best_test_acc: 67.53 | test acc:67.61 | time:22.5ms
Run 5/10, best test accuracy: 67.53, acc(last): 67.69, total time: 5.99s
The split is 6
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 6/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.8339 | train acc:34.78 | val acc:29.52 | test_acc_best_val: 32.23  | best_test_acc: 32.23 | test acc:32.23 | time:23.8ms
epoch:20 | loss:1.5407 | train acc:92.03 | val acc:63.02 | test_acc_best_val: 63.56  | best_test_acc: 63.56 | test acc:65.53 | time:21.3ms
epoch:40 | loss:1.4940 | train acc:98.55 | val acc:63.65 | test_acc_best_val: 65.49  | best_test_acc: 65.49 | test acc:66.23 | time:21.9ms
epoch:60 | loss:1.4744 | train acc:99.28 | val acc:65.40 | test_acc_best_val: 66.78  | best_test_acc: 66.78 | test acc:66.78 | time:20.7ms
epoch:80 | loss:1.4999 | train acc:99.28 | val acc:66.03 | test_acc_best_val: 66.94  | best_test_acc: 66.94 | test acc:67.10 | time:19.2ms
epoch:100 | loss:1.4568 | train acc:99.28 | val acc:66.51 | test_acc_best_val: 66.94  | best_test_acc: 66.94 | test acc:67.18 | time:22.6ms
epoch:120 | loss:1.4629 | train acc:99.28 | val acc:66.83 | test_acc_best_val: 67.26  | best_test_acc: 67.26 | test acc:67.26 | time:20.9ms
epoch:140 | loss:1.4684 | train acc:99.28 | val acc:66.98 | test_acc_best_val: 67.61  | best_test_acc: 67.61 | test acc:67.45 | time:21.4ms
epoch:160 | loss:1.5063 | train acc:99.28 | val acc:66.67 | test_acc_best_val: 67.61  | best_test_acc: 67.61 | test acc:67.26 | time:19.6ms
epoch:180 | loss:1.4761 | train acc:99.28 | val acc:66.83 | test_acc_best_val: 67.61  | best_test_acc: 67.61 | test acc:67.26 | time:21.2ms
Run 6/10, best test accuracy: 67.61, acc(last): 67.18, total time: 5.99s
The split is 7
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 7/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.8735 | train acc:31.88 | val acc:27.46 | test_acc_best_val: 27.04  | best_test_acc: 27.04 | test acc:27.04 | time:25.4ms
epoch:20 | loss:1.5690 | train acc:89.13 | val acc:60.16 | test_acc_best_val: 57.55  | best_test_acc: 57.55 | test acc:57.55 | time:21.7ms
epoch:40 | loss:1.5327 | train acc:96.38 | val acc:64.76 | test_acc_best_val: 62.93  | best_test_acc: 62.93 | test acc:62.70 | time:21.8ms
epoch:60 | loss:1.5430 | train acc:97.10 | val acc:63.81 | test_acc_best_val: 62.93  | best_test_acc: 62.93 | test acc:62.22 | time:18.6ms
epoch:80 | loss:1.4950 | train acc:99.28 | val acc:64.60 | test_acc_best_val: 62.93  | best_test_acc: 62.93 | test acc:62.11 | time:20.9ms
epoch:100 | loss:1.5155 | train acc:98.55 | val acc:63.81 | test_acc_best_val: 62.93  | best_test_acc: 62.93 | test acc:61.99 | time:21.2ms
epoch:120 | loss:1.4726 | train acc:97.83 | val acc:62.70 | test_acc_best_val: 62.93  | best_test_acc: 62.93 | test acc:61.91 | time:21.1ms
epoch:140 | loss:1.5086 | train acc:97.83 | val acc:62.54 | test_acc_best_val: 62.93  | best_test_acc: 62.93 | test acc:61.79 | time:21.3ms
epoch:160 | loss:1.4767 | train acc:98.55 | val acc:62.86 | test_acc_best_val: 62.93  | best_test_acc: 62.93 | test acc:61.91 | time:22.0ms
epoch:180 | loss:1.4848 | train acc:99.28 | val acc:62.54 | test_acc_best_val: 62.93  | best_test_acc: 62.93 | test acc:61.83 | time:20.6ms
Run 7/10, best test accuracy: 62.93, acc(last): 62.30, total time: 6.29s
The split is 8
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 8/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.8116 | train acc:38.41 | val acc:33.33 | test_acc_best_val: 36.79  | best_test_acc: 36.79 | test acc:36.79 | time:31.4ms
epoch:20 | loss:1.5122 | train acc:95.65 | val acc:60.48 | test_acc_best_val: 61.08  | best_test_acc: 61.08 | test acc:61.08 | time:27.2ms
epoch:40 | loss:1.4722 | train acc:99.28 | val acc:64.13 | test_acc_best_val: 63.17  | best_test_acc: 63.17 | test acc:63.40 | time:21.9ms
epoch:60 | loss:1.4993 | train acc:100.00 | val acc:62.54 | test_acc_best_val: 63.52  | best_test_acc: 63.52 | test acc:63.17 | time:27.2ms
epoch:80 | loss:1.4448 | train acc:100.00 | val acc:63.17 | test_acc_best_val: 63.52  | best_test_acc: 63.52 | test acc:63.68 | time:26.9ms
epoch:100 | loss:1.5030 | train acc:100.00 | val acc:65.24 | test_acc_best_val: 63.68  | best_test_acc: 63.68 | test acc:63.68 | time:23.3ms
epoch:120 | loss:1.4668 | train acc:100.00 | val acc:65.24 | test_acc_best_val: 63.80  | best_test_acc: 63.80 | test acc:64.31 | time:24.6ms
epoch:140 | loss:1.4766 | train acc:100.00 | val acc:64.13 | test_acc_best_val: 63.80  | best_test_acc: 63.80 | test acc:64.23 | time:35.3ms
epoch:160 | loss:1.4860 | train acc:100.00 | val acc:63.81 | test_acc_best_val: 63.80  | best_test_acc: 63.80 | test acc:64.39 | time:30.4ms
epoch:180 | loss:1.4764 | train acc:100.00 | val acc:63.49 | test_acc_best_val: 63.80  | best_test_acc: 63.80 | test acc:64.47 | time:25.7ms
Run 8/10, best test accuracy: 63.80, acc(last): 64.74, total time: 7.43s
The split is 9
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 9/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.8502 | train acc:48.55 | val acc:41.75 | test_acc_best_val: 42.10  | best_test_acc: 42.10 | test acc:42.10 | time:22.2ms
epoch:20 | loss:1.5255 | train acc:91.30 | val acc:65.24 | test_acc_best_val: 60.06  | best_test_acc: 60.06 | test acc:60.06 | time:21.2ms
epoch:40 | loss:1.4722 | train acc:97.83 | val acc:64.29 | test_acc_best_val: 61.40  | best_test_acc: 61.40 | test acc:62.66 | time:21.7ms
epoch:60 | loss:1.4936 | train acc:100.00 | val acc:65.40 | test_acc_best_val: 62.85  | best_test_acc: 62.85 | test acc:62.70 | time:22.7ms
epoch:80 | loss:1.4753 | train acc:99.28 | val acc:64.44 | test_acc_best_val: 62.85  | best_test_acc: 62.85 | test acc:62.58 | time:30.5ms
epoch:100 | loss:1.4895 | train acc:98.55 | val acc:65.24 | test_acc_best_val: 62.85  | best_test_acc: 62.85 | test acc:63.13 | time:27.8ms
epoch:120 | loss:1.4552 | train acc:98.55 | val acc:66.03 | test_acc_best_val: 62.85  | best_test_acc: 62.85 | test acc:63.17 | time:19.5ms
epoch:140 | loss:1.4604 | train acc:98.55 | val acc:65.87 | test_acc_best_val: 62.85  | best_test_acc: 62.85 | test acc:62.93 | time:20.9ms
epoch:160 | loss:1.4620 | train acc:98.55 | val acc:67.30 | test_acc_best_val: 63.21  | best_test_acc: 63.21 | test acc:63.21 | time:24.7ms
epoch:180 | loss:1.4592 | train acc:98.55 | val acc:66.83 | test_acc_best_val: 63.17  | best_test_acc: 63.17 | test acc:63.40 | time:20.7ms
Run 9/10, best test accuracy: 63.17, acc(last): 63.88, total time: 6.79s
The split is 10
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 10/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.8602 | train acc:50.72 | val acc:40.32 | test_acc_best_val: 40.76  | best_test_acc: 40.76 | test acc:40.76 | time:25.1ms
epoch:20 | loss:1.4760 | train acc:95.65 | val acc:63.17 | test_acc_best_val: 60.26  | best_test_acc: 60.26 | test acc:60.26 | time:20.6ms
epoch:40 | loss:1.5260 | train acc:97.83 | val acc:63.02 | test_acc_best_val: 60.46  | best_test_acc: 60.46 | test acc:60.14 | time:27.0ms
epoch:60 | loss:1.4813 | train acc:99.28 | val acc:64.44 | test_acc_best_val: 61.95  | best_test_acc: 61.95 | test acc:62.26 | time:24.1ms
epoch:80 | loss:1.4982 | train acc:99.28 | val acc:65.56 | test_acc_best_val: 62.97  | best_test_acc: 62.97 | test acc:62.81 | time:23.6ms
epoch:100 | loss:1.4698 | train acc:99.28 | val acc:65.56 | test_acc_best_val: 62.93  | best_test_acc: 62.93 | test acc:63.01 | time:26.0ms
epoch:120 | loss:1.4944 | train acc:99.28 | val acc:66.35 | test_acc_best_val: 63.80  | best_test_acc: 63.80 | test acc:63.84 | time:26.9ms
epoch:140 | loss:1.4584 | train acc:99.28 | val acc:65.87 | test_acc_best_val: 63.80  | best_test_acc: 63.80 | test acc:64.23 | time:27.7ms
epoch:160 | loss:1.5025 | train acc:99.28 | val acc:66.98 | test_acc_best_val: 64.74  | best_test_acc: 64.74 | test acc:64.74 | time:24.3ms
epoch:180 | loss:1.4682 | train acc:99.28 | val acc:66.35 | test_acc_best_val: 64.74  | best_test_acc: 64.74 | test acc:63.88 | time:27.8ms
Run 10/10, best test accuracy: 64.74, acc(last): 64.43, total time: 7.08s
The split is 1
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 1/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.8448 | train acc:21.74 | val acc:20.79 | test_acc_best_val: 21.66  | best_test_acc: 21.66 | test acc:21.66 | time:22.8ms
epoch:20 | loss:1.5486 | train acc:89.86 | val acc:67.94 | test_acc_best_val: 62.97  | best_test_acc: 62.97 | test acc:62.97 | time:20.2ms
epoch:40 | loss:1.5042 | train acc:96.38 | val acc:67.14 | test_acc_best_val: 62.97  | best_test_acc: 62.97 | test acc:64.35 | time:22.6ms
epoch:60 | loss:1.5330 | train acc:98.55 | val acc:67.62 | test_acc_best_val: 65.09  | best_test_acc: 65.09 | test acc:65.29 | time:21.7ms
epoch:80 | loss:1.4847 | train acc:99.28 | val acc:67.78 | test_acc_best_val: 65.09  | best_test_acc: 65.09 | test acc:65.84 | time:21.6ms
epoch:100 | loss:1.4763 | train acc:98.55 | val acc:68.10 | test_acc_best_val: 65.09  | best_test_acc: 65.09 | test acc:66.39 | time:19.9ms
epoch:120 | loss:1.5178 | train acc:99.28 | val acc:68.41 | test_acc_best_val: 66.71  | best_test_acc: 66.71 | test acc:66.63 | time:21.8ms
epoch:140 | loss:1.4703 | train acc:99.28 | val acc:68.10 | test_acc_best_val: 66.71  | best_test_acc: 66.71 | test acc:66.39 | time:20.6ms
epoch:160 | loss:1.4826 | train acc:99.28 | val acc:68.41 | test_acc_best_val: 66.71  | best_test_acc: 66.71 | test acc:66.27 | time:22.3ms
epoch:180 | loss:1.5438 | train acc:99.28 | val acc:69.21 | test_acc_best_val: 66.71  | best_test_acc: 66.71 | test acc:66.98 | time:21.7ms
Run 1/10, best test accuracy: 67.10, acc(last): 67.26, total time: 6.41s
The split is 2
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 2/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.8722 | train acc:42.75 | val acc:26.67 | test_acc_best_val: 29.95  | best_test_acc: 29.95 | test acc:29.95 | time:21.1ms
epoch:20 | loss:1.5292 | train acc:88.41 | val acc:63.02 | test_acc_best_val: 63.05  | best_test_acc: 63.05 | test acc:63.05 | time:21.0ms
epoch:40 | loss:1.5369 | train acc:98.55 | val acc:64.92 | test_acc_best_val: 65.09  | best_test_acc: 65.09 | test acc:65.13 | time:21.6ms
epoch:60 | loss:1.5032 | train acc:100.00 | val acc:64.76 | test_acc_best_val: 65.41  | best_test_acc: 65.41 | test acc:65.64 | time:21.2ms
epoch:80 | loss:1.4988 | train acc:100.00 | val acc:65.87 | test_acc_best_val: 66.35  | best_test_acc: 66.35 | test acc:66.35 | time:21.3ms
epoch:100 | loss:1.5073 | train acc:100.00 | val acc:65.71 | test_acc_best_val: 66.63  | best_test_acc: 66.63 | test acc:66.78 | time:21.0ms
epoch:120 | loss:1.4770 | train acc:100.00 | val acc:65.71 | test_acc_best_val: 66.63  | best_test_acc: 66.63 | test acc:67.30 | time:21.9ms
epoch:140 | loss:1.4904 | train acc:100.00 | val acc:65.40 | test_acc_best_val: 66.63  | best_test_acc: 66.63 | test acc:67.45 | time:21.8ms
epoch:160 | loss:1.4946 | train acc:100.00 | val acc:67.94 | test_acc_best_val: 67.89  | best_test_acc: 67.89 | test acc:68.00 | time:21.2ms
epoch:180 | loss:1.4894 | train acc:100.00 | val acc:67.30 | test_acc_best_val: 67.96  | best_test_acc: 67.96 | test acc:67.22 | time:21.9ms
Run 2/10, best test accuracy: 67.96, acc(last): 67.10, total time: 6.05s
The split is 3
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 3/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.8093 | train acc:39.13 | val acc:36.03 | test_acc_best_val: 33.33  | best_test_acc: 33.33 | test acc:33.33 | time:21.7ms
epoch:20 | loss:1.5429 | train acc:95.65 | val acc:63.81 | test_acc_best_val: 63.88  | best_test_acc: 63.88 | test acc:64.54 | time:21.5ms
epoch:40 | loss:1.5306 | train acc:98.55 | val acc:66.51 | test_acc_best_val: 65.61  | best_test_acc: 65.61 | test acc:65.61 | time:19.7ms
epoch:60 | loss:1.5308 | train acc:99.28 | val acc:64.92 | test_acc_best_val: 65.72  | best_test_acc: 65.72 | test acc:65.49 | time:23.4ms
epoch:80 | loss:1.5416 | train acc:99.28 | val acc:66.35 | test_acc_best_val: 65.72  | best_test_acc: 65.72 | test acc:66.51 | time:23.3ms
epoch:100 | loss:1.5156 | train acc:99.28 | val acc:66.83 | test_acc_best_val: 66.98  | best_test_acc: 66.98 | test acc:67.14 | time:30.4ms
epoch:120 | loss:1.4939 | train acc:99.28 | val acc:66.67 | test_acc_best_val: 66.98  | best_test_acc: 66.98 | test acc:66.86 | time:26.2ms
epoch:140 | loss:1.4887 | train acc:99.28 | val acc:66.51 | test_acc_best_val: 66.98  | best_test_acc: 66.98 | test acc:67.10 | time:19.7ms
epoch:160 | loss:1.5563 | train acc:99.28 | val acc:67.30 | test_acc_best_val: 67.45  | best_test_acc: 67.45 | test acc:67.45 | time:22.1ms
epoch:180 | loss:1.4711 | train acc:99.28 | val acc:66.67 | test_acc_best_val: 67.65  | best_test_acc: 67.65 | test acc:67.77 | time:28.5ms
Run 3/10, best test accuracy: 67.65, acc(last): 67.61, total time: 6.46s
The split is 4
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 4/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.8519 | train acc:31.16 | val acc:24.92 | test_acc_best_val: 25.24  | best_test_acc: 25.24 | test acc:25.24 | time:21.1ms
epoch:20 | loss:1.5582 | train acc:86.96 | val acc:62.86 | test_acc_best_val: 61.16  | best_test_acc: 61.16 | test acc:61.16 | time:24.4ms
epoch:40 | loss:1.5092 | train acc:97.10 | val acc:63.65 | test_acc_best_val: 62.74  | best_test_acc: 62.74 | test acc:62.74 | time:29.3ms
epoch:60 | loss:1.5286 | train acc:97.83 | val acc:64.76 | test_acc_best_val: 62.85  | best_test_acc: 62.85 | test acc:63.01 | time:27.2ms
epoch:80 | loss:1.4836 | train acc:99.28 | val acc:64.29 | test_acc_best_val: 63.05  | best_test_acc: 63.05 | test acc:63.60 | time:20.4ms
epoch:100 | loss:1.4707 | train acc:98.55 | val acc:64.60 | test_acc_best_val: 63.05  | best_test_acc: 63.05 | test acc:63.64 | time:28.6ms
epoch:120 | loss:1.4977 | train acc:99.28 | val acc:64.92 | test_acc_best_val: 63.56  | best_test_acc: 63.56 | test acc:63.95 | time:19.1ms
epoch:140 | loss:1.5533 | train acc:99.28 | val acc:64.92 | test_acc_best_val: 63.56  | best_test_acc: 63.56 | test acc:63.99 | time:27.3ms
epoch:160 | loss:1.4866 | train acc:98.55 | val acc:64.29 | test_acc_best_val: 63.56  | best_test_acc: 63.56 | test acc:63.76 | time:29.6ms
epoch:180 | loss:1.5063 | train acc:98.55 | val acc:65.08 | test_acc_best_val: 63.56  | best_test_acc: 63.56 | test acc:63.52 | time:22.1ms
Run 4/10, best test accuracy: 63.68, acc(last): 63.84, total time: 6.70s
The split is 5
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 5/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.8646 | train acc:29.71 | val acc:22.70 | test_acc_best_val: 25.86  | best_test_acc: 25.86 | test acc:25.86 | time:20.0ms
epoch:20 | loss:1.5590 | train acc:94.93 | val acc:60.95 | test_acc_best_val: 62.46  | best_test_acc: 62.46 | test acc:62.89 | time:21.0ms
epoch:40 | loss:1.4764 | train acc:100.00 | val acc:64.60 | test_acc_best_val: 64.98  | best_test_acc: 64.98 | test acc:65.25 | time:21.4ms
epoch:60 | loss:1.5005 | train acc:100.00 | val acc:64.76 | test_acc_best_val: 65.92  | best_test_acc: 65.92 | test acc:65.64 | time:19.8ms
epoch:80 | loss:1.4915 | train acc:100.00 | val acc:64.76 | test_acc_best_val: 65.92  | best_test_acc: 65.92 | test acc:66.47 | time:25.1ms
epoch:100 | loss:1.4979 | train acc:100.00 | val acc:65.71 | test_acc_best_val: 66.47  | best_test_acc: 66.47 | test acc:67.57 | time:27.1ms
epoch:120 | loss:1.5214 | train acc:100.00 | val acc:67.14 | test_acc_best_val: 67.77  | best_test_acc: 67.77 | test acc:67.77 | time:29.0ms
epoch:140 | loss:1.5459 | train acc:100.00 | val acc:66.67 | test_acc_best_val: 67.77  | best_test_acc: 67.77 | test acc:67.41 | time:20.7ms
epoch:160 | loss:1.4676 | train acc:100.00 | val acc:67.46 | test_acc_best_val: 67.30  | best_test_acc: 67.30 | test acc:67.22 | time:22.5ms
epoch:180 | loss:1.4881 | train acc:100.00 | val acc:66.83 | test_acc_best_val: 67.61  | best_test_acc: 67.61 | test acc:68.04 | time:22.8ms
Run 5/10, best test accuracy: 67.61, acc(last): 68.24, total time: 6.27s
The split is 6
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 6/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.8116 | train acc:41.30 | val acc:36.19 | test_acc_best_val: 39.54  | best_test_acc: 39.54 | test acc:39.54 | time:20.9ms
epoch:20 | loss:1.5613 | train acc:92.03 | val acc:64.29 | test_acc_best_val: 63.64  | best_test_acc: 63.64 | test acc:64.62 | time:24.6ms
epoch:40 | loss:1.5437 | train acc:97.83 | val acc:65.40 | test_acc_best_val: 66.39  | best_test_acc: 66.39 | test acc:66.08 | time:19.3ms
epoch:60 | loss:1.4873 | train acc:99.28 | val acc:65.40 | test_acc_best_val: 65.80  | best_test_acc: 65.80 | test acc:65.80 | time:20.0ms
epoch:80 | loss:1.5006 | train acc:99.28 | val acc:64.92 | test_acc_best_val: 65.80  | best_test_acc: 65.80 | test acc:66.43 | time:20.9ms
epoch:100 | loss:1.4768 | train acc:99.28 | val acc:64.92 | test_acc_best_val: 65.80  | best_test_acc: 65.80 | test acc:66.39 | time:26.8ms
epoch:120 | loss:1.4937 | train acc:99.28 | val acc:66.35 | test_acc_best_val: 66.67  | best_test_acc: 66.67 | test acc:66.59 | time:26.6ms
epoch:140 | loss:1.5003 | train acc:99.28 | val acc:65.71 | test_acc_best_val: 66.67  | best_test_acc: 66.67 | test acc:66.63 | time:21.9ms
epoch:160 | loss:1.5160 | train acc:99.28 | val acc:65.71 | test_acc_best_val: 66.67  | best_test_acc: 66.67 | test acc:66.27 | time:27.7ms
epoch:180 | loss:1.5417 | train acc:99.28 | val acc:66.03 | test_acc_best_val: 66.67  | best_test_acc: 66.67 | test acc:66.67 | time:28.1ms
Run 6/10, best test accuracy: 66.67, acc(last): 66.71, total time: 7.13s
The split is 7
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 7/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.7898 | train acc:33.33 | val acc:36.67 | test_acc_best_val: 35.77  | best_test_acc: 35.77 | test acc:35.77 | time:26.3ms
epoch:20 | loss:1.5805 | train acc:90.58 | val acc:59.05 | test_acc_best_val: 59.79  | best_test_acc: 59.79 | test acc:59.79 | time:21.9ms
epoch:40 | loss:1.5398 | train acc:97.83 | val acc:59.68 | test_acc_best_val: 61.64  | best_test_acc: 61.64 | test acc:61.83 | time:26.8ms
epoch:60 | loss:1.4787 | train acc:98.55 | val acc:62.38 | test_acc_best_val: 63.09  | best_test_acc: 63.09 | test acc:62.38 | time:23.4ms
epoch:80 | loss:1.4643 | train acc:98.55 | val acc:62.70 | test_acc_best_val: 63.09  | best_test_acc: 63.09 | test acc:62.34 | time:30.3ms
epoch:100 | loss:1.5291 | train acc:98.55 | val acc:61.75 | test_acc_best_val: 63.09  | best_test_acc: 63.09 | test acc:61.83 | time:23.2ms
epoch:120 | loss:1.5159 | train acc:99.28 | val acc:61.59 | test_acc_best_val: 63.09  | best_test_acc: 63.09 | test acc:61.64 | time:21.3ms
epoch:140 | loss:1.4898 | train acc:98.55 | val acc:61.75 | test_acc_best_val: 63.09  | best_test_acc: 63.09 | test acc:62.03 | time:21.2ms
epoch:160 | loss:1.4574 | train acc:98.55 | val acc:61.59 | test_acc_best_val: 63.09  | best_test_acc: 63.09 | test acc:62.34 | time:22.3ms
epoch:180 | loss:1.5511 | train acc:98.55 | val acc:62.54 | test_acc_best_val: 63.09  | best_test_acc: 63.09 | test acc:62.42 | time:24.6ms
Run 7/10, best test accuracy: 63.09, acc(last): 62.93, total time: 6.86s
The split is 8
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 8/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.8230 | train acc:28.99 | val acc:27.46 | test_acc_best_val: 27.75  | best_test_acc: 27.75 | test acc:27.75 | time:28.9ms
epoch:20 | loss:1.5163 | train acc:93.48 | val acc:59.84 | test_acc_best_val: 61.28  | best_test_acc: 61.28 | test acc:62.54 | time:21.1ms
epoch:40 | loss:1.5344 | train acc:100.00 | val acc:61.27 | test_acc_best_val: 63.64  | best_test_acc: 63.64 | test acc:63.80 | time:26.2ms
epoch:60 | loss:1.4759 | train acc:100.00 | val acc:61.59 | test_acc_best_val: 63.01  | best_test_acc: 63.01 | test acc:63.01 | time:22.5ms
epoch:80 | loss:1.4216 | train acc:100.00 | val acc:62.54 | test_acc_best_val: 63.05  | best_test_acc: 63.05 | test acc:62.89 | time:24.8ms
epoch:100 | loss:1.4540 | train acc:100.00 | val acc:63.17 | test_acc_best_val: 63.25  | best_test_acc: 63.25 | test acc:63.48 | time:22.1ms
epoch:120 | loss:1.4806 | train acc:100.00 | val acc:64.60 | test_acc_best_val: 64.23  | best_test_acc: 64.23 | test acc:63.95 | time:27.1ms
epoch:140 | loss:1.4758 | train acc:100.00 | val acc:63.81 | test_acc_best_val: 64.23  | best_test_acc: 64.23 | test acc:63.80 | time:22.9ms
epoch:160 | loss:1.4600 | train acc:100.00 | val acc:64.76 | test_acc_best_val: 64.23  | best_test_acc: 64.23 | test acc:63.72 | time:21.6ms
epoch:180 | loss:1.5181 | train acc:100.00 | val acc:64.76 | test_acc_best_val: 64.23  | best_test_acc: 64.23 | test acc:64.31 | time:23.8ms
Run 8/10, best test accuracy: 64.23, acc(last): 64.19, total time: 6.67s
The split is 9
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 9/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.8478 | train acc:43.48 | val acc:30.48 | test_acc_best_val: 32.19  | best_test_acc: 32.19 | test acc:32.19 | time:23.7ms
epoch:20 | loss:1.5032 | train acc:89.86 | val acc:60.95 | test_acc_best_val: 60.85  | best_test_acc: 60.85 | test acc:60.85 | time:26.8ms
epoch:40 | loss:1.4982 | train acc:98.55 | val acc:60.16 | test_acc_best_val: 60.85  | best_test_acc: 60.85 | test acc:62.66 | time:24.2ms
epoch:60 | loss:1.5042 | train acc:98.55 | val acc:60.95 | test_acc_best_val: 63.72  | best_test_acc: 63.72 | test acc:64.27 | time:24.7ms
epoch:80 | loss:1.4723 | train acc:98.55 | val acc:61.27 | test_acc_best_val: 63.95  | best_test_acc: 63.95 | test acc:63.25 | time:24.8ms
epoch:100 | loss:1.4758 | train acc:98.55 | val acc:62.06 | test_acc_best_val: 63.76  | best_test_acc: 63.76 | test acc:64.54 | time:19.7ms
epoch:120 | loss:1.5085 | train acc:98.55 | val acc:62.54 | test_acc_best_val: 64.31  | best_test_acc: 64.31 | test acc:64.31 | time:21.2ms
epoch:140 | loss:1.4567 | train acc:98.55 | val acc:62.38 | test_acc_best_val: 64.15  | best_test_acc: 64.15 | test acc:64.19 | time:23.2ms
epoch:160 | loss:1.5069 | train acc:98.55 | val acc:63.17 | test_acc_best_val: 64.98  | best_test_acc: 64.98 | test acc:64.98 | time:31.1ms
epoch:180 | loss:1.4965 | train acc:98.55 | val acc:63.33 | test_acc_best_val: 65.29  | best_test_acc: 65.29 | test acc:65.17 | time:26.9ms
Run 9/10, best test accuracy: 65.29, acc(last): 64.90, total time: 6.92s
The split is 10
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 10/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.8012 | train acc:37.68 | val acc:28.89 | test_acc_best_val: 28.77  | best_test_acc: 28.77 | test acc:28.77 | time:24.2ms
epoch:20 | loss:1.5305 | train acc:90.58 | val acc:57.46 | test_acc_best_val: 60.61  | best_test_acc: 60.61 | test acc:61.28 | time:19.3ms
epoch:40 | loss:1.4711 | train acc:97.10 | val acc:61.43 | test_acc_best_val: 62.19  | best_test_acc: 62.19 | test acc:62.19 | time:25.6ms
epoch:60 | loss:1.4835 | train acc:99.28 | val acc:62.22 | test_acc_best_val: 63.01  | best_test_acc: 63.01 | test acc:63.17 | time:21.6ms
epoch:80 | loss:1.4398 | train acc:99.28 | val acc:64.13 | test_acc_best_val: 63.68  | best_test_acc: 63.68 | test acc:63.52 | time:21.0ms
epoch:100 | loss:1.4734 | train acc:99.28 | val acc:63.65 | test_acc_best_val: 63.80  | best_test_acc: 63.80 | test acc:63.95 | time:19.8ms
epoch:120 | loss:1.4729 | train acc:99.28 | val acc:63.33 | test_acc_best_val: 63.80  | best_test_acc: 63.80 | test acc:64.31 | time:20.7ms
epoch:140 | loss:1.4464 | train acc:99.28 | val acc:63.97 | test_acc_best_val: 63.80  | best_test_acc: 63.80 | test acc:64.90 | time:24.7ms
epoch:160 | loss:1.4848 | train acc:99.28 | val acc:63.65 | test_acc_best_val: 63.80  | best_test_acc: 63.80 | test acc:64.70 | time:28.9ms
epoch:180 | loss:1.4908 | train acc:99.28 | val acc:63.02 | test_acc_best_val: 63.80  | best_test_acc: 63.80 | test acc:64.90 | time:22.6ms
Run 10/10, best test accuracy: 63.80, acc(last): 64.47, total time: 6.69s
The split is 1
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 1/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.8605 | train acc:17.39 | val acc:20.79 | test_acc_best_val: 20.60  | best_test_acc: 20.60 | test acc:20.60 | time:21.4ms
epoch:20 | loss:1.5463 | train acc:86.96 | val acc:61.11 | test_acc_best_val: 61.75  | best_test_acc: 61.75 | test acc:61.75 | time:27.3ms
epoch:40 | loss:1.5333 | train acc:96.38 | val acc:64.44 | test_acc_best_val: 65.25  | best_test_acc: 65.25 | test acc:65.72 | time:25.2ms
epoch:60 | loss:1.5001 | train acc:98.55 | val acc:63.97 | test_acc_best_val: 65.96  | best_test_acc: 65.96 | test acc:66.04 | time:20.7ms
epoch:80 | loss:1.4998 | train acc:98.55 | val acc:64.76 | test_acc_best_val: 66.23  | best_test_acc: 66.23 | test acc:66.71 | time:20.2ms
epoch:100 | loss:1.4943 | train acc:99.28 | val acc:64.60 | test_acc_best_val: 66.23  | best_test_acc: 66.23 | test acc:66.55 | time:20.5ms
epoch:120 | loss:1.4885 | train acc:99.28 | val acc:65.08 | test_acc_best_val: 67.37  | best_test_acc: 67.37 | test acc:67.49 | time:20.1ms
epoch:140 | loss:1.5058 | train acc:99.28 | val acc:64.44 | test_acc_best_val: 67.37  | best_test_acc: 67.37 | test acc:67.18 | time:20.3ms
epoch:160 | loss:1.5504 | train acc:99.28 | val acc:65.08 | test_acc_best_val: 66.90  | best_test_acc: 66.90 | test acc:66.82 | time:20.9ms
epoch:180 | loss:1.4774 | train acc:99.28 | val acc:65.40 | test_acc_best_val: 66.90  | best_test_acc: 66.90 | test acc:66.51 | time:25.9ms
Run 1/10, best test accuracy: 66.67, acc(last): 67.30, total time: 6.26s
The split is 2
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 2/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.8634 | train acc:33.33 | val acc:36.98 | test_acc_best_val: 33.37  | best_test_acc: 33.37 | test acc:33.37 | time:23.4ms
epoch:20 | loss:1.5184 | train acc:95.65 | val acc:66.51 | test_acc_best_val: 64.39  | best_test_acc: 64.39 | test acc:64.39 | time:20.4ms
epoch:40 | loss:1.4581 | train acc:99.28 | val acc:65.08 | test_acc_best_val: 64.58  | best_test_acc: 64.58 | test acc:64.23 | time:21.2ms
epoch:60 | loss:1.5010 | train acc:100.00 | val acc:66.03 | test_acc_best_val: 64.58  | best_test_acc: 64.58 | test acc:64.82 | time:34.6ms
epoch:80 | loss:1.4797 | train acc:100.00 | val acc:65.56 | test_acc_best_val: 64.58  | best_test_acc: 64.58 | test acc:66.43 | time:21.9ms
epoch:100 | loss:1.4942 | train acc:100.00 | val acc:67.14 | test_acc_best_val: 66.75  | best_test_acc: 66.75 | test acc:66.59 | time:23.0ms
epoch:120 | loss:1.5184 | train acc:100.00 | val acc:66.67 | test_acc_best_val: 66.75  | best_test_acc: 66.75 | test acc:66.43 | time:25.2ms
epoch:140 | loss:1.5280 | train acc:100.00 | val acc:66.67 | test_acc_best_val: 66.75  | best_test_acc: 66.75 | test acc:66.39 | time:21.9ms
epoch:160 | loss:1.5037 | train acc:100.00 | val acc:67.62 | test_acc_best_val: 66.82  | best_test_acc: 66.82 | test acc:66.82 | time:23.4ms
epoch:180 | loss:1.4692 | train acc:100.00 | val acc:66.98 | test_acc_best_val: 66.82  | best_test_acc: 66.82 | test acc:66.98 | time:21.3ms
Run 2/10, best test accuracy: 67.26, acc(last): 67.30, total time: 6.29s
The split is 3
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 3/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.8318 | train acc:31.16 | val acc:25.56 | test_acc_best_val: 24.88  | best_test_acc: 24.88 | test acc:24.88 | time:25.4ms
epoch:20 | loss:1.5712 | train acc:94.20 | val acc:64.92 | test_acc_best_val: 63.29  | best_test_acc: 63.29 | test acc:63.68 | time:20.4ms
epoch:40 | loss:1.5360 | train acc:97.83 | val acc:65.87 | test_acc_best_val: 65.72  | best_test_acc: 65.72 | test acc:65.88 | time:21.2ms
epoch:60 | loss:1.5161 | train acc:99.28 | val acc:67.30 | test_acc_best_val: 66.31  | best_test_acc: 66.31 | test acc:66.31 | time:25.2ms
epoch:80 | loss:1.5279 | train acc:99.28 | val acc:67.14 | test_acc_best_val: 66.31  | best_test_acc: 66.31 | test acc:66.39 | time:24.1ms
epoch:100 | loss:1.5050 | train acc:99.28 | val acc:66.35 | test_acc_best_val: 65.72  | best_test_acc: 65.72 | test acc:66.67 | time:21.6ms
epoch:120 | loss:1.5372 | train acc:99.28 | val acc:67.46 | test_acc_best_val: 66.55  | best_test_acc: 66.55 | test acc:66.31 | time:32.3ms
epoch:140 | loss:1.5080 | train acc:99.28 | val acc:67.78 | test_acc_best_val: 66.55  | best_test_acc: 66.55 | test acc:66.43 | time:27.2ms
epoch:160 | loss:1.5295 | train acc:99.28 | val acc:68.25 | test_acc_best_val: 67.30  | best_test_acc: 67.30 | test acc:67.22 | time:41.3ms
epoch:180 | loss:1.4788 | train acc:99.28 | val acc:68.25 | test_acc_best_val: 67.30  | best_test_acc: 67.30 | test acc:67.57 | time:28.2ms
Run 3/10, best test accuracy: 67.30, acc(last): 67.45, total time: 7.01s
The split is 4
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 4/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.8092 | train acc:31.88 | val acc:26.35 | test_acc_best_val: 26.73  | best_test_acc: 26.73 | test acc:26.73 | time:24.9ms
epoch:20 | loss:1.5603 | train acc:89.86 | val acc:62.06 | test_acc_best_val: 62.34  | best_test_acc: 62.34 | test acc:62.26 | time:22.9ms
epoch:40 | loss:1.4873 | train acc:97.83 | val acc:62.38 | test_acc_best_val: 62.34  | best_test_acc: 62.34 | test acc:62.97 | time:45.9ms
epoch:60 | loss:1.5087 | train acc:98.55 | val acc:64.92 | test_acc_best_val: 63.84  | best_test_acc: 63.84 | test acc:63.84 | time:27.8ms
epoch:80 | loss:1.5182 | train acc:98.55 | val acc:63.49 | test_acc_best_val: 64.19  | best_test_acc: 64.19 | test acc:63.84 | time:23.1ms
epoch:100 | loss:1.4651 | train acc:98.55 | val acc:62.86 | test_acc_best_val: 64.19  | best_test_acc: 64.19 | test acc:64.35 | time:21.2ms
epoch:120 | loss:1.5281 | train acc:98.55 | val acc:62.70 | test_acc_best_val: 64.19  | best_test_acc: 64.19 | test acc:63.99 | time:21.1ms
epoch:140 | loss:1.4970 | train acc:98.55 | val acc:63.17 | test_acc_best_val: 64.19  | best_test_acc: 64.19 | test acc:64.82 | time:24.5ms
epoch:160 | loss:1.5032 | train acc:99.28 | val acc:62.86 | test_acc_best_val: 64.19  | best_test_acc: 64.19 | test acc:64.47 | time:30.7ms
epoch:180 | loss:1.5221 | train acc:99.28 | val acc:63.81 | test_acc_best_val: 64.19  | best_test_acc: 64.19 | test acc:64.90 | time:27.5ms
Run 4/10, best test accuracy: 64.19, acc(last): 65.21, total time: 6.75s
The split is 5
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 5/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.8332 | train acc:47.10 | val acc:34.92 | test_acc_best_val: 36.12  | best_test_acc: 36.12 | test acc:36.12 | time:26.5ms
epoch:20 | loss:1.5889 | train acc:95.65 | val acc:59.68 | test_acc_best_val: 63.40  | best_test_acc: 63.40 | test acc:63.95 | time:23.2ms
epoch:40 | loss:1.5483 | train acc:99.28 | val acc:60.95 | test_acc_best_val: 64.98  | best_test_acc: 64.98 | test acc:65.29 | time:21.4ms
epoch:60 | loss:1.4838 | train acc:100.00 | val acc:62.86 | test_acc_best_val: 66.31  | best_test_acc: 66.31 | test acc:66.75 | time:19.6ms
epoch:80 | loss:1.4602 | train acc:100.00 | val acc:63.97 | test_acc_best_val: 67.49  | best_test_acc: 67.49 | test acc:67.53 | time:22.3ms
epoch:100 | loss:1.4887 | train acc:100.00 | val acc:64.92 | test_acc_best_val: 67.81  | best_test_acc: 67.81 | test acc:67.49 | time:20.6ms
epoch:120 | loss:1.4614 | train acc:100.00 | val acc:64.76 | test_acc_best_val: 67.81  | best_test_acc: 67.81 | test acc:67.57 | time:21.2ms
epoch:140 | loss:1.4865 | train acc:100.00 | val acc:63.81 | test_acc_best_val: 67.81  | best_test_acc: 67.81 | test acc:67.53 | time:24.2ms
epoch:160 | loss:1.4954 | train acc:100.00 | val acc:65.40 | test_acc_best_val: 67.81  | best_test_acc: 67.81 | test acc:67.02 | time:23.3ms
epoch:180 | loss:1.4359 | train acc:100.00 | val acc:64.92 | test_acc_best_val: 67.81  | best_test_acc: 67.81 | test acc:66.94 | time:28.4ms
Run 5/10, best test accuracy: 67.81, acc(last): 67.45, total time: 6.63s
The split is 6
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 6/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.8188 | train acc:44.20 | val acc:41.75 | test_acc_best_val: 43.71  | best_test_acc: 43.71 | test acc:43.71 | time:29.0ms
epoch:20 | loss:1.5338 | train acc:92.03 | val acc:63.97 | test_acc_best_val: 63.92  | best_test_acc: 63.92 | test acc:64.23 | time:29.0ms
epoch:40 | loss:1.4928 | train acc:96.38 | val acc:63.81 | test_acc_best_val: 64.27  | best_test_acc: 64.27 | test acc:64.70 | time:23.9ms
epoch:60 | loss:1.5082 | train acc:98.55 | val acc:64.92 | test_acc_best_val: 64.78  | best_test_acc: 64.78 | test acc:65.37 | time:21.6ms
epoch:80 | loss:1.5034 | train acc:99.28 | val acc:65.71 | test_acc_best_val: 66.04  | best_test_acc: 66.04 | test acc:66.16 | time:21.2ms
epoch:100 | loss:1.4887 | train acc:99.28 | val acc:67.46 | test_acc_best_val: 66.31  | best_test_acc: 66.31 | test acc:66.31 | time:20.0ms
epoch:120 | loss:1.4557 | train acc:99.28 | val acc:66.51 | test_acc_best_val: 66.31  | best_test_acc: 66.31 | test acc:66.04 | time:22.0ms
epoch:140 | loss:1.5434 | train acc:99.28 | val acc:67.14 | test_acc_best_val: 66.31  | best_test_acc: 66.31 | test acc:66.19 | time:25.5ms
epoch:160 | loss:1.4559 | train acc:99.28 | val acc:66.67 | test_acc_best_val: 66.31  | best_test_acc: 66.31 | test acc:66.51 | time:30.8ms
epoch:180 | loss:1.4790 | train acc:99.28 | val acc:67.14 | test_acc_best_val: 66.31  | best_test_acc: 66.31 | test acc:66.59 | time:23.1ms
Run 6/10, best test accuracy: 66.31, acc(last): 66.55, total time: 6.73s
The split is 7
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 7/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.8685 | train acc:41.30 | val acc:36.83 | test_acc_best_val: 37.42  | best_test_acc: 37.42 | test acc:37.42 | time:22.3ms
epoch:20 | loss:1.5399 | train acc:89.13 | val acc:61.75 | test_acc_best_val: 59.55  | best_test_acc: 59.55 | test acc:59.98 | time:22.7ms
epoch:40 | loss:1.5212 | train acc:95.65 | val acc:63.49 | test_acc_best_val: 61.67  | best_test_acc: 61.67 | test acc:61.99 | time:21.4ms
epoch:60 | loss:1.5148 | train acc:97.83 | val acc:63.49 | test_acc_best_val: 62.11  | best_test_acc: 62.11 | test acc:63.33 | time:19.6ms
epoch:80 | loss:1.5674 | train acc:98.55 | val acc:64.92 | test_acc_best_val: 64.31  | best_test_acc: 64.31 | test acc:64.54 | time:22.2ms
epoch:100 | loss:1.4829 | train acc:98.55 | val acc:65.40 | test_acc_best_val: 64.62  | best_test_acc: 64.62 | test acc:64.27 | time:52.3ms
epoch:120 | loss:1.5027 | train acc:99.28 | val acc:65.08 | test_acc_best_val: 64.27  | best_test_acc: 64.27 | test acc:64.70 | time:19.9ms
epoch:140 | loss:1.4650 | train acc:98.55 | val acc:64.76 | test_acc_best_val: 64.27  | best_test_acc: 64.27 | test acc:64.62 | time:21.9ms
epoch:160 | loss:1.5175 | train acc:98.55 | val acc:65.56 | test_acc_best_val: 64.27  | best_test_acc: 64.27 | test acc:65.29 | time:20.1ms
epoch:180 | loss:1.5498 | train acc:97.83 | val acc:64.60 | test_acc_best_val: 64.27  | best_test_acc: 64.27 | test acc:64.66 | time:22.7ms
Run 7/10, best test accuracy: 64.27, acc(last): 64.27, total time: 6.38s
The split is 8
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 8/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.8380 | train acc:39.13 | val acc:28.25 | test_acc_best_val: 30.50  | best_test_acc: 30.50 | test acc:30.50 | time:19.1ms
epoch:20 | loss:1.5420 | train acc:92.03 | val acc:61.43 | test_acc_best_val: 60.93  | best_test_acc: 60.93 | test acc:61.01 | time:28.5ms
epoch:40 | loss:1.4969 | train acc:99.28 | val acc:61.43 | test_acc_best_val: 61.99  | best_test_acc: 61.99 | test acc:62.03 | time:22.6ms
epoch:60 | loss:1.4619 | train acc:100.00 | val acc:60.79 | test_acc_best_val: 61.99  | best_test_acc: 61.99 | test acc:63.13 | time:21.3ms
epoch:80 | loss:1.4336 | train acc:100.00 | val acc:60.79 | test_acc_best_val: 61.99  | best_test_acc: 61.99 | test acc:63.44 | time:22.7ms
epoch:100 | loss:1.4501 | train acc:100.00 | val acc:62.22 | test_acc_best_val: 63.84  | best_test_acc: 63.84 | test acc:63.84 | time:21.5ms
epoch:120 | loss:1.4430 | train acc:100.00 | val acc:62.70 | test_acc_best_val: 63.48  | best_test_acc: 63.48 | test acc:63.80 | time:22.2ms
epoch:140 | loss:1.4754 | train acc:100.00 | val acc:62.54 | test_acc_best_val: 63.48  | best_test_acc: 63.48 | test acc:64.66 | time:21.7ms
epoch:160 | loss:1.5149 | train acc:100.00 | val acc:61.75 | test_acc_best_val: 63.48  | best_test_acc: 63.48 | test acc:64.66 | time:28.0ms
epoch:180 | loss:1.4677 | train acc:100.00 | val acc:62.86 | test_acc_best_val: 63.48  | best_test_acc: 63.48 | test acc:65.37 | time:21.1ms
Run 8/10, best test accuracy: 65.64, acc(last): 65.25, total time: 6.09s
The split is 9
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 9/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.8235 | train acc:44.93 | val acc:33.81 | test_acc_best_val: 37.19  | best_test_acc: 37.19 | test acc:37.19 | time:22.7ms
epoch:20 | loss:1.5072 | train acc:89.86 | val acc:61.43 | test_acc_best_val: 61.79  | best_test_acc: 61.79 | test acc:61.79 | time:20.2ms
epoch:40 | loss:1.4750 | train acc:95.65 | val acc:62.54 | test_acc_best_val: 62.66  | best_test_acc: 62.66 | test acc:63.05 | time:20.2ms
epoch:60 | loss:1.5130 | train acc:99.28 | val acc:63.49 | test_acc_best_val: 63.48  | best_test_acc: 63.48 | test acc:63.92 | time:22.4ms
epoch:80 | loss:1.4839 | train acc:99.28 | val acc:63.65 | test_acc_best_val: 63.48  | best_test_acc: 63.48 | test acc:63.95 | time:20.3ms
epoch:100 | loss:1.4633 | train acc:98.55 | val acc:64.13 | test_acc_best_val: 64.11  | best_test_acc: 64.11 | test acc:64.15 | time:21.0ms
epoch:120 | loss:1.5039 | train acc:98.55 | val acc:64.92 | test_acc_best_val: 64.27  | best_test_acc: 64.27 | test acc:64.27 | time:20.9ms
epoch:140 | loss:1.4569 | train acc:98.55 | val acc:64.13 | test_acc_best_val: 64.27  | best_test_acc: 64.27 | test acc:64.39 | time:20.9ms
epoch:160 | loss:1.5151 | train acc:99.28 | val acc:63.97 | test_acc_best_val: 64.27  | best_test_acc: 64.27 | test acc:65.06 | time:21.1ms
epoch:180 | loss:1.5079 | train acc:98.55 | val acc:64.13 | test_acc_best_val: 64.27  | best_test_acc: 64.27 | test acc:65.21 | time:21.7ms
Run 9/10, best test accuracy: 65.33, acc(last): 65.02, total time: 6.29s
The split is 10
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 10/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.8406 | train acc:44.20 | val acc:36.19 | test_acc_best_val: 39.19  | best_test_acc: 39.19 | test acc:39.19 | time:21.0ms
epoch:20 | loss:1.5480 | train acc:94.93 | val acc:60.00 | test_acc_best_val: 60.34  | best_test_acc: 60.34 | test acc:60.57 | time:25.9ms
epoch:40 | loss:1.5234 | train acc:98.55 | val acc:62.38 | test_acc_best_val: 62.19  | best_test_acc: 62.19 | test acc:62.11 | time:27.0ms
epoch:60 | loss:1.4852 | train acc:98.55 | val acc:61.59 | test_acc_best_val: 62.15  | best_test_acc: 62.15 | test acc:62.54 | time:19.4ms
epoch:80 | loss:1.4534 | train acc:99.28 | val acc:62.70 | test_acc_best_val: 62.62  | best_test_acc: 62.62 | test acc:62.93 | time:55.5ms
epoch:100 | loss:1.4731 | train acc:99.28 | val acc:62.54 | test_acc_best_val: 62.62  | best_test_acc: 62.62 | test acc:63.60 | time:21.1ms
epoch:120 | loss:1.4695 | train acc:98.55 | val acc:62.38 | test_acc_best_val: 62.62  | best_test_acc: 62.62 | test acc:64.47 | time:22.4ms
epoch:140 | loss:1.4912 | train acc:99.28 | val acc:63.81 | test_acc_best_val: 64.07  | best_test_acc: 64.07 | test acc:63.95 | time:21.6ms
epoch:160 | loss:1.4666 | train acc:99.28 | val acc:62.22 | test_acc_best_val: 64.07  | best_test_acc: 64.07 | test acc:64.66 | time:20.5ms
epoch:180 | loss:1.4487 | train acc:99.28 | val acc:62.54 | test_acc_best_val: 64.07  | best_test_acc: 64.07 | test acc:65.02 | time:24.3ms
Run 10/10, best test accuracy: 64.07, acc(last): 64.82, total time: 6.16s
The split is 1
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 1/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.8153 | train acc:23.91 | val acc:25.56 | test_acc_best_val: 21.93  | best_test_acc: 21.93 | test acc:21.93 | time:21.4ms
epoch:20 | loss:1.5569 | train acc:90.58 | val acc:65.71 | test_acc_best_val: 64.07  | best_test_acc: 64.07 | test acc:64.07 | time:21.1ms
epoch:40 | loss:1.4973 | train acc:97.83 | val acc:67.30 | test_acc_best_val: 65.88  | best_test_acc: 65.88 | test acc:66.08 | time:20.6ms
epoch:60 | loss:1.4940 | train acc:99.28 | val acc:67.46 | test_acc_best_val: 66.19  | best_test_acc: 66.19 | test acc:66.31 | time:20.4ms
epoch:80 | loss:1.4666 | train acc:99.28 | val acc:68.25 | test_acc_best_val: 66.47  | best_test_acc: 66.47 | test acc:66.47 | time:20.9ms
epoch:100 | loss:1.4918 | train acc:98.55 | val acc:68.73 | test_acc_best_val: 66.75  | best_test_acc: 66.75 | test acc:66.78 | time:19.5ms
epoch:120 | loss:1.5351 | train acc:99.28 | val acc:68.73 | test_acc_best_val: 66.78  | best_test_acc: 66.78 | test acc:66.78 | time:25.8ms
epoch:140 | loss:1.4985 | train acc:99.28 | val acc:67.94 | test_acc_best_val: 66.75  | best_test_acc: 66.75 | test acc:66.90 | time:22.4ms
epoch:160 | loss:1.5055 | train acc:98.55 | val acc:68.57 | test_acc_best_val: 66.67  | best_test_acc: 66.67 | test acc:66.31 | time:22.5ms
epoch:180 | loss:1.5090 | train acc:97.83 | val acc:68.89 | test_acc_best_val: 66.67  | best_test_acc: 66.67 | test acc:66.51 | time:24.4ms
Run 1/10, best test accuracy: 66.67, acc(last): 66.82, total time: 6.10s
The split is 2
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 2/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.8049 | train acc:43.48 | val acc:38.10 | test_acc_best_val: 35.77  | best_test_acc: 35.77 | test acc:35.77 | time:21.4ms
epoch:20 | loss:1.4990 | train acc:94.93 | val acc:66.03 | test_acc_best_val: 63.60  | best_test_acc: 63.60 | test acc:63.60 | time:21.2ms
epoch:40 | loss:1.5015 | train acc:100.00 | val acc:66.51 | test_acc_best_val: 63.60  | best_test_acc: 63.60 | test acc:64.82 | time:21.7ms
epoch:60 | loss:1.5035 | train acc:100.00 | val acc:66.98 | test_acc_best_val: 65.06  | best_test_acc: 65.06 | test acc:65.06 | time:21.0ms
epoch:80 | loss:1.4900 | train acc:100.00 | val acc:66.83 | test_acc_best_val: 65.06  | best_test_acc: 65.06 | test acc:65.57 | time:20.2ms
epoch:100 | loss:1.4714 | train acc:100.00 | val acc:66.83 | test_acc_best_val: 65.57  | best_test_acc: 65.57 | test acc:65.80 | time:21.7ms
epoch:120 | loss:1.4872 | train acc:100.00 | val acc:67.62 | test_acc_best_val: 65.84  | best_test_acc: 65.84 | test acc:65.88 | time:20.7ms
epoch:140 | loss:1.4614 | train acc:100.00 | val acc:67.78 | test_acc_best_val: 65.84  | best_test_acc: 65.84 | test acc:66.35 | time:22.9ms
epoch:160 | loss:1.4571 | train acc:100.00 | val acc:67.30 | test_acc_best_val: 65.84  | best_test_acc: 65.84 | test acc:66.27 | time:20.9ms
epoch:180 | loss:1.5202 | train acc:100.00 | val acc:67.46 | test_acc_best_val: 65.84  | best_test_acc: 65.84 | test acc:66.35 | time:20.0ms
Run 2/10, best test accuracy: 66.51, acc(last): 66.51, total time: 6.12s
The split is 3
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 3/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.8131 | train acc:31.16 | val acc:27.62 | test_acc_best_val: 27.71  | best_test_acc: 27.71 | test acc:27.71 | time:20.1ms
epoch:20 | loss:1.6043 | train acc:93.48 | val acc:62.54 | test_acc_best_val: 63.09  | best_test_acc: 63.09 | test acc:63.09 | time:21.6ms
epoch:40 | loss:1.5321 | train acc:99.28 | val acc:62.54 | test_acc_best_val: 65.68  | best_test_acc: 65.68 | test acc:66.51 | time:24.2ms
epoch:60 | loss:1.5610 | train acc:99.28 | val acc:62.70 | test_acc_best_val: 65.68  | best_test_acc: 65.68 | test acc:66.19 | time:20.5ms
epoch:80 | loss:1.5380 | train acc:99.28 | val acc:63.97 | test_acc_best_val: 66.19  | best_test_acc: 66.19 | test acc:67.10 | time:22.5ms
epoch:100 | loss:1.4655 | train acc:99.28 | val acc:63.81 | test_acc_best_val: 66.19  | best_test_acc: 66.19 | test acc:66.43 | time:20.2ms
epoch:120 | loss:1.5153 | train acc:99.28 | val acc:64.60 | test_acc_best_val: 67.45  | best_test_acc: 67.45 | test acc:67.69 | time:22.9ms
epoch:140 | loss:1.5040 | train acc:99.28 | val acc:66.03 | test_acc_best_val: 67.61  | best_test_acc: 67.61 | test acc:67.61 | time:26.7ms
epoch:160 | loss:1.4942 | train acc:99.28 | val acc:65.71 | test_acc_best_val: 67.49  | best_test_acc: 67.49 | test acc:66.98 | time:20.7ms
epoch:180 | loss:1.5149 | train acc:99.28 | val acc:66.35 | test_acc_best_val: 67.49  | best_test_acc: 67.49 | test acc:67.06 | time:24.8ms
Run 3/10, best test accuracy: 67.49, acc(last): 66.82, total time: 6.52s
The split is 4
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 4/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.8084 | train acc:25.36 | val acc:23.81 | test_acc_best_val: 25.55  | best_test_acc: 25.55 | test acc:25.55 | time:23.5ms
epoch:20 | loss:1.5148 | train acc:86.23 | val acc:62.38 | test_acc_best_val: 58.77  | best_test_acc: 58.77 | test acc:58.92 | time:20.7ms
epoch:40 | loss:1.5261 | train acc:97.10 | val acc:63.33 | test_acc_best_val: 60.14  | best_test_acc: 60.14 | test acc:62.97 | time:27.3ms
epoch:60 | loss:1.5128 | train acc:98.55 | val acc:63.33 | test_acc_best_val: 63.56  | best_test_acc: 63.56 | test acc:63.84 | time:20.5ms
epoch:80 | loss:1.5055 | train acc:99.28 | val acc:63.33 | test_acc_best_val: 63.56  | best_test_acc: 63.56 | test acc:64.11 | time:23.1ms
epoch:100 | loss:1.5218 | train acc:99.28 | val acc:63.17 | test_acc_best_val: 63.56  | best_test_acc: 63.56 | test acc:64.86 | time:19.9ms
epoch:120 | loss:1.4743 | train acc:99.28 | val acc:63.17 | test_acc_best_val: 63.56  | best_test_acc: 63.56 | test acc:64.82 | time:31.1ms
epoch:140 | loss:1.5186 | train acc:99.28 | val acc:63.33 | test_acc_best_val: 63.56  | best_test_acc: 63.56 | test acc:64.58 | time:25.4ms
epoch:160 | loss:1.4774 | train acc:99.28 | val acc:62.70 | test_acc_best_val: 63.56  | best_test_acc: 63.56 | test acc:64.74 | time:22.1ms
epoch:180 | loss:1.4934 | train acc:98.55 | val acc:63.49 | test_acc_best_val: 63.56  | best_test_acc: 63.56 | test acc:64.78 | time:22.5ms
Run 4/10, best test accuracy: 63.56, acc(last): 64.82, total time: 6.85s
The split is 5
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 5/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.8302 | train acc:29.71 | val acc:25.87 | test_acc_best_val: 26.26  | best_test_acc: 26.26 | test acc:26.26 | time:20.4ms
epoch:20 | loss:1.4960 | train acc:94.20 | val acc:63.49 | test_acc_best_val: 64.39  | best_test_acc: 64.39 | test acc:64.39 | time:21.8ms
epoch:40 | loss:1.4853 | train acc:99.28 | val acc:63.33 | test_acc_best_val: 64.39  | best_test_acc: 64.39 | test acc:64.58 | time:21.8ms
epoch:60 | loss:1.4957 | train acc:100.00 | val acc:64.44 | test_acc_best_val: 64.90  | best_test_acc: 64.90 | test acc:64.90 | time:29.4ms
epoch:80 | loss:1.4771 | train acc:100.00 | val acc:63.81 | test_acc_best_val: 64.90  | best_test_acc: 64.90 | test acc:64.43 | time:20.3ms
epoch:100 | loss:1.4588 | train acc:100.00 | val acc:64.92 | test_acc_best_val: 64.90  | best_test_acc: 64.90 | test acc:64.94 | time:22.8ms
epoch:120 | loss:1.4633 | train acc:100.00 | val acc:66.35 | test_acc_best_val: 65.57  | best_test_acc: 65.57 | test acc:66.04 | time:29.7ms
epoch:140 | loss:1.5183 | train acc:100.00 | val acc:66.51 | test_acc_best_val: 65.96  | best_test_acc: 65.96 | test acc:65.92 | time:20.8ms
epoch:160 | loss:1.5227 | train acc:100.00 | val acc:65.56 | test_acc_best_val: 66.51  | best_test_acc: 66.51 | test acc:66.39 | time:22.8ms
epoch:180 | loss:1.5035 | train acc:100.00 | val acc:66.98 | test_acc_best_val: 66.51  | best_test_acc: 66.51 | test acc:66.71 | time:28.5ms
Run 5/10, best test accuracy: 66.51, acc(last): 66.75, total time: 6.96s
The split is 6
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 6/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.8265 | train acc:22.46 | val acc:25.56 | test_acc_best_val: 22.96  | best_test_acc: 22.96 | test acc:22.96 | time:20.4ms
epoch:20 | loss:1.5427 | train acc:92.75 | val acc:64.60 | test_acc_best_val: 63.80  | best_test_acc: 63.80 | test acc:63.80 | time:23.5ms
epoch:40 | loss:1.4894 | train acc:98.55 | val acc:65.56 | test_acc_best_val: 64.74  | best_test_acc: 64.74 | test acc:65.02 | time:20.2ms
epoch:60 | loss:1.4833 | train acc:99.28 | val acc:65.40 | test_acc_best_val: 64.70  | best_test_acc: 64.70 | test acc:65.21 | time:21.2ms
epoch:80 | loss:1.4940 | train acc:99.28 | val acc:65.71 | test_acc_best_val: 64.70  | best_test_acc: 64.70 | test acc:65.37 | time:22.3ms
epoch:100 | loss:1.4760 | train acc:99.28 | val acc:67.14 | test_acc_best_val: 66.12  | best_test_acc: 66.12 | test acc:66.08 | time:21.6ms
epoch:120 | loss:1.5070 | train acc:99.28 | val acc:66.83 | test_acc_best_val: 66.12  | best_test_acc: 66.12 | test acc:66.23 | time:21.0ms
epoch:140 | loss:1.4910 | train acc:99.28 | val acc:66.19 | test_acc_best_val: 66.12  | best_test_acc: 66.12 | test acc:65.92 | time:21.8ms
epoch:160 | loss:1.4677 | train acc:99.28 | val acc:65.71 | test_acc_best_val: 66.12  | best_test_acc: 66.12 | test acc:65.49 | time:22.1ms
epoch:180 | loss:1.4900 | train acc:99.28 | val acc:65.71 | test_acc_best_val: 66.12  | best_test_acc: 66.12 | test acc:66.08 | time:29.2ms
Run 6/10, best test accuracy: 66.12, acc(last): 65.96, total time: 6.35s
The split is 7
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 7/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.8317 | train acc:36.23 | val acc:33.17 | test_acc_best_val: 32.55  | best_test_acc: 32.55 | test acc:32.55 | time:20.9ms
epoch:20 | loss:1.5780 | train acc:89.13 | val acc:62.22 | test_acc_best_val: 59.83  | best_test_acc: 59.83 | test acc:59.83 | time:24.0ms
epoch:40 | loss:1.5135 | train acc:97.10 | val acc:62.38 | test_acc_best_val: 59.87  | best_test_acc: 59.87 | test acc:62.26 | time:24.1ms
epoch:60 | loss:1.5119 | train acc:98.55 | val acc:63.65 | test_acc_best_val: 62.46  | best_test_acc: 62.46 | test acc:61.99 | time:21.5ms
epoch:80 | loss:1.4953 | train acc:98.55 | val acc:62.38 | test_acc_best_val: 62.46  | best_test_acc: 62.46 | test acc:62.03 | time:21.6ms
epoch:100 | loss:1.4952 | train acc:98.55 | val acc:65.40 | test_acc_best_val: 62.78  | best_test_acc: 62.78 | test acc:63.21 | time:21.7ms
epoch:120 | loss:1.4965 | train acc:98.55 | val acc:64.60 | test_acc_best_val: 62.78  | best_test_acc: 62.78 | test acc:61.67 | time:27.0ms
epoch:140 | loss:1.5117 | train acc:97.83 | val acc:61.75 | test_acc_best_val: 62.78  | best_test_acc: 62.78 | test acc:60.42 | time:21.1ms
epoch:160 | loss:1.4791 | train acc:97.83 | val acc:63.49 | test_acc_best_val: 62.78  | best_test_acc: 62.78 | test acc:61.87 | time:24.6ms
epoch:180 | loss:1.5225 | train acc:98.55 | val acc:63.65 | test_acc_best_val: 62.78  | best_test_acc: 62.78 | test acc:62.54 | time:21.6ms
Run 7/10, best test accuracy: 62.78, acc(last): 61.91, total time: 6.38s
The split is 8
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 8/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.8289 | train acc:54.35 | val acc:40.95 | test_acc_best_val: 40.61  | best_test_acc: 40.61 | test acc:40.61 | time:22.6ms
epoch:20 | loss:1.5374 | train acc:94.93 | val acc:59.21 | test_acc_best_val: 59.98  | best_test_acc: 59.98 | test acc:59.98 | time:22.6ms
epoch:40 | loss:1.5211 | train acc:99.28 | val acc:60.63 | test_acc_best_val: 61.40  | best_test_acc: 61.40 | test acc:63.13 | time:21.6ms
epoch:60 | loss:1.5108 | train acc:100.00 | val acc:60.79 | test_acc_best_val: 63.40  | best_test_acc: 63.40 | test acc:63.29 | time:21.9ms
epoch:80 | loss:1.4833 | train acc:100.00 | val acc:61.11 | test_acc_best_val: 63.40  | best_test_acc: 63.40 | test acc:63.76 | time:29.2ms
epoch:100 | loss:1.4547 | train acc:100.00 | val acc:61.75 | test_acc_best_val: 63.68  | best_test_acc: 63.68 | test acc:63.72 | time:22.5ms
epoch:120 | loss:1.4275 | train acc:100.00 | val acc:62.06 | test_acc_best_val: 64.54  | best_test_acc: 64.54 | test acc:64.47 | time:21.9ms
epoch:140 | loss:1.4954 | train acc:100.00 | val acc:60.95 | test_acc_best_val: 64.54  | best_test_acc: 64.54 | test acc:64.47 | time:29.7ms
epoch:160 | loss:1.4747 | train acc:100.00 | val acc:61.90 | test_acc_best_val: 64.54  | best_test_acc: 64.54 | test acc:65.09 | time:22.1ms
epoch:180 | loss:1.4652 | train acc:100.00 | val acc:63.02 | test_acc_best_val: 65.25  | best_test_acc: 65.25 | test acc:65.25 | time:22.0ms
Run 8/10, best test accuracy: 64.74, acc(last): 64.82, total time: 6.96s
The split is 9
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 9/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.8463 | train acc:33.33 | val acc:31.11 | test_acc_best_val: 32.94  | best_test_acc: 32.94 | test acc:32.94 | time:22.8ms
epoch:20 | loss:1.5053 | train acc:89.86 | val acc:62.06 | test_acc_best_val: 62.58  | best_test_acc: 62.58 | test acc:62.58 | time:21.4ms
epoch:40 | loss:1.4540 | train acc:94.93 | val acc:62.70 | test_acc_best_val: 63.33  | best_test_acc: 63.33 | test acc:63.36 | time:23.8ms
epoch:60 | loss:1.4984 | train acc:98.55 | val acc:62.06 | test_acc_best_val: 63.33  | best_test_acc: 63.33 | test acc:62.85 | time:21.1ms
epoch:80 | loss:1.4895 | train acc:98.55 | val acc:62.22 | test_acc_best_val: 63.33  | best_test_acc: 63.33 | test acc:62.74 | time:22.5ms
epoch:100 | loss:1.4748 | train acc:98.55 | val acc:62.38 | test_acc_best_val: 63.33  | best_test_acc: 63.33 | test acc:63.09 | time:28.2ms
epoch:120 | loss:1.4929 | train acc:97.83 | val acc:63.17 | test_acc_best_val: 63.33  | best_test_acc: 63.33 | test acc:63.72 | time:21.4ms
epoch:140 | loss:1.4846 | train acc:97.83 | val acc:62.70 | test_acc_best_val: 63.33  | best_test_acc: 63.33 | test acc:63.80 | time:22.0ms
epoch:160 | loss:1.4793 | train acc:97.83 | val acc:62.38 | test_acc_best_val: 63.33  | best_test_acc: 63.33 | test acc:63.88 | time:26.8ms
epoch:180 | loss:1.4712 | train acc:97.83 | val acc:62.54 | test_acc_best_val: 63.33  | best_test_acc: 63.33 | test acc:63.88 | time:28.5ms
Run 9/10, best test accuracy: 63.33, acc(last): 64.27, total time: 6.84s
The split is 10
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 10/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.8082 | train acc:34.06 | val acc:25.40 | test_acc_best_val: 25.51  | best_test_acc: 25.51 | test acc:25.51 | time:24.8ms
epoch:20 | loss:1.5265 | train acc:92.03 | val acc:57.46 | test_acc_best_val: 60.89  | best_test_acc: 60.89 | test acc:60.81 | time:25.3ms
epoch:40 | loss:1.4701 | train acc:97.83 | val acc:60.48 | test_acc_best_val: 62.97  | best_test_acc: 62.97 | test acc:62.89 | time:20.1ms
epoch:60 | loss:1.4386 | train acc:98.55 | val acc:61.90 | test_acc_best_val: 62.54  | best_test_acc: 62.54 | test acc:62.62 | time:22.8ms
epoch:80 | loss:1.4909 | train acc:99.28 | val acc:61.75 | test_acc_best_val: 62.42  | best_test_acc: 62.42 | test acc:63.33 | time:26.5ms
epoch:100 | loss:1.5386 | train acc:99.28 | val acc:60.79 | test_acc_best_val: 62.42  | best_test_acc: 62.42 | test acc:63.29 | time:20.0ms
epoch:120 | loss:1.5032 | train acc:99.28 | val acc:62.22 | test_acc_best_val: 62.42  | best_test_acc: 62.42 | test acc:63.84 | time:37.1ms
epoch:140 | loss:1.4434 | train acc:99.28 | val acc:62.38 | test_acc_best_val: 64.03  | best_test_acc: 64.03 | test acc:63.92 | time:20.6ms
epoch:160 | loss:1.4817 | train acc:99.28 | val acc:63.17 | test_acc_best_val: 64.27  | best_test_acc: 64.27 | test acc:63.99 | time:20.5ms
epoch:180 | loss:1.4681 | train acc:99.28 | val acc:62.70 | test_acc_best_val: 64.27  | best_test_acc: 64.27 | test acc:64.11 | time:28.2ms
Run 10/10, best test accuracy: 64.15, acc(last): 64.31, total time: 6.82s
The split is 1
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 1/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.8357 | train acc:18.84 | val acc:22.54 | test_acc_best_val: 20.36  | best_test_acc: 20.36 | test acc:20.36 | time:28.2ms
epoch:20 | loss:1.5776 | train acc:89.86 | val acc:65.87 | test_acc_best_val: 63.80  | best_test_acc: 63.80 | test acc:63.80 | time:22.1ms
epoch:40 | loss:1.5134 | train acc:97.10 | val acc:67.94 | test_acc_best_val: 66.31  | best_test_acc: 66.31 | test acc:66.31 | time:30.5ms
epoch:60 | loss:1.5523 | train acc:99.28 | val acc:67.94 | test_acc_best_val: 66.12  | best_test_acc: 66.12 | test acc:66.59 | time:21.5ms
epoch:80 | loss:1.5177 | train acc:99.28 | val acc:68.41 | test_acc_best_val: 66.12  | best_test_acc: 66.12 | test acc:66.78 | time:24.1ms
epoch:100 | loss:1.4812 | train acc:99.28 | val acc:67.30 | test_acc_best_val: 66.12  | best_test_acc: 66.12 | test acc:66.67 | time:26.7ms
epoch:120 | loss:1.4986 | train acc:99.28 | val acc:67.94 | test_acc_best_val: 66.12  | best_test_acc: 66.12 | test acc:66.75 | time:20.5ms
epoch:140 | loss:1.5334 | train acc:99.28 | val acc:67.94 | test_acc_best_val: 66.12  | best_test_acc: 66.12 | test acc:67.22 | time:20.9ms
epoch:160 | loss:1.5169 | train acc:98.55 | val acc:68.57 | test_acc_best_val: 66.12  | best_test_acc: 66.12 | test acc:67.26 | time:29.8ms
epoch:180 | loss:1.4761 | train acc:98.55 | val acc:67.30 | test_acc_best_val: 66.12  | best_test_acc: 66.12 | test acc:67.57 | time:21.8ms
Run 1/10, best test accuracy: 66.12, acc(last): 67.61, total time: 6.87s
The split is 2
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 2/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.8624 | train acc:42.75 | val acc:34.76 | test_acc_best_val: 34.04  | best_test_acc: 34.04 | test acc:34.04 | time:28.7ms
epoch:20 | loss:1.5517 | train acc:96.38 | val acc:66.98 | test_acc_best_val: 63.25  | best_test_acc: 63.25 | test acc:63.25 | time:27.0ms
epoch:40 | loss:1.4992 | train acc:99.28 | val acc:66.51 | test_acc_best_val: 64.62  | best_test_acc: 64.62 | test acc:64.82 | time:22.1ms
epoch:60 | loss:1.4977 | train acc:100.00 | val acc:65.56 | test_acc_best_val: 64.62  | best_test_acc: 64.62 | test acc:64.70 | time:28.6ms
epoch:80 | loss:1.4473 | train acc:100.00 | val acc:65.87 | test_acc_best_val: 64.62  | best_test_acc: 64.62 | test acc:64.98 | time:22.2ms
epoch:100 | loss:1.4779 | train acc:100.00 | val acc:65.87 | test_acc_best_val: 64.62  | best_test_acc: 64.62 | test acc:65.13 | time:21.0ms
epoch:120 | loss:1.4453 | train acc:100.00 | val acc:67.62 | test_acc_best_val: 65.76  | best_test_acc: 65.76 | test acc:65.76 | time:28.9ms
epoch:140 | loss:1.5042 | train acc:100.00 | val acc:67.30 | test_acc_best_val: 65.84  | best_test_acc: 65.84 | test acc:65.17 | time:22.0ms
epoch:160 | loss:1.5224 | train acc:100.00 | val acc:67.30 | test_acc_best_val: 65.84  | best_test_acc: 65.84 | test acc:65.64 | time:21.5ms
epoch:180 | loss:1.4757 | train acc:100.00 | val acc:67.94 | test_acc_best_val: 65.96  | best_test_acc: 65.96 | test acc:65.88 | time:19.8ms
Run 2/10, best test accuracy: 66.08, acc(last): 66.39, total time: 6.68s
The split is 3
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 3/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.8400 | train acc:44.93 | val acc:41.11 | test_acc_best_val: 38.84  | best_test_acc: 38.84 | test acc:38.84 | time:22.0ms
epoch:20 | loss:1.5495 | train acc:91.30 | val acc:63.97 | test_acc_best_val: 65.06  | best_test_acc: 65.06 | test acc:65.06 | time:20.1ms
epoch:40 | loss:1.5644 | train acc:97.83 | val acc:63.49 | test_acc_best_val: 65.06  | best_test_acc: 65.06 | test acc:66.51 | time:20.6ms
epoch:60 | loss:1.5034 | train acc:99.28 | val acc:63.33 | test_acc_best_val: 65.06  | best_test_acc: 65.06 | test acc:67.65 | time:22.8ms
epoch:80 | loss:1.5490 | train acc:99.28 | val acc:66.19 | test_acc_best_val: 67.61  | best_test_acc: 67.61 | test acc:67.81 | time:22.2ms
epoch:100 | loss:1.5066 | train acc:100.00 | val acc:66.67 | test_acc_best_val: 67.73  | best_test_acc: 67.73 | test acc:67.73 | time:22.0ms
epoch:120 | loss:1.4811 | train acc:100.00 | val acc:67.30 | test_acc_best_val: 67.14  | best_test_acc: 67.14 | test acc:67.10 | time:21.9ms
epoch:140 | loss:1.5072 | train acc:100.00 | val acc:67.46 | test_acc_best_val: 66.98  | best_test_acc: 66.98 | test acc:66.82 | time:21.0ms
epoch:160 | loss:1.5038 | train acc:100.00 | val acc:67.62 | test_acc_best_val: 66.98  | best_test_acc: 66.98 | test acc:67.41 | time:23.1ms
epoch:180 | loss:1.5130 | train acc:99.28 | val acc:66.98 | test_acc_best_val: 67.45  | best_test_acc: 67.45 | test acc:67.77 | time:22.3ms
Run 3/10, best test accuracy: 67.45, acc(last): 68.16, total time: 6.20s
The split is 4
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 4/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.8594 | train acc:41.30 | val acc:30.32 | test_acc_best_val: 30.58  | best_test_acc: 30.58 | test acc:30.58 | time:20.9ms
epoch:20 | loss:1.5561 | train acc:88.41 | val acc:58.89 | test_acc_best_val: 61.48  | best_test_acc: 61.48 | test acc:61.79 | time:20.7ms
epoch:40 | loss:1.4799 | train acc:98.55 | val acc:61.11 | test_acc_best_val: 63.36  | best_test_acc: 63.36 | test acc:63.29 | time:27.3ms
epoch:60 | loss:1.4836 | train acc:98.55 | val acc:61.11 | test_acc_best_val: 63.33  | best_test_acc: 63.33 | test acc:63.72 | time:23.1ms
epoch:80 | loss:1.5150 | train acc:99.28 | val acc:61.75 | test_acc_best_val: 64.27  | best_test_acc: 64.27 | test acc:64.35 | time:19.2ms
epoch:100 | loss:1.4766 | train acc:99.28 | val acc:61.27 | test_acc_best_val: 64.27  | best_test_acc: 64.27 | test acc:64.03 | time:20.5ms
epoch:120 | loss:1.4999 | train acc:99.28 | val acc:60.48 | test_acc_best_val: 64.27  | best_test_acc: 64.27 | test acc:64.07 | time:26.7ms
epoch:140 | loss:1.5108 | train acc:99.28 | val acc:61.27 | test_acc_best_val: 64.27  | best_test_acc: 64.27 | test acc:64.43 | time:27.2ms
epoch:160 | loss:1.4959 | train acc:98.55 | val acc:61.75 | test_acc_best_val: 64.27  | best_test_acc: 64.27 | test acc:64.19 | time:25.0ms
epoch:180 | loss:1.4846 | train acc:99.28 | val acc:62.06 | test_acc_best_val: 64.27  | best_test_acc: 64.27 | test acc:64.74 | time:21.7ms
Run 4/10, best test accuracy: 65.57, acc(last): 65.45, total time: 6.40s
The split is 5
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 5/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.8007 | train acc:53.62 | val acc:36.19 | test_acc_best_val: 35.18  | best_test_acc: 35.18 | test acc:35.18 | time:26.8ms
epoch:20 | loss:1.5139 | train acc:94.20 | val acc:63.81 | test_acc_best_val: 63.68  | best_test_acc: 63.68 | test acc:63.84 | time:23.7ms
epoch:40 | loss:1.5264 | train acc:99.28 | val acc:64.13 | test_acc_best_val: 63.68  | best_test_acc: 63.68 | test acc:65.92 | time:21.8ms
epoch:60 | loss:1.5077 | train acc:100.00 | val acc:64.92 | test_acc_best_val: 63.68  | best_test_acc: 63.68 | test acc:64.90 | time:22.3ms
epoch:80 | loss:1.4871 | train acc:100.00 | val acc:65.56 | test_acc_best_val: 65.13  | best_test_acc: 65.13 | test acc:65.25 | time:20.4ms
epoch:100 | loss:1.4860 | train acc:100.00 | val acc:65.56 | test_acc_best_val: 65.68  | best_test_acc: 65.68 | test acc:65.96 | time:21.3ms
epoch:120 | loss:1.5091 | train acc:100.00 | val acc:65.71 | test_acc_best_val: 65.68  | best_test_acc: 65.68 | test acc:66.23 | time:23.2ms
epoch:140 | loss:1.4588 | train acc:100.00 | val acc:66.67 | test_acc_best_val: 66.39  | best_test_acc: 66.39 | test acc:66.31 | time:27.7ms
epoch:160 | loss:1.4787 | train acc:99.28 | val acc:65.71 | test_acc_best_val: 66.27  | best_test_acc: 66.27 | test acc:66.00 | time:20.8ms
epoch:180 | loss:1.5120 | train acc:99.28 | val acc:65.40 | test_acc_best_val: 66.27  | best_test_acc: 66.27 | test acc:66.47 | time:22.7ms
Run 5/10, best test accuracy: 66.27, acc(last): 66.55, total time: 6.52s
The split is 6
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 6/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.8139 | train acc:18.84 | val acc:21.27 | test_acc_best_val: 21.89  | best_test_acc: 21.89 | test acc:21.89 | time:22.3ms
epoch:20 | loss:1.5330 | train acc:93.48 | val acc:62.70 | test_acc_best_val: 63.21  | best_test_acc: 63.21 | test acc:63.48 | time:27.9ms
epoch:40 | loss:1.4553 | train acc:97.83 | val acc:64.29 | test_acc_best_val: 65.41  | best_test_acc: 65.41 | test acc:65.41 | time:23.7ms
epoch:60 | loss:1.4561 | train acc:98.55 | val acc:63.97 | test_acc_best_val: 65.45  | best_test_acc: 65.45 | test acc:65.84 | time:25.5ms
epoch:80 | loss:1.4836 | train acc:99.28 | val acc:62.38 | test_acc_best_val: 65.45  | best_test_acc: 65.45 | test acc:65.41 | time:21.4ms
epoch:100 | loss:1.4617 | train acc:99.28 | val acc:62.86 | test_acc_best_val: 65.45  | best_test_acc: 65.45 | test acc:64.82 | time:25.4ms
epoch:120 | loss:1.5092 | train acc:99.28 | val acc:62.38 | test_acc_best_val: 65.45  | best_test_acc: 65.45 | test acc:64.90 | time:26.5ms
epoch:140 | loss:1.5034 | train acc:99.28 | val acc:63.02 | test_acc_best_val: 65.45  | best_test_acc: 65.45 | test acc:65.64 | time:22.9ms
epoch:160 | loss:1.5022 | train acc:99.28 | val acc:63.81 | test_acc_best_val: 65.45  | best_test_acc: 65.45 | test acc:66.12 | time:27.8ms
epoch:180 | loss:1.5145 | train acc:99.28 | val acc:64.60 | test_acc_best_val: 66.43  | best_test_acc: 66.43 | test acc:66.82 | time:27.6ms
Run 6/10, best test accuracy: 66.43, acc(last): 66.31, total time: 7.16s
The split is 7
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 7/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.8008 | train acc:38.41 | val acc:25.56 | test_acc_best_val: 25.55  | best_test_acc: 25.55 | test acc:25.55 | time:25.1ms
epoch:20 | loss:1.5703 | train acc:89.13 | val acc:56.83 | test_acc_best_val: 56.68  | best_test_acc: 56.68 | test acc:57.08 | time:23.3ms
epoch:40 | loss:1.5181 | train acc:96.38 | val acc:60.00 | test_acc_best_val: 62.03  | best_test_acc: 62.03 | test acc:63.13 | time:21.4ms
epoch:60 | loss:1.4882 | train acc:97.10 | val acc:62.54 | test_acc_best_val: 63.80  | best_test_acc: 63.80 | test acc:64.03 | time:22.8ms
epoch:80 | loss:1.4782 | train acc:97.83 | val acc:60.95 | test_acc_best_val: 63.80  | best_test_acc: 63.80 | test acc:62.97 | time:28.9ms
epoch:100 | loss:1.4951 | train acc:97.83 | val acc:60.48 | test_acc_best_val: 63.80  | best_test_acc: 63.80 | test acc:63.01 | time:28.0ms
epoch:120 | loss:1.5073 | train acc:99.28 | val acc:61.43 | test_acc_best_val: 63.80  | best_test_acc: 63.80 | test acc:64.11 | time:21.1ms
epoch:140 | loss:1.4982 | train acc:98.55 | val acc:59.84 | test_acc_best_val: 63.80  | best_test_acc: 63.80 | test acc:62.89 | time:22.1ms
epoch:160 | loss:1.4946 | train acc:98.55 | val acc:60.48 | test_acc_best_val: 63.80  | best_test_acc: 63.80 | test acc:63.64 | time:26.9ms
epoch:180 | loss:1.4869 | train acc:99.28 | val acc:60.16 | test_acc_best_val: 63.80  | best_test_acc: 63.80 | test acc:62.50 | time:25.3ms
Run 7/10, best test accuracy: 63.80, acc(last): 63.29, total time: 7.31s
The split is 8
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 8/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.7977 | train acc:42.75 | val acc:35.24 | test_acc_best_val: 38.05  | best_test_acc: 38.05 | test acc:38.05 | time:29.2ms
epoch:20 | loss:1.4878 | train acc:96.38 | val acc:61.11 | test_acc_best_val: 61.75  | best_test_acc: 61.75 | test acc:61.75 | time:29.0ms
epoch:40 | loss:1.5338 | train acc:100.00 | val acc:62.70 | test_acc_best_val: 63.40  | best_test_acc: 63.40 | test acc:64.19 | time:22.9ms
epoch:60 | loss:1.4630 | train acc:100.00 | val acc:63.17 | test_acc_best_val: 64.62  | best_test_acc: 64.62 | test acc:64.94 | time:25.9ms
epoch:80 | loss:1.4709 | train acc:100.00 | val acc:64.44 | test_acc_best_val: 65.45  | best_test_acc: 65.45 | test acc:65.61 | time:27.6ms
epoch:100 | loss:1.4781 | train acc:100.00 | val acc:64.29 | test_acc_best_val: 65.45  | best_test_acc: 65.45 | test acc:64.47 | time:24.5ms
epoch:120 | loss:1.5091 | train acc:100.00 | val acc:63.65 | test_acc_best_val: 65.45  | best_test_acc: 65.45 | test acc:65.06 | time:20.9ms
epoch:140 | loss:1.4540 | train acc:100.00 | val acc:64.92 | test_acc_best_val: 65.45  | best_test_acc: 65.45 | test acc:65.76 | time:22.0ms
epoch:160 | loss:1.4648 | train acc:100.00 | val acc:64.76 | test_acc_best_val: 65.45  | best_test_acc: 65.45 | test acc:65.64 | time:23.0ms
epoch:180 | loss:1.5058 | train acc:100.00 | val acc:63.65 | test_acc_best_val: 65.45  | best_test_acc: 65.45 | test acc:65.80 | time:27.6ms
Run 8/10, best test accuracy: 65.45, acc(last): 65.45, total time: 7.02s
The split is 9
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 9/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.8339 | train acc:29.71 | val acc:31.11 | test_acc_best_val: 32.43  | best_test_acc: 32.43 | test acc:32.43 | time:30.0ms
epoch:20 | loss:1.5116 | train acc:92.75 | val acc:57.62 | test_acc_best_val: 61.16  | best_test_acc: 61.16 | test acc:61.16 | time:28.6ms
epoch:40 | loss:1.5016 | train acc:97.83 | val acc:60.16 | test_acc_best_val: 63.33  | best_test_acc: 63.33 | test acc:63.88 | time:29.2ms
epoch:60 | loss:1.4833 | train acc:98.55 | val acc:59.52 | test_acc_best_val: 63.84  | best_test_acc: 63.84 | test acc:63.56 | time:21.7ms
epoch:80 | loss:1.5236 | train acc:98.55 | val acc:59.37 | test_acc_best_val: 63.84  | best_test_acc: 63.84 | test acc:63.36 | time:26.5ms
epoch:100 | loss:1.5195 | train acc:98.55 | val acc:60.95 | test_acc_best_val: 63.84  | best_test_acc: 63.84 | test acc:63.92 | time:24.5ms
epoch:120 | loss:1.4476 | train acc:98.55 | val acc:61.27 | test_acc_best_val: 63.99  | best_test_acc: 63.99 | test acc:64.03 | time:29.8ms
epoch:140 | loss:1.4659 | train acc:98.55 | val acc:61.90 | test_acc_best_val: 63.84  | best_test_acc: 63.84 | test acc:64.39 | time:21.7ms
epoch:160 | loss:1.4599 | train acc:98.55 | val acc:61.43 | test_acc_best_val: 64.43  | best_test_acc: 64.43 | test acc:64.50 | time:21.7ms
epoch:180 | loss:1.5014 | train acc:98.55 | val acc:61.75 | test_acc_best_val: 64.43  | best_test_acc: 64.43 | test acc:65.53 | time:22.9ms
Run 9/10, best test accuracy: 65.88, acc(last): 66.00, total time: 6.99s
The split is 10
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 10/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.8126 | train acc:46.38 | val acc:34.44 | test_acc_best_val: 35.34  | best_test_acc: 35.34 | test acc:35.34 | time:22.4ms
epoch:20 | loss:1.5174 | train acc:89.86 | val acc:57.46 | test_acc_best_val: 60.06  | best_test_acc: 60.06 | test acc:60.53 | time:29.0ms
epoch:40 | loss:1.4870 | train acc:97.83 | val acc:58.10 | test_acc_best_val: 61.71  | best_test_acc: 61.71 | test acc:61.87 | time:22.0ms
epoch:60 | loss:1.4605 | train acc:99.28 | val acc:58.25 | test_acc_best_val: 62.38  | best_test_acc: 62.38 | test acc:62.38 | time:22.9ms
epoch:80 | loss:1.4663 | train acc:99.28 | val acc:59.37 | test_acc_best_val: 63.48  | best_test_acc: 63.48 | test acc:63.48 | time:25.9ms
epoch:100 | loss:1.4773 | train acc:99.28 | val acc:59.84 | test_acc_best_val: 63.80  | best_test_acc: 63.80 | test acc:63.44 | time:25.7ms
epoch:120 | loss:1.4811 | train acc:99.28 | val acc:60.32 | test_acc_best_val: 63.21  | best_test_acc: 63.21 | test acc:63.76 | time:21.3ms
epoch:140 | loss:1.5048 | train acc:99.28 | val acc:61.27 | test_acc_best_val: 64.39  | best_test_acc: 64.39 | test acc:64.82 | time:25.1ms
epoch:160 | loss:1.4540 | train acc:99.28 | val acc:62.54 | test_acc_best_val: 64.54  | best_test_acc: 64.54 | test acc:64.50 | time:27.4ms
epoch:180 | loss:1.4954 | train acc:99.28 | val acc:62.54 | test_acc_best_val: 64.70  | best_test_acc: 64.70 | test acc:64.43 | time:28.4ms
Run 10/10, best test accuracy: 64.62, acc(last): 64.82, total time: 7.12s
The split is 1
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 1/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.8513 | train acc:21.01 | val acc:20.63 | test_acc_best_val: 22.01  | best_test_acc: 22.01 | test acc:22.01 | time:47.6ms
epoch:20 | loss:1.5376 | train acc:87.68 | val acc:64.13 | test_acc_best_val: 63.29  | best_test_acc: 63.29 | test acc:63.29 | time:21.3ms
epoch:40 | loss:1.5135 | train acc:97.83 | val acc:67.14 | test_acc_best_val: 65.53  | best_test_acc: 65.53 | test acc:65.53 | time:21.1ms
epoch:60 | loss:1.4899 | train acc:98.55 | val acc:67.62 | test_acc_best_val: 66.00  | best_test_acc: 66.00 | test acc:65.84 | time:23.4ms
epoch:80 | loss:1.4997 | train acc:99.28 | val acc:67.94 | test_acc_best_val: 66.12  | best_test_acc: 66.12 | test acc:66.47 | time:28.1ms
epoch:100 | loss:1.4963 | train acc:99.28 | val acc:69.37 | test_acc_best_val: 65.80  | best_test_acc: 65.80 | test acc:65.64 | time:22.4ms
epoch:120 | loss:1.4834 | train acc:99.28 | val acc:69.52 | test_acc_best_val: 65.68  | best_test_acc: 65.68 | test acc:66.23 | time:21.8ms
epoch:140 | loss:1.4606 | train acc:99.28 | val acc:70.48 | test_acc_best_val: 66.19  | best_test_acc: 66.19 | test acc:66.27 | time:23.4ms
epoch:160 | loss:1.5001 | train acc:99.28 | val acc:69.84 | test_acc_best_val: 66.23  | best_test_acc: 66.23 | test acc:66.04 | time:23.3ms
epoch:180 | loss:1.4938 | train acc:99.28 | val acc:70.32 | test_acc_best_val: 66.23  | best_test_acc: 66.23 | test acc:66.43 | time:22.9ms
Run 1/10, best test accuracy: 66.23, acc(last): 66.12, total time: 7.14s
The split is 2
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 2/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.8438 | train acc:44.20 | val acc:40.00 | test_acc_best_val: 39.90  | best_test_acc: 39.90 | test acc:39.90 | time:21.0ms
epoch:20 | loss:1.5470 | train acc:92.75 | val acc:65.08 | test_acc_best_val: 63.48  | best_test_acc: 63.48 | test acc:63.76 | time:26.9ms
epoch:40 | loss:1.5151 | train acc:98.55 | val acc:65.40 | test_acc_best_val: 64.03  | best_test_acc: 64.03 | test acc:64.15 | time:21.4ms
epoch:60 | loss:1.5000 | train acc:100.00 | val acc:65.40 | test_acc_best_val: 63.99  | best_test_acc: 63.99 | test acc:64.98 | time:23.0ms
epoch:80 | loss:1.5010 | train acc:100.00 | val acc:65.56 | test_acc_best_val: 63.99  | best_test_acc: 63.99 | test acc:64.70 | time:28.5ms
epoch:100 | loss:1.5109 | train acc:100.00 | val acc:66.51 | test_acc_best_val: 65.68  | best_test_acc: 65.68 | test acc:65.68 | time:23.8ms
epoch:120 | loss:1.4487 | train acc:100.00 | val acc:67.78 | test_acc_best_val: 65.72  | best_test_acc: 65.72 | test acc:65.72 | time:23.0ms
epoch:140 | loss:1.4365 | train acc:100.00 | val acc:67.78 | test_acc_best_val: 65.92  | best_test_acc: 65.92 | test acc:65.64 | time:27.7ms
epoch:160 | loss:1.5006 | train acc:100.00 | val acc:67.46 | test_acc_best_val: 65.92  | best_test_acc: 65.92 | test acc:66.04 | time:21.6ms
epoch:180 | loss:1.4727 | train acc:100.00 | val acc:66.51 | test_acc_best_val: 65.92  | best_test_acc: 65.92 | test acc:66.98 | time:21.7ms
Run 2/10, best test accuracy: 65.92, acc(last): 66.90, total time: 7.00s
The split is 3
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 3/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.8179 | train acc:39.86 | val acc:28.57 | test_acc_best_val: 32.04  | best_test_acc: 32.04 | test acc:32.04 | time:21.2ms
epoch:20 | loss:1.5986 | train acc:96.38 | val acc:61.90 | test_acc_best_val: 63.95  | best_test_acc: 63.95 | test acc:63.95 | time:21.4ms
epoch:40 | loss:1.5744 | train acc:100.00 | val acc:62.06 | test_acc_best_val: 64.78  | best_test_acc: 64.78 | test acc:64.78 | time:26.8ms
epoch:60 | loss:1.5085 | train acc:99.28 | val acc:63.97 | test_acc_best_val: 65.76  | best_test_acc: 65.76 | test acc:65.84 | time:19.7ms
epoch:80 | loss:1.5208 | train acc:100.00 | val acc:66.51 | test_acc_best_val: 66.27  | best_test_acc: 66.27 | test acc:66.35 | time:20.3ms
epoch:100 | loss:1.5392 | train acc:100.00 | val acc:66.35 | test_acc_best_val: 66.27  | best_test_acc: 66.27 | test acc:66.75 | time:22.6ms
epoch:120 | loss:1.5328 | train acc:100.00 | val acc:66.51 | test_acc_best_val: 66.90  | best_test_acc: 66.90 | test acc:67.18 | time:21.7ms
epoch:140 | loss:1.5225 | train acc:100.00 | val acc:66.83 | test_acc_best_val: 66.90  | best_test_acc: 66.90 | test acc:66.86 | time:24.4ms
epoch:160 | loss:1.4998 | train acc:99.28 | val acc:67.14 | test_acc_best_val: 67.73  | best_test_acc: 67.73 | test acc:67.73 | time:21.2ms
epoch:180 | loss:1.5066 | train acc:99.28 | val acc:67.78 | test_acc_best_val: 67.65  | best_test_acc: 67.65 | test acc:67.65 | time:20.9ms
Run 3/10, best test accuracy: 67.61, acc(last): 67.26, total time: 6.30s
The split is 4
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 4/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.8021 | train acc:33.33 | val acc:28.89 | test_acc_best_val: 27.79  | best_test_acc: 27.79 | test acc:27.79 | time:20.3ms
epoch:20 | loss:1.5502 | train acc:89.86 | val acc:63.65 | test_acc_best_val: 60.22  | best_test_acc: 60.22 | test acc:60.22 | time:21.8ms
epoch:40 | loss:1.5276 | train acc:97.10 | val acc:65.71 | test_acc_best_val: 62.89  | best_test_acc: 62.89 | test acc:62.89 | time:23.0ms
epoch:60 | loss:1.4669 | train acc:98.55 | val acc:67.30 | test_acc_best_val: 63.64  | best_test_acc: 63.64 | test acc:63.56 | time:22.3ms
epoch:80 | loss:1.4924 | train acc:99.28 | val acc:66.35 | test_acc_best_val: 63.64  | best_test_acc: 63.64 | test acc:63.92 | time:27.2ms
epoch:100 | loss:1.4640 | train acc:99.28 | val acc:65.87 | test_acc_best_val: 63.64  | best_test_acc: 63.64 | test acc:63.64 | time:21.1ms
epoch:120 | loss:1.4823 | train acc:98.55 | val acc:66.03 | test_acc_best_val: 63.64  | best_test_acc: 63.64 | test acc:63.99 | time:20.5ms
epoch:140 | loss:1.4802 | train acc:99.28 | val acc:66.19 | test_acc_best_val: 63.64  | best_test_acc: 63.64 | test acc:64.66 | time:21.4ms
epoch:160 | loss:1.4925 | train acc:97.83 | val acc:66.03 | test_acc_best_val: 63.64  | best_test_acc: 63.64 | test acc:64.31 | time:24.6ms
epoch:180 | loss:1.5500 | train acc:98.55 | val acc:66.35 | test_acc_best_val: 63.64  | best_test_acc: 63.64 | test acc:64.47 | time:25.5ms
Run 4/10, best test accuracy: 63.64, acc(last): 63.99, total time: 6.33s
The split is 5
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 5/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.8790 | train acc:36.96 | val acc:26.98 | test_acc_best_val: 29.52  | best_test_acc: 29.52 | test acc:29.52 | time:22.1ms
epoch:20 | loss:1.5238 | train acc:94.20 | val acc:61.11 | test_acc_best_val: 65.53  | best_test_acc: 65.53 | test acc:65.61 | time:21.5ms
epoch:40 | loss:1.5298 | train acc:100.00 | val acc:60.16 | test_acc_best_val: 65.53  | best_test_acc: 65.53 | test acc:64.90 | time:22.2ms
epoch:60 | loss:1.4885 | train acc:100.00 | val acc:61.59 | test_acc_best_val: 65.53  | best_test_acc: 65.53 | test acc:66.75 | time:21.8ms
epoch:80 | loss:1.4617 | train acc:100.00 | val acc:61.75 | test_acc_best_val: 66.67  | best_test_acc: 66.67 | test acc:65.88 | time:24.4ms
epoch:100 | loss:1.4412 | train acc:100.00 | val acc:62.06 | test_acc_best_val: 66.67  | best_test_acc: 66.67 | test acc:66.71 | time:22.6ms
epoch:120 | loss:1.4842 | train acc:100.00 | val acc:61.75 | test_acc_best_val: 66.67  | best_test_acc: 66.67 | test acc:66.43 | time:22.4ms
epoch:140 | loss:1.4644 | train acc:100.00 | val acc:61.90 | test_acc_best_val: 66.55  | best_test_acc: 66.55 | test acc:66.43 | time:24.5ms
epoch:160 | loss:1.4659 | train acc:100.00 | val acc:62.38 | test_acc_best_val: 66.55  | best_test_acc: 66.55 | test acc:66.71 | time:20.5ms
epoch:180 | loss:1.4660 | train acc:100.00 | val acc:61.43 | test_acc_best_val: 66.55  | best_test_acc: 66.55 | test acc:66.71 | time:21.8ms
Run 5/10, best test accuracy: 66.55, acc(last): 66.39, total time: 6.42s
The split is 6
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 6/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.8127 | train acc:42.03 | val acc:33.97 | test_acc_best_val: 37.74  | best_test_acc: 37.74 | test acc:37.74 | time:21.0ms
epoch:20 | loss:1.5314 | train acc:88.41 | val acc:63.17 | test_acc_best_val: 63.99  | best_test_acc: 63.99 | test acc:64.31 | time:28.0ms
epoch:40 | loss:1.5388 | train acc:98.55 | val acc:66.67 | test_acc_best_val: 66.75  | best_test_acc: 66.75 | test acc:66.82 | time:23.6ms
epoch:60 | loss:1.4987 | train acc:99.28 | val acc:67.14 | test_acc_best_val: 66.12  | best_test_acc: 66.12 | test acc:66.23 | time:21.7ms
epoch:80 | loss:1.4640 | train acc:99.28 | val acc:66.83 | test_acc_best_val: 66.67  | best_test_acc: 66.67 | test acc:66.59 | time:24.6ms
epoch:100 | loss:1.4958 | train acc:99.28 | val acc:66.35 | test_acc_best_val: 66.67  | best_test_acc: 66.67 | test acc:66.67 | time:22.8ms
epoch:120 | loss:1.4676 | train acc:99.28 | val acc:66.19 | test_acc_best_val: 66.67  | best_test_acc: 66.67 | test acc:67.26 | time:23.2ms
epoch:140 | loss:1.4944 | train acc:99.28 | val acc:66.03 | test_acc_best_val: 66.67  | best_test_acc: 66.67 | test acc:67.06 | time:21.3ms
epoch:160 | loss:1.4515 | train acc:99.28 | val acc:65.08 | test_acc_best_val: 66.67  | best_test_acc: 66.67 | test acc:67.30 | time:24.5ms
epoch:180 | loss:1.4900 | train acc:99.28 | val acc:66.35 | test_acc_best_val: 66.67  | best_test_acc: 66.67 | test acc:67.49 | time:19.9ms
Run 6/10, best test accuracy: 66.67, acc(last): 67.18, total time: 6.64s
The split is 7
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 7/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.8051 | train acc:39.13 | val acc:27.14 | test_acc_best_val: 32.82  | best_test_acc: 32.82 | test acc:32.82 | time:20.7ms
epoch:20 | loss:1.5759 | train acc:85.51 | val acc:55.87 | test_acc_best_val: 57.55  | best_test_acc: 57.55 | test acc:57.55 | time:19.8ms
epoch:40 | loss:1.5495 | train acc:96.38 | val acc:59.52 | test_acc_best_val: 61.24  | best_test_acc: 61.24 | test acc:61.48 | time:23.6ms
epoch:60 | loss:1.5389 | train acc:97.83 | val acc:60.95 | test_acc_best_val: 61.32  | best_test_acc: 61.32 | test acc:61.60 | time:23.9ms
epoch:80 | loss:1.4991 | train acc:97.83 | val acc:63.17 | test_acc_best_val: 62.03  | best_test_acc: 62.03 | test acc:62.03 | time:26.6ms
epoch:100 | loss:1.5455 | train acc:97.83 | val acc:61.75 | test_acc_best_val: 62.03  | best_test_acc: 62.03 | test acc:62.34 | time:22.0ms
epoch:120 | loss:1.5027 | train acc:97.83 | val acc:61.75 | test_acc_best_val: 62.03  | best_test_acc: 62.03 | test acc:62.46 | time:26.3ms
epoch:140 | loss:1.4810 | train acc:98.55 | val acc:60.95 | test_acc_best_val: 62.03  | best_test_acc: 62.03 | test acc:62.78 | time:22.2ms
epoch:160 | loss:1.4698 | train acc:98.55 | val acc:61.11 | test_acc_best_val: 62.03  | best_test_acc: 62.03 | test acc:62.58 | time:21.7ms
epoch:180 | loss:1.5249 | train acc:97.83 | val acc:60.63 | test_acc_best_val: 62.03  | best_test_acc: 62.03 | test acc:62.70 | time:23.1ms
Run 7/10, best test accuracy: 62.03, acc(last): 61.64, total time: 6.59s
The split is 8
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 8/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.8749 | train acc:60.14 | val acc:44.92 | test_acc_best_val: 46.46  | best_test_acc: 46.46 | test acc:46.46 | time:24.5ms
epoch:20 | loss:1.5641 | train acc:91.30 | val acc:58.89 | test_acc_best_val: 59.36  | best_test_acc: 59.36 | test acc:59.36 | time:21.9ms
epoch:40 | loss:1.5320 | train acc:97.83 | val acc:63.33 | test_acc_best_val: 62.89  | best_test_acc: 62.89 | test acc:63.33 | time:20.6ms
epoch:60 | loss:1.4658 | train acc:100.00 | val acc:63.49 | test_acc_best_val: 64.27  | best_test_acc: 64.27 | test acc:64.39 | time:23.3ms
epoch:80 | loss:1.4392 | train acc:100.00 | val acc:63.49 | test_acc_best_val: 64.27  | best_test_acc: 64.27 | test acc:64.07 | time:26.1ms
epoch:100 | loss:1.4892 | train acc:100.00 | val acc:63.02 | test_acc_best_val: 64.27  | best_test_acc: 64.27 | test acc:64.54 | time:22.0ms
epoch:120 | loss:1.4829 | train acc:100.00 | val acc:64.92 | test_acc_best_val: 65.25  | best_test_acc: 65.25 | test acc:65.33 | time:22.4ms
epoch:140 | loss:1.4852 | train acc:100.00 | val acc:65.24 | test_acc_best_val: 64.98  | best_test_acc: 64.98 | test acc:64.50 | time:26.8ms
epoch:160 | loss:1.4672 | train acc:100.00 | val acc:64.92 | test_acc_best_val: 64.98  | best_test_acc: 64.98 | test acc:65.06 | time:21.4ms
epoch:180 | loss:1.4617 | train acc:100.00 | val acc:65.24 | test_acc_best_val: 64.98  | best_test_acc: 64.98 | test acc:64.98 | time:21.6ms
Run 8/10, best test accuracy: 64.98, acc(last): 64.50, total time: 6.79s
The split is 9
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 9/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.8273 | train acc:52.17 | val acc:39.84 | test_acc_best_val: 38.92  | best_test_acc: 38.92 | test acc:38.92 | time:22.1ms
epoch:20 | loss:1.5001 | train acc:85.51 | val acc:59.68 | test_acc_best_val: 59.47  | best_test_acc: 59.47 | test acc:60.10 | time:22.0ms
epoch:40 | loss:1.4701 | train acc:96.38 | val acc:63.65 | test_acc_best_val: 62.38  | best_test_acc: 62.38 | test acc:62.46 | time:21.9ms
epoch:60 | loss:1.5125 | train acc:98.55 | val acc:64.92 | test_acc_best_val: 62.46  | best_test_acc: 62.46 | test acc:62.46 | time:21.9ms
epoch:80 | loss:1.4643 | train acc:98.55 | val acc:64.29 | test_acc_best_val: 62.46  | best_test_acc: 62.46 | test acc:63.17 | time:22.5ms
epoch:100 | loss:1.4817 | train acc:98.55 | val acc:64.60 | test_acc_best_val: 62.46  | best_test_acc: 62.46 | test acc:63.80 | time:25.5ms
epoch:120 | loss:1.4649 | train acc:97.83 | val acc:64.60 | test_acc_best_val: 63.72  | best_test_acc: 63.72 | test acc:63.40 | time:27.8ms
epoch:140 | loss:1.4610 | train acc:98.55 | val acc:63.81 | test_acc_best_val: 63.72  | best_test_acc: 63.72 | test acc:63.64 | time:21.3ms
epoch:160 | loss:1.4661 | train acc:99.28 | val acc:64.92 | test_acc_best_val: 63.72  | best_test_acc: 63.72 | test acc:64.03 | time:26.7ms
epoch:180 | loss:1.4622 | train acc:99.28 | val acc:65.40 | test_acc_best_val: 64.27  | best_test_acc: 64.27 | test acc:64.27 | time:24.8ms
Run 9/10, best test accuracy: 64.23, acc(last): 64.11, total time: 6.75s
The split is 10
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 10/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.8267 | train acc:43.48 | val acc:31.27 | test_acc_best_val: 29.95  | best_test_acc: 29.95 | test acc:29.95 | time:22.0ms
epoch:20 | loss:1.5013 | train acc:94.93 | val acc:60.63 | test_acc_best_val: 59.94  | best_test_acc: 59.94 | test acc:61.20 | time:23.4ms
epoch:40 | loss:1.5249 | train acc:98.55 | val acc:61.90 | test_acc_best_val: 62.50  | best_test_acc: 62.50 | test acc:62.50 | time:21.7ms
epoch:60 | loss:1.5101 | train acc:99.28 | val acc:63.49 | test_acc_best_val: 62.89  | best_test_acc: 62.89 | test acc:62.89 | time:28.0ms
epoch:80 | loss:1.4365 | train acc:99.28 | val acc:63.33 | test_acc_best_val: 62.89  | best_test_acc: 62.89 | test acc:63.09 | time:21.9ms
epoch:100 | loss:1.4895 | train acc:99.28 | val acc:62.86 | test_acc_best_val: 63.21  | best_test_acc: 63.21 | test acc:62.97 | time:22.8ms
epoch:120 | loss:1.4391 | train acc:99.28 | val acc:61.75 | test_acc_best_val: 63.21  | best_test_acc: 63.21 | test acc:62.93 | time:21.9ms
epoch:140 | loss:1.5213 | train acc:99.28 | val acc:62.70 | test_acc_best_val: 63.21  | best_test_acc: 63.21 | test acc:63.52 | time:23.2ms
epoch:160 | loss:1.4724 | train acc:99.28 | val acc:63.49 | test_acc_best_val: 63.21  | best_test_acc: 63.21 | test acc:64.39 | time:22.6ms
epoch:180 | loss:1.4697 | train acc:99.28 | val acc:63.49 | test_acc_best_val: 64.54  | best_test_acc: 64.54 | test acc:64.19 | time:21.6ms
Run 10/10, best test accuracy: 64.54, acc(last): 63.84, total time: 6.39s
The split is 1
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 1/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.8363 | train acc:26.09 | val acc:24.92 | test_acc_best_val: 22.96  | best_test_acc: 22.96 | test acc:22.96 | time:26.1ms
epoch:20 | loss:1.5750 | train acc:89.86 | val acc:63.49 | test_acc_best_val: 63.95  | best_test_acc: 63.95 | test acc:64.54 | time:19.4ms
epoch:40 | loss:1.4738 | train acc:97.10 | val acc:64.44 | test_acc_best_val: 65.21  | best_test_acc: 65.21 | test acc:66.39 | time:22.2ms
epoch:60 | loss:1.4875 | train acc:99.28 | val acc:64.76 | test_acc_best_val: 66.47  | best_test_acc: 66.47 | test acc:67.06 | time:29.7ms
epoch:80 | loss:1.4663 | train acc:99.28 | val acc:64.60 | test_acc_best_val: 66.47  | best_test_acc: 66.47 | test acc:66.98 | time:26.3ms
epoch:100 | loss:1.5264 | train acc:99.28 | val acc:64.44 | test_acc_best_val: 66.47  | best_test_acc: 66.47 | test acc:66.63 | time:23.8ms
epoch:120 | loss:1.5045 | train acc:99.28 | val acc:64.92 | test_acc_best_val: 66.47  | best_test_acc: 66.47 | test acc:67.37 | time:21.8ms
epoch:140 | loss:1.5077 | train acc:99.28 | val acc:64.92 | test_acc_best_val: 66.47  | best_test_acc: 66.47 | test acc:66.98 | time:22.3ms
epoch:160 | loss:1.5178 | train acc:99.28 | val acc:65.08 | test_acc_best_val: 66.47  | best_test_acc: 66.47 | test acc:66.98 | time:22.4ms
epoch:180 | loss:1.4346 | train acc:99.28 | val acc:65.56 | test_acc_best_val: 66.47  | best_test_acc: 66.47 | test acc:67.14 | time:22.7ms
Run 1/10, best test accuracy: 67.33, acc(last): 67.57, total time: 6.92s
The split is 2
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 2/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.8190 | train acc:42.03 | val acc:32.70 | test_acc_best_val: 32.00  | best_test_acc: 32.00 | test acc:32.00 | time:26.3ms
epoch:20 | loss:1.5233 | train acc:94.93 | val acc:65.87 | test_acc_best_val: 64.15  | best_test_acc: 64.15 | test acc:64.82 | time:23.9ms
epoch:40 | loss:1.4782 | train acc:99.28 | val acc:66.67 | test_acc_best_val: 65.29  | best_test_acc: 65.29 | test acc:65.57 | time:23.2ms
epoch:60 | loss:1.4987 | train acc:100.00 | val acc:66.35 | test_acc_best_val: 65.29  | best_test_acc: 65.29 | test acc:66.71 | time:24.8ms
epoch:80 | loss:1.4672 | train acc:100.00 | val acc:67.62 | test_acc_best_val: 66.51  | best_test_acc: 66.51 | test acc:66.31 | time:21.9ms
epoch:100 | loss:1.4775 | train acc:100.00 | val acc:65.24 | test_acc_best_val: 66.51  | best_test_acc: 66.51 | test acc:66.94 | time:38.1ms
epoch:120 | loss:1.5347 | train acc:100.00 | val acc:66.51 | test_acc_best_val: 66.51  | best_test_acc: 66.51 | test acc:66.98 | time:21.4ms
epoch:140 | loss:1.5040 | train acc:100.00 | val acc:67.94 | test_acc_best_val: 67.65  | best_test_acc: 67.65 | test acc:67.65 | time:22.1ms
epoch:160 | loss:1.4917 | train acc:100.00 | val acc:67.94 | test_acc_best_val: 67.73  | best_test_acc: 67.73 | test acc:67.85 | time:22.7ms
epoch:180 | loss:1.4782 | train acc:100.00 | val acc:67.94 | test_acc_best_val: 67.22  | best_test_acc: 67.22 | test acc:67.26 | time:26.2ms
Run 2/10, best test accuracy: 67.22, acc(last): 68.32, total time: 6.74s
The split is 3
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 3/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.7992 | train acc:27.54 | val acc:19.52 | test_acc_best_val: 24.02  | best_test_acc: 24.02 | test acc:24.02 | time:20.9ms
epoch:20 | loss:1.5563 | train acc:92.03 | val acc:63.81 | test_acc_best_val: 60.93  | best_test_acc: 60.93 | test acc:60.93 | time:21.1ms
epoch:40 | loss:1.5029 | train acc:98.55 | val acc:67.94 | test_acc_best_val: 64.03  | best_test_acc: 64.03 | test acc:64.15 | time:22.8ms
epoch:60 | loss:1.5312 | train acc:98.55 | val acc:68.25 | test_acc_best_val: 64.50  | best_test_acc: 64.50 | test acc:64.23 | time:24.3ms
epoch:80 | loss:1.5574 | train acc:99.28 | val acc:69.21 | test_acc_best_val: 65.06  | best_test_acc: 65.06 | test acc:65.17 | time:21.1ms
epoch:100 | loss:1.5374 | train acc:99.28 | val acc:69.84 | test_acc_best_val: 65.13  | best_test_acc: 65.13 | test acc:64.94 | time:22.3ms
epoch:120 | loss:1.5203 | train acc:99.28 | val acc:70.16 | test_acc_best_val: 64.74  | best_test_acc: 64.74 | test acc:64.94 | time:30.6ms
epoch:140 | loss:1.5510 | train acc:99.28 | val acc:70.63 | test_acc_best_val: 65.61  | best_test_acc: 65.61 | test acc:65.61 | time:29.7ms
epoch:160 | loss:1.4894 | train acc:99.28 | val acc:70.63 | test_acc_best_val: 65.96  | best_test_acc: 65.96 | test acc:66.16 | time:22.0ms
epoch:180 | loss:1.4827 | train acc:99.28 | val acc:70.00 | test_acc_best_val: 65.96  | best_test_acc: 65.96 | test acc:66.39 | time:31.4ms
Run 3/10, best test accuracy: 65.96, acc(last): 66.51, total time: 6.96s
The split is 4
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 4/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.8783 | train acc:35.51 | val acc:25.24 | test_acc_best_val: 26.57  | best_test_acc: 26.57 | test acc:26.57 | time:26.9ms
epoch:20 | loss:1.5279 | train acc:89.13 | val acc:60.95 | test_acc_best_val: 60.61  | best_test_acc: 60.61 | test acc:60.61 | time:23.6ms
epoch:40 | loss:1.5211 | train acc:98.55 | val acc:64.13 | test_acc_best_val: 63.05  | best_test_acc: 63.05 | test acc:62.93 | time:21.1ms
epoch:60 | loss:1.4882 | train acc:99.28 | val acc:65.08 | test_acc_best_val: 63.52  | best_test_acc: 63.52 | test acc:63.95 | time:22.1ms
epoch:80 | loss:1.4668 | train acc:99.28 | val acc:64.60 | test_acc_best_val: 63.52  | best_test_acc: 63.52 | test acc:63.92 | time:23.3ms
epoch:100 | loss:1.4790 | train acc:99.28 | val acc:64.60 | test_acc_best_val: 63.52  | best_test_acc: 63.52 | test acc:64.11 | time:30.2ms
epoch:120 | loss:1.4300 | train acc:99.28 | val acc:64.76 | test_acc_best_val: 63.52  | best_test_acc: 63.52 | test acc:63.99 | time:27.2ms
epoch:140 | loss:1.4279 | train acc:99.28 | val acc:64.60 | test_acc_best_val: 63.52  | best_test_acc: 63.52 | test acc:63.84 | time:22.5ms
epoch:160 | loss:1.4493 | train acc:99.28 | val acc:64.13 | test_acc_best_val: 63.52  | best_test_acc: 63.52 | test acc:64.78 | time:24.7ms
epoch:180 | loss:1.4925 | train acc:99.28 | val acc:65.56 | test_acc_best_val: 63.52  | best_test_acc: 63.52 | test acc:64.94 | time:23.7ms
Run 4/10, best test accuracy: 63.52, acc(last): 64.23, total time: 6.94s
The split is 5
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 5/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.8327 | train acc:31.88 | val acc:29.37 | test_acc_best_val: 28.22  | best_test_acc: 28.22 | test acc:28.22 | time:23.9ms
epoch:20 | loss:1.5608 | train acc:95.65 | val acc:66.19 | test_acc_best_val: 63.40  | best_test_acc: 63.40 | test acc:63.40 | time:94.0ms
epoch:40 | loss:1.4832 | train acc:100.00 | val acc:66.35 | test_acc_best_val: 64.74  | best_test_acc: 64.74 | test acc:64.62 | time:47.0ms
epoch:60 | loss:1.4877 | train acc:100.00 | val acc:68.89 | test_acc_best_val: 64.82  | best_test_acc: 64.82 | test acc:64.82 | time:27.2ms
epoch:80 | loss:1.4756 | train acc:100.00 | val acc:69.05 | test_acc_best_val: 64.98  | best_test_acc: 64.98 | test acc:65.96 | time:22.2ms
epoch:100 | loss:1.4576 | train acc:100.00 | val acc:68.73 | test_acc_best_val: 64.98  | best_test_acc: 64.98 | test acc:67.33 | time:23.8ms
epoch:120 | loss:1.4998 | train acc:100.00 | val acc:68.73 | test_acc_best_val: 67.33  | best_test_acc: 67.33 | test acc:66.75 | time:35.9ms
epoch:140 | loss:1.4679 | train acc:100.00 | val acc:68.73 | test_acc_best_val: 67.33  | best_test_acc: 67.33 | test acc:66.67 | time:22.4ms
epoch:160 | loss:1.5116 | train acc:100.00 | val acc:68.89 | test_acc_best_val: 67.33  | best_test_acc: 67.33 | test acc:66.82 | time:21.8ms
epoch:180 | loss:1.5054 | train acc:100.00 | val acc:69.52 | test_acc_best_val: 67.33  | best_test_acc: 67.33 | test acc:67.10 | time:22.2ms
Run 5/10, best test accuracy: 67.33, acc(last): 66.90, total time: 7.21s
The split is 6
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 6/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.8209 | train acc:31.88 | val acc:31.11 | test_acc_best_val: 31.60  | best_test_acc: 31.60 | test acc:31.60 | time:25.8ms
epoch:20 | loss:1.5361 | train acc:92.75 | val acc:62.22 | test_acc_best_val: 62.74  | best_test_acc: 62.74 | test acc:63.60 | time:22.5ms
epoch:40 | loss:1.4947 | train acc:98.55 | val acc:64.92 | test_acc_best_val: 65.13  | best_test_acc: 65.13 | test acc:65.29 | time:23.4ms
epoch:60 | loss:1.5006 | train acc:98.55 | val acc:66.98 | test_acc_best_val: 65.57  | best_test_acc: 65.57 | test acc:65.57 | time:29.0ms
epoch:80 | loss:1.4686 | train acc:99.28 | val acc:68.10 | test_acc_best_val: 65.88  | best_test_acc: 65.88 | test acc:65.92 | time:21.5ms
epoch:100 | loss:1.4918 | train acc:99.28 | val acc:68.25 | test_acc_best_val: 66.47  | best_test_acc: 66.47 | test acc:66.75 | time:23.7ms
epoch:120 | loss:1.5169 | train acc:99.28 | val acc:67.94 | test_acc_best_val: 66.47  | best_test_acc: 66.47 | test acc:66.86 | time:22.3ms
epoch:140 | loss:1.4600 | train acc:99.28 | val acc:68.25 | test_acc_best_val: 66.47  | best_test_acc: 66.47 | test acc:66.98 | time:22.8ms
epoch:160 | loss:1.4898 | train acc:99.28 | val acc:68.57 | test_acc_best_val: 66.78  | best_test_acc: 66.78 | test acc:66.98 | time:28.2ms
epoch:180 | loss:1.4581 | train acc:99.28 | val acc:68.41 | test_acc_best_val: 66.78  | best_test_acc: 66.78 | test acc:66.75 | time:22.8ms
Run 6/10, best test accuracy: 66.78, acc(last): 66.75, total time: 7.17s
The split is 7
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 7/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.8267 | train acc:39.86 | val acc:31.75 | test_acc_best_val: 34.55  | best_test_acc: 34.55 | test acc:34.55 | time:23.1ms
epoch:20 | loss:1.5248 | train acc:87.68 | val acc:53.65 | test_acc_best_val: 57.47  | best_test_acc: 57.47 | test acc:58.65 | time:21.6ms
epoch:40 | loss:1.5293 | train acc:97.10 | val acc:56.83 | test_acc_best_val: 62.46  | best_test_acc: 62.46 | test acc:62.46 | time:25.1ms
epoch:60 | loss:1.5133 | train acc:99.28 | val acc:57.78 | test_acc_best_val: 63.01  | best_test_acc: 63.01 | test acc:62.50 | time:21.6ms
epoch:80 | loss:1.5015 | train acc:99.28 | val acc:58.25 | test_acc_best_val: 63.01  | best_test_acc: 63.01 | test acc:62.26 | time:26.2ms
epoch:100 | loss:1.5235 | train acc:99.28 | val acc:60.48 | test_acc_best_val: 64.19  | best_test_acc: 64.19 | test acc:64.19 | time:22.1ms
epoch:120 | loss:1.4683 | train acc:99.28 | val acc:59.84 | test_acc_best_val: 64.58  | best_test_acc: 64.58 | test acc:64.31 | time:21.9ms
epoch:140 | loss:1.5357 | train acc:99.28 | val acc:60.48 | test_acc_best_val: 64.58  | best_test_acc: 64.58 | test acc:64.19 | time:23.1ms
epoch:160 | loss:1.5049 | train acc:98.55 | val acc:60.32 | test_acc_best_val: 64.58  | best_test_acc: 64.58 | test acc:65.13 | time:26.7ms
epoch:180 | loss:1.4922 | train acc:98.55 | val acc:59.37 | test_acc_best_val: 64.58  | best_test_acc: 64.58 | test acc:64.31 | time:21.5ms
Run 7/10, best test accuracy: 64.58, acc(last): 64.43, total time: 6.20s
The split is 8
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 8/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.8191 | train acc:47.83 | val acc:38.73 | test_acc_best_val: 38.92  | best_test_acc: 38.92 | test acc:38.92 | time:22.6ms
epoch:20 | loss:1.5547 | train acc:89.86 | val acc:63.17 | test_acc_best_val: 61.12  | best_test_acc: 61.12 | test acc:61.12 | time:21.1ms
epoch:40 | loss:1.5177 | train acc:99.28 | val acc:65.24 | test_acc_best_val: 63.44  | best_test_acc: 63.44 | test acc:63.44 | time:21.6ms
epoch:60 | loss:1.4853 | train acc:100.00 | val acc:64.44 | test_acc_best_val: 63.68  | best_test_acc: 63.68 | test acc:63.52 | time:21.0ms
epoch:80 | loss:1.5354 | train acc:100.00 | val acc:65.40 | test_acc_best_val: 63.68  | best_test_acc: 63.68 | test acc:63.44 | time:22.2ms
epoch:100 | loss:1.4962 | train acc:100.00 | val acc:64.76 | test_acc_best_val: 63.33  | best_test_acc: 63.33 | test acc:62.66 | time:20.3ms
epoch:120 | loss:1.4977 | train acc:100.00 | val acc:64.92 | test_acc_best_val: 63.33  | best_test_acc: 63.33 | test acc:63.52 | time:19.6ms
epoch:140 | loss:1.5118 | train acc:100.00 | val acc:65.87 | test_acc_best_val: 63.80  | best_test_acc: 63.80 | test acc:63.56 | time:22.3ms
epoch:160 | loss:1.4484 | train acc:100.00 | val acc:65.56 | test_acc_best_val: 63.80  | best_test_acc: 63.80 | test acc:63.99 | time:20.1ms
epoch:180 | loss:1.4879 | train acc:100.00 | val acc:65.40 | test_acc_best_val: 63.80  | best_test_acc: 63.80 | test acc:64.19 | time:27.8ms
Run 8/10, best test accuracy: 63.80, acc(last): 64.50, total time: 6.23s
The split is 9
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 9/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.8787 | train acc:34.06 | val acc:23.65 | test_acc_best_val: 24.84  | best_test_acc: 24.84 | test acc:24.84 | time:22.7ms
epoch:20 | loss:1.5248 | train acc:91.30 | val acc:61.11 | test_acc_best_val: 59.59  | best_test_acc: 59.59 | test acc:60.14 | time:21.9ms
epoch:40 | loss:1.4656 | train acc:97.10 | val acc:62.70 | test_acc_best_val: 61.44  | best_test_acc: 61.44 | test acc:61.67 | time:22.6ms
epoch:60 | loss:1.4673 | train acc:97.83 | val acc:62.06 | test_acc_best_val: 61.44  | best_test_acc: 61.44 | test acc:61.79 | time:53.0ms
epoch:80 | loss:1.4699 | train acc:98.55 | val acc:62.38 | test_acc_best_val: 61.44  | best_test_acc: 61.44 | test acc:62.26 | time:20.8ms
epoch:100 | loss:1.4619 | train acc:99.28 | val acc:62.54 | test_acc_best_val: 61.44  | best_test_acc: 61.44 | test acc:61.95 | time:19.6ms
epoch:120 | loss:1.4553 | train acc:98.55 | val acc:63.17 | test_acc_best_val: 61.44  | best_test_acc: 61.44 | test acc:62.50 | time:22.4ms
epoch:140 | loss:1.4448 | train acc:98.55 | val acc:62.86 | test_acc_best_val: 62.42  | best_test_acc: 62.42 | test acc:63.09 | time:21.3ms
epoch:160 | loss:1.4635 | train acc:98.55 | val acc:63.49 | test_acc_best_val: 62.42  | best_test_acc: 62.42 | test acc:63.17 | time:22.3ms
epoch:180 | loss:1.4982 | train acc:98.55 | val acc:62.70 | test_acc_best_val: 62.42  | best_test_acc: 62.42 | test acc:63.88 | time:21.1ms
Run 9/10, best test accuracy: 62.42, acc(last): 64.07, total time: 6.20s
The split is 10
number of hyperedges is 1079
The hypergraph citeseer/cocitation has 1079 hyperedges where authors are hyperedges
The average hyperedge contains 3.200185356811863 nodes
Run 10/10, Total Epochs: 200
UniGNN(
  (conv_out): UniGATConv(64, 6, heads=1)
  (convs): ModuleList(
    (0): UniGATConv(3703, 8, heads=8)
  )
  (act): ReLU()
  (input_drop): Dropout(p=0.6, inplace=False)
  (dropout): Dropout(p=0.6, inplace=False)
)
total_params:237516
epoch:0 | loss:1.7868 | train acc:39.13 | val acc:28.89 | test_acc_best_val: 29.48  | best_test_acc: 29.48 | test acc:29.48 | time:21.9ms
epoch:20 | loss:1.5443 | train acc:91.30 | val acc:58.73 | test_acc_best_val: 58.96  | best_test_acc: 58.96 | test acc:61.12 | time:22.2ms
epoch:40 | loss:1.4707 | train acc:98.55 | val acc:60.16 | test_acc_best_val: 61.24  | best_test_acc: 61.24 | test acc:63.25 | time:24.1ms
epoch:60 | loss:1.4756 | train acc:98.55 | val acc:63.02 | test_acc_best_val: 64.70  | best_test_acc: 64.70 | test acc:64.70 | time:21.5ms
epoch:80 | loss:1.4831 | train acc:99.28 | val acc:63.49 | test_acc_best_val: 64.70  | best_test_acc: 64.70 | test acc:64.66 | time:21.4ms
epoch:100 | loss:1.4734 | train acc:99.28 | val acc:64.29 | test_acc_best_val: 65.21  | best_test_acc: 65.21 | test acc:65.25 | time:20.8ms
epoch:120 | loss:1.5001 | train acc:99.28 | val acc:64.92 | test_acc_best_val: 65.21  | best_test_acc: 65.21 | test acc:65.02 | time:22.7ms
epoch:140 | loss:1.4935 | train acc:99.28 | val acc:65.08 | test_acc_best_val: 65.33  | best_test_acc: 65.33 | test acc:65.64 | time:20.1ms
epoch:160 | loss:1.4599 | train acc:99.28 | val acc:64.92 | test_acc_best_val: 65.33  | best_test_acc: 65.33 | test acc:65.76 | time:21.4ms
epoch:180 | loss:1.4539 | train acc:99.28 | val acc:63.49 | test_acc_best_val: 65.33  | best_test_acc: 65.33 | test acc:66.12 | time:22.9ms
Run 10/10, best test accuracy: 65.33, acc(last): 65.96, total time: 6.32s
We had 80 runs
Average test accuracy for best val: 65.55621065199375  1.6141439836859306
Average final test accuracy: 65.6942804902792  1.58919700818203
Average best test accuracy: 65.55621065199375  1.6141439836859306
