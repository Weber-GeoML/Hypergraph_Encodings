{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Notebook to analyze the structure of coauthorship and cocitation data.\n",
    "\n",
    "Mimicks Hypergraph_Encodings/scripts/compute_encodings/analyze_cc_ca_data.py\n",
    "\n",
    "This script loads and examines the data structure for:\n",
    "- coauthorship: cora, dblp\n",
    "- cocitation: citeseer, cora, pubmed\n",
    "\n",
    "It prints examples and statistics to understand the data format.\n",
    "\n",
    "These files have:\n",
    "\n",
    "Features: Sparse matrices (scipy.sparse.csr.csr_matrix)\n",
    "Hypergraph: Dictionary with hyperedges\n",
    "Labels: List of node labels\n",
    "Splits: Dictionary with 'train' and 'test' splits (10 different splits)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from typing import Dict, Any, List\n",
    "import warnings\n",
    "import hypernetx as hnx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from encodings_hnns.expansions import compute_clique_expansion\n",
    "import networkx as nx\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "\n",
    "def load_pickle_file(file_path: str) -> Any:\n",
    "    \"\"\"Load a pickle file and return its contents.\n",
    "\n",
    "    Args:\n",
    "        file_path: Path to the pickle file\n",
    "\n",
    "    Returns:\n",
    "        Contents of the pickle file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as handle:\n",
    "            return pickle.load(handle)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return None\n",
    "    \n",
    "\n",
    "def analyze_dataset(dataset_path: str, dataset_name: str) -> None:\n",
    "    \"\"\"Analyze a single dataset and print its structure.\n",
    "\n",
    "    Args:\n",
    "        dataset_path: Path to the dataset directory\n",
    "        dataset_name: Name of the dataset\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ANALYZING DATASET: {dataset_name.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # Load features\n",
    "    features_path = os.path.join(dataset_path, \"features.pickle\")\n",
    "    features = load_pickle_file(features_path)\n",
    "    if features is not None:\n",
    "        print(f\"Features type: {type(features)}\")\n",
    "        if isinstance(features, np.ndarray):\n",
    "            print(f\"Features shape: {features.shape}\")\n",
    "            print(f\"Features dtype: {features.dtype}\")\n",
    "            print(f\"Features sample (first 5 elements): {features[:5]}\")\n",
    "        else:\n",
    "            print(f\"Features content (first 100 chars): {str(features)[:100]}...\")\n",
    "\n",
    "    # Load hypergraph\n",
    "    hypergraph_path = os.path.join(dataset_path, \"hypergraph.pickle\")\n",
    "    hypergraph = load_pickle_file(hypergraph_path)\n",
    "    if hypergraph is not None:\n",
    "        print(f\"\\nHypergraph type: {type(hypergraph)}\")\n",
    "        if isinstance(hypergraph, dict):\n",
    "            print(f\"Number of hyperedges: {len(hypergraph)}\")\n",
    "            print(f\"Sample hyperedges (first 3):\")\n",
    "            for i, (key, value) in enumerate(list(hypergraph.items())[:3]):\n",
    "                print(f\"  {key}: {value}\")\n",
    "\n",
    "            # Analyze hyperedge sizes\n",
    "            edge_sizes = [len(edge) for edge in hypergraph.values()]\n",
    "            print(f\"Hyperedge size statistics:\")\n",
    "            print(f\"  Min: {min(edge_sizes)}\")\n",
    "            print(f\"  Max: {max(edge_sizes)}\")\n",
    "            print(f\"  Mean: {np.mean(edge_sizes):.2f}\")\n",
    "            print(f\"  Median: {np.median(edge_sizes):.2f}\")\n",
    "\n",
    "            # Get all unique nodes\n",
    "            all_nodes = set()\n",
    "            for edge in hypergraph.values():\n",
    "                all_nodes.update(edge)\n",
    "            print(f\"Total unique nodes: {len(all_nodes)}\")\n",
    "\n",
    "\n",
    "    # Load labels\n",
    "    labels_path = os.path.join(dataset_path, \"labels.pickle\")\n",
    "    labels = load_pickle_file(labels_path)\n",
    "    if labels is not None:\n",
    "        print(f\"\\nLabels type: {type(labels)}\")\n",
    "        if isinstance(labels, np.ndarray):\n",
    "            print(f\"Labels shape: {labels.shape}\")\n",
    "            print(f\"Labels dtype: {labels.dtype}\")\n",
    "            print(f\"Labels sample (first 10): {labels[:10]}\")\n",
    "            print(f\"Unique labels: {np.unique(labels)}\")\n",
    "            print(f\"Number of classes: {len(np.unique(labels))}\")\n",
    "        else:\n",
    "            print(f\"Labels content (first 100 chars): {str(labels)[:100]}...\")\n",
    "\n",
    "    # Load splits\n",
    "    splits_dir = os.path.join(dataset_path, \"splits\")\n",
    "    if os.path.exists(splits_dir):\n",
    "        print(f\"\\nSplits directory: {splits_dir}\")\n",
    "        split_files = [f for f in os.listdir(splits_dir) if f.endswith(\".pickle\")]\n",
    "        print(f\"Number of split files: {len(split_files)}\")\n",
    "\n",
    "        # Analyze first split\n",
    "        if split_files:\n",
    "            first_split_path = os.path.join(splits_dir, split_files[0])\n",
    "            split_data = load_pickle_file(first_split_path)\n",
    "            if split_data is not None:\n",
    "                print(f\"First split ({split_files[0]}) type: {type(split_data)}\")\n",
    "                if isinstance(split_data, dict):\n",
    "                    print(f\"Split keys: {list(split_data.keys())}\")\n",
    "                    for key, value in split_data.items():\n",
    "                        if isinstance(value, (list, np.ndarray)):\n",
    "                            print(f\"  {key}: {len(value)} elements\")\n",
    "                        else:\n",
    "                            print(f\"  {key}: {type(value)}\")\n",
    "                elif isinstance(split_data, (list, tuple)):\n",
    "                    print(f\"Split contains {len(split_data)} elements\")\n",
    "                    if len(split_data) >= 3:\n",
    "                        print(f\"  Train: {len(split_data[0])} elements\")\n",
    "                        print(f\"  Val: {len(split_data[1])} elements\")\n",
    "                        print(f\"  Test: {len(split_data[2])} elements\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANALYZING COAUTHORSHIP AND COCITATION DATASETS\n",
      "============================================================\n",
      "Data directory: ../data\n",
      "\n",
      "Found coauthorship directory: ../data/coauthorship\n",
      "\n",
      "============================================================\n",
      "ANALYZING DATASET: COAUTHORSHIP_CORA\n",
      "============================================================\n",
      "Features type: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Features content (first 100 chars): <Compressed Sparse Row sparse matrix of dtype 'float32'\n",
      "\twith 49216 stored elements and shape (2708,...\n",
      "\n",
      "Hypergraph type: <class 'dict'>\n",
      "Number of hyperedges: 1072\n",
      "Sample hyperedges (first 3):\n",
      "  V Gupta: [235, 355]\n",
      "  A Srinivasan: [1133, 1666, 1888]\n",
      "  J Zavrel: [783, 785]\n",
      "Hyperedge size statistics:\n",
      "  Min: 2\n",
      "  Max: 43\n",
      "  Mean: 4.28\n",
      "  Median: 3.00\n",
      "Total unique nodes: 2388\n",
      "\n",
      "Labels type: <class 'list'>\n",
      "Labels content (first 100 chars): [3, 2, 6, 6, 5, 5, 1, 3, 3, 1, 3, 0, 5, 4, 3, 3, 6, 3, 3, 3, 1, 6, 0, 5, 6, 1, 3, 5, 3, 2, 4, 3, 4, ...\n",
      "\n",
      "Splits directory: ../data/coauthorship/cora/splits\n",
      "Number of split files: 10\n",
      "First split (9.pickle) type: <class 'dict'>\n",
      "Split keys: ['train', 'test']\n",
      "  train: 140 elements\n",
      "  test: 2568 elements\n",
      "\n",
      "============================================================\n",
      "ANALYZING DATASET: COAUTHORSHIP_DBLP\n",
      "============================================================\n",
      "Features type: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Features content (first 100 chars): <Compressed Sparse Row sparse matrix of dtype 'float32'\n",
      "\twith 1602890 stored elements and shape (413...\n",
      "\n",
      "Hypergraph type: <class 'dict'>\n",
      "Number of hyperedges: 22363\n",
      "Sample hyperedges (first 3):\n",
      "  franz baader: [0, 14492, 12731, 12732, 12734, 12736, 12738, 39025]\n",
      "  carsten lutz: [0, 29187, 14607, 13661, 12732, 19117, 29746, 14720, 14658, 29848, 5027, 29184]\n",
      "  ulrike sattler: [0, 29765, 26519, 26553, 13661, 283, 14492, 13387, 14437, 13389, 13390, 13391, 14658, 26689]\n",
      "Hyperedge size statistics:\n",
      "  Min: 2\n",
      "  Max: 202\n",
      "  Mean: 4.45\n",
      "  Median: 3.00\n",
      "Total unique nodes: 41302\n",
      "\n",
      "Labels type: <class 'list'>\n",
      "Labels content (first 100 chars): [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "\n",
      "Splits directory: ../data/coauthorship/dblp/splits\n",
      "Number of split files: 10\n",
      "First split (9.pickle) type: <class 'dict'>\n",
      "Split keys: ['train', 'test']\n",
      "  train: 1740 elements\n",
      "  test: 39562 elements\n",
      "\n",
      "Found cocitation directory: ../data/cocitation\n",
      "\n",
      "============================================================\n",
      "ANALYZING DATASET: COCITATION_CITESEER\n",
      "============================================================\n",
      "Features type: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Features content (first 100 chars): <Compressed Sparse Row sparse matrix of dtype 'float32'\n",
      "\twith 105165 stored elements and shape (3312...\n",
      "\n",
      "Hypergraph type: <class 'dict'>\n",
      "Number of hyperedges: 1079\n",
      "Sample hyperedges (first 3):\n",
      "  415: {0, 953}\n",
      "  514: {0, 384, 323, 173, 383}\n",
      "  585: {0, 86}\n",
      "Hyperedge size statistics:\n",
      "  Min: 2\n",
      "  Max: 26\n",
      "  Mean: 3.20\n",
      "  Median: 2.00\n",
      "Total unique nodes: 1458\n",
      "\n",
      "Labels type: <class 'list'>\n",
      "Labels content (first 100 chars): [1, 4, 1, 0, 5, 5, 1, 4, 5, 2, 4, 1, 0, 4, 0, 4, 0, 3, 1, 1, 1, 4, 4, 0, 4, 3, 0, 1, 4, 3, 3, 3, 3, ...\n",
      "\n",
      "Splits directory: ../data/cocitation/citeseer/splits\n",
      "Number of split files: 10\n",
      "First split (9.pickle) type: <class 'dict'>\n",
      "Split keys: ['train', 'test']\n",
      "  train: 138 elements\n",
      "  test: 3174 elements\n",
      "\n",
      "============================================================\n",
      "ANALYZING DATASET: COCITATION_CORA\n",
      "============================================================\n",
      "Features type: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Features content (first 100 chars): <Compressed Sparse Row sparse matrix of dtype 'float32'\n",
      "\twith 49216 stored elements and shape (2708,...\n",
      "\n",
      "Hypergraph type: <class 'dict'>\n",
      "Number of hyperedges: 1579\n",
      "Sample hyperedges (first 3):\n",
      "  402: {538, 163, 219}\n",
      "  1696: {1114, 163}\n",
      "  2295: {881, 427, 2667, 163}\n",
      "Hyperedge size statistics:\n",
      "  Min: 2\n",
      "  Max: 5\n",
      "  Mean: 3.03\n",
      "  Median: 3.00\n",
      "Total unique nodes: 1434\n",
      "\n",
      "Labels type: <class 'list'>\n",
      "Labels content (first 100 chars): [3, 2, 6, 6, 5, 5, 1, 3, 3, 1, 3, 0, 5, 4, 3, 3, 6, 3, 3, 3, 1, 6, 0, 5, 6, 1, 3, 5, 3, 2, 4, 3, 4, ...\n",
      "\n",
      "Splits directory: ../data/cocitation/cora/splits\n",
      "Number of split files: 10\n",
      "First split (9.pickle) type: <class 'dict'>\n",
      "Split keys: ['train', 'test']\n",
      "  train: 140 elements\n",
      "  test: 2568 elements\n",
      "\n",
      "============================================================\n",
      "ANALYZING DATASET: COCITATION_PUBMED\n",
      "============================================================\n",
      "Features type: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Features content (first 100 chars): <Compressed Sparse Row sparse matrix of dtype 'float32'\n",
      "\twith 988031 stored elements and shape (1971...\n",
      "\n",
      "Hypergraph type: <class 'dict'>\n",
      "Number of hyperedges: 7963\n",
      "Sample hyperedges (first 3):\n",
      "  0: {14618, 12388, 2918, 14295}\n",
      "  2: {9547, 10612}\n",
      "  4: {5652, 12621}\n",
      "Hyperedge size statistics:\n",
      "  Min: 2\n",
      "  Max: 171\n",
      "  Mean: 4.35\n",
      "  Median: 3.00\n",
      "Total unique nodes: 3840\n",
      "\n",
      "Labels type: <class 'list'>\n",
      "Labels content (first 100 chars): [0, 0, 0, 1, 0, 0, 2, 2, 2, 1, 0, 2, 0, 2, 2, 1, 0, 2, 2, 2, 1, 2, 2, 0, 1, 2, 1, 2, 0, 2, 0, 1, 2, ...\n",
      "\n",
      "Splits directory: ../data/cocitation/pubmed/splits\n",
      "Number of split files: 10\n",
      "First split (9.pickle) type: <class 'dict'>\n",
      "Split keys: ['train', 'test']\n",
      "  train: 78 elements\n",
      "  test: 19639 elements\n"
     ]
    }
   ],
   "source": [
    "def main() -> None:\n",
    "    \"\"\"Main function to analyze all datasets.\"\"\"\n",
    "    print(\"ANALYZING COAUTHORSHIP AND COCITATION DATASETS\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    data_dir = os.path.join(\"../data\")\n",
    "\n",
    "    print(f\"Data directory: {data_dir}\")\n",
    "\n",
    "    # Analyze coauthorship datasets\n",
    "    coauthorship_dir = os.path.join(data_dir, \"coauthorship\")\n",
    "    if os.path.exists(coauthorship_dir):\n",
    "        print(f\"\\nFound coauthorship directory: {coauthorship_dir}\")\n",
    "        for dataset in [\"cora\", \"dblp\"]:\n",
    "            dataset_path = os.path.join(coauthorship_dir, dataset)\n",
    "            if os.path.exists(dataset_path):\n",
    "                analyze_dataset(dataset_path, f\"coauthorship_{dataset}\")\n",
    "            else:\n",
    "                print(f\"Dataset {dataset} not found in coauthorship\")\n",
    "    else:\n",
    "        print(\"Coauthorship directory not found\")\n",
    "\n",
    "    # Analyze cocitation datasets\n",
    "    cocitation_dir = os.path.join(data_dir, \"cocitation\")\n",
    "    if os.path.exists(cocitation_dir):\n",
    "        print(f\"\\nFound cocitation directory: {cocitation_dir}\")\n",
    "        for dataset in [\"citeseer\", \"cora\", \"pubmed\"]:\n",
    "            dataset_path = os.path.join(cocitation_dir, dataset)\n",
    "            if os.path.exists(dataset_path):\n",
    "                analyze_dataset(dataset_path, f\"cocitation_{dataset}\")\n",
    "            else:\n",
    "                print(f\"Dataset {dataset} not found in cocitation\")\n",
    "    else:\n",
    "        print(\"Cocitation directory not found\")\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other data (in data/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clean_repo_5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
