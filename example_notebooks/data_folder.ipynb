{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Notebook to analyze the structure of coauthorship and cocitation data.\n",
    "\n",
    "Mimicks Hypergraph_Encodings/scripts/compute_encodings/analyze_cc_ca_data.py\n",
    "\n",
    "This script loads and examines the data structure for:\n",
    "- coauthorship: cora, dblp\n",
    "- cocitation: citeseer, cora, pubmed\n",
    "\n",
    "It prints examples and statistics to understand the data format.\n",
    "\n",
    "These files have:\n",
    "\n",
    "Features: Sparse matrices (scipy.sparse.csr.csr_matrix)\n",
    "Hypergraph: Dictionary with hyperedges\n",
    "Labels: List of node labels\n",
    "Splits: Dictionary with 'train' and 'test' splits (10 different splits)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from typing import Dict, Any, List\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "\n",
    "def load_pickle_file(file_path: str) -> Any:\n",
    "    \"\"\"Load a pickle file and return its contents.\n",
    "\n",
    "    Args:\n",
    "        file_path: Path to the pickle file\n",
    "\n",
    "    Returns:\n",
    "        Contents of the pickle file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as handle:\n",
    "            return pickle.load(handle)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def analyze_dataset(dataset_path: str, dataset_name: str) -> None:\n",
    "    \"\"\"Analyze a single dataset and print its structure.\n",
    "\n",
    "    Args:\n",
    "        dataset_path: Path to the dataset directory\n",
    "        dataset_name: Name of the dataset\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ANALYZING DATASET: {dataset_name.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # Load features\n",
    "    features_path = os.path.join(dataset_path, \"features.pickle\")\n",
    "    features = load_pickle_file(features_path)\n",
    "    if features is not None:\n",
    "        print(f\"Features type: {type(features)}\")\n",
    "        if isinstance(features, np.ndarray):\n",
    "            print(f\"Features shape: {features.shape}\")\n",
    "            print(f\"Features dtype: {features.dtype}\")\n",
    "            print(f\"Features sample (first 5 elements): {features[:5]}\")\n",
    "        else:\n",
    "            print(f\"Features content (first 100 chars): {str(features)[:100]}...\")\n",
    "\n",
    "    # Load hypergraph\n",
    "    hypergraph_path = os.path.join(dataset_path, \"hypergraph.pickle\")\n",
    "    hypergraph = load_pickle_file(hypergraph_path)\n",
    "    if hypergraph is not None:\n",
    "        print(f\"\\nHypergraph type: {type(hypergraph)}\")\n",
    "        if isinstance(hypergraph, dict):\n",
    "            print(f\"Number of hyperedges: {len(hypergraph)}\")\n",
    "            print(f\"Sample hyperedges (first 3):\")\n",
    "            for i, (key, value) in enumerate(list(hypergraph.items())[:3]):\n",
    "                print(f\"  {key}: {value}\")\n",
    "\n",
    "            # Analyze hyperedge sizes\n",
    "            edge_sizes = [len(edge) for edge in hypergraph.values()]\n",
    "            print(f\"Hyperedge size statistics:\")\n",
    "            print(f\"  Min: {min(edge_sizes)}\")\n",
    "            print(f\"  Max: {max(edge_sizes)}\")\n",
    "            print(f\"  Mean: {np.mean(edge_sizes):.2f}\")\n",
    "            print(f\"  Median: {np.median(edge_sizes):.2f}\")\n",
    "\n",
    "            # Get all unique nodes\n",
    "            all_nodes = set()\n",
    "            for edge in hypergraph.values():\n",
    "                all_nodes.update(edge)\n",
    "            print(f\"Total unique nodes: {len(all_nodes)}\")\n",
    "\n",
    "    # Load labels\n",
    "    labels_path = os.path.join(dataset_path, \"labels.pickle\")\n",
    "    labels = load_pickle_file(labels_path)\n",
    "    if labels is not None:\n",
    "        print(f\"\\nLabels type: {type(labels)}\")\n",
    "        if isinstance(labels, np.ndarray):\n",
    "            print(f\"Labels shape: {labels.shape}\")\n",
    "            print(f\"Labels dtype: {labels.dtype}\")\n",
    "            print(f\"Labels sample (first 10): {labels[:10]}\")\n",
    "            print(f\"Unique labels: {np.unique(labels)}\")\n",
    "            print(f\"Number of classes: {len(np.unique(labels))}\")\n",
    "        else:\n",
    "            print(f\"Labels content (first 100 chars): {str(labels)[:100]}...\")\n",
    "\n",
    "    # Load splits\n",
    "    splits_dir = os.path.join(dataset_path, \"splits\")\n",
    "    if os.path.exists(splits_dir):\n",
    "        print(f\"\\nSplits directory: {splits_dir}\")\n",
    "        split_files = [f for f in os.listdir(splits_dir) if f.endswith(\".pickle\")]\n",
    "        print(f\"Number of split files: {len(split_files)}\")\n",
    "\n",
    "        # Analyze first split\n",
    "        if split_files:\n",
    "            first_split_path = os.path.join(splits_dir, split_files[0])\n",
    "            split_data = load_pickle_file(first_split_path)\n",
    "            if split_data is not None:\n",
    "                print(f\"First split ({split_files[0]}) type: {type(split_data)}\")\n",
    "                if isinstance(split_data, dict):\n",
    "                    print(f\"Split keys: {list(split_data.keys())}\")\n",
    "                    for key, value in split_data.items():\n",
    "                        if isinstance(value, (list, np.ndarray)):\n",
    "                            print(f\"  {key}: {len(value)} elements\")\n",
    "                        else:\n",
    "                            print(f\"  {key}: {type(value)}\")\n",
    "                elif isinstance(split_data, (list, tuple)):\n",
    "                    print(f\"Split contains {len(split_data)} elements\")\n",
    "                    if len(split_data) >= 3:\n",
    "                        print(f\"  Train: {len(split_data[0])} elements\")\n",
    "                        print(f\"  Val: {len(split_data[1])} elements\")\n",
    "                        print(f\"  Test: {len(split_data[2])} elements\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANALYZING COAUTHORSHIP AND COCITATION DATASETS\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 141\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m60\u001b[39m)\n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m# Get the data directory\u001b[39;00m\n\u001b[0;32m--> 141\u001b[0m current_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(\u001b[38;5;18;43m__file__\u001b[39;49m))\n\u001b[1;32m    142\u001b[0m project_root \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(current_dir))\n\u001b[1;32m    143\u001b[0m data_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(project_root, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "def main() -> None:\n",
    "    \"\"\"Main function to analyze all datasets.\"\"\"\n",
    "    print(\"ANALYZING COAUTHORSHIP AND COCITATION DATASETS\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Get the data directory\n",
    "    current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    project_root = os.path.dirname(os.path.dirname(current_dir))\n",
    "    data_dir = os.path.join(project_root, \"data\")\n",
    "\n",
    "    print(f\"Data directory: {data_dir}\")\n",
    "\n",
    "    # Analyze coauthorship datasets\n",
    "    coauthorship_dir = os.path.join(data_dir, \"coauthorship\")\n",
    "    if os.path.exists(coauthorship_dir):\n",
    "        print(f\"\\nFound coauthorship directory: {coauthorship_dir}\")\n",
    "        for dataset in [\"cora\", \"dblp\"]:\n",
    "            dataset_path = os.path.join(coauthorship_dir, dataset)\n",
    "            if os.path.exists(dataset_path):\n",
    "                analyze_dataset(dataset_path, f\"coauthorship_{dataset}\")\n",
    "            else:\n",
    "                print(f\"Dataset {dataset} not found in coauthorship\")\n",
    "    else:\n",
    "        print(\"Coauthorship directory not found\")\n",
    "\n",
    "    # Analyze cocitation datasets\n",
    "    cocitation_dir = os.path.join(data_dir, \"cocitation\")\n",
    "    if os.path.exists(cocitation_dir):\n",
    "        print(f\"\\nFound cocitation directory: {cocitation_dir}\")\n",
    "        for dataset in [\"citeseer\", \"cora\", \"pubmed\"]:\n",
    "            dataset_path = os.path.join(cocitation_dir, dataset)\n",
    "            if os.path.exists(dataset_path):\n",
    "                analyze_dataset(dataset_path, f\"cocitation_{dataset}\")\n",
    "            else:\n",
    "                print(f\"Dataset {dataset} not found in cocitation\")\n",
    "    else:\n",
    "        print(\"Cocitation directory not found\")\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clean_repo_5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
