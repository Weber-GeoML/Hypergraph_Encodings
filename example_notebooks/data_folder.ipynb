{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coauthorship and cotitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cocitation: citeseer, cora, pubmed\n",
    "Coauthorship: cora, deblp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just one hypergraph per folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Notebook to analyze the structure of coauthorship and cocitation data.\n",
    "\n",
    "Mimicks Hypergraph_Encodings/scripts/compute_encodings/analyze_cc_ca_data.py\n",
    "\n",
    "This script loads and examines the data structure for:\n",
    "- coauthorship: cora, dblp\n",
    "- cocitation: citeseer, cora, pubmed\n",
    "\n",
    "It prints examples and statistics to understand the data format.\n",
    "\n",
    "These files have:\n",
    "\n",
    "Features: Sparse matrices (scipy.sparse.csr.csr_matrix)\n",
    "Hypergraph: Dictionary with hyperedges\n",
    "Labels: List of node labels\n",
    "Splits: Dictionary with 'train' and 'test' splits (10 different splits)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from typing import Any\n",
    "import warnings\n",
    "\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "\n",
    "def load_pickle_file(file_path: str) -> Any:\n",
    "    \"\"\"Load a pickle file and return its contents.\n",
    "\n",
    "    Args:\n",
    "        file_path: Path to the pickle file\n",
    "\n",
    "    Returns:\n",
    "        Contents of the pickle file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as handle:\n",
    "            return pickle.load(handle)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return None\n",
    "    \n",
    "\n",
    "def analyze_dataset(dataset_path: str, dataset_name: str) -> None:\n",
    "    \"\"\"Analyze a single dataset and print its structure.\n",
    "\n",
    "    Args:\n",
    "        dataset_path: Path to the dataset directory\n",
    "        dataset_name: Name of the dataset\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ANALYZING DATASET: {dataset_name.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # Load features\n",
    "    features_path = os.path.join(dataset_path, \"features.pickle\")\n",
    "    features = load_pickle_file(features_path)\n",
    "    if features is not None:\n",
    "        print(f\"Features type: {type(features)}\")\n",
    "        if isinstance(features, np.ndarray):\n",
    "            print(f\"Features shape: {features.shape}\")\n",
    "            print(f\"Features dtype: {features.dtype}\")\n",
    "            print(f\"Features sample (first 5 elements): {features[:5]}\")\n",
    "        else:\n",
    "            print(f\"Features content (first 100 chars): {str(features)[:100]}...\")\n",
    "\n",
    "    # Load hypergraph\n",
    "    hypergraph_path = os.path.join(dataset_path, \"hypergraph.pickle\")\n",
    "    hypergraph = load_pickle_file(hypergraph_path)\n",
    "    if hypergraph is not None:\n",
    "        print(f\"\\nHypergraph type: {type(hypergraph)}\")\n",
    "        if isinstance(hypergraph, dict):\n",
    "            print(f\"Number of hyperedges: {len(hypergraph)}\")\n",
    "            print(\"Sample hyperedges (first 3):\")\n",
    "            for i, (key, value) in enumerate(list(hypergraph.items())[:3]):\n",
    "                print(f\"  {key}: {value}\")\n",
    "\n",
    "            # Analyze hyperedge sizes\n",
    "            edge_sizes = [len(edge) for edge in hypergraph.values()]\n",
    "            print(\"Hyperedge size statistics:\")\n",
    "            print(f\"  Min: {min(edge_sizes)}\")\n",
    "            print(f\"  Max: {max(edge_sizes)}\")\n",
    "            print(f\"  Mean: {np.mean(edge_sizes):.2f}\")\n",
    "            print(f\"  Median: {np.median(edge_sizes):.2f}\")\n",
    "\n",
    "            # Get all unique nodes\n",
    "            all_nodes = set()\n",
    "            for edge in hypergraph.values():\n",
    "                all_nodes.update(edge)\n",
    "            print(f\"Total unique nodes: {len(all_nodes)}\")\n",
    "\n",
    "            # dict_hypergraph = {\n",
    "            #     \"hypergraph\": hypergraph,\n",
    "            #     \"n\": len(all_nodes),\n",
    "            #     \"features\": [[1.0] for _ in range(len(all_nodes))],\n",
    "            #     \"labels\": [[1] for _ in range(len(all_nodes))],\n",
    "            # }\n",
    "\n",
    "            # # do a clique expansion and plot\n",
    "            # expanded_graph = compute_clique_expansion(dict_hypergraph)\n",
    "            # G = nx.Graph()\n",
    "            # edge_index = expanded_graph.edge_index.t().tolist()\n",
    "            # G.add_edges_from(edge_index)\n",
    "            # pos = nx.spring_layout(G)\n",
    "            # nx.draw(G, pos, with_labels=True, node_color=\"lightblue\", node_size=500, font_size=12, font_weight=\"bold\")\n",
    "            # plt.title(\"Clique Expansion\", fontsize=20)\n",
    "\n",
    "\n",
    "    # Load labels\n",
    "    labels_path = os.path.join(dataset_path, \"labels.pickle\")\n",
    "    labels = load_pickle_file(labels_path)\n",
    "    if labels is not None:\n",
    "        print(f\"\\nLabels type: {type(labels)}\")\n",
    "        if isinstance(labels, np.ndarray):\n",
    "            print(f\"Labels shape: {labels.shape}\")\n",
    "            print(f\"Labels dtype: {labels.dtype}\")\n",
    "            print(f\"Labels sample (first 10): {labels[:10]}\")\n",
    "            print(f\"Unique labels: {np.unique(labels)}\")\n",
    "            print(f\"Number of classes: {len(np.unique(labels))}\")\n",
    "        else:\n",
    "            print(f\"Labels content (first 100 chars): {str(labels)[:100]}...\")\n",
    "\n",
    "    # Load splits\n",
    "    splits_dir = os.path.join(dataset_path, \"splits\")\n",
    "    if os.path.exists(splits_dir):\n",
    "        print(f\"\\nSplits directory: {splits_dir}\")\n",
    "        split_files = [f for f in os.listdir(splits_dir) if f.endswith(\".pickle\")]\n",
    "        print(f\"Number of split files: {len(split_files)}\")\n",
    "\n",
    "        # Analyze first split\n",
    "        if split_files:\n",
    "            first_split_path = os.path.join(splits_dir, split_files[0])\n",
    "            split_data = load_pickle_file(first_split_path)\n",
    "            if split_data is not None:\n",
    "                print(f\"First split ({split_files[0]}) type: {type(split_data)}\")\n",
    "                if isinstance(split_data, dict):\n",
    "                    print(f\"Split keys: {list(split_data.keys())}\")\n",
    "                    for key, value in split_data.items():\n",
    "                        if isinstance(value, (list, np.ndarray)):\n",
    "                            print(f\"  {key}: {len(value)} elements\")\n",
    "                        else:\n",
    "                            print(f\"  {key}: {type(value)}\")\n",
    "                elif isinstance(split_data, (list, tuple)):\n",
    "                    print(f\"Split contains {len(split_data)} elements\")\n",
    "                    if len(split_data) >= 3:\n",
    "                        print(f\"  Train: {len(split_data[0])} elements\")\n",
    "                        print(f\"  Val: {len(split_data[1])} elements\")\n",
    "                        print(f\"  Test: {len(split_data[2])} elements\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANALYZING COAUTHORSHIP AND COCITATION DATASETS\n",
      "============================================================\n",
      "Data directory: ../data\n",
      "\n",
      "Found coauthorship directory: ../data/coauthorship\n",
      "\n",
      "============================================================\n",
      "ANALYZING DATASET: COAUTHORSHIP_CORA\n",
      "============================================================\n",
      "Features type: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Features content (first 100 chars): <Compressed Sparse Row sparse matrix of dtype 'float32'\n",
      "\twith 49216 stored elements and shape (2708,...\n",
      "\n",
      "Hypergraph type: <class 'dict'>\n",
      "Number of hyperedges: 1072\n",
      "Sample hyperedges (first 3):\n",
      "  V Gupta: [235, 355]\n",
      "  A Srinivasan: [1133, 1666, 1888]\n",
      "  J Zavrel: [783, 785]\n",
      "Hyperedge size statistics:\n",
      "  Min: 2\n",
      "  Max: 43\n",
      "  Mean: 4.28\n",
      "  Median: 3.00\n",
      "Total unique nodes: 2388\n",
      "\n",
      "Labels type: <class 'list'>\n",
      "Labels content (first 100 chars): [3, 2, 6, 6, 5, 5, 1, 3, 3, 1, 3, 0, 5, 4, 3, 3, 6, 3, 3, 3, 1, 6, 0, 5, 6, 1, 3, 5, 3, 2, 4, 3, 4, ...\n",
      "\n",
      "Splits directory: ../data/coauthorship/cora/splits\n",
      "Number of split files: 10\n",
      "First split (9.pickle) type: <class 'dict'>\n",
      "Split keys: ['train', 'test']\n",
      "  train: 140 elements\n",
      "  test: 2568 elements\n",
      "\n",
      "============================================================\n",
      "ANALYZING DATASET: COAUTHORSHIP_DBLP\n",
      "============================================================\n",
      "Features type: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Features content (first 100 chars): <Compressed Sparse Row sparse matrix of dtype 'float32'\n",
      "\twith 1602890 stored elements and shape (413...\n",
      "\n",
      "Hypergraph type: <class 'dict'>\n",
      "Number of hyperedges: 22363\n",
      "Sample hyperedges (first 3):\n",
      "  franz baader: [0, 14492, 12731, 12732, 12734, 12736, 12738, 39025]\n",
      "  carsten lutz: [0, 29187, 14607, 13661, 12732, 19117, 29746, 14720, 14658, 29848, 5027, 29184]\n",
      "  ulrike sattler: [0, 29765, 26519, 26553, 13661, 283, 14492, 13387, 14437, 13389, 13390, 13391, 14658, 26689]\n",
      "Hyperedge size statistics:\n",
      "  Min: 2\n",
      "  Max: 202\n",
      "  Mean: 4.45\n",
      "  Median: 3.00\n",
      "Total unique nodes: 41302\n",
      "\n",
      "Labels type: <class 'list'>\n",
      "Labels content (first 100 chars): [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "\n",
      "Splits directory: ../data/coauthorship/dblp/splits\n",
      "Number of split files: 10\n",
      "First split (9.pickle) type: <class 'dict'>\n",
      "Split keys: ['train', 'test']\n",
      "  train: 1740 elements\n",
      "  test: 39562 elements\n",
      "\n",
      "Found cocitation directory: ../data/cocitation\n",
      "\n",
      "============================================================\n",
      "ANALYZING DATASET: COCITATION_CITESEER\n",
      "============================================================\n",
      "Features type: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Features content (first 100 chars): <Compressed Sparse Row sparse matrix of dtype 'float32'\n",
      "\twith 105165 stored elements and shape (3312...\n",
      "\n",
      "Hypergraph type: <class 'dict'>\n",
      "Number of hyperedges: 1079\n",
      "Sample hyperedges (first 3):\n",
      "  415: {0, 953}\n",
      "  514: {0, 384, 323, 173, 383}\n",
      "  585: {0, 86}\n",
      "Hyperedge size statistics:\n",
      "  Min: 2\n",
      "  Max: 26\n",
      "  Mean: 3.20\n",
      "  Median: 2.00\n",
      "Total unique nodes: 1458\n",
      "\n",
      "Labels type: <class 'list'>\n",
      "Labels content (first 100 chars): [1, 4, 1, 0, 5, 5, 1, 4, 5, 2, 4, 1, 0, 4, 0, 4, 0, 3, 1, 1, 1, 4, 4, 0, 4, 3, 0, 1, 4, 3, 3, 3, 3, ...\n",
      "\n",
      "Splits directory: ../data/cocitation/citeseer/splits\n",
      "Number of split files: 10\n",
      "First split (9.pickle) type: <class 'dict'>\n",
      "Split keys: ['train', 'test']\n",
      "  train: 138 elements\n",
      "  test: 3174 elements\n",
      "\n",
      "============================================================\n",
      "ANALYZING DATASET: COCITATION_CORA\n",
      "============================================================\n",
      "Features type: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Features content (first 100 chars): <Compressed Sparse Row sparse matrix of dtype 'float32'\n",
      "\twith 49216 stored elements and shape (2708,...\n",
      "\n",
      "Hypergraph type: <class 'dict'>\n",
      "Number of hyperedges: 1579\n",
      "Sample hyperedges (first 3):\n",
      "  402: {538, 163, 219}\n",
      "  1696: {1114, 163}\n",
      "  2295: {881, 427, 2667, 163}\n",
      "Hyperedge size statistics:\n",
      "  Min: 2\n",
      "  Max: 5\n",
      "  Mean: 3.03\n",
      "  Median: 3.00\n",
      "Total unique nodes: 1434\n",
      "\n",
      "Labels type: <class 'list'>\n",
      "Labels content (first 100 chars): [3, 2, 6, 6, 5, 5, 1, 3, 3, 1, 3, 0, 5, 4, 3, 3, 6, 3, 3, 3, 1, 6, 0, 5, 6, 1, 3, 5, 3, 2, 4, 3, 4, ...\n",
      "\n",
      "Splits directory: ../data/cocitation/cora/splits\n",
      "Number of split files: 10\n",
      "First split (9.pickle) type: <class 'dict'>\n",
      "Split keys: ['train', 'test']\n",
      "  train: 140 elements\n",
      "  test: 2568 elements\n",
      "\n",
      "============================================================\n",
      "ANALYZING DATASET: COCITATION_PUBMED\n",
      "============================================================\n",
      "Features type: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Features content (first 100 chars): <Compressed Sparse Row sparse matrix of dtype 'float32'\n",
      "\twith 988031 stored elements and shape (1971...\n",
      "\n",
      "Hypergraph type: <class 'dict'>\n",
      "Number of hyperedges: 7963\n",
      "Sample hyperedges (first 3):\n",
      "  0: {14618, 12388, 2918, 14295}\n",
      "  2: {9547, 10612}\n",
      "  4: {5652, 12621}\n",
      "Hyperedge size statistics:\n",
      "  Min: 2\n",
      "  Max: 171\n",
      "  Mean: 4.35\n",
      "  Median: 3.00\n",
      "Total unique nodes: 3840\n",
      "\n",
      "Labels type: <class 'list'>\n",
      "Labels content (first 100 chars): [0, 0, 0, 1, 0, 0, 2, 2, 2, 1, 0, 2, 0, 2, 2, 1, 0, 2, 2, 2, 1, 2, 2, 0, 1, 2, 1, 2, 0, 2, 0, 1, 2, ...\n",
      "\n",
      "Splits directory: ../data/cocitation/pubmed/splits\n",
      "Number of split files: 10\n",
      "First split (9.pickle) type: <class 'dict'>\n",
      "Split keys: ['train', 'test']\n",
      "  train: 78 elements\n",
      "  test: 19639 elements\n"
     ]
    }
   ],
   "source": [
    "def analsse_cc_ca_data() -> None:\n",
    "    \"\"\"Main function to analyze all datasets.\"\"\"\n",
    "    print(\"ANALYZING COAUTHORSHIP AND COCITATION DATASETS\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    data_dir = os.path.join(\"../data\")\n",
    "\n",
    "    print(f\"Data directory: {data_dir}\")\n",
    "\n",
    "    # Analyze coauthorship datasets\n",
    "    coauthorship_dir = os.path.join(data_dir, \"coauthorship\")\n",
    "    if os.path.exists(coauthorship_dir):\n",
    "        print(f\"\\nFound coauthorship directory: {coauthorship_dir}\")\n",
    "        for dataset in [\"cora\", \"dblp\"]:\n",
    "            dataset_path = os.path.join(coauthorship_dir, dataset)\n",
    "            if os.path.exists(dataset_path):\n",
    "                analyze_dataset(dataset_path, f\"coauthorship_{dataset}\")\n",
    "            else:\n",
    "                print(f\"Dataset {dataset} not found in coauthorship\")\n",
    "    else:\n",
    "        print(\"Coauthorship directory not found\")\n",
    "\n",
    "    # Analyze cocitation datasets\n",
    "    cocitation_dir = os.path.join(data_dir, \"cocitation\")\n",
    "    if os.path.exists(cocitation_dir):\n",
    "        print(f\"\\nFound cocitation directory: {cocitation_dir}\")\n",
    "        for dataset in [\"citeseer\", \"cora\", \"pubmed\"]:\n",
    "            dataset_path = os.path.join(cocitation_dir, dataset)\n",
    "            if os.path.exists(dataset_path):\n",
    "                analyze_dataset(dataset_path, f\"cocitation_{dataset}\")\n",
    "            else:\n",
    "                print(f\"Dataset {dataset} not found in cocitation\")\n",
    "    else:\n",
    "        print(\"Cocitation directory not found\")\n",
    "\n",
    "analsse_cc_ca_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# graph_classification_datasets & hypegraph_classification_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of graphs (presented as a list of dicts, with graph, features, etc as key-pairs in each dict)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANALYZING GRAPH AND HYPERGRAPH CLASSIFICATION DATASETS\n",
      "================================================================================\n",
      "Graph classification directory: /Users/pellegrinraphael/Desktop/Repos_GNN/Hypergraph_Encodings/data/graph_classification_datasets\n",
      "Hypergraph classification directory: /Users/pellegrinraphael/Desktop/Repos_GNN/Hypergraph_Encodings/data/hypergraph_classification_datasets\n",
      "\n",
      "============================================================\n",
      "ANALYZING GRAPH CLASSIFICATION DATASETS\n",
      "============================================================\n",
      "\n",
      "================================================================================\n",
      "ANALYZING DATASET: PEPTIDESSTRUCT_HYPERGRAPHS_TEST\n",
      "================================================================================\n",
      "Dataset type: <class 'list'>\n",
      "Number of samples: 2331\n",
      "\n",
      "First sample type: <class 'dict'>\n",
      "First sample: dict_keys(['hypergraph', 'features', 'labels', 'n'])\n",
      "\n",
      "================================================================================\n",
      "ANALYZING DATASET: PEPTIDESSTRUCT_HYPERGRAPHS\n",
      "================================================================================\n",
      "Dataset type: <class 'list'>\n",
      "Number of samples: 15535\n",
      "\n",
      "First sample type: <class 'dict'>\n",
      "First sample: dict_keys(['hypergraph', 'features', 'labels', 'n'])\n",
      "\n",
      "================================================================================\n",
      "ANALYZING DATASET: PEPTIDESSTRUCT_HYPERGRAPHS_VAL\n",
      "================================================================================\n",
      "Dataset type: <class 'list'>\n",
      "Number of samples: 2331\n",
      "\n",
      "First sample type: <class 'dict'>\n",
      "First sample: dict_keys(['hypergraph', 'features', 'labels', 'n'])\n",
      "\n",
      "================================================================================\n",
      "ANALYZING DATASET: PEPTIDESSTRUCT_HYPERGRAPHS_TRAIN\n",
      "================================================================================\n",
      "Dataset type: <class 'list'>\n",
      "Number of samples: 10873\n",
      "\n",
      "First sample type: <class 'dict'>\n",
      "First sample: dict_keys(['hypergraph', 'features', 'labels', 'n'])\n",
      "\n",
      "============================================================\n",
      "ANALYZING HYPERGRAPH CLASSIFICATION DATASETS\n",
      "============================================================\n",
      "\n",
      "================================================================================\n",
      "ANALYZING DATASET: COLLAB_BASE\n",
      "================================================================================\n",
      "Dataset type: <class 'list'>\n",
      "Number of samples: 5000\n",
      "\n",
      "First sample type: <class 'dict'>\n",
      "First sample: dict_keys(['hypergraph', 'n', 'features', 'labels'])\n",
      "\n",
      "================================================================================\n",
      "ANALYZING DATASET: IMDB_BASE\n",
      "================================================================================\n",
      "Dataset type: <class 'list'>\n",
      "Number of samples: 1000\n",
      "\n",
      "First sample type: <class 'dict'>\n",
      "First sample: dict_keys(['hypergraph', 'n', 'features', 'labels'])\n",
      "\n",
      "================================================================================\n",
      "ANALYZING DATASET: MUTAG_BASE\n",
      "================================================================================\n",
      "Dataset type: <class 'list'>\n",
      "Number of samples: 188\n",
      "\n",
      "First sample type: <class 'dict'>\n",
      "First sample: dict_keys(['hypergraph', 'n', 'features', 'labels'])\n",
      "\n",
      "================================================================================\n",
      "ANALYZING DATASET: ENZYMES_BASE\n",
      "================================================================================\n",
      "Dataset type: <class 'list'>\n",
      "Number of samples: 600\n",
      "\n",
      "First sample type: <class 'dict'>\n",
      "First sample: dict_keys(['hypergraph', 'n', 'features', 'labels'])\n",
      "\n",
      "================================================================================\n",
      "ANALYZING DATASET: PROTEINS_BASE\n",
      "================================================================================\n",
      "Dataset type: <class 'list'>\n",
      "Number of samples: 1113\n",
      "\n",
      "First sample type: <class 'dict'>\n",
      "First sample: dict_keys(['hypergraph', 'n', 'features', 'labels'])\n",
      "\n",
      "================================================================================\n",
      "ANALYZING DATASET: REDDIT_BASE\n",
      "================================================================================\n",
      "Dataset type: <class 'list'>\n",
      "Number of samples: 2000\n",
      "\n",
      "First sample type: <class 'dict'>\n",
      "First sample: dict_keys(['hypergraph', 'n', 'features', 'labels'])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Notebook to analyze the structure of graph and hypergraph classification datasets.\n",
    "\n",
    "This script loads and examines the data structure for:\n",
    "- Graph classification datasets: peptidesstruct\n",
    "- Hypergraph classification datasets: collab, imdb, mutag, enzymes, proteins, reddit\n",
    "\n",
    "It prints examples and statistics to understand the data format.\n",
    "\n",
    "These files contain lists of (hypergraph, features, labels) tuples.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def analyze_classification_dataset(file_path: str, dataset_name: str) -> None:\n",
    "    \"\"\"Analyze a single classification dataset file and print its structure.\n",
    "\n",
    "    Args:\n",
    "        file_path: Path to the pickle file\n",
    "        dataset_name: Name of the dataset\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ANALYZING DATASET: {dataset_name.upper()}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Load the dataset\n",
    "    data = load_pickle_file(file_path)\n",
    "    if data is None:\n",
    "        print(\"Failed to load dataset\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Dataset type: {type(data)}\")\n",
    "    \n",
    "    if isinstance(data, list):\n",
    "        print(f\"Number of samples: {len(data)}\")\n",
    "        \n",
    "        if len(data) == 0:\n",
    "            print(\"Dataset is empty\")\n",
    "            return\n",
    "        \n",
    "        # Analyze first sample\n",
    "        first_sample = data[0]\n",
    "        print(f\"\\nFirst sample type: {type(first_sample)}\")\n",
    "        print(f\"First sample: {first_sample.keys()}\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"Unexpected data format: {type(data)}\")\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    \"\"\"Main function to analyze all classification datasets.\"\"\"\n",
    "    print(\"ANALYZING GRAPH AND HYPERGRAPH CLASSIFICATION DATASETS\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Define the data directories\n",
    "    graph_classification_dir = \"/Users/pellegrinraphael/Desktop/Repos_GNN/Hypergraph_Encodings/data/graph_classification_datasets\"\n",
    "    hypergraph_classification_dir = \"/Users/pellegrinraphael/Desktop/Repos_GNN/Hypergraph_Encodings/data/hypergraph_classification_datasets\"\n",
    "\n",
    "    print(f\"Graph classification directory: {graph_classification_dir}\")\n",
    "    print(f\"Hypergraph classification directory: {hypergraph_classification_dir}\")\n",
    "\n",
    "    # Analyze graph classification datasets\n",
    "    if os.path.exists(graph_classification_dir):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"ANALYZING GRAPH CLASSIFICATION DATASETS\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Look for peptidesstruct files\n",
    "        for filename in os.listdir(graph_classification_dir):\n",
    "            if filename.startswith(\"peptidesstruct\") and filename.endswith(\".pickle\") and \"with_encodings\" not in filename:\n",
    "                file_path = os.path.join(graph_classification_dir, filename)\n",
    "                dataset_name = filename.replace(\".pickle\", \"\")\n",
    "                analyze_classification_dataset(file_path, dataset_name)\n",
    "    else:\n",
    "        print(\"Graph classification directory not found\")\n",
    "\n",
    "    # Analyze hypergraph classification datasets\n",
    "    if os.path.exists(hypergraph_classification_dir):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"ANALYZING HYPERGRAPH CLASSIFICATION DATASETS\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Look for dataset files\n",
    "        datasets = [\"collab\", \"imdb\", \"mutag\", \"enzymes\", \"proteins\", \"reddit\"]\n",
    "        \n",
    "        for dataset in datasets:\n",
    "            # First try to find the base dataset (without encodings)\n",
    "            base_file = f\"{dataset}_hypergraphs.pickle\"\n",
    "            base_path = os.path.join(hypergraph_classification_dir, base_file)\n",
    "            \n",
    "            if os.path.exists(base_path):\n",
    "                analyze_classification_dataset(base_path, f\"{dataset}_base\")\n",
    "            \n",
    "    else:\n",
    "        print(\"Hypergraph classification directory not found\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clean_repo_5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
